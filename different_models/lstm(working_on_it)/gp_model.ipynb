{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import concurrent.futures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import concurrent\n",
    "from concurrent.futures import ThreadPoolExecutor  # Importing specific executor for clarity\n",
    "\n",
    "# Nearest neighbor search\n",
    "import sklearn\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# Special functions and optimizations\n",
    "from scipy.special import gamma, kv  # Bessel function and gamma function\n",
    "from scipy.stats import multivariate_normal  # Simulation\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import cdist  # For space and time distance\n",
    "from scipy.spatial import distance  # Find closest spatial point\n",
    "from scipy.optimize import differential_evolution\n",
    "from typing import Callable, Union, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "\n",
    "# Custom imports\n",
    "from GEMS_TCO import orbitmap\n",
    "from GEMS_TCO import kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate instsance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\joonw\\\\TCO\\\\data_engineering\\\\data_2024\\\\data_24_07_0131_N510_E110120.csv\")\n",
    "instance = orbitmap.MakeOrbitdata(df, 5,10,110,120)\n",
    "\n",
    "# Load the dictionary from the pickle file\n",
    "filepath = \"C:\\\\Users\\\\joonw\\\\TCO\\\\data_engineering\\\\data_2024\\\\sparse_cen_map24_01.pkl\"\n",
    "\n",
    "with open(filepath, 'rb') as pickle_file:\n",
    "    coarse_dict_24_1 = pickle.load(pickle_file)\n",
    "\n",
    "sample_df = coarse_dict_24_1['y24m01day01_hm02:12']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set coarse coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "rho_lat = 20\n",
    "rho_lon = 20\n",
    "lat_n = sample_df['Latitude'].unique()[::rho_lat]\n",
    "lon_n = sample_df['Longitude'].unique()[::rho_lon]\n",
    "\n",
    "print(len(lat_n))\n",
    "print(len(lon_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that\n",
    "\n",
    "# Efficiency: isin is vectorized, leveraging pandas' optimized internals for filtering\n",
    "# tr  = ddf.apply(lambda x: (x.Latitude in lat_n) and (x.Longitude in lon_n), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 2023 1.\n",
      "Finished processing 2023 2.\n",
      "Finished processing 2023 3.\n",
      "Finished processing 2023 4.\n",
      "Finished processing 2023 5.\n",
      "Finished processing 2023 6.\n",
      "Finished processing 2023 7.\n",
      "Finished processing 2023 8.\n",
      "Finished processing 2023 9.\n",
      "Finished processing 2023 10.\n",
      "Finished processing 2023 11.\n",
      "Finished processing 2023 12.\n",
      "Finished processing 2024 1.\n",
      "Finished processing 2024 2.\n",
      "Finished processing 2024 3.\n",
      "Finished processing 2024 4.\n",
      "Finished processing 2024 5.\n",
      "Finished processing 2024 6.\n",
      "Finished processing 2024 7.\n",
      "Finished processing 2024 8.\n",
      "Finished processing 2024 9.\n",
      "Finished processing 2024 10.\n",
      "Finished processing 2024 11.\n",
      "Finished processing 2024 12.\n"
     ]
    }
   ],
   "source": [
    "# Set coarse coordinates\n",
    "\n",
    "coarse_dicts = {}\n",
    "\n",
    "years = ['2023','2024']\n",
    "for year in years:\n",
    "    for month in range(1, 13):  # Iterate over all months\n",
    "        filepath = f\"C:\\\\Users\\\\joonw\\\\TCO\\\\data_engineering\\\\data_{year}\\\\sparse_cen_map{year[2:]}_{month:02d}.pkl\"\n",
    "        with open(filepath, 'rb') as pickle_file:\n",
    "            loaded_map = pickle.load(pickle_file)\n",
    "            for key in loaded_map:\n",
    "                tmp_df = loaded_map[key]\n",
    "                coarse_filter = (tmp_df['Latitude'].isin(lat_n)) & (tmp_df['Longitude'].isin(lon_n))\n",
    "                coarse_dicts[f\"{year}_{month:02d}_{key}\"] = tmp_df[coarse_filter].reset_index(drop=True)\n",
    "\n",
    "        print(f\"Finished processing {year} {month}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large-scale DataFrame concatenation, the list-based approach is the preferred way to go. both faster and more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "for key in coarse_dicts:\n",
    "    df_list.append(coarse_dicts[key])\n",
    "\n",
    "df_entire = pd.concat(df_list, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = list(coarse_dicts.keys())\n",
    "train_set = []\n",
    "test_set = []\n",
    "for i in range(len(coarse_dicts)):\n",
    "    if i<= 4000:\n",
    "        train_set.append(coarse_dicts[key_list[i]]) \n",
    "    else:\n",
    "        test_set.append(coarse_dicts[key_list[i]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.concat(train_set, axis=0, ignore_index=True)\n",
    "test_set = pd.concat(test_set, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gp testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 149. GiB for an array with shape (20009901225,) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m coordinates \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLatitude\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m train_set\u001b[38;5;241m.\u001b[39miterrows()])\n\u001b[0;32m     60\u001b[0m ozone_values \u001b[38;5;241m=\u001b[39m train_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mColumnAmountO3\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m---> 61\u001b[0m gp_extractor\u001b[38;5;241m.\u001b[39mfit(coordinates, ozone_values)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[0;32m     64\u001b[0m mean_features, std_features \u001b[38;5;241m=\u001b[39m gp_extractor\u001b[38;5;241m.\u001b[39mextract_features(coordinates)\n",
      "Cell \u001b[1;32mIn[19], line 8\u001b[0m, in \u001b[0;36mGPFeatureExtractor.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgp\u001b[38;5;241m.\u001b[39mfit(X, y)\n",
      "File \u001b[1;32mc:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:307\u001b[0m, in \u001b[0;36mGaussianProcessRegressor.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_marginal_likelihood(theta, clone_kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# First optimize starting from theta specified in kernel\u001b[39;00m\n\u001b[0;32m    305\u001b[0m optima \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    306\u001b[0m     (\n\u001b[1;32m--> 307\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constrained_optimization(\n\u001b[0;32m    308\u001b[0m             obj_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_\u001b[38;5;241m.\u001b[39mtheta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_\u001b[38;5;241m.\u001b[39mbounds\n\u001b[0;32m    309\u001b[0m         )\n\u001b[0;32m    310\u001b[0m     )\n\u001b[0;32m    311\u001b[0m ]\n\u001b[0;32m    313\u001b[0m \u001b[38;5;66;03m# Additional runs are performed from log-uniform chosen initial\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;66;03m# theta\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_restarts_optimizer \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:652\u001b[0m, in \u001b[0;36mGaussianProcessRegressor._constrained_optimization\u001b[1;34m(self, obj_func, initial_theta, bounds)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constrained_optimization\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj_func, initial_theta, bounds):\n\u001b[0;32m    651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin_l_bfgs_b\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 652\u001b[0m         opt_res \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39moptimize\u001b[38;5;241m.\u001b[39mminimize(\n\u001b[0;32m    653\u001b[0m             obj_func,\n\u001b[0;32m    654\u001b[0m             initial_theta,\n\u001b[0;32m    655\u001b[0m             method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL-BFGS-B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    656\u001b[0m             jac\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    657\u001b[0m             bounds\u001b[38;5;241m=\u001b[39mbounds,\n\u001b[0;32m    658\u001b[0m         )\n\u001b[0;32m    659\u001b[0m         _check_optimize_result(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m, opt_res)\n\u001b[0;32m    660\u001b[0m         theta_opt, func_min \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[1;32mc:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:731\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    728\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    729\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 731\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    732\u001b[0m                            callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    734\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    735\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:347\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    344\u001b[0m         iprint \u001b[38;5;241m=\u001b[39m disp\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# _prepare_scalar_function can use bounds=None to represent no bounds\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m sf \u001b[38;5;241m=\u001b[39m _prepare_scalar_function(fun, x0, jac\u001b[38;5;241m=\u001b[39mjac, args\u001b[38;5;241m=\u001b[39margs, epsilon\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    348\u001b[0m                               bounds\u001b[38;5;241m=\u001b[39mbounds,\n\u001b[0;32m    349\u001b[0m                               finite_diff_rel_step\u001b[38;5;241m=\u001b[39mfinite_diff_rel_step)\n\u001b[0;32m    351\u001b[0m func_and_grad \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun_and_grad\n\u001b[0;32m    353\u001b[0m fortran_int \u001b[38;5;241m=\u001b[39m _lbfgsb\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mintvar\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[1;32mc:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:288\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    284\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m sf \u001b[38;5;241m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0;32m    289\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[38;5;241m=\u001b[39mepsilon)\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32mc:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:222\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    219\u001b[0m     finite_diff_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_linear_operator\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m# Initial function evaluation\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# Initial gradient evaluation\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped_grad, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ngev \u001b[38;5;241m=\u001b[39m _wrapper_grad(\n\u001b[0;32m    226\u001b[0m     grad,\n\u001b[0;32m    227\u001b[0m     fun\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped_fun,\n\u001b[0;32m    228\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m    229\u001b[0m     finite_diff_options\u001b[38;5;241m=\u001b[39mfinite_diff_options\n\u001b[0;32m    230\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:294\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 294\u001b[0m         fx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped_fun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m fx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lowest_f:\n\u001b[0;32m    296\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lowest_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx\n",
      "File \u001b[1;32mc:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:20\u001b[0m, in \u001b[0;36m_wrapper_fun.<locals>.wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     16\u001b[0m ncalls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m fx \u001b[38;5;241m=\u001b[39m fun(np\u001b[38;5;241m.\u001b[39mcopy(x), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:79\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m     78\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_if_needed(x, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[1;32mc:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:73\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 73\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfun(x, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:297\u001b[0m, in \u001b[0;36mGaussianProcessRegressor.fit.<locals>.obj_func\u001b[1;34m(theta, eval_gradient)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(theta, eval_gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_gradient:\n\u001b[1;32m--> 297\u001b[0m         lml, grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_marginal_likelihood(\n\u001b[0;32m    298\u001b[0m             theta, eval_gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clone_kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    299\u001b[0m         )\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mlml, \u001b[38;5;241m-\u001b[39mgrad\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:576\u001b[0m, in \u001b[0;36mGaussianProcessRegressor.log_marginal_likelihood\u001b[1;34m(self, theta, eval_gradient, clone_kernel)\u001b[0m\n\u001b[0;32m    573\u001b[0m     kernel\u001b[38;5;241m.\u001b[39mtheta \u001b[38;5;241m=\u001b[39m theta\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_gradient:\n\u001b[1;32m--> 576\u001b[0m     K, K_gradient \u001b[38;5;241m=\u001b[39m kernel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train_, eval_gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    578\u001b[0m     K \u001b[38;5;241m=\u001b[39m kernel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train_)\n",
      "File \u001b[1;32mc:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:1713\u001b[0m, in \u001b[0;36mMatern.__call__\u001b[1;34m(self, X, Y, eval_gradient)\u001b[0m\n\u001b[0;32m   1711\u001b[0m length_scale \u001b[38;5;241m=\u001b[39m _check_length_scale(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength_scale)\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1713\u001b[0m     dists \u001b[38;5;241m=\u001b[39m pdist(X \u001b[38;5;241m/\u001b[39m length_scale, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1715\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_gradient:\n",
      "File \u001b[1;32mc:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\scipy\\spatial\\distance.py:2180\u001b[0m, in \u001b[0;36mpdist\u001b[1;34m(X, metric, out, **kwargs)\u001b[0m\n\u001b[0;32m   2178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metric_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2179\u001b[0m     pdist_fn \u001b[38;5;241m=\u001b[39m metric_info\u001b[38;5;241m.\u001b[39mpdist_func\n\u001b[1;32m-> 2180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pdist_fn(X, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2181\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mstr\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2182\u001b[0m     metric_info \u001b[38;5;241m=\u001b[39m _TEST_METRICS\u001b[38;5;241m.\u001b[39mget(mstr, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 149. GiB for an array with shape (20009901225,) and data type float64"
     ]
    }
   ],
   "source": [
    "# Gaussian Process-based Feature Extraction\n",
    "class GPFeatureExtractor:\n",
    "    def __init__(self, kernel=None):\n",
    "        self.kernel = kernel if kernel else Matern(length_scale=1.0, nu=1.5)\n",
    "        self.gp = GaussianProcessRegressor(kernel=self.kernel)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.gp.fit(X, y)\n",
    "\n",
    "    def extract_features(self, X):\n",
    "        mean, std = self.gp.predict(X, return_std=True)\n",
    "        return mean, std\n",
    "\n",
    "# Custom Dataset for Ozone Data\n",
    "class OzoneDataset(Dataset):\n",
    "    def __init__(self, data, num_latitude, num_longitude):\n",
    "        self.data = data\n",
    "        self.num_latitude = num_latitude\n",
    "        self.num_longitude = num_longitude\n",
    "        self.latitudes = sorted(data['Latitude'].unique())\n",
    "        self.longitudes = sorted(data['Longitude'].unique())\n",
    "        self.lat_lon_map = {\n",
    "            (row['Latitude'], row['Longitude']): (self.latitudes.index(row['Latitude']), self.longitudes.index(row['Longitude']))\n",
    "            for _, row in data.iterrows()\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        lat_idx, lon_idx = self.lat_lon_map[(row['Latitude'], row['Longitude'])]\n",
    "        return torch.tensor([lat_idx, lon_idx], dtype=torch.float32), torch.tensor(row['ColumnAmountO3'], dtype=torch.float32)\n",
    "\n",
    "# Parameters\n",
    "lat_number = 5\n",
    "lon_number = 10\n",
    "batch_size = 16\n",
    "train_dataset = OzoneDataset(test_set, lat_number, lon_number)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.25):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        return output\n",
    "\n",
    "\n",
    "# Initialize GP feature extractor and train\n",
    "gp_extractor = GPFeatureExtractor()\n",
    "coordinates = np.array([(row['Latitude'], row['Longitude']) for _, row in train_set.iterrows()])\n",
    "ozone_values = train_set['ColumnAmountO3'].values\n",
    "gp_extractor.fit(coordinates, ozone_values)\n",
    "\n",
    "# Extract features\n",
    "mean_features, std_features = gp_extractor.extract_features(coordinates)\n",
    "\n",
    "# Initialize LSTM model\n",
    "input_size = 2  # Lat and Lon\n",
    "hidden_size = 128\n",
    "output_size = lat_number * lon_number\n",
    "model = LSTMModel(input_size, hidden_size, output_size)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    for coordinates, ozone_values in train_loader:\n",
    "        coordinates, ozone_values = coordinates.to(device), ozone_values.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(coordinates)\n",
    "        loss = criterion(predictions, ozone_values)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    end_time = time.time()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}, Time: {end_time - start_time:.2f}s\")\n",
    "\n",
    "# Save the model\n",
    "model_path = f'/home/jl2815/tco/models/save_models/gp_lstm_{lat_number}_{lon_number}.pth'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Pretrained CNN for Feature Extraction\n",
    "The CNN processes spatial grids (latitude × longitude) and outputs flattened features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "\n",
    "class FeatureExtractorCNN(nn.Module):\n",
    "    def __init__(self, cnn_channels, output_size):\n",
    "        super(FeatureExtractorCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(cnn_channels, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(64 * 5 * 10, output_size)  # Adjust based on grid size (5 × 10 here)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Dataset Preparation\n",
    "Precompute CNN features for spatial grids and prepare multi-scale sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OzoneDataset(Dataset):\n",
    "    def __init__(self, data, cnn_model, num_latitude, num_longitude, daily_cycle_len, monthly_cycle_len, three_month_cycle_len):\n",
    "        self.data = data\n",
    "        self.cnn_model = cnn_model.eval()  # Use the CNN in evaluation mode for preprocessing\n",
    "        self.num_latitude = num_latitude\n",
    "        self.num_longitude = num_longitude\n",
    "        self.daily_cycle_len = daily_cycle_len\n",
    "        self.monthly_cycle_len = monthly_cycle_len\n",
    "        self.three_month_cycle_len = three_month_cycle_len\n",
    "        self.cnn_features = self.precompute_cnn_features()\n",
    "        self.prepared_data = self.prepare_data()\n",
    "\n",
    "    def precompute_cnn_features(self):\n",
    "        latitudes = sorted(self.data['Latitude'].unique())\n",
    "        longitudes = sorted(self.data['Longitude'].unique())\n",
    "        time_steps = sorted(self.data['Hours_elapsed'].unique())\n",
    "\n",
    "        # Precompute CNN features for all grids\n",
    "        cnn_features = []\n",
    "        for t in time_steps:\n",
    "            sub_data = self.data[self.data['Hours_elapsed'] == t]\n",
    "            grid = np.zeros((self.num_latitude, self.num_longitude))\n",
    "            for _, row in sub_data.iterrows():\n",
    "                lat_idx = latitudes.index(row['Latitude'])\n",
    "                lon_idx = longitudes.index(row['Longitude'])\n",
    "                grid[lat_idx, lon_idx] = row['ColumnAmountO3']\n",
    "\n",
    "            # Convert grid to tensor and pass through CNN\n",
    "            grid_tensor = torch.tensor(grid, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Add batch and channel dims\n",
    "            with torch.no_grad():\n",
    "                spatial_features = self.cnn_model(grid_tensor).view(-1)  # Flattened CNN output\n",
    "            cnn_features.append(spatial_features.numpy())\n",
    "        return np.array(cnn_features)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        X_daily, X_monthly, X_three_month, y = [], [], [], []\n",
    "        for i in range(len(self.cnn_features) - self.three_month_cycle_len):\n",
    "            # Daily cycle: Last few intervals (3 time steps)\n",
    "            daily_seq = self.cnn_features[i:i + self.daily_cycle_len]\n",
    "\n",
    "            # Monthly cycle: Consecutive 90 intervals\n",
    "            monthly_seq = self.cnn_features[i:i + self.monthly_cycle_len]\n",
    "\n",
    "            # Three-month cycle: Snapshots at 1-month intervals\n",
    "            three_month_seq = self.cnn_features[i:i + self.three_month_cycle_len:self.monthly_cycle_len]\n",
    "\n",
    "            # Append inputs and target\n",
    "            X_daily.append(daily_seq)\n",
    "            X_monthly.append(monthly_seq)\n",
    "            X_three_month.append(three_month_seq)\n",
    "            y.append(self.cnn_features[i + self.three_month_cycle_len])  # Target is the CNN feature of the next step\n",
    "        return np.array(X_daily), np.array(X_monthly), np.array(X_three_month), np.array(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prepared_data[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_daily, X_monthly, X_three_month, y = self.prepared_data\n",
    "        return (\n",
    "            torch.tensor(X_daily[idx], dtype=torch.float32),\n",
    "            torch.tensor(X_monthly[idx], dtype=torch.float32),\n",
    "            torch.tensor(X_three_month[idx], dtype=torch.float32),\n",
    "            torch.tensor(y[idx], dtype=torch.float32)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Multi-Scale LSTM Model\n",
    "Each cycle is processed by its own LSTM, and the outputs are combined for the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleLSTM(nn.Module):\n",
    "    def __init__(self, lstm_hidden_size, lstm_num_layers=1, lstm_dropout=0.25):\n",
    "        super(MultiScaleLSTM, self).__init__()\n",
    "        # LSTM layers for daily, monthly, and three-month cycles\n",
    "        self.lstm_daily = nn.LSTM(64, lstm_hidden_size, num_layers=lstm_num_layers, dropout=lstm_dropout, batch_first=True)\n",
    "        self.lstm_monthly = nn.LSTM(64, lstm_hidden_size, num_layers=lstm_num_layers, dropout=lstm_dropout, batch_first=True)\n",
    "        self.lstm_three_month = nn.LSTM(64, lstm_hidden_size, num_layers=lstm_num_layers, dropout=lstm_dropout, batch_first=True)\n",
    "\n",
    "        # Fully connected layer for combining outputs\n",
    "        self.fc = nn.Linear(lstm_hidden_size * 3, 1)\n",
    "\n",
    "    def forward(self, X_daily, X_monthly, X_three_month):\n",
    "        # Process each cycle with its LSTM\n",
    "        lstm_out_daily, _ = self.lstm_daily(X_daily)\n",
    "        lstm_out_monthly, _ = self.lstm_monthly(X_monthly)\n",
    "        lstm_out_three_month, _ = self.lstm_three_month(X_three_month)\n",
    "\n",
    "        # Concatenate the outputs\n",
    "        combined_features = torch.cat((lstm_out_daily[:, -1, :], lstm_out_monthly[:, -1, :], lstm_out_three_month[:, -1, :]), dim=1)\n",
    "\n",
    "        # Final prediction\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 training pipe line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  warnings.warn(\n",
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([1024])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([960])) that is different to the input size (torch.Size([15, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 39.3152\n",
      "Epoch 2/20, Loss: 41.6272\n",
      "Epoch 3/20, Loss: 39.7652\n",
      "Epoch 4/20, Loss: 39.6117\n",
      "Epoch 5/20, Loss: 41.7547\n",
      "Epoch 6/20, Loss: 41.4694\n",
      "Epoch 7/20, Loss: 40.2820\n",
      "Epoch 8/20, Loss: 42.5225\n",
      "Epoch 9/20, Loss: 40.7666\n",
      "Epoch 10/20, Loss: 41.1398\n",
      "Epoch 11/20, Loss: 39.9845\n",
      "Epoch 12/20, Loss: 39.5781\n",
      "Epoch 13/20, Loss: 40.1565\n",
      "Epoch 14/20, Loss: 40.5254\n",
      "Epoch 15/20, Loss: 40.8880\n",
      "Epoch 16/20, Loss: 39.6964\n",
      "Epoch 17/20, Loss: 40.6008\n",
      "Epoch 18/20, Loss: 41.7216\n",
      "Epoch 19/20, Loss: 41.5865\n",
      "Epoch 20/20, Loss: 40.4669\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "num_latitude = 5\n",
    "num_longitude = 10\n",
    "cnn_channels = 1  # like grayscale images, ozone is represented as a single value\n",
    "cnn_output_size = 64  # It is not tied to the size of the input grid but rather to the complexity of the patterns we expect the CNN to learn.\n",
    "daily_cycle_len = 8\n",
    "monthly_cycle_len = 240\n",
    "three_month_cycle_len = 720\n",
    "lstm_hidden_size = 128 # Increasing lstm_hidden_size allows the LSTM to better capture long-term dependencies. but computational cost\n",
    "\n",
    "# Load your data\n",
    "data = df_entire\n",
    "\n",
    "# Initialize CNN for feature extraction\n",
    "cnn_model = FeatureExtractorCNN(cnn_channels, cnn_output_size)\n",
    "cnn_model.eval()  # Set CNN to evaluation mode for feature extraction\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = OzoneDataset(data, cnn_model, num_latitude, num_longitude, daily_cycle_len, monthly_cycle_len, three_month_cycle_len)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Initialize Multi-Scale LSTM\n",
    "model = MultiScaleLSTM(lstm_hidden_size=128, lstm_num_layers=1, lstm_dropout=0.1)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    for X_daily, X_monthly, X_three_month, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(X_daily, X_monthly, X_three_month)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(predictions, y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 59500.1015\n",
      "Epoch 2/20, Loss: 48166.1589\n",
      "Epoch 3/20, Loss: 38610.7289\n",
      "Epoch 4/20, Loss: 30544.2431\n",
      "Epoch 5/20, Loss: 23785.4705\n",
      "Epoch 6/20, Loss: 18185.5642\n",
      "Epoch 7/20, Loss: 13590.4080\n",
      "Epoch 8/20, Loss: 9898.8490\n",
      "Epoch 9/20, Loss: 6988.8460\n",
      "Epoch 10/20, Loss: 4770.5174\n",
      "Epoch 11/20, Loss: 3127.2470\n",
      "Epoch 12/20, Loss: 1966.9871\n",
      "Epoch 13/20, Loss: 1189.1370\n",
      "Epoch 14/20, Loss: 701.2340\n",
      "Epoch 15/20, Loss: 419.0379\n",
      "Epoch 16/20, Loss: 271.5537\n",
      "Epoch 17/20, Loss: 203.9514\n",
      "Epoch 18/20, Loss: 176.7939\n",
      "Epoch 19/20, Loss: 168.1362\n",
      "Epoch 20/20, Loss: 166.6006\n"
     ]
    }
   ],
   "source": [
    "# CNN Model for Spatial Feature Extraction\n",
    "class FeatureExtractorCNN(nn.Module):\n",
    "    def __init__(self, cnn_channels, output_size):\n",
    "        super(FeatureExtractorCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(cnn_channels, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(64 * 5 * 10, output_size)  # Adjust based on grid size (5x10 here)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Custom Dataset for Ozone Data\n",
    "class OzoneDataset(Dataset):\n",
    "    def __init__(self, data, num_latitude, num_longitude, daily_cycle_len, monthly_cycle_len, three_month_cycle_len):\n",
    "        self.data = data\n",
    "        self.num_latitude = num_latitude\n",
    "        self.num_longitude = num_longitude\n",
    "        self.daily_cycle_len = daily_cycle_len\n",
    "        self.monthly_cycle_len = monthly_cycle_len\n",
    "        self.three_month_cycle_len = three_month_cycle_len\n",
    "\n",
    "        # Cache unique latitude/longitude indices for faster grid construction\n",
    "        self.latitudes = sorted(data['Latitude'].unique())\n",
    "        self.longitudes = sorted(data['Longitude'].unique())\n",
    "        self.time_steps = sorted(data['Hours_elapsed'].unique())\n",
    "        self.lat_lon_map = {\n",
    "            (row['Latitude'], row['Longitude']): (self.latitudes.index(row['Latitude']), self.longitudes.index(row['Longitude']))\n",
    "            for _, row in data.iterrows()\n",
    "        }\n",
    "\n",
    "        # Prepare data for sequences\n",
    "        self.prepared_data = self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        X_daily, X_monthly, X_three_month, y = [], [], [], []\n",
    "        for i in range(len(self.time_steps) - self.three_month_cycle_len):\n",
    "            daily_seq, monthly_seq, three_month_seq = [], [], []\n",
    "\n",
    "            for t in range(i, i + self.three_month_cycle_len):\n",
    "                sub_data = self.data[self.data['Hours_elapsed'] == self.time_steps[t]]\n",
    "                grid = np.zeros((self.num_latitude, self.num_longitude))\n",
    "                \n",
    "                for _, row in sub_data.iterrows():\n",
    "                    lat_idx, lon_idx = self.lat_lon_map[(row['Latitude'], row['Longitude'])]\n",
    "                    grid[lat_idx, lon_idx] = row['ColumnAmountO3']\n",
    "\n",
    "                if t < i + self.daily_cycle_len:\n",
    "                    daily_seq.append(grid)\n",
    "                if t < i + self.monthly_cycle_len:\n",
    "                    monthly_seq.append(grid)\n",
    "                three_month_seq.append(grid)\n",
    "\n",
    "            X_daily.append(torch.tensor(np.array(daily_seq), dtype=torch.float32))\n",
    "            X_monthly.append(torch.tensor(np.array(monthly_seq), dtype=torch.float32))\n",
    "            X_three_month.append(torch.tensor(np.array(three_month_seq), dtype=torch.float32))\n",
    "            y.append(torch.tensor(np.array(three_month_seq[-1]), dtype=torch.float32))  # Target is last grid in the sequence\n",
    "\n",
    "        return X_daily, X_monthly, X_three_month, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prepared_data[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_daily, X_monthly, X_three_month, y = self.prepared_data\n",
    "        return X_daily[idx], X_monthly[idx], X_three_month[idx], y[idx]\n",
    "\n",
    "# Multi-Scale LSTM for Temporal Modeling\n",
    "class MultiScaleLSTM(nn.Module):\n",
    "    def __init__(self, cnn_channels, cnn_output_size, lstm_hidden_size, lstm_num_layers=1, lstm_dropout=0.25):\n",
    "        super(MultiScaleLSTM, self).__init__()\n",
    "        self.cnn = FeatureExtractorCNN(cnn_channels, cnn_output_size)\n",
    "        self.lstm_daily = nn.LSTM(cnn_output_size, lstm_hidden_size, num_layers=lstm_num_layers, dropout=lstm_dropout, batch_first=True)\n",
    "        self.lstm_monthly = nn.LSTM(cnn_output_size, lstm_hidden_size, num_layers=lstm_num_layers, dropout=lstm_dropout, batch_first=True)\n",
    "        self.lstm_three_month = nn.LSTM(cnn_output_size, lstm_hidden_size, num_layers=lstm_num_layers, dropout=lstm_dropout, batch_first=True)\n",
    "        # self.fc = nn.Linear(lstm_hidden_size * 3, 1)\n",
    "        self.fc = nn.Linear(lstm_hidden_size * 3, 5 * 10)\n",
    "\n",
    "    def forward(self, X_daily, X_monthly, X_three_month):\n",
    "        def extract_features(X_seq):\n",
    "            batch_size, seq_len, height, width = X_seq.shape\n",
    "            # Process each timestep with CNN\n",
    "            features = [self.cnn(X_seq[:, t].unsqueeze(1)) for t in range(seq_len)]\n",
    "            return torch.stack(features, dim=1)  # Shape: [batch_size, seq_len, cnn_output_size]\n",
    "\n",
    "        # Extract spatial features for each sequence\n",
    "        daily_features = extract_features(X_daily)\n",
    "        monthly_features = extract_features(X_monthly)\n",
    "        three_month_features = extract_features(X_three_month)\n",
    "\n",
    "        # Process temporal features using LSTMs\n",
    "        lstm_out_daily, _ = self.lstm_daily(daily_features)\n",
    "        lstm_out_monthly, _ = self.lstm_monthly(monthly_features)\n",
    "        lstm_out_three_month, _ = self.lstm_three_month(three_month_features)\n",
    "\n",
    "        # Concatenate the final outputs from each LSTM\n",
    "        combined_features = torch.cat((lstm_out_daily[:, -1, :], lstm_out_monthly[:, -1, :], lstm_out_three_month[:, -1, :]), dim=1)\n",
    "\n",
    "        output = self.fc(combined_features)\n",
    "        output = output.view(-1, 5, 10)  # Reshape to grid dimensions\n",
    "\n",
    "        # Final prediction\n",
    "        return output\n",
    "\n",
    "# Parameters\n",
    "num_latitude = 5\n",
    "num_longitude = 10\n",
    "cnn_channels = 1  # Grayscale-like input\n",
    "cnn_output_size = 64  # Number of features extracted by CNN\n",
    "daily_cycle_len = 8\n",
    "monthly_cycle_len = 24    #240\n",
    "three_month_cycle_len = 60  # 720 tmp for week\n",
    "lstm_hidden_size = 64 # 128\n",
    "\n",
    "# Load dataset (example)\n",
    "data = df_entire\n",
    "dataset = OzoneDataset(data, num_latitude, num_longitude, daily_cycle_len, monthly_cycle_len, three_month_cycle_len)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "  \n",
    "# Initialize model\n",
    "model = MultiScaleLSTM(cnn_channels, cnn_output_size, lstm_hidden_size)\n",
    "model.train()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()  # Start time for the epoch\n",
    "    for X_daily, X_monthly, X_three_month, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_daily, X_monthly, X_three_month)\n",
    "        loss = criterion(predictions,   y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    end_time = time.time()  # End time for the epoch\n",
    "    epoch_duration = end_time - start_time  # Calculate duration\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader):.4f}, Time: {epoch_duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model until sofar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 54995.0567\n",
      "Epoch 2/20, Loss: 35152.9516\n",
      "Epoch 3/20, Loss: 21103.9720\n",
      "Epoch 4/20, Loss: 11925.7865\n",
      "Epoch 5/20, Loss: 6259.6006\n",
      "Epoch 6/20, Loss: 3025.9508\n",
      "Epoch 7/20, Loss: 1359.7929\n",
      "Epoch 8/20, Loss: 601.0122\n",
      "Epoch 9/20, Loss: 303.0626\n",
      "Epoch 10/20, Loss: 205.2377\n",
      "Epoch 11/20, Loss: 179.0557\n",
      "Epoch 12/20, Loss: 173.4082\n",
      "Epoch 13/20, Loss: 172.4943\n",
      "Epoch 14/20, Loss: 172.4172\n",
      "Epoch 15/20, Loss: 172.4189\n",
      "Epoch 16/20, Loss: 172.4139\n",
      "Epoch 17/20, Loss: 172.4108\n",
      "Epoch 18/20, Loss: 172.4078\n",
      "Epoch 19/20, Loss: 172.4732\n",
      "Epoch 20/20, Loss: 172.4732\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CNN Model for Spatial Feature Extraction\n",
    "class FeatureExtractorCNN(nn.Module):\n",
    "    def __init__(self, cnn_channels, output_size):\n",
    "        super(FeatureExtractorCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(cnn_channels, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16,  32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(64 * 5 * 10, output_size)  # Adjust based on grid size (5x10 here)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Custom Dataset for Ozone Data\n",
    "class OzoneDataset(Dataset):\n",
    "    def __init__(self, data, num_latitude, num_longitude, daily_cycle_len, monthly_cycle_len, three_month_cycle_len):\n",
    "        self.data = data\n",
    "        self.num_latitude = num_latitude\n",
    "        self.num_longitude = num_longitude\n",
    "        self.daily_cycle_len = daily_cycle_len\n",
    "        self.monthly_cycle_len = monthly_cycle_len\n",
    "        self.three_month_cycle_len = three_month_cycle_len\n",
    "\n",
    "        # Cache unique latitude/longitude indices for faster grid construction\n",
    "        self.latitudes = sorted(data['Latitude'].unique())\n",
    "        self.longitudes = sorted(data['Longitude'].unique())\n",
    "        self.time_steps = sorted(data['Hours_elapsed'].unique())\n",
    "        self.lat_lon_map = {\n",
    "            (row['Latitude'], row['Longitude']): (self.latitudes.index(row['Latitude']), self.longitudes.index(row['Longitude']))\n",
    "            for _, row in data.iterrows()\n",
    "        }\n",
    "\n",
    "        # Prepare data for sequences\n",
    "        self.prepared_data = self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        X_daily, X_monthly, X_three_month, y = [], [], [], []\n",
    "        for i in range(len(self.time_steps) - self.three_month_cycle_len):\n",
    "            daily_seq, monthly_seq, three_month_seq = [], [], []\n",
    "\n",
    "            for t in range(i, i + self.three_month_cycle_len):\n",
    "                sub_data = self.data[self.data['Hours_elapsed'] == self.time_steps[t]]\n",
    "                grid = np.zeros((self.num_latitude, self.num_longitude))\n",
    "                \n",
    "                for _, row in sub_data.iterrows():\n",
    "                    lat_idx, lon_idx = self.lat_lon_map[(row['Latitude'], row['Longitude'])]\n",
    "                    grid[lat_idx, lon_idx] = row['ColumnAmountO3']\n",
    "\n",
    "                if t < i + self.daily_cycle_len:\n",
    "                    daily_seq.append(grid)\n",
    "                if t < i + self.monthly_cycle_len:\n",
    "                    monthly_seq.append(grid)\n",
    "                three_month_seq.append(grid)\n",
    "\n",
    "            X_daily.append(torch.tensor(np.array(daily_seq), dtype=torch.float32))\n",
    "            X_monthly.append(torch.tensor(np.array(monthly_seq), dtype=torch.float32))\n",
    "            X_three_month.append(torch.tensor(np.array(three_month_seq), dtype=torch.float32))\n",
    "            y.append(torch.tensor(np.array(three_month_seq[-1]), dtype=torch.float32))  # Target is last grid in the sequence\n",
    "\n",
    "        return X_daily, X_monthly, X_three_month, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prepared_data[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_daily, X_monthly, X_three_month, y = self.prepared_data\n",
    "        return X_daily[idx], X_monthly[idx], X_three_month[idx], y[idx]\n",
    "\n",
    "# Multi-Scale LSTM for Temporal Modeling\n",
    "class MultiScaleLSTM(nn.Module):\n",
    "    def __init__(self, cnn_channels, cnn_output_size, lstm_hidden_size, lstm_num_layers=1, lstm_dropout=0.25):\n",
    "        super(MultiScaleLSTM, self).__init__()\n",
    "        self.cnn = FeatureExtractorCNN(cnn_channels, cnn_output_size)\n",
    "        self.lstm_daily = nn.LSTM(cnn_output_size, lstm_hidden_size, num_layers=lstm_num_layers, dropout=lstm_dropout, batch_first=True)\n",
    "        self.lstm_monthly = nn.LSTM(cnn_output_size, lstm_hidden_size, num_layers=lstm_num_layers, dropout=lstm_dropout, batch_first=True)\n",
    "        self.lstm_three_month = nn.LSTM(cnn_output_size, lstm_hidden_size, num_layers=lstm_num_layers, dropout=lstm_dropout, batch_first=True)\n",
    "        # self.fc = nn.Linear(lstm_hidden_size * 3, 1)\n",
    "        self.fc = nn.Linear(lstm_hidden_size * 3, 5 * 10)\n",
    "\n",
    "    def forward(self, X_daily, X_monthly, X_three_month):\n",
    "        def extract_features(X_seq):\n",
    "            batch_size, seq_len, height, width = X_seq.shape\n",
    "            # Process each timestep with CNN\n",
    "            features = [self.cnn(X_seq[:, t].unsqueeze(1)) for t in range(seq_len)]\n",
    "            return torch.stack(features, dim=1)  # Shape: [batch_size, seq_len, cnn_output_size]\n",
    "\n",
    "        # Extract spatial features for each sequence\n",
    "        daily_features = extract_features(X_daily)\n",
    "        monthly_features = extract_features(X_monthly)\n",
    "        three_month_features = extract_features(X_three_month)\n",
    "\n",
    "        # Process temporal features using LSTMs\n",
    "        lstm_out_daily, _ = self.lstm_daily(daily_features)\n",
    "        lstm_out_monthly, _ = self.lstm_monthly(monthly_features)\n",
    "        lstm_out_three_month, _ = self.lstm_three_month(three_month_features)\n",
    "\n",
    "        # Concatenate the final outputs from each LSTM\n",
    "        combined_features = torch.cat((lstm_out_daily[:, -1, :], lstm_out_monthly[:, -1, :], lstm_out_three_month[:, -1, :]), dim=1)\n",
    "\n",
    "        output = self.fc(combined_features)\n",
    "        output = output.view(-1, 5, 10)  # Reshape to grid dimensions\n",
    "\n",
    "        # Final prediction\n",
    "        return output\n",
    "\n",
    "# Parameters\n",
    "num_latitude = 5\n",
    "num_longitude = 10\n",
    "cnn_channels = 1  # Grayscale-like input\n",
    "cnn_output_size = 64  # Number of features extracted by CNN\n",
    "daily_cycle_len = 8\n",
    "monthly_cycle_len = 24    #240\n",
    "three_month_cycle_len = 60  # 720 tmp for week\n",
    "lstm_hidden_size = 128 # 128\n",
    "\n",
    "# Load dataset (example)\n",
    "data = train_set\n",
    "dataset = OzoneDataset(data, num_latitude, num_longitude, daily_cycle_len, monthly_cycle_len, three_month_cycle_len)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "  \n",
    "# Initialize model\n",
    "model = MultiScaleLSTM(cnn_channels, cnn_output_size, lstm_hidden_size)\n",
    "model.train()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for X_daily, X_monthly, X_three_month, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_daily, X_monthly, X_three_month)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "model_path = f'C:\\\\Users\\\\joonw\\\\TCO\\\\GEMS_TCO-1\\\\models\\\\saved_models\\\\cnn_lstm_{5}_{10}1.pth'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "# Parameters\n",
    "num_latitude = 5\n",
    "num_longitude = 10\n",
    "cnn_channels = 1  # Grayscale-like input\n",
    "cnn_output_size = 64  # Number of features extracted by CNN\n",
    "daily_cycle_len = 8\n",
    "monthly_cycle_len = 24    #240\n",
    "three_month_cycle_len = 60  # 720 tmp for week\n",
    "lstm_hidden_size = 128 # 128\n",
    "###\n",
    "model_path = f'C:\\\\Users\\\\joonw\\\\TCO\\\\GEMS_TCO-1\\\\models\\\\saved_models\\\\cnn_lstm_{5}_{10}1.pth'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 12183.6316\n",
      "Epoch 2/20, Loss: 165.1870\n",
      "Epoch 3/20, Loss: 164.6268\n",
      "Epoch 4/20, Loss: 165.0410\n",
      "Epoch 5/20, Loss: 164.8529\n",
      "Epoch 6/20, Loss: 164.8094\n",
      "Epoch 7/20, Loss: 165.3400\n",
      "Epoch 8/20, Loss: 165.0061\n",
      "Epoch 9/20, Loss: 164.9474\n",
      "Epoch 10/20, Loss: 165.8635\n",
      "Epoch 11/20, Loss: 165.3053\n",
      "Epoch 12/20, Loss: 165.8042\n",
      "Epoch 13/20, Loss: 166.0905\n",
      "Epoch 14/20, Loss: 166.1279\n",
      "Epoch 15/20, Loss: 166.2415\n",
      "Epoch 16/20, Loss: 167.3728\n",
      "Epoch 17/20, Loss: 167.0428\n",
      "Epoch 18/20, Loss: 166.5574\n",
      "Epoch 19/20, Loss: 166.5962\n",
      "Epoch 20/20, Loss: 166.8578\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CNN Model for Spatial Feature Extraction\n",
    "class FeatureExtractorCNN(nn.Module):\n",
    "    def __init__(self, cnn_channels, output_size):\n",
    "        super(FeatureExtractorCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(cnn_channels, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(64 * 5 * 10, output_size)  # Adjust based on grid size (5x10 here)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Custom Dataset for Ozone Data\n",
    "class OzoneDataset(Dataset):\n",
    "    def __init__(self, data, num_latitude, num_longitude, daily_cycle_len, monthly_cycle_len, three_month_cycle_len):\n",
    "        self.data = data\n",
    "        self.num_latitude = num_latitude\n",
    "        self.num_longitude = num_longitude\n",
    "        self.daily_cycle_len = daily_cycle_len\n",
    "        self.monthly_cycle_len = monthly_cycle_len\n",
    "        self.three_month_cycle_len = three_month_cycle_len\n",
    "\n",
    "        # Cache unique latitude/longitude indices for faster grid construction\n",
    "        self.latitudes = sorted(data['Latitude'].unique())\n",
    "        self.longitudes = sorted(data['Longitude'].unique())\n",
    "        self.time_steps = sorted(data['Hours_elapsed'].unique())\n",
    "        self.lat_lon_map = {\n",
    "            (row['Latitude'], row['Longitude']): (self.latitudes.index(row['Latitude']), self.longitudes.index(row['Longitude']))\n",
    "            for _, row in data.iterrows()\n",
    "        }\n",
    "\n",
    "        # Prepare data for sequences\n",
    "        self.prepared_data = self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        X_daily, X_monthly, X_three_month, y = [], [], [], []\n",
    "        for i in range(len(self.time_steps) - self.three_month_cycle_len):\n",
    "            daily_seq, monthly_seq, three_month_seq = [], [], []\n",
    "\n",
    "            for t in range(i, i + self.three_month_cycle_len):\n",
    "                sub_data = self.data[self.data['Hours_elapsed'] == self.time_steps[t]]\n",
    "                grid = np.zeros((self.num_latitude, self.num_longitude))\n",
    "                \n",
    "                for _, row in sub_data.iterrows():\n",
    "                    lat_idx, lon_idx = self.lat_lon_map[(row['Latitude'], row['Longitude'])]\n",
    "                    grid[lat_idx, lon_idx] = row['ColumnAmountO3']\n",
    "\n",
    "                if t < i + self.daily_cycle_len:\n",
    "                    daily_seq.append(grid)\n",
    "                if t < i + self.monthly_cycle_len:\n",
    "                    monthly_seq.append(grid)\n",
    "                three_month_seq.append(grid)\n",
    "\n",
    "            X_daily.append(torch.tensor(np.array(daily_seq), dtype=torch.float32))\n",
    "            X_monthly.append(torch.tensor(np.array(monthly_seq), dtype=torch.float32))\n",
    "            X_three_month.append(torch.tensor(np.array(three_month_seq), dtype=torch.float32))\n",
    "            y.append(torch.tensor(np.array(three_month_seq[-1]), dtype=torch.float32))  # Target is last grid in the sequence\n",
    "\n",
    "        return X_daily, X_monthly, X_three_month, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prepared_data[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_daily, X_monthly, X_three_month, y = self.prepared_data\n",
    "        return X_daily[idx], X_monthly[idx], X_three_month[idx], y[idx]\n",
    "\n",
    "# Multi-Scale LSTM for Temporal Modeling\n",
    "class MultiScaleLSTM(nn.Module):\n",
    "    def __init__(self, cnn_channels, cnn_output_size, lstm_hidden_size, lstm_num_layers=1, lstm_dropout=0.25):\n",
    "        super(MultiScaleLSTM, self).__init__()\n",
    "        self.cnn = FeatureExtractorCNN(cnn_channels, cnn_output_size)\n",
    "        self.lstm_daily = nn.LSTM(cnn_output_size, lstm_hidden_size, num_layers=lstm_num_layers, dropout=lstm_dropout, batch_first=True)\n",
    "        self.lstm_monthly = nn.LSTM(cnn_output_size, lstm_hidden_size, num_layers=lstm_num_layers, dropout=lstm_dropout, batch_first=True)\n",
    "        self.lstm_three_month = nn.LSTM(cnn_output_size, lstm_hidden_size, num_layers=lstm_num_layers, dropout=lstm_dropout, batch_first=True)\n",
    "        # self.fc = nn.Linear(lstm_hidden_size * 3, 1)\n",
    "        self.fc = nn.Linear(lstm_hidden_size * 3, 5 * 10)\n",
    "\n",
    "    def forward(self, X_daily, X_monthly, X_three_month):\n",
    "        def extract_features(X_seq):\n",
    "            batch_size, seq_len, height, width = X_seq.shape\n",
    "            # Process each timestep with CNN\n",
    "            features = [self.cnn(X_seq[:, t].unsqueeze(1)) for t in range(seq_len)]\n",
    "            return torch.stack(features, dim=1)  # Shape: [batch_size, seq_len, cnn_output_size]\n",
    "\n",
    "        # Extract spatial features for each sequence\n",
    "        daily_features = extract_features(X_daily)\n",
    "        monthly_features = extract_features(X_monthly)\n",
    "        three_month_features = extract_features(X_three_month)\n",
    "\n",
    "        # Process temporal features using LSTMs\n",
    "        lstm_out_daily, _ = self.lstm_daily(daily_features)\n",
    "        lstm_out_monthly, _ = self.lstm_monthly(monthly_features)\n",
    "        lstm_out_three_month, _ = self.lstm_three_month(three_month_features)\n",
    "\n",
    "        # Concatenate the final outputs from each LSTM\n",
    "        combined_features = torch.cat((lstm_out_daily[:, -1, :], lstm_out_monthly[:, -1, :], lstm_out_three_month[:, -1, :]), dim=1)\n",
    "\n",
    "        output = self.fc(combined_features)\n",
    "        output = output.view(-1, 5, 10)  # Reshape to grid dimensions\n",
    "\n",
    "        # Final prediction\n",
    "        return output\n",
    "\n",
    "# Parameters\n",
    "num_latitude = 5\n",
    "num_longitude = 10\n",
    "cnn_channels = 1  # Grayscale-like input\n",
    "cnn_output_size = 64  # Number of features extracted by CNN\n",
    "daily_cycle_len = 8\n",
    "monthly_cycle_len = 24    #240\n",
    "three_month_cycle_len = 60  # 720 tmp for week\n",
    "lstm_hidden_size = 128 # 128\n",
    "\n",
    "# Load dataset (example)\n",
    "data = df_entire\n",
    "dataset = OzoneDataset(data, num_latitude, num_longitude, daily_cycle_len, monthly_cycle_len, three_month_cycle_len)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "  \n",
    "# Initialize model\n",
    "model = MultiScaleLSTM(cnn_channels, cnn_output_size, lstm_hidden_size)\n",
    "model.train()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for X_daily, X_monthly, X_three_month, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_daily, X_monthly, X_three_month)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "model_path = f'C:\\\\Users\\\\joonw\\\\TCO\\\\GEMS_TCO-1\\\\models\\\\saved_models\\\\cnn_lstm_{num_latitude}_{num_longitude}2.pth'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the Code\n",
    "Feature Extraction:\n",
    "\n",
    "A pretrained CNN extracts spatial features from the ozone grid.\n",
    "These features are precomputed to improve efficiency during training.\n",
    "Multi-Scale Modeling:\n",
    "\n",
    "Separate sequences for daily, monthly, and three-month cycles are created.\n",
    "Each sequence is processed by a dedicated LSTM to capture patterns specific to that cycle.\n",
    "Prediction:\n",
    "\n",
    "Outputs from all LSTMs are concatenated and passed through a fully connected layer for the final prediction.\n",
    "This code ensures efficiency, scalability, and the ability to capture patterns at different temporal scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to replace cnn with gaussian process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model for Spatial Feature Extraction\n",
    "class FeatureExtractorCNN(nn.Module):\n",
    "    def __init__(self, cnn_channels, output_size):\n",
    "        super(FeatureExtractorCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(cnn_channels, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(64 * 5 * 10, output_size)  # Adjust based on grid size (5x10 here)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Custom Dataset for Ozone Data\n",
    "class OzoneDataset(Dataset):\n",
    "    def __init__(self, data, num_latitude, num_longitude, daily_cycle_len, monthly_cycle_len, three_month_cycle_len):\n",
    "        self.data = data\n",
    "        self.num_latitude = num_latitude\n",
    "        self.num_longitude = num_longitude\n",
    "        self.daily_cycle_len = daily_cycle_len\n",
    "        self.monthly_cycle_len = monthly_cycle_len\n",
    "        self.three_month_cycle_len = three_month_cycle_len\n",
    "\n",
    "        # Cache unique latitude/longitude indices for faster grid construction\n",
    "        self.latitudes = sorted(data['Latitude'].unique())\n",
    "        self.longitudes = sorted(data['Longitude'].unique())\n",
    "        self.time_steps = sorted(data['Hours_elapsed'].unique())\n",
    "        self.lat_lon_map = {\n",
    "            (row['Latitude'], row['Longitude']): (self.latitudes.index(row['Latitude']), self.longitudes.index(row['Longitude']))\n",
    "            for _, row in data.iterrows()\n",
    "        }\n",
    "\n",
    "        # Prepare data for sequences\n",
    "        self.prepared_data = self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        X_daily, X_monthly, X_three_month, y = [], [], [], []\n",
    "        for i in range(len(self.time_steps) - self.three_month_cycle_len):\n",
    "            daily_seq, monthly_seq, three_month_seq = [], [], []\n",
    "\n",
    "            for t in range(i, i + self.three_month_cycle_len):\n",
    "                sub_data = self.data[self.data['Hours_elapsed'] == self.time_steps[t]]\n",
    "                grid = np.zeros((self.num_latitude, self.num_longitude))\n",
    "                \n",
    "                for _, row in sub_data.iterrows():\n",
    "                    lat_idx, lon_idx = self.lat_lon_map[(row['Latitude'], row['Longitude'])]\n",
    "                    grid[lat_idx, lon_idx] = row['ColumnAmountO3']\n",
    "\n",
    "                if t < i + self.daily_cycle_len:\n",
    "                    daily_seq.append(grid)\n",
    "                if t < i + self.monthly_cycle_len:\n",
    "                    monthly_seq.append(grid)\n",
    "                three_month_seq.append(grid)\n",
    "\n",
    "            X_daily.append(torch.tensor(np.array(daily_seq), dtype=torch.float32))\n",
    "            X_monthly.append(torch.tensor(np.array(monthly_seq), dtype=torch.float32))\n",
    "            X_three_month.append(torch.tensor(np.array(three_month_seq), dtype=torch.float32))\n",
    "            y.append(torch.tensor(np.array(three_month_seq[-1]), dtype=torch.float32))  # Target is last grid in the sequence\n",
    "\n",
    "        return X_daily, X_monthly, X_three_month, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prepared_data[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_daily, X_monthly, X_three_month, y = self.prepared_data\n",
    "        return X_daily[idx], X_monthly[idx], X_three_month[idx], y[idx]\n",
    "\n",
    "# Multi-Scale LSTM for Temporal Modeling\n",
    "class MultiScaleLSTM(nn.Module):\n",
    "    def __init__(self, cnn_channels, cnn_output_size, lstm_hidden_size, lstm_num_layers=1, lstm_dropout=0.25):\n",
    "        super(MultiScaleLSTM, self).__init__()\n",
    "        self.cnn = FeatureExtractorCNN(cnn_channels, cnn_output_size)\n",
    "        self.lstm_daily = nn.LSTM(cnn_output_size, lstm_hidden_size, num_layers=lstm_num_layers, dropout=lstm_dropout, batch_first=True)\n",
    "        self.lstm_monthly = nn.LSTM(cnn_output_size, lstm_hidden_size, num_layers=lstm_num_layers, dropout=lstm_dropout, batch_first=True)\n",
    "        self.lstm_three_month = nn.LSTM(cnn_output_size, lstm_hidden_size, num_layers=lstm_num_layers, dropout=lstm_dropout, batch_first=True)\n",
    "        # self.fc = nn.Linear(lstm_hidden_size * 3, 1)\n",
    "        self.fc = nn.Linear(lstm_hidden_size * 3, 5 * 10)\n",
    "\n",
    "    def forward(self, X_daily, X_monthly, X_three_month):\n",
    "        def extract_features(X_seq):\n",
    "            batch_size, seq_len, height, width = X_seq.shape\n",
    "            # Process each timestep with CNN\n",
    "            features = [self.cnn(X_seq[:, t].unsqueeze(1)) for t in range(seq_len)]\n",
    "            return torch.stack(features, dim=1)  # Shape: [batch_size, seq_len, cnn_output_size]\n",
    "\n",
    "        # Extract spatial features for each sequence\n",
    "        daily_features = extract_features(X_daily)\n",
    "        monthly_features = extract_features(X_monthly)\n",
    "        three_month_features = extract_features(X_three_month)\n",
    "\n",
    "        # Process temporal features using LSTMs\n",
    "        lstm_out_daily, _ = self.lstm_daily(daily_features)\n",
    "        lstm_out_monthly, _ = self.lstm_monthly(monthly_features)\n",
    "        lstm_out_three_month, _ = self.lstm_three_month(three_month_features)\n",
    "\n",
    "        # Concatenate the final outputs from each LSTM\n",
    "        combined_features = torch.cat((lstm_out_daily[:, -1, :], lstm_out_monthly[:, -1, :], lstm_out_three_month[:, -1, :]), dim=1)\n",
    "\n",
    "        output = self.fc(combined_features)\n",
    "        output = output.view(-1, 5, 10)  # Reshape to grid dimensions\n",
    "\n",
    "        # Final prediction\n",
    "        return output\n",
    "\n",
    "# Parameters\n",
    "num_latitude = 5\n",
    "num_longitude = 10\n",
    "cnn_channels = 1  # Grayscale-like input\n",
    "cnn_output_size = 64  # Number of features extracted by CNN\n",
    "daily_cycle_len = 8\n",
    "monthly_cycle_len = 24    #240\n",
    "three_month_cycle_len = 60  # 720 tmp for week\n",
    "lstm_hidden_size = 128 # 128\n",
    "\n",
    "# Load dataset (example)\n",
    "data = df_entire\n",
    "dataset = OzoneDataset(data, num_latitude, num_longitude, daily_cycle_len, monthly_cycle_len, three_month_cycle_len)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "  \n",
    "# Initialize model\n",
    "model = MultiScaleLSTM(cnn_channels, cnn_output_size, lstm_hidden_size)\n",
    "model.train()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for X_daily, X_monthly, X_three_month, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_daily, X_monthly, X_three_month)\n",
    "        loss = criterion(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2d = np.array(x_train).reshape(x_train.shape[0], -1)\n",
    "\n",
    "# Define the best hyperparameters from cross-validation\n",
    "\n",
    "best_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mse',\n",
    "    'num_leaves': 31,           # Increase for more splits\n",
    "    'min_data_in_leaf': 15,     # Adjust for dense data\n",
    "    'learning_rate': 0.01,      # Lower learning rate for better convergence\n",
    "    'lambda_l1': 0.1,           # Regularization to prevent overfitting\n",
    "    'lambda_l2': 0.5,           # Regularization for dense data\n",
    "    'feature_fraction': 0.8,    # Use only 80% of features per split\n",
    "    'bagging_fraction': 0.8,    # Use only 80% of data for bagging\n",
    "    'bagging_freq': 5,          # Perform bagging every 5 iterations\n",
    "    'max_depth': 12,            # Adjust depth for complexity\n",
    "    'random_state': 42,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Create the full LightGBM dataset\n",
    "full_train_dataset = lgb.Dataset(x_train_2d, label=y_train)\n",
    "\n",
    "# Define a logging callback\n",
    "callbacks = [lgb.log_evaluation(period=50)]\n",
    "\n",
    "# Train the model on the full training set\n",
    "final_model = lgb.train(\n",
    "    best_params,\n",
    "    full_train_dataset,\n",
    "    num_boost_round=2000,  # Use a high value to allow full convergence\n",
    "    valid_sets=[full_train_dataset],  # round 2000 to 5000 not helpful\n",
    "    valid_names=['train'],\n",
    "    callbacks=callbacks  # Use callbacks for logging\n",
    ")\n",
    "\n",
    "# Save the model for future use\n",
    "final_model.save_model('final_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lgb.plot_importance(final_model, max_num_features=30, importance_type='gain')\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jl2815",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
