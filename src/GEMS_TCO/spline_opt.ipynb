{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631d9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# when python interpreter is different, add path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings\n",
    "from GEMS_TCO import load_data\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "import time\n",
    "\n",
    "# Special functions and optimizations\n",
    "from typing import Callable, Union, Tuple\n",
    "from scipy.spatial.distance import cdist  # For space and time distance\n",
    "from scipy.special import gamma, kv  # Bessel function and gamma function\n",
    "# Special functions and optimizations\n",
    "\n",
    "\n",
    "from scipy.interpolate import splrep, splev\n",
    "\n",
    "# Fit your \"spline\" by just storing the x and y\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchcubicspline import natural_cubic_spline_coeffs, NaturalCubicSpline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a1b70ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Day 12 data size per day: 50.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lat_lon_resolution = [20,20]\n",
    "years = ['2024']\n",
    "month_range =[7,8]\n",
    "nheads = 200\n",
    "\n",
    "# input_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/Exercises/st_model/estimates\"\n",
    "input_path = \"C:\\\\Users\\\\joonw\\\\tco\\\\GEMS_TCO-2\\\\Exercises\\\\st_model\\\\estimates\\\\\"   # window\n",
    "input_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/Exercises/st_model/estimates/save/Apirl_16_24smooth_0.5\"   # mac\n",
    "# input_filename = \"vecc_extra_estimates_50_july24.pkl\"\n",
    "# input_filename = \"vecc_inter_estimates_1250_july24.pkl\"\n",
    "\n",
    "# input_filename = \"vecc_inter_estimates_5000_july24.pkl\"\n",
    "# input_filename = \"estimation_200_july24.pkl\"\n",
    "input_filename = \"full_estimates_1250_july24.pkl\"\n",
    "input_filepath = os.path.join(input_path, input_filename)\n",
    "# Load pickle\n",
    "with open(input_filepath, 'rb') as pickle_file:\n",
    "    amarel_map1250= pickle.load(pickle_file)\n",
    "\n",
    "# Assuming df_1250 is your DataFrame\n",
    "df_1250 = pd.DataFrame()\n",
    "for key in amarel_map1250:\n",
    "    tmp = pd.DataFrame(amarel_map1250[key][0].reshape(1, -1), columns=['sigmasq', 'range_lat', 'range_lon', 'advec_lat', 'advec_lon', 'beta', 'nugget'])\n",
    "    tmp['loss'] = amarel_map1250[key][1]\n",
    "    df_1250 = pd.concat((df_1250, tmp), axis=0)\n",
    "\n",
    "# Generate date range\n",
    "date_range = pd.date_range(start='07-01-24', end='07-31-24')\n",
    "\n",
    "# Ensure the number of dates matches the number of rows in df_1250\n",
    "if len(date_range) == len(df_1250):\n",
    "    df_1250.index = date_range\n",
    "else:\n",
    "    print(\"The number of dates does not match the number of rows in the DataFrame.\")\n",
    "\n",
    "df = df_1250\n",
    "# Save DataFrame to CSV\n",
    "output_filename = 'full_estimates_1250_july24.csv'\n",
    "output_csv_path = os.path.join(input_path, output_filename)\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "for day in range(12,13):\n",
    "    print(f'\\n Day {day} data size per day: { (200/lat_lon_resolution[0])*(100/lat_lon_resolution[0])  } \\n')\n",
    "\n",
    "    # parameters\n",
    "    mm_cond_number = 10 \n",
    "    idx_for_datamap= [ 8*(day),8*(day+1)]\n",
    "    # params = [ 27.25, 2.18, 2.294, 4.099e-4, -0.07915, 0.0999, 3.65]   #200\n",
    "    params = list(df.iloc[day-1][:-1])\n",
    "    params = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "    # data\n",
    "    # input_path = Path(\"C:\\\\Users\\\\joonw\\\\tco\\\\Extracted_data\")  # window\n",
    "    input_path = Path(\"/Users/joonwonlee/Documents/GEMS_DATA\")\n",
    "    instance = load_data(input_path)\n",
    "    map, ord_mm, nns_map= instance.load_mm20k_data_bymonthyear( lat_lon_resolution= lat_lon_resolution, mm_cond_number=mm_cond_number,years_=years, months_=month_range)\n",
    "    analysis_data_map, aggregated_data = instance.load_working_data_byday( map, ord_mm, nns_map, idx_for_datamap= idx_for_datamap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7194b220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(604.0889, dtype=torch.float64, grad_fn=<MulBackward0>) tensor(604.0889, dtype=torch.float64, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "spline_instance = kernels.spline(epsilon = 1e-17, coarse_factor=5, k=3, smooth = 0.5, input_map= analysis_data_map, aggregated_data= aggregated_data, nns_map=nns_map, mm_cond_number=10)\n",
    "spline_object, distances = spline_instance.fit_cublic_spline(params)\n",
    "cov_matrix =  spline_instance.interpolate_cubic_spline( params, spline_object, distances)\n",
    "full_ll = spline_instance.full_likelihood_using_spline( params, cov_matrix)\n",
    "full_ll\n",
    "\n",
    "instance_2 = kernels.vecchia_experiment(1.0, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "excat_ll = instance_2.full_likelihood(params, aggregated_data[:,:4], aggregated_data[:,2], instance_2.matern_cov_anisotropy_v05)\n",
    "\n",
    "print(excat_ll, full_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9acc1f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([23.0360,  2.3000,  3.3470, -0.0543,  0.1150,  0.1102,  1.6049],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "Epoch 1, Gradients: [ -0.67774733  -0.37667714  -3.33159631  31.58859341 -13.40030352\n",
      " -95.99902942  -4.98647   ]\n",
      " Loss: 604.0888771854491, Parameters: [23.03603363  2.29999757  3.3469553  -0.0542808   0.11497638  0.11015477\n",
      "  1.6048981 ]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Output 0 of UnbindBackward0 is a view and its base or another view of its base has been modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# optimizer, scheduler =  instance.optimizer_fun(params, lr= 0.01 , betas=(0.9, 0.99), eps=1e-8, step_size= 5, gamma=0.1)    \u001b[39;00m\n\u001b[32m      4\u001b[39m optimizer, scheduler = spline_instance.optimizer_fun(params, lr=\u001b[32m0.03\u001b[39m, betas=(\u001b[32m0.9\u001b[39m, \u001b[32m0.99\u001b[39m), eps=\u001b[32m1e-8\u001b[39m, step_size=\u001b[32m100\u001b[39m, gamma=\u001b[32m0.9\u001b[39m)  \n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m out, epoch = \u001b[43mspline_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/kernels.py:333\u001b[39m, in \u001b[36mspline.run_full\u001b[39m\u001b[34m(self, params, optimizer, scheduler, cov_matrix, epochs)\u001b[39m\n\u001b[32m    330\u001b[39m optimizer.zero_grad()  \u001b[38;5;66;03m# Zero the gradients \u001b[39;00m\n\u001b[32m    332\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.compute_full_nll(params, cov_matrix)\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backpropagate the loss\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[38;5;66;03m# Print gradients and parameters every 10th epoch\u001b[39;00m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Output 0 of UnbindBackward0 is a view and its base or another view of its base has been modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one."
     ]
    }
   ],
   "source": [
    "print(params)\n",
    "\n",
    "# optimizer, scheduler =  instance.optimizer_fun(params, lr= 0.01 , betas=(0.9, 0.99), eps=1e-8, step_size= 5, gamma=0.1)    \n",
    "optimizer, scheduler = spline_instance.optimizer_fun(params, lr=0.03, betas=(0.9, 0.99), eps=1e-8, step_size=100, gamma=0.9)  \n",
    "out, epoch = spline_instance.run_full(params, optimizer,scheduler, cov_matrix, epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ff815b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ce1e4e2",
   "metadata": {},
   "source": [
    "# Saved files below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81cd5e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class spline:\n",
    "    def __init__(self, epsilon, coarse_factor, k, smooth):\n",
    "        self.smooth = torch.tensor(smooth, dtype= torch.float64)\n",
    "        self.k = k\n",
    "        self.coarse_factor = coarse_factor\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def compute_cov(self, params) :\n",
    "         # fit_distances and flat_distances both 1d\n",
    "        sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "        distances, non_zero_indices = instance_2.precompute_coords_anisotropy(params, aggregated_data[:,:4],aggregated_data[:,:4])\n",
    "        \n",
    "        flat_distances = distances.flatten()\n",
    "        fit_distances = torch.linspace(self.epsilon, torch.max(flat_distances), len(flat_distances) // self.coarse_factor)\n",
    "\n",
    "        # fit_distances = torch.zeros_like(distances)\n",
    "        # print(fit_distances.shape)\n",
    "        # Compute the covariance for non-zero distances\n",
    "        non_zero_indices = fit_distances != 0\n",
    "        out = torch.zeros_like(fit_distances, dtype= torch.float64)\n",
    "\n",
    "        if torch.any(non_zero_indices):\n",
    "            tmp = torch.tensor( kv(self.smooth, torch.sqrt(fit_distances[non_zero_indices])), dtype=torch.float64).clone()\n",
    "            out[non_zero_indices] = (sigmasq * (2**(1-self.smooth)) / gamma(self.smooth) *\n",
    "                                    (torch.sqrt(fit_distances[non_zero_indices]) ) ** self.smooth *\n",
    "                                    tmp)\n",
    "        out[~non_zero_indices] = sigmasq\n",
    "\n",
    "        # print(out.shape)\n",
    "        #         \n",
    "        # Compute spline coefficients\n",
    "        coeffs = natural_cubic_spline_coeffs(fit_distances, out.unsqueeze(1))\n",
    "\n",
    "        # Create spline object\n",
    "        spline = NaturalCubicSpline(coeffs)\n",
    "        # Interpolate using the spline\n",
    "        out = spline.evaluate(distances)\n",
    "        out = out.reshape(distances.shape)\n",
    "        out += torch.eye(out.shape[0], dtype=torch.float64) * nugget \n",
    "        return \n",
    "     \n",
    "    def full_likelihood(self,params: torch.Tensor, input_np: torch.Tensor, y: torch.Tensor, cov_matrix) -> torch.Tensor:\n",
    "        input_arr = input_np[:, :4]  ## input_np is aggregated data over a day.\n",
    "        y_arr = y\n",
    "\n",
    "        # Compute the covariance matrix\n",
    "        # cov_matrix = covariance_function(params=params, y=input_arr, x=input_arr)\n",
    "        \n",
    "        # Compute the log determinant of the covariance matrix\n",
    "        sign, log_det = torch.slogdet(cov_matrix)\n",
    "        # if sign <= 0:\n",
    "        #     raise ValueError(\"Covariance matrix is not positive definite\")\n",
    "        \n",
    "        # Extract locations\n",
    "        locs = input_arr[:, :2]\n",
    "\n",
    "        # Compute beta\n",
    "        tmp1 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, locs))\n",
    "        tmp2 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, y_arr))\n",
    "        beta = torch.linalg.solve(tmp1, tmp2)\n",
    "\n",
    "        # Compute the mean\n",
    "        mu = torch.matmul(locs, beta)\n",
    "        y_mu = y_arr - mu\n",
    "\n",
    "        # Compute the quadratic form\n",
    "        quad_form = torch.matmul(y_mu, torch.linalg.solve(cov_matrix, y_mu))\n",
    "\n",
    "        # Compute the negative log likelihood\n",
    "        neg_log_lik = 0.5 * (log_det + quad_form)\n",
    "     \n",
    "        return  neg_log_lik\n",
    "    \n",
    "    def compute_full_nll(self, params, covariance_function):\n",
    "        cov_mat = covariance_function(params) \n",
    "        nll = self.full_likelihood( params,aggregated_data[:,:4], aggregated_data[:,2], cov_mat)\n",
    "        return nll\n",
    "\n",
    "    def optimizer_fun(self, params, lr=0.01, betas=(0.9, 0.8), eps=1e-8, step_size=40, gamma=0.5):\n",
    "        optimizer = torch.optim.Adam([params], lr=lr, betas=betas, eps=eps)\n",
    "        scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)  # Decrease LR by a factor of 0.1 every 10 epochs\n",
    "        return optimizer, scheduler\n",
    "\n",
    "   # use adpating lr\n",
    "    def run_full(self, params, optimizer, scheduler,  covariance_function, epochs=10 ):\n",
    "        prev_loss= float('inf')\n",
    "\n",
    "        tol = 1e-4  # Convergence tolerance\n",
    "        for epoch in range(epochs):  # Number of epochs\n",
    "            optimizer.zero_grad()  # Zero the gradients \n",
    "            \n",
    "            loss = self.compute_full_nll(params, covariance_function)\n",
    "            loss.backward()  # Backpropagate the loss\n",
    "            \n",
    "            # Print gradients and parameters every 10th epoch\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "            \n",
    "            # if epoch % 500 == 0:\n",
    "            #     print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "            \n",
    "            optimizer.step()  # Update the parameters\n",
    "            scheduler.step()  # Update the learning rate\n",
    "            # Check for convergence\n",
    "            if abs(prev_loss - loss.item()) < tol:\n",
    "                print(f\"Converged at epoch {epoch}\")\n",
    "                print(f'Epoch {epoch+1}, : Loss: {loss.item()}, \\n vecc Parameters: {params.detach().numpy()}')\n",
    "                break\n",
    "\n",
    "            prev_loss = loss.item()\n",
    "        print(f'FINAL STATE: Epoch {epoch+1}, Loss: {loss.item()}, \\n vecc Parameters: {params.detach().numpy()}')\n",
    "        return params.detach().numpy().tolist() + [ loss.item()], epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e5ea7",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "426935a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([23.0360,  2.3000,  3.3470, -0.0543,  0.1150,  0.1102,  1.6049],\n",
      "       dtype=torch.float64, requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9p/53hd4c7d2fl193h4jwp194wc0000gn/T/ipykernel_7939/2848326105.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tmp = torch.tensor( kv(self.smooth, torch.sqrt(fit_distances[non_zero_indices])), dtype=torch.float64).clone()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "slogdet(): argument 'input' (position 1) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# optimizer, scheduler =  instance.optimizer_fun(params, lr= 0.01 , betas=(0.9, 0.99), eps=1e-8, step_size= 5, gamma=0.1)    \u001b[39;00m\n\u001b[32m      6\u001b[39m optimizer, scheduler = instance.optimizer_fun(params, lr=\u001b[32m0.03\u001b[39m, betas=(\u001b[32m0.9\u001b[39m, \u001b[32m0.99\u001b[39m), eps=\u001b[32m1e-8\u001b[39m, step_size=\u001b[32m100\u001b[39m, gamma=\u001b[32m0.9\u001b[39m)  \n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m out, epoch = \u001b[43minstance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_cov\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mspline.run_full\u001b[39m\u001b[34m(self, params, optimizer, scheduler, covariance_function, epochs)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):  \u001b[38;5;66;03m# Number of epochs\u001b[39;00m\n\u001b[32m     90\u001b[39m     optimizer.zero_grad()  \u001b[38;5;66;03m# Zero the gradients \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_full_nll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcovariance_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     loss.backward()  \u001b[38;5;66;03m# Backpropagate the loss\u001b[39;00m\n\u001b[32m     95\u001b[39m     \u001b[38;5;66;03m# Print gradients and parameters every 10th epoch\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 76\u001b[39m, in \u001b[36mspline.compute_full_nll\u001b[39m\u001b[34m(self, params, covariance_function)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_full_nll\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, covariance_function):\n\u001b[32m     75\u001b[39m     cov_mat = covariance_function(params) \n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     nll = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfull_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43maggregated_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregated_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov_mat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m nll\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mspline.full_likelihood\u001b[39m\u001b[34m(self, params, input_np, y, cov_matrix)\u001b[39m\n\u001b[32m     44\u001b[39m y_arr = y\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Compute the covariance matrix\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# cov_matrix = covariance_function(params=params, y=input_arr, x=input_arr)\u001b[39;00m\n\u001b[32m     48\u001b[39m \n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Compute the log determinant of the covariance matrix\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m sign, log_det = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mslogdet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcov_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# if sign <= 0:\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m#     raise ValueError(\"Covariance matrix is not positive definite\")\u001b[39;00m\n\u001b[32m     53\u001b[39m \n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Extract locations\u001b[39;00m\n\u001b[32m     55\u001b[39m locs = input_arr[:, :\u001b[32m2\u001b[39m]\n",
      "\u001b[31mTypeError\u001b[39m: slogdet(): argument 'input' (position 1) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "print(params)\n",
    "\n",
    "instance_2 = kernels.vecchia_experiment(1.0, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "instance = spline( epsilon = 1e-8, coarse_factor = 4, k=3, smooth= 0.5)\n",
    "# optimizer, scheduler =  instance.optimizer_fun(params, lr= 0.01 , betas=(0.9, 0.99), eps=1e-8, step_size= 5, gamma=0.1)    \n",
    "optimizer, scheduler = instance.optimizer_fun(params, lr=0.03, betas=(0.9, 0.99), eps=1e-8, step_size=100, gamma=0.9)  \n",
    "out, epoch = instance.run_full(params, optimizer,scheduler, instance.compute_cov, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "85136da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9p/53hd4c7d2fl193h4jwp194wc0000gn/T/ipykernel_6150/985977276.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tmp = torch.tensor( kv(smooth, torch.sqrt(fit_distances[non_zero_indices])), dtype=torch.float64).clone()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cubic spline: \n",
      " tensor([[24.6387,  4.0899,  5.2544,  ...,  8.4224, 10.1567,  7.5467],\n",
      "        [ 4.0899, 24.6387,  0.9367,  ...,  3.3871,  3.1544,  2.5753],\n",
      "        [ 5.2544,  0.9367, 24.6387,  ...,  3.3957,  4.7013,  3.8674],\n",
      "        ...,\n",
      "        [ 8.4224,  3.3871,  3.3957,  ..., 24.6387,  9.1852, 17.0864],\n",
      "        [10.1567,  3.1544,  4.7013,  ...,  9.1852, 24.6387,  8.0202],\n",
      "        [ 7.5467,  2.5753,  3.8674,  ..., 17.0864,  8.0202, 24.6387]],\n",
      "       dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "\n",
      "exact \n",
      " tensor([[24.6409,  4.0899,  5.2544,  ...,  8.4224, 10.1567,  7.5467],\n",
      "        [ 4.0899, 24.6409,  0.9367,  ...,  3.3871,  3.1544,  2.5753],\n",
      "        [ 5.2544,  0.9367, 24.6409,  ...,  3.3957,  4.7013,  3.8674],\n",
      "        ...,\n",
      "        [ 8.4224,  3.3871,  3.3957,  ..., 24.6409,  9.1852, 17.0864],\n",
      "        [10.1567,  3.1544,  4.7013,  ...,  9.1852, 24.6409,  8.0202],\n",
      "        [ 7.5467,  2.5753,  3.8674,  ..., 17.0864,  8.0202, 24.6409]],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "coarse_factor=1\n",
    "\n",
    "fit_distances = torch.linspace(epsilon, torch.max(flat_distances), len(flat_distances) // coarse_factor)\n",
    "\n",
    "non_zero_indices = fit_distances != 0\n",
    "out = torch.zeros_like(fit_distances, dtype= torch.float64)\n",
    "\n",
    "if torch.any(non_zero_indices):\n",
    "    tmp = torch.tensor( kv(smooth, torch.sqrt(fit_distances[non_zero_indices])), dtype=torch.float64).clone()\n",
    "    out[non_zero_indices] = (  (2**(1-smooth)) / gamma(smooth) *\n",
    "                            (torch.sqrt(fit_distances[non_zero_indices]) ) ** smooth *\n",
    "                            tmp)\n",
    "    \n",
    "out[~non_zero_indices] = 1\n",
    "\n",
    "\n",
    "# Compute spline coefficients\n",
    "coeffs = natural_cubic_spline_coeffs(fit_distances, out.unsqueeze(1))\n",
    "\n",
    "# Create spline object\n",
    "splinenn = NaturalCubicSpline(coeffs)\n",
    "\n",
    "# Interpolate using the spline\n",
    "out1 = splinenn.evaluate(distances)\n",
    "out1 = out1.reshape(distances.shape)\n",
    "\n",
    "out1 *= sigmasq\n",
    "\n",
    "# Assuming out1, sigmasq, and nugget are already defined\n",
    "identity_matrix = torch.eye(out1.shape[0], dtype=torch.float64) * ( nugget)\n",
    "out1 += identity_matrix\n",
    "\n",
    "print(f'cubic spline: \\n {out1}')\n",
    "\n",
    "print(f'\\nexact \\n {out2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41c5a49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40000])\n",
      "torch.Size([40000])\n",
      "tensor([[24.6386,  4.0899,  5.2544,  ...,  8.4224, 10.1567,  7.5467],\n",
      "        [ 4.0899, 24.6386,  0.9367,  ...,  3.3871,  3.1544,  2.5753],\n",
      "        [ 5.2544,  0.9367, 24.6386,  ...,  3.3957,  4.7013,  3.8674],\n",
      "        ...,\n",
      "        [ 8.4224,  3.3871,  3.3957,  ..., 24.6386,  9.1852, 17.0864],\n",
      "        [10.1567,  3.1544,  4.7013,  ...,  9.1852, 24.6386,  8.0202],\n",
      "        [ 7.5467,  2.5753,  3.8674,  ..., 17.0864,  8.0202, 24.6386]],\n",
      "       dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "tensor([[24.6409,  4.0899,  5.2544,  ...,  8.4224, 10.1567,  7.5467],\n",
      "        [ 4.0899, 24.6409,  0.9367,  ...,  3.3871,  3.1544,  2.5753],\n",
      "        [ 5.2544,  0.9367, 24.6409,  ...,  3.3957,  4.7013,  3.8674],\n",
      "        ...,\n",
      "        [ 8.4224,  3.3871,  3.3957,  ..., 24.6409,  9.1852, 17.0864],\n",
      "        [10.1567,  3.1544,  4.7013,  ...,  9.1852, 24.6409,  8.0202],\n",
      "        [ 7.5467,  2.5753,  3.8674,  ..., 17.0864,  8.0202, 24.6409]],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(604.1003, dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smooth = 0.5\n",
    "\n",
    "instance_2 = kernels.vecchia_experiment(smooth, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "instance = spline( epsilon = 1e-8, coarse_factor = 4, k=3, smooth= smooth)\n",
    "\n",
    "distances, non_zero_indices = instance_2.precompute_coords_anisotropy(params, aggregated_data[:,:4],aggregated_data[:,:4])\n",
    "\n",
    "flat_distances = distances.flatten()\n",
    "sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "epsilon = 1e-8\n",
    "coarse_factor = 4\n",
    "\n",
    "fit_distances = torch.linspace(epsilon, torch.max(flat_distances), len(flat_distances) // coarse_factor)\n",
    "print(fit_distances.shape)\n",
    "# Compute the covariance for non-zero distances\n",
    "non_zero_indices = fit_distances != 0\n",
    "out = torch.zeros_like(fit_distances, dtype= torch.float64)\n",
    "\n",
    "if torch.any(non_zero_indices):\n",
    "    tmp = kv(smooth, torch.sqrt(fit_distances[non_zero_indices])).double().clone()\n",
    "    out[non_zero_indices] = (sigmasq * (2**(1-smooth)) / gamma(smooth) *\n",
    "                            (torch.sqrt(fit_distances[non_zero_indices]) ) ** smooth *\n",
    "                            tmp)\n",
    "    \n",
    "out[~non_zero_indices] = sigmasq\n",
    "\n",
    "print(out.shape)\n",
    "\n",
    "# Compute spline coefficients\n",
    "coeffs = natural_cubic_spline_coeffs(fit_distances, out.unsqueeze(1))\n",
    "\n",
    "# Create spline object\n",
    "splinenn = NaturalCubicSpline(coeffs)\n",
    "\n",
    "# Interpolate using the spline\n",
    "out1 = splinenn.evaluate(distances)\n",
    "out1 = out1.reshape(distances.shape)\n",
    "out1 += torch.eye(out1.shape[0], dtype=torch.float64) * nugget \n",
    "\n",
    "print(out1)\n",
    "out2 = instance_2.matern_cov_anisotropy_kv(params, aggregated_data[:,:4],aggregated_data[:,:4])\n",
    "\n",
    "\n",
    "print(out2)\n",
    "instance.full_likelihood( params,aggregated_data[:,:4], aggregated_data[:,2], out1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
