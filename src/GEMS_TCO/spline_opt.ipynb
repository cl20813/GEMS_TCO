{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631d9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# when python interpreter is different, add path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings\n",
    "from GEMS_TCO import load_data\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import copy                    # clone tensor\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "import time\n",
    "\n",
    "# Special functions and optimizations\n",
    "from typing import Callable, Union, Tuple\n",
    "from scipy.spatial.distance import cdist  # For space and time distance\n",
    "from scipy.special import gamma, kv  # Bessel function and gamma function\n",
    "# Special functions and optimizations\n",
    "from scipy.spatial.distance import cdist  # For space and time distance\n",
    "from scipy.special import gamma, kv  # Bessel function and gamma function\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import basinhopping, minimize\n",
    "from scipy.stats import norm,uniform\n",
    "from scipy.stats import t\n",
    "from scipy.interpolate import splrep, splev\n",
    "from scipy.special import kv, gamma\n",
    "\n",
    "# Fit your \"spline\" by just storing the x and y\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torchcubicspline import natural_cubic_spline_coeffs, NaturalCubicSpline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a1b70ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Day 12 data size per day: 50.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lat_lon_resolution = [20,20]\n",
    "years = ['2024']\n",
    "month_range =[7,8]\n",
    "nheads = 200\n",
    "\n",
    "# input_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/Exercises/st_model/estimates\"\n",
    "input_path = \"C:\\\\Users\\\\joonw\\\\tco\\\\GEMS_TCO-2\\\\Exercises\\\\st_model\\\\estimates\\\\\"   # window\n",
    "input_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/Exercises/st_model/estimates/save/Apirl_16_24smooth_0.5\"   # mac\n",
    "# input_filename = \"vecc_extra_estimates_50_july24.pkl\"\n",
    "# input_filename = \"vecc_inter_estimates_1250_july24.pkl\"\n",
    "\n",
    "# input_filename = \"vecc_inter_estimates_5000_july24.pkl\"\n",
    "# input_filename = \"estimation_200_july24.pkl\"\n",
    "input_filename = \"full_estimates_1250_july24.pkl\"\n",
    "input_filepath = os.path.join(input_path, input_filename)\n",
    "# Load pickle\n",
    "with open(input_filepath, 'rb') as pickle_file:\n",
    "    amarel_map1250= pickle.load(pickle_file)\n",
    "\n",
    "# Assuming df_1250 is your DataFrame\n",
    "df_1250 = pd.DataFrame()\n",
    "for key in amarel_map1250:\n",
    "    tmp = pd.DataFrame(amarel_map1250[key][0].reshape(1, -1), columns=['sigmasq', 'range_lat', 'range_lon', 'advec_lat', 'advec_lon', 'beta', 'nugget'])\n",
    "    tmp['loss'] = amarel_map1250[key][1]\n",
    "    df_1250 = pd.concat((df_1250, tmp), axis=0)\n",
    "\n",
    "# Generate date range\n",
    "date_range = pd.date_range(start='07-01-24', end='07-31-24')\n",
    "\n",
    "# Ensure the number of dates matches the number of rows in df_1250\n",
    "if len(date_range) == len(df_1250):\n",
    "    df_1250.index = date_range\n",
    "else:\n",
    "    print(\"The number of dates does not match the number of rows in the DataFrame.\")\n",
    "\n",
    "df = df_1250\n",
    "# Save DataFrame to CSV\n",
    "output_filename = 'full_estimates_1250_july24.csv'\n",
    "output_csv_path = os.path.join(input_path, output_filename)\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "for day in range(12,13):\n",
    "    print(f'\\n Day {day} data size per day: { (200/lat_lon_resolution[0])*(100/lat_lon_resolution[0])  } \\n')\n",
    "\n",
    "    # parameters\n",
    "    mm_cond_number = 10 \n",
    "    idx_for_datamap= [ 8*(day),8*(day+1)]\n",
    "    # params = [ 27.25, 2.18, 2.294, 4.099e-4, -0.07915, 0.0999, 3.65]   #200\n",
    "    params = list(df.iloc[day-1][:-1])\n",
    "    params = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "    # data\n",
    "    # input_path = Path(\"C:\\\\Users\\\\joonw\\\\tco\\\\Extracted_data\")  # window\n",
    "    input_path = Path(\"/Users/joonwonlee/Documents/GEMS_DATA\")\n",
    "    instance = load_data(input_path)\n",
    "    map, ord_mm, nns_map= instance.load_mm20k_data_bymonthyear( lat_lon_resolution= lat_lon_resolution, mm_cond_number=mm_cond_number,years_=years, months_=month_range)\n",
    "    analysis_data_map, aggregated_data = instance.load_working_data_byday( map, ord_mm, nns_map, idx_for_datamap= idx_for_datamap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81cd5e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class spline:\n",
    "    def __init__(self, epsilon, coarse_factor, k, smooth):\n",
    "        self.smooth = torch.tensor(smooth, dtype= torch.float64)\n",
    "        self.k = k\n",
    "        self.coarse_factor = coarse_factor\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def compute_cov(self, params ):\n",
    "         # fit_distances and flat_distances both 1d\n",
    "        sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "        distances, non_zero_indices = instance_2.precompute_coords_anisotropy(params, aggregated_data[:,:4],aggregated_data[:,:4])\n",
    "        \n",
    "        flat_distances = distances.flatten()\n",
    "        fit_distances = torch.linspace(self.epsilon, torch.max(flat_distances), len(flat_distances) // self.coarse_factor)\n",
    "\n",
    "        # fit_distances = torch.zeros_like(distances)\n",
    "        # print(fit_distances.shape)\n",
    "        # Compute the covariance for non-zero distances\n",
    "        non_zero_indices = fit_distances != 0\n",
    "        out = torch.zeros_like(fit_distances, dtype= torch.float64)\n",
    "\n",
    "        if torch.any(non_zero_indices):\n",
    "            tmp = torch.tensor( kv(self.smooth, torch.sqrt(fit_distances[non_zero_indices])), dtype=torch.float64).clone()\n",
    "            out[non_zero_indices] = (sigmasq * (2**(1-self.smooth)) / gamma(self.smooth) *\n",
    "                                    (torch.sqrt(fit_distances[non_zero_indices]) ) ** self.smooth *\n",
    "                                    tmp)\n",
    "        out[~non_zero_indices] = sigmasq\n",
    "\n",
    "        # print(out.shape)\n",
    "        #         \n",
    "        # Compute spline coefficients\n",
    "        coeffs = natural_cubic_spline_coeffs(fit_distances, out.unsqueeze(1))\n",
    "\n",
    "        # Create spline object\n",
    "        spline = NaturalCubicSpline(coeffs)\n",
    "        # Interpolate using the spline\n",
    "        out = spline.evaluate(distances)\n",
    "        out = out.reshape(distances.shape)\n",
    "        out += torch.eye(out.shape[0], dtype=torch.float64) * nugget \n",
    "        return \n",
    "     \n",
    "    def full_likelihood(self,params: torch.Tensor, input_np: torch.Tensor, y: torch.Tensor, cov_matrix) -> torch.Tensor:\n",
    "        input_arr = input_np[:, :4]  ## input_np is aggregated data over a day.\n",
    "        y_arr = y\n",
    "\n",
    "        # Compute the covariance matrix\n",
    "        # cov_matrix = covariance_function(params=params, y=input_arr, x=input_arr)\n",
    "        \n",
    "        # Compute the log determinant of the covariance matrix\n",
    "        sign, log_det = torch.slogdet(cov_matrix)\n",
    "        # if sign <= 0:\n",
    "        #     raise ValueError(\"Covariance matrix is not positive definite\")\n",
    "        \n",
    "        # Extract locations\n",
    "        locs = input_arr[:, :2]\n",
    "\n",
    "        # Compute beta\n",
    "        tmp1 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, locs))\n",
    "        tmp2 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, y_arr))\n",
    "        beta = torch.linalg.solve(tmp1, tmp2)\n",
    "\n",
    "        # Compute the mean\n",
    "        mu = torch.matmul(locs, beta)\n",
    "        y_mu = y_arr - mu\n",
    "\n",
    "        # Compute the quadratic form\n",
    "        quad_form = torch.matmul(y_mu, torch.linalg.solve(cov_matrix, y_mu))\n",
    "\n",
    "        # Compute the negative log likelihood\n",
    "        neg_log_lik = 0.5 * (log_det + quad_form)\n",
    "     \n",
    "        return  neg_log_lik\n",
    "    \n",
    "    def compute_full_nll(self, params, covariance_function):\n",
    "        cov_mat = covariance_function(params) \n",
    "        nll = self.full_likelihood( params,aggregated_data[:,:4], aggregated_data[:,2], cov_mat)\n",
    "        return nll\n",
    "\n",
    "    def optimizer_fun(self, params, lr=0.01, betas=(0.9, 0.8), eps=1e-8, step_size=40, gamma=0.5):\n",
    "        optimizer = torch.optim.Adam([params], lr=lr, betas=betas, eps=eps)\n",
    "        scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)  # Decrease LR by a factor of 0.1 every 10 epochs\n",
    "        return optimizer, scheduler\n",
    "\n",
    "   # use adpating lr\n",
    "    def run_full(self, params, optimizer, scheduler,  covariance_function, epochs=10 ):\n",
    "        prev_loss= float('inf')\n",
    "\n",
    "        tol = 1e-4  # Convergence tolerance\n",
    "        for epoch in range(epochs):  # Number of epochs\n",
    "            optimizer.zero_grad()  # Zero the gradients \n",
    "            \n",
    "            loss = self.compute_full_nll(params, covariance_function)\n",
    "            loss.backward()  # Backpropagate the loss\n",
    "            \n",
    "            # Print gradients and parameters every 10th epoch\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "            \n",
    "            # if epoch % 500 == 0:\n",
    "            #     print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "            \n",
    "            optimizer.step()  # Update the parameters\n",
    "            scheduler.step()  # Update the learning rate\n",
    "            # Check for convergence\n",
    "            if abs(prev_loss - loss.item()) < tol:\n",
    "                print(f\"Converged at epoch {epoch}\")\n",
    "                print(f'Epoch {epoch+1}, : Loss: {loss.item()}, \\n vecc Parameters: {params.detach().numpy()}')\n",
    "                break\n",
    "\n",
    "            prev_loss = loss.item()\n",
    "        print(f'FINAL STATE: Epoch {epoch+1}, Loss: {loss.item()}, \\n vecc Parameters: {params.detach().numpy()}')\n",
    "        return params.detach().numpy().tolist() + [ loss.item()], epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e5ea7",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426935a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params)\n",
    "\n",
    "instance_2 = kernels.vecchia_experiment(1.0, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "instance = spline( epsilon = 1e-8, coarse_factor = 4, k=3, smooth= 0.5)\n",
    "# optimizer, scheduler =  instance.optimizer_fun(params, lr= 0.01 , betas=(0.9, 0.99), eps=1e-8, step_size= 5, gamma=0.1)    \n",
    "optimizer, scheduler = instance.optimizer_fun(params, lr=0.03, betas=(0.9, 0.99), eps=1e-8, step_size=100, gamma=0.9)  \n",
    "out, epoch = instance.run_full(params, optimizer,scheduler, instance.compute_cov, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "85136da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9p/53hd4c7d2fl193h4jwp194wc0000gn/T/ipykernel_6150/3459816693.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tmp = torch.tensor( kv(smooth, torch.sqrt(fit_distances[non_zero_indices])), dtype=torch.float64).clone()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24.6386,  4.0899,  5.2544,  ...,  8.4224, 10.1567,  7.5467],\n",
      "        [ 4.0899, 24.6386,  0.9367,  ...,  3.3871,  3.1544,  2.5753],\n",
      "        [ 5.2544,  0.9367, 24.6386,  ...,  3.3957,  4.7013,  3.8674],\n",
      "        ...,\n",
      "        [ 8.4224,  3.3871,  3.3957,  ..., 24.6386,  9.1852, 17.0864],\n",
      "        [10.1567,  3.1544,  4.7013,  ...,  9.1852, 24.6386,  8.0202],\n",
      "        [ 7.5467,  2.5753,  3.8674,  ..., 17.0864,  8.0202, 24.6386]],\n",
      "       dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "tensor([[24.6409,  4.0899,  5.2544,  ...,  8.4224, 10.1567,  7.5467],\n",
      "        [ 4.0899, 24.6409,  0.9367,  ...,  3.3871,  3.1544,  2.5753],\n",
      "        [ 5.2544,  0.9367, 24.6409,  ...,  3.3957,  4.7013,  3.8674],\n",
      "        ...,\n",
      "        [ 8.4224,  3.3871,  3.3957,  ..., 24.6409,  9.1852, 17.0864],\n",
      "        [10.1567,  3.1544,  4.7013,  ...,  9.1852, 24.6409,  8.0202],\n",
      "        [ 7.5467,  2.5753,  3.8674,  ..., 17.0864,  8.0202, 24.6409]],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "fit_distances = torch.linspace(epsilon, torch.max(flat_distances), len(flat_distances) // coarse_factor)\n",
    "\n",
    "non_zero_indices = fit_distances != 0\n",
    "out = torch.zeros_like(fit_distances, dtype= torch.float64)\n",
    "\n",
    "if torch.any(non_zero_indices):\n",
    "    tmp = torch.tensor( kv(smooth, torch.sqrt(fit_distances[non_zero_indices])), dtype=torch.float64).clone()\n",
    "    out[non_zero_indices] = (  (2**(1-smooth)) / gamma(smooth) *\n",
    "                            (torch.sqrt(fit_distances[non_zero_indices]) ) ** smooth *\n",
    "                            tmp)\n",
    "    \n",
    "out[~non_zero_indices] = 1\n",
    "\n",
    "\n",
    "# Compute spline coefficients\n",
    "coeffs = natural_cubic_spline_coeffs(fit_distances, out.unsqueeze(1))\n",
    "\n",
    "# Create spline object\n",
    "splinenn = NaturalCubicSpline(coeffs)\n",
    "\n",
    "# Interpolate using the spline\n",
    "out1 = splinenn.evaluate(distances)\n",
    "out1 = out1.reshape(distances.shape)\n",
    "\n",
    "out1 *= sigmasq\n",
    "\n",
    "# Assuming out1, sigmasq, and nugget are already defined\n",
    "identity_matrix = torch.eye(out1.shape[0], dtype=torch.float64) * ( nugget)\n",
    "out1 += identity_matrix\n",
    "\n",
    "print(out1)\n",
    "\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41c5a49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40000])\n",
      "torch.Size([40000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9p/53hd4c7d2fl193h4jwp194wc0000gn/T/ipykernel_6150/468670627.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tmp = torch.tensor( kv(smooth, torch.sqrt(fit_distances[non_zero_indices])), dtype=torch.float64).clone()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24.6386,  4.0899,  5.2544,  ...,  8.4224, 10.1567,  7.5467],\n",
      "        [ 4.0899, 24.6386,  0.9367,  ...,  3.3871,  3.1544,  2.5753],\n",
      "        [ 5.2544,  0.9367, 24.6386,  ...,  3.3957,  4.7013,  3.8674],\n",
      "        ...,\n",
      "        [ 8.4224,  3.3871,  3.3957,  ..., 24.6386,  9.1852, 17.0864],\n",
      "        [10.1567,  3.1544,  4.7013,  ...,  9.1852, 24.6386,  8.0202],\n",
      "        [ 7.5467,  2.5753,  3.8674,  ..., 17.0864,  8.0202, 24.6386]],\n",
      "       dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "tensor([[24.6409,  4.0899,  5.2544,  ...,  8.4224, 10.1567,  7.5467],\n",
      "        [ 4.0899, 24.6409,  0.9367,  ...,  3.3871,  3.1544,  2.5753],\n",
      "        [ 5.2544,  0.9367, 24.6409,  ...,  3.3957,  4.7013,  3.8674],\n",
      "        ...,\n",
      "        [ 8.4224,  3.3871,  3.3957,  ..., 24.6409,  9.1852, 17.0864],\n",
      "        [10.1567,  3.1544,  4.7013,  ...,  9.1852, 24.6409,  8.0202],\n",
      "        [ 7.5467,  2.5753,  3.8674,  ..., 17.0864,  8.0202, 24.6409]],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(604.1003, dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smooth = 0.5\n",
    "\n",
    "instance_2 = kernels.vecchia_experiment(smooth, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "instance = spline( epsilon = 1e-8, coarse_factor = 4, k=3, smooth= smooth)\n",
    "\n",
    "distances, non_zero_indices = instance_2.precompute_coords_anisotropy(params, aggregated_data[:,:4],aggregated_data[:,:4])\n",
    "\n",
    "flat_distances = distances.flatten()\n",
    "sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "epsilon = 1e-8\n",
    "coarse_factor = 4\n",
    "\n",
    "fit_distances = torch.linspace(epsilon, torch.max(flat_distances), len(flat_distances) // coarse_factor)\n",
    "print(fit_distances.shape)\n",
    "# Compute the covariance for non-zero distances\n",
    "non_zero_indices = fit_distances != 0\n",
    "out = torch.zeros_like(fit_distances, dtype= torch.float64)\n",
    "\n",
    "if torch.any(non_zero_indices):\n",
    "    tmp = torch.tensor( kv(smooth, torch.sqrt(fit_distances[non_zero_indices])), dtype=torch.float64).clone()\n",
    "    out[non_zero_indices] = (sigmasq * (2**(1-smooth)) / gamma(smooth) *\n",
    "                            (torch.sqrt(fit_distances[non_zero_indices]) ) ** smooth *\n",
    "                            tmp)\n",
    "    \n",
    "out[~non_zero_indices] = sigmasq\n",
    "\n",
    "print(out.shape)\n",
    "\n",
    "# Compute spline coefficients\n",
    "coeffs = natural_cubic_spline_coeffs(fit_distances, out.unsqueeze(1))\n",
    "\n",
    "# Create spline object\n",
    "splinenn = NaturalCubicSpline(coeffs)\n",
    "\n",
    "# Interpolate using the spline\n",
    "out1 = splinenn.evaluate(distances)\n",
    "out1 = out1.reshape(distances.shape)\n",
    "out1 += torch.eye(out1.shape[0], dtype=torch.float64) * nugget \n",
    "\n",
    "print(out1)\n",
    "out2 = instance_2.matern_cov_anisotropy_kv(params, aggregated_data[:,:4],aggregated_data[:,:4])\n",
    "\n",
    "\n",
    "print(out2)\n",
    "instance.full_likelihood( params,aggregated_data[:,:4], aggregated_data[:,2], out1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de5265bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(704.8110, dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance_2.full_likelihood(params, aggregated_data[:,:4], aggregated_data[:,2], instance_2.matern_cov_anisotropy_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d5e0d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchcubicspline import natural_cubic_spline_coeffs, NaturalCubicSpline\n",
    "\n",
    "# Example data\n",
    "fit_distances = torch.tensor([0, 1, 2, 3, 4, 5], dtype=torch.float32)\n",
    "out = torch.tensor([0, 0.8, 0.9, 0.1, -0.8, -1], dtype=torch.float32).unsqueeze(1)  # Add a dimension to make it (6, 1)\n",
    "distances = torch.linspace(0, 5, 100)\n",
    "\n",
    "# Compute spline coefficients\n",
    "coeffs = natural_cubic_spline_coeffs(fit_distances, out)\n",
    "\n",
    "# Create spline object\n",
    "spline = NaturalCubicSpline(coeffs)\n",
    "\n",
    "# Interpolate using the spline\n",
    "out1 = spline.evaluate(distances)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9c1aa7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000],\n",
       "        [ 0.8000],\n",
       "        [ 0.9000],\n",
       "        [ 0.1000],\n",
       "        [-0.8000],\n",
       "        [-1.0000]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
