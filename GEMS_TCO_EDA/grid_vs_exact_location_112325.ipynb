{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdfafc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from GEMS_TCO import data_preprocess \n",
    "from GEMS_TCO import  kernels_reparam_space_time as kernels_reparam_space_time\n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO.data_loader import load_data2, exact_location_filter\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "from GEMS_TCO import debiased_whittle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d98f99",
   "metadata": {},
   "source": [
    "Load monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "450a6c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting data to lat: [0.0, 5.0], lon: [123.0, 133.0]\n"
     ]
    }
   ],
   "source": [
    "space: List[str] = ['1', '1']\n",
    "lat_lon_resolution = [int(s) for s in space]\n",
    "mm_cond_number: int = 20\n",
    "years = ['2024']\n",
    "month_range = [7] \n",
    "\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "data_load_instance = load_data2(config.mac_data_load_path)\n",
    "\n",
    "\n",
    "df_map, ord_mm, nns_map = data_load_instance.load_maxmin_ordered_data_bymonthyear(\n",
    "lat_lon_resolution=lat_lon_resolution, \n",
    "mm_cond_number=mm_cond_number,\n",
    "years_=years, \n",
    "months_=month_range,\n",
    "lat_range=[0.0, 5.0],      \n",
    "lon_range=[123.0, 133.0] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1451a758",
   "metadata": {},
   "source": [
    "Load daily data applying max-min ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bd02742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([145008, 4])\n",
      "tensor([[  4.9760, 132.9840, 260.9815,  21.0000],\n",
      "        [  4.9760, 132.9210, 261.5255,  21.0000],\n",
      "        [  4.9760, 132.8580, 262.9575,  21.0000],\n",
      "        [  4.9760, 132.7950, 262.6640,  21.0000],\n",
      "        [  4.9760, 132.7320, 261.0027,  21.0000],\n",
      "        [  4.9760, 132.6690, 260.6678,  21.0000],\n",
      "        [  4.9760, 132.6060, 261.8147,  21.0000],\n",
      "        [  4.9760, 132.5430, 263.7160,  21.0000],\n",
      "        [  4.9760, 132.4800, 263.3271,  21.0000],\n",
      "        [  4.9760, 132.4170, 263.4973,  21.0000]])\n"
     ]
    }
   ],
   "source": [
    "daily_aggregated_tensors = [] \n",
    "daily_hourly_maps = []      \n",
    "\n",
    "daily_aggregated_tensors2 = [] \n",
    "daily_hourly_maps2 = []    \n",
    "\n",
    "for day_index in range(31):\n",
    "    hour_start_index = day_index * 8\n",
    "    hour_end_index = (day_index + 1) * 8\n",
    "    #hour_end_index = day_index*8 + 1\n",
    "    hour_indices = [hour_start_index, hour_end_index]\n",
    "    \n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "    df_map, \n",
    "    hour_indices, \n",
    "    ord_mm= None,  # or just omit it\n",
    "    dtype=torch.float, # or just omit it \n",
    "    keep_ori=False\n",
    "    )\n",
    "\n",
    "    daily_aggregated_tensors.append( day_aggregated_tensor )\n",
    "    daily_hourly_maps.append( day_hourly_map )\n",
    "\n",
    "\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "    df_map, \n",
    "    hour_indices, \n",
    "    ord_mm= None,  # or just omit it\n",
    "    dtype=torch.float, # or just omit it \n",
    "    keep_ori=True\n",
    "    )\n",
    "\n",
    "    daily_aggregated_tensors2.append( day_aggregated_tensor )\n",
    "    daily_hourly_maps2.append( day_hourly_map )\n",
    "\n",
    "\n",
    "print(daily_aggregated_tensors[0].shape)\n",
    "print(daily_aggregated_tensors[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ac7b823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Deviation Filter (Days 0-30) ---\n",
      "Thresholds: Lat <= 0.9, Lon <= 0.4\n",
      "Day 0 (Hourly): 18126 -> 16372 rows kept (90.32%)\n",
      "Day 0 (Aggregated): 145008 -> 130976 rows kept (Shape consistent with 16372 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 1 (Hourly): 18126 -> 16391 rows kept (90.43%)\n",
      "Day 1 (Aggregated): 145008 -> 131128 rows kept (Shape consistent with 16391 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 2 (Hourly): 18126 -> 16389 rows kept (90.42%)\n",
      "Day 2 (Aggregated): 145008 -> 131112 rows kept (Shape consistent with 16389 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 3 (Hourly): 18126 -> 16392 rows kept (90.43%)\n",
      "Day 3 (Aggregated): 145008 -> 131136 rows kept (Shape consistent with 16392 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 4 (Hourly): 18126 -> 16389 rows kept (90.42%)\n",
      "Day 4 (Aggregated): 145008 -> 131112 rows kept (Shape consistent with 16389 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 5 (Hourly): 18126 -> 16395 rows kept (90.45%)\n",
      "Day 5 (Aggregated): 145008 -> 131160 rows kept (Shape consistent with 16395 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 6 (Hourly): 18126 -> 16396 rows kept (90.46%)\n",
      "Day 6 (Aggregated): 145008 -> 131168 rows kept (Shape consistent with 16396 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 7 (Hourly): 18126 -> 16276 rows kept (89.79%)\n",
      "Day 7 (Aggregated): 145008 -> 130208 rows kept (Shape consistent with 16276 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 8 (Hourly): 18126 -> 16286 rows kept (89.85%)\n",
      "Day 8 (Aggregated): 145008 -> 130288 rows kept (Shape consistent with 16286 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 9 (Hourly): 18126 -> 16368 rows kept (90.30%)\n",
      "Day 9 (Aggregated): 145008 -> 130944 rows kept (Shape consistent with 16368 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 10 (Hourly): 18126 -> 15062 rows kept (83.10%)\n",
      "Day 10 (Aggregated): 145008 -> 120496 rows kept (Shape consistent with 15062 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 11 (Hourly): 18126 -> 16291 rows kept (89.88%)\n",
      "Day 11 (Aggregated): 145008 -> 130328 rows kept (Shape consistent with 16291 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 12 (Hourly): 18126 -> 16166 rows kept (89.19%)\n",
      "Day 12 (Aggregated): 145008 -> 129328 rows kept (Shape consistent with 16166 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 13 (Hourly): 18126 -> 15950 rows kept (88.00%)\n",
      "Day 13 (Aggregated): 145008 -> 127600 rows kept (Shape consistent with 15950 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 14 (Hourly): 18126 -> 16370 rows kept (90.31%)\n",
      "Day 14 (Aggregated): 145008 -> 130960 rows kept (Shape consistent with 16370 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 15 (Hourly): 18126 -> 16278 rows kept (89.80%)\n",
      "Day 15 (Aggregated): 145008 -> 130224 rows kept (Shape consistent with 16278 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 16 (Hourly): 18126 -> 16373 rows kept (90.33%)\n",
      "Day 16 (Aggregated): 145008 -> 130984 rows kept (Shape consistent with 16373 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 17 (Hourly): 18126 -> 16372 rows kept (90.32%)\n",
      "Day 17 (Aggregated): 145008 -> 130976 rows kept (Shape consistent with 16372 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 18 (Hourly): 18126 -> 16382 rows kept (90.38%)\n",
      "Day 18 (Aggregated): 145008 -> 131056 rows kept (Shape consistent with 16382 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 19 (Hourly): 18126 -> 16391 rows kept (90.43%)\n",
      "Day 19 (Aggregated): 145008 -> 131128 rows kept (Shape consistent with 16391 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 20 (Hourly): 18126 -> 16392 rows kept (90.43%)\n",
      "Day 20 (Aggregated): 145008 -> 131136 rows kept (Shape consistent with 16392 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 21 (Hourly): 18126 -> 16391 rows kept (90.43%)\n",
      "Day 21 (Aggregated): 145008 -> 131128 rows kept (Shape consistent with 16391 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 22 (Hourly): 18126 -> 16382 rows kept (90.38%)\n",
      "Day 22 (Aggregated): 145008 -> 131056 rows kept (Shape consistent with 16382 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 23 (Hourly): 18126 -> 16362 rows kept (90.27%)\n",
      "Day 23 (Aggregated): 145008 -> 130896 rows kept (Shape consistent with 16362 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 24 (Hourly): 18126 -> 16338 rows kept (90.14%)\n",
      "Day 24 (Aggregated): 145008 -> 130704 rows kept (Shape consistent with 16338 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 25 (Hourly): 18126 -> 16349 rows kept (90.20%)\n",
      "Day 25 (Aggregated): 145008 -> 130792 rows kept (Shape consistent with 16349 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 26 (Hourly): 18126 -> 16361 rows kept (90.26%)\n",
      "Day 26 (Aggregated): 145008 -> 130888 rows kept (Shape consistent with 16361 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 27 (Hourly): 18126 -> 16372 rows kept (90.32%)\n",
      "Day 27 (Aggregated): 145008 -> 130976 rows kept (Shape consistent with 16372 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 28 (Hourly): 18126 -> 16385 rows kept (90.40%)\n",
      "Day 28 (Aggregated): 145008 -> 131080 rows kept (Shape consistent with 16385 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 29 (Hourly): 18126 -> 16397 rows kept (90.46%)\n",
      "Day 29 (Aggregated): 145008 -> 131176 rows kept (Shape consistent with 16397 filtered hourly rows).\n",
      "------------------------------\n",
      "Day 30 (Hourly): 18126 -> 16367 rows kept (90.30%)\n",
      "Day 30 (Aggregated): 145008 -> 130936 rows kept (Shape consistent with 16367 filtered hourly rows).\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "Filtering Complete.\n",
      "Filtered 31 days of hourly data.\n",
      "Filtered 31 days of aggregated data.\n"
     ]
    }
   ],
   "source": [
    "lat_d = 0.9\n",
    "lon_d = 0.4\n",
    "\n",
    "daily_hourly_filtered_maps, daily_aggregated_filtered_maps = exact_location_filter.filter_by_location_deviation(daily_hourly_maps, daily_hourly_maps2, daily_aggregated_tensors, daily_aggregated_tensors2, lat_d, lon_d)\n",
    "\n",
    "#ord_mm, nns_map = exact_location_filter.get_spatial_ordering(daily_hourly_filtered_maps[0], 10)\n",
    "#print(nns_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21a0b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0.5 # smooth\n",
    "mm_cond_number = 8\n",
    "nheads = 300\n",
    "#nheads = 1230\n",
    "#lr = 0.01\n",
    "#step = 80\n",
    "#gamma_par = 0.5\n",
    "\n",
    "# --- Placeholder Global Variables ---\n",
    "# ðŸ’¥ REVISED: Added lr, patience, factor. Removed step, gamma_par\n",
    "lr=0.1\n",
    "patience = 5       # Scheduler: Epochs to wait for improvement\n",
    "factor = 0.5         # Scheduler: Factor to reduce LR by (e.g., 0.5 = 50% cut)\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d345ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Day 2 (2024-07-2) ---\n",
      "Data size per day: 15181.0, smooth: 0.5\n",
      "mm_cond_number: 8,\n",
      "initial parameters: \n",
      "\n",
      "  Param 0: 4.2042\n",
      "  Param 1: 1.6348\n",
      "  Param 2: 0.4721\n",
      "  Param 3: -2.5562\n",
      "  Param 4: 0.0218\n",
      "  Param 5: -0.1689\n",
      "  Param 6: -1.3984\n",
      "--- Starting L-BFGS Optimization ---\n",
      "--- Step 1/10 / Loss: 1.490161 ---\n",
      "  Param 0: Value=3.3634, Grad=1.3591195052272828e-06\n",
      "  Param 1: Value=1.4000, Grad=4.953914267371197e-07\n",
      "  Param 2: Value=-1.5226, Grad=1.8429332092548725e-08\n",
      "  Param 3: Value=-3.1225, Grad=1.4741271117473528e-07\n",
      "  Param 4: Value=0.1083, Grad=1.400311751110981e-07\n",
      "  Param 5: Value=-0.2207, Grad=3.759825409581295e-06\n",
      "  Param 6: Value=1.1187, Grad=2.8386796741426228e-06\n",
      "  Max Abs Grad: 3.759825e-06\n",
      "------------------------------\n",
      "\n",
      "Converged on gradient norm (max|grad| < 1e-05) at step 1\n",
      "FINAL STATE: Step 1, Loss: 1.490160846159536\n",
      "  Raw (vecc) Parameters: [3.36336504695141, 1.4000489357246877, -1.5226391671330102, -3.122542129387705, 0.10834442729589913, -0.22068132726903175, 1.1186604833527585]\n",
      "  Interpretable Parameters:\n",
      "    sigma_sq  : 7.122908\n",
      "    range_lon : 0.246585\n",
      "    range_lat : 0.527963\n",
      "    range_time: 1.174946\n",
      "    advec_lat : 0.108344\n",
      "    advec_lon : -0.220681\n",
      "    nugget    : 3.060752\n",
      "Day 2 optimization finished in 3994.17s over 1 L-BFGS steps.\n",
      "Day 2 final results (raw params + loss): [3.36336504695141, 1.4000489357246877, -1.5226391671330102, -3.122542129387705, 0.10834442729589913, -0.22068132726903175, 1.1186604833527585, 1.490160846159536]\n"
     ]
    }
   ],
   "source": [
    "# --- L-BFGS SPECIFIC GLOBAL PARAMETERS ---\n",
    "LBFGS_LR = 1.0\n",
    "LBFGS_MAX_STEPS = 10       # Number of outer optimization steps\n",
    "LBFGS_HISTORY_SIZE = 100   # Memory for Hessian approximation\n",
    "LBFGS_MAX_EVAL = 50        # Max evaluations (line search) per step\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "day_indices = [1] # 0 index \n",
    "for day_idx in day_indices:  \n",
    "\n",
    "    # Assuming data access is correct\n",
    "    daily_hourly_map = daily_hourly_filtered_maps[day_idx]\n",
    "    daily_aggregated_tensor = daily_aggregated_filtered_maps[day_idx]\n",
    "\n",
    "    ord_mm, nns_map = exact_location_filter.get_spatial_ordering(daily_hourly_filtered_maps[0], mm_cond_number)\n",
    "\n",
    "    for map in daily_hourly_map:\n",
    "        daily_hourly_map[map] = daily_hourly_map[map][ord_mm]\n",
    "\n",
    "    # --- Parameter Initialization (SPATIO-TEMPORAL) ---\n",
    "    '''  \n",
    "    init_sigmasq   = 15.0\n",
    "    init_range_lat = 0.66 \n",
    "    init_range_lon = 0.7 \n",
    "    init_nugget    = 1.5\n",
    "    init_range_time = 0.1\n",
    "    init_advec_lat = 0.02\n",
    "    init_advec_lon = -0.08\n",
    "    '''\n",
    "    init_sigmasq   = 13.059\n",
    "    init_range_lat = 0.154 \n",
    "    init_range_lon = 0.195 \n",
    "    init_nugget    = 0.247\n",
    "    init_range_time = 0.7\n",
    "    init_advec_lat = 0.0218\n",
    "    init_advec_lon = -0.1689\n",
    "\n",
    "    \n",
    "    # Map model parameters to the 'phi' reparameterization\n",
    "    init_phi2 = 1.0 / init_range_lon                # 1/range_lon\n",
    "    init_phi1 = init_sigmasq * init_phi2            # sigmasq / range_lon\n",
    "    init_phi3 = (init_range_lon / init_range_lat)**2  # (range_lon / range_lat)^2\n",
    "    init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # 7-parameter spatio-temporal list (Log/Linear)\n",
    "    params_list = [\n",
    "        torch.tensor([np.log(init_phi1)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [0] log(phi1)\n",
    "        torch.tensor([np.log(init_phi2)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [1] log(phi2)\n",
    "        torch.tensor([np.log(init_phi3)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [2] log(phi3)\n",
    "        torch.tensor([np.log(init_phi4)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [3] log(phi4)\n",
    "        torch.tensor([init_advec_lat],         requires_grad=True, dtype=torch.float64, device=device_str ), # [4] advec_lat (linear)\n",
    "        torch.tensor([init_advec_lon],         requires_grad=True, dtype=torch.float64, device=device_str ), # [5] advec_lon (linear)\n",
    "        torch.tensor([np.log(init_nugget)],    requires_grad=True, dtype=torch.float64, device=device_str )  # [6] log(nugget)\n",
    "    ]\n",
    "\n",
    "    # --- Define parameter groups ---\n",
    "    lr_all = LBFGS_LR\n",
    "    all_indices = [0, 1, 2, 3, 4, 5, 6] \n",
    "    \n",
    "    # L-BFGS requires the parameters to be iterable in a single list or group\n",
    "    param_groups = [\n",
    "        {'params': [params_list[idx] for idx in all_indices], 'lr': lr_all, 'name': 'all_params'}\n",
    "    ]\n",
    "\n",
    "    # --- Print Job Info (using placeholder print variables) ---\n",
    "    res_calc = (113 // lat_lon_resolution[0]) * (158 // lat_lon_resolution[0]) \n",
    "    print(f'\\n--- Starting Day {day_idx+1} (2024-07-{day_idx+1}) ---')\n",
    "    print(f'Data size per day: { daily_aggregated_tensor.shape[0]/8}, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n')\n",
    "    for i, p in enumerate(params_list):\n",
    "        print(f\"  Param {i}: {p.item():.4f}\")\n",
    "            \n",
    "    # --- ðŸ’¥ Instantiate the L-BFGS Class ---\n",
    "    # NOTE: Assuming fit_vecchia_lbfgs is available via kernels_reparam_space_time\n",
    "    model_instance = kernels_reparam_space_time.fit_vecchia_lbfgs(\n",
    "            smooth = v,\n",
    "            input_map = daily_hourly_map,\n",
    "            aggregated_data = daily_aggregated_tensor,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- ðŸ’¥ Set L-BFGS Optimizer ---\n",
    "    # L-BFGS specific arguments are passed here\n",
    "    optimizer = model_instance.set_optimizer(\n",
    "            param_groups,     \n",
    "            lr=LBFGS_LR,            \n",
    "            max_iter=LBFGS_MAX_EVAL,        # max_iter in LBFGS is the line search limit\n",
    "            history_size=LBFGS_HISTORY_SIZE \n",
    "        )\n",
    "\n",
    "    # --- ðŸ’¥ Call the L-BFGS Fit Method ---\n",
    "    out, steps_ran = model_instance.fit_vecc_lbfgs(\n",
    "            params_list,\n",
    "            optimizer,\n",
    "            model_instance.matern_cov_aniso_STABLE_log_reparam, \n",
    "            max_steps=LBFGS_MAX_STEPS # Outer loop steps\n",
    "        )\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Day {day_idx+1} optimization finished in {epoch_time:.2f}s over {steps_ran+1} L-BFGS steps.\")\n",
    "    print(f\"Day {day_idx+1} final results (raw params + loss): {out}\")\n",
    "# day 0 61m "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a9ea6a",
   "metadata": {},
   "source": [
    "full nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a4dc08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   -1    -1    -1 ...    -1    -1    -1]\n",
      " [    0    -1    -1 ...    -1    -1    -1]\n",
      " [    0     1    -1 ...    -1    -1    -1]\n",
      " ...\n",
      " [ 2309   129  3182 ...  6499 12043  3169]\n",
      " [ 2267   947  3193 ... 11176 11662  2524]\n",
      " [  122  2445  3181 ... 11779 11978 12045]]\n"
     ]
    }
   ],
   "source": [
    "ord_mm, nns_map = exact_location_filter.get_spatial_ordering(daily_hourly_filtered_maps[0], 10)\n",
    "print(nns_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "135d6d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145008\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "nn = daily_aggregated_tensors[0].shape[0]\n",
    "print(nn)\n",
    "day_idx = 0\n",
    "lat_range= [1,3]\n",
    "lon_range= [125.0, 129.0]\n",
    "\n",
    "day2_va = [4.263671207178608, 1.6401967914088247, 0.25443837667152014, -4.020499340539446, 0.026886942200999114, -0.16593421415829276, -0.6337447467203996]\n",
    "day2_vl = [4.363671207178608, 1.6401967914088247, 0.25443837667152014, -4.020499340539446, 0.026886942200999114, -0.16593421415829276, -0.6337447467203996]\n",
    "day2_dwl = [4.163671207178608, 1.6401967914088247, 0.25443837667152014, -4.020499340539446, 0.026886942200999114, -0.16593421415829276, -0.6337447467203996]\n",
    "\n",
    "day = [ day2_va, day2_vl, day2_dwl]\n",
    "\n",
    "for i,model_params in enumerate(day):\n",
    "    instance = debiased_whittle.full_vecc_dw_likelihoods(daily_aggregated_tensors, daily_hourly_maps, day_idx=day_idx, params_list=model_params, lat_range=lat_range, lon_range=lon_range)\n",
    "    v = 0.5\n",
    "    nheads = 300\n",
    "    instance.initiate_model_instance_vecchia(v, nns_map, mm_cond_number, nheads)\n",
    "    res = instance.likelihood_wrapper()\n",
    "    print(f' full likelihood: {torch.round(res[0]*nn, decimals=2)},\\n vecchia: {torch.round(res[1]*nn, decimals=2)}, \\n whittle de-biased: {torch.round(res[2], decimals = 2)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
