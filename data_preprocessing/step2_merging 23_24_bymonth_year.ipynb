{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages and set the directory to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work with jl2815 environment\n",
    "import xarray as xr # for netCDF4 \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "# Ignore warnings due to duplicated dimension names\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"xarray\")\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "from GEMS_TCO import configuration as config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2024 01: hours: 00 to 05   N3035_E100110\n",
    "2023 04: hours: 00 to 07   N3035_E100110\n",
    "2023 01: hours: 00 to 05   N3035_E100110\n",
    "\n",
    "2023: y23m04day20_8 and y23m07day13_8 missing !! for N3035_E100110\n",
    "2023: y23m04day20_8 !! for N3035_E110120\n",
    "\n",
    "\n",
    "2023 04: hours: 00 to 07   N3035_E110120           \n",
    "2023 07: hours: 00 to 07   N3035_E110120\n",
    "2024 04: hours: 00 to 07   N3035_E110120\n",
    "2024 07: hours: 00 to 07   N3035_E110120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Groupd in the NetCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groups in the NetCDF file:\n",
      "Data Fields\n",
      "Geolocation Fields\n",
      "METADATA\n",
      "\n",
      "Variables in group 'Data Fields':\n",
      "AveragingKernel\n",
      "CloudPressure\n",
      "ColumnAmountO3\n",
      "DegreesOfFreedomForSignal\n",
      "EstimatedError\n",
      "FinalAlgorithmFlags\n",
      "LayerEfficiency\n",
      "Nvalue\n",
      "O3BelowCloud\n",
      "Reflectivity340\n",
      "Reflectivity380\n",
      "Residue\n",
      "StepOneO3\n",
      "StepTwoO3\n",
      "TerrainPressure\n",
      "dNdR\n",
      "dR_dl\n",
      "EffectiveCloudFraction\n",
      "\n",
      "Variables in group 'Geolocation Fields':\n",
      "Latitude\n",
      "Longitude\n",
      "RelativeAzimuthAngle\n",
      "SolarZenithAngle\n",
      "ViewingZenithAngle\n",
      "Time\n",
      "GroundPixelQualityFlags\n",
      "\n",
      "Variables in group 'METADATA':\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/xarray/namedarray/core.py:264: UserWarning: Duplicate dimension names present: dimensions {'nlayer2'} appear more than once in dims=('nlayer2', 'nlayer2', 'spatial', 'image'). We do not yet support duplicate dimension names, but we do allow initial construction of the object. We recommend you rename the dims immediately to become distinct, as most xarray functionality is likely to fail silently if you do not. To rename the dimensions you will need to set the ``.dims`` attribute of each variable, ``e.g. var.dims=('x0', 'x1')``.\n",
      "  self._dims = self._parse_dimensions(dims)\n",
      "/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/xarray/namedarray/core.py:264: UserWarning: Duplicate dimension names present: dimensions {'nlayer2'} appear more than once in dims=('nlayer2', 'nlayer2', 'spatial', 'image'). We do not yet support duplicate dimension names, but we do allow initial construction of the object. We recommend you rename the dims immediately to become distinct, as most xarray functionality is likely to fail silently if you do not. To rename the dimensions you will need to set the ``.dims`` attribute of each variable, ``e.g. var.dims=('x0', 'x1')``.\n",
      "  self._dims = self._parse_dimensions(dims)\n"
     ]
    }
   ],
   "source": [
    "# file_path = \"/Volumes/Backup Plus/GEMS_UNZIPPED/2024070131/20240701_0045.nc\"  # portable device\n",
    "file_path = \"/Users/joonwonlee/Documents/GEMS_DATA/2024070131/20240701_0045.nc\" # mac\n",
    "location = xr.open_dataset(file_path, group='Geolocation Fields')\n",
    "Z = xr.open_dataset(file_path, group='Data Fields')\n",
    "\n",
    "\n",
    "import netCDF4 as nc\n",
    "\n",
    "dataset = nc.Dataset(file_path)\n",
    "\n",
    "# Print all group names\n",
    "print(\"Groups in the NetCDF file:\")\n",
    "for group_name in dataset.groups:\n",
    "    print(group_name)\n",
    "\n",
    "# Optionally, print variables in each group\n",
    "for group_name, group in dataset.groups.items():\n",
    "    print(f\"\\nVariables in group '{group_name}':\")\n",
    "    for var_name in group.variables:\n",
    "        print(var_name)\n",
    "\n",
    "# Close the dataset\n",
    "dataset.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemsORITocsvHour:          \n",
    "    def __init__(self, lat_s,lat_e,lon_s,lon_e):\n",
    "        self.lat_s = lat_s \n",
    "        self.lat_e = lat_e  \n",
    "        self.lon_s = lon_s\n",
    "        self.lon_e = lon_e                         \n",
    "  \n",
    "    def extract_data(self,file_path):\n",
    "        location = xr.open_dataset(file_path, group='Geolocation Fields')\n",
    "        Z = xr.open_dataset(file_path, group='Data Fields')\n",
    "        \n",
    "        location_variables = ['Latitude', 'Longitude', 'Time']\n",
    "        tmp1 = location[location_variables]\n",
    "\n",
    "        location_df = tmp1.to_dataframe().reset_index() # Convert xarray.Dataset to pandas DataFrame\n",
    "        location_df = location_df[location_variables]   # remove spatial (2048), image (695) indices\n",
    "\n",
    "        Z_variables = ['ColumnAmountO3','FinalAlgorithmFlags']\n",
    "        tmp2 = Z[Z_variables]\n",
    "\n",
    "        Z_df = tmp2.to_dataframe().reset_index()      \n",
    "        Z_df = Z_df[Z_variables]\n",
    "\n",
    "        mydata = pd.concat([location_df, Z_df], axis=1)      # both rows are 2048*695\n",
    "  \n",
    "        # Close the NetCDF file\n",
    "        location.close()\n",
    "        Z.close()\n",
    "        return mydata\n",
    "    \n",
    "    def dropna(self, file_path):\n",
    "        mydata = self.extract_data(file_path)\n",
    "        mydata = mydata.dropna(subset=['Latitude', 'Longitude','Time','ColumnAmountO3','FinalAlgorithmFlags'])\n",
    "        return mydata\n",
    "\n",
    "    def ORItocsv(self, file_path):\n",
    "\n",
    "        df = self.dropna(file_path)  \n",
    "        truncated_df = df[ (df['Latitude']<= self.lat_e) & (df['Latitude']>= self.lat_s) & (df['Longitude']>= self.lon_s) & (df['Longitude']<= self.lon_e) ]\n",
    "        \n",
    "        # Cut off missing values\n",
    "        truncated_df= truncated_df[truncated_df.iloc[:,3]<1000]    \n",
    "\n",
    "        truncated_df['Time'] = np.mean(truncated_df.iloc[:,2])\n",
    "\n",
    "        # Convert 'Time' column to datetime type\n",
    "        # print(df2['Time'])\n",
    "\n",
    "        truncated_df['Time'] = pd.to_datetime(truncated_df['Time'], unit='h')\n",
    "        truncated_df['Time'] = truncated_df['Time'].dt.floor('min')  \n",
    "        \n",
    "        return truncated_df\n",
    "\n",
    "class file_path_list:\n",
    "    def __init__(self, year:int, month:int, computer_path:str):\n",
    "        self.year = year\n",
    "        self.month = month\n",
    "        self.computer_path = computer_path\n",
    "\n",
    "    def file_names_july24(self):\n",
    "        if self.month == 2:\n",
    "            self.day_str = \"0128\"  # Handle February specifically\n",
    "        else:\n",
    "            self.day_str = \"0131\" if (self.month in [1, 3, 5, 7, 8, 10, 12]) else \"0130\"\n",
    "\n",
    "        last_day_range = int(self.day_str[2:])+1\n",
    "        base_directory = f'{self.year}{self.month:02d}{self.day_str}/'\n",
    "        file_prefixes = []\n",
    "        for i in range(1,last_day_range):\n",
    "            file_prefixes.append(f'{self.year}{self.month:02d}{i:02d}_')\n",
    "        \n",
    "        file_paths_list = [f\"{self.computer_path}{base_directory}{prefix}{hour:02d}45.nc\" for prefix in file_prefixes for hour in range(0, 8)] # 6 for january 8 for else\n",
    "        return file_paths_list\n",
    "    \n",
    "class MonthAggregatedCSV(GemsORITocsvHour):\n",
    "    def __init__(self, lat_start, lat_end, lon_start, lon_end):\n",
    "        super().__init__(lat_start, lat_end, lon_start, lon_end)\n",
    "\n",
    "    def aggregate_july24tocsv(self, file_paths_list):\n",
    "        aggregated_data = []\n",
    "        for i, filepath in enumerate(file_paths_list):\n",
    "            try:\n",
    "                # Attempt to transform netCDF file into csv for hourly data\n",
    "                cur_data = self.ORItocsv(filepath)\n",
    "                aggregated_data.append(cur_data)\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: File not found - {filepath}. Skipping this file.\")\n",
    "                continue\n",
    "\n",
    "        # Concatenate all DataFrames at once (more efficient than repeated concat)\n",
    "        aggregated_df =  pd.concat(aggregated_data, ignore_index=True) if aggregated_data else pd.DataFrame()\n",
    "\n",
    "\n",
    "        aggregated_df['Hours_elapsed'] = aggregated_df['Time'].astype('int64') // 10**9/3600\n",
    "        GoodQualityData = aggregated_df[aggregated_df['FinalAlgorithmFlags']<=2]\n",
    "        # frequency_table3= gqdata['FinalAlgorithmFlags'].value_counts()\n",
    "        # print(frequency_table3)\n",
    "        return GoodQualityData\n",
    "    \n",
    "    def save(self, GoodqualityData, year,month, computer_path):\n",
    "        # computer_path = config.mac.data_path\n",
    "        if month == 2:\n",
    "            day_str = \"0128\"  # Handle February specifically\n",
    "        else:\n",
    "            day_str = \"0131\" if (month in [1, 3, 5, 7, 8, 10, 12]) else \"0130\"\n",
    "\n",
    "        output_file = f'data_{int(str(year)[2:4])}_{month:02d}_{day_str}_N{str(self.lat_s)+str(self.lat_e)}_E{str(self.lon_s)+str(self.lon_e)}.csv' \n",
    "        output_csv_path = Path(computer_path)/output_file\n",
    "        \n",
    "        # csv_file_path = os.path.join(r\"C:\\\\Users\\\\joonw\\tco\\\\data_engineering\", tmp_path)\n",
    "        GoodqualityData.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computer_path = config.mac_data_load_path  # mac\n",
    "# portable_disk = \"/Volumes/Backup Plus/GEMS_UNZIPPED/\" \n",
    "year=2024\n",
    "month = 7\n",
    "filelist_instance = file_path_list(year, month, computer_path)\n",
    "# filelist_instance = file_path_list(year, month, portable_disk)\n",
    "file_paths_list = filelist_instance.file_names_july24() \n",
    "\n",
    "instance= MonthAggregatedCSV(4.9, 10.1, 109.9, 124.1)\n",
    "GoodQualityData = instance.aggregate_july24tocsv(file_paths_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance.save(GoodQualityData, 2024, 7, computer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create csv file by uing loop for days given month and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "data_23_09_0130_N510_E110120.csv\n"
     ]
    }
   ],
   "source": [
    "year = 2024\n",
    "portable_disk = \"/Volumes/Backup Plus/GEMS_UNZIPPED/\" \n",
    "for month in range(7, 8):  # From January to November\n",
    "    # filelist_instance = file_path_list(year, month, computer_path)\n",
    "    filelist_instance = file_path_list(year, month, portable_disk)\n",
    "    file_paths_list = filelist_instance.file_names_july24() \n",
    "\n",
    "    instance= MonthAggregatedCSV(4.9, 10.1, 109.9, 124.1)\n",
    "    GoodQualityData = instance.aggregate_july24tocsv(file_paths_list)\n",
    "    instance.save(GoodQualityData, year, month, computer_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You may ignore belows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# different library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = \"/Users/joonwonlee/Documents/GEMS_DATA/2024070131/20240701_0045.nc\"\n",
    "\n",
    "# Open the NetCDF file\n",
    "dataset = nc.Dataset(file_path)\n",
    "\n",
    "# Access specific groups\n",
    "geolocation_group = dataset.groups['Geolocation Fields']\n",
    "data_group = dataset.groups['Data Fields']\n",
    "meta_group = dataset.groups['METADATA']\n",
    "\n",
    "# Extract variables from the 'Geolocation Fields' group\n",
    "latitude = geolocation_group.variables['Latitude'][:]\n",
    "longitude = geolocation_group.variables['Longitude'][:]\n",
    "time = geolocation_group.variables['Time'][:]\n",
    "\n",
    "# Ensure time has the same shape as latitude and longitude\n",
    "if time.shape != latitude.shape:\n",
    "    time = np.broadcast_to(time, latitude.shape)\n",
    "\n",
    "# Extract variables from the 'Data Fields' group\n",
    "column_amount_o3 = data_group.variables['ColumnAmountO3'][:]\n",
    "final_algorithm_flags = data_group.variables['FinalAlgorithmFlags'][:]\n",
    "\n",
    "# Create DataFrames\n",
    "location_df = pd.DataFrame({\n",
    "    'Latitude': latitude.flatten(),  # Flattening the arrays to 1D\n",
    "    'Longitude': longitude.flatten(),\n",
    "    'Time': time.flatten()\n",
    "})\n",
    "\n",
    "data_df = pd.DataFrame({\n",
    "    'ColumnAmountO3': column_amount_o3.flatten(),\n",
    "    'FinalAlgorithmFlags': final_algorithm_flags.flatten()\n",
    "})\n",
    "\n",
    "# Combine DataFrames\n",
    "mydata = pd.concat([location_df, data_df], axis=1)\n",
    "\n",
    "# Close the NetCDF file\n",
    "dataset.close()\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(mydata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
