{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages and set the directory to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work with jl2815 environment\n",
    "import xarray as xr # for netCDF4 \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\joonw\\TCO\\newpipeline\n",
      "Updated working directory: D:\\GEMS_UNZIPPED\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(\"Current working directory:\", current_directory)\n",
    "new_directory = \"D:\\\\GEMS_UNZIPPED\"\n",
    "os.chdir(new_directory)\n",
    "updated_directory = os.getcwd()\n",
    "print(\"Updated working directory:\", updated_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2024 01: hours: 00 to 05   N3035_E100110\n",
    "2023 04: hours: 00 to 07   N3035_E100110\n",
    "2023 01: hours: 00 to 05   N3035_E100110\n",
    "\n",
    "2023: y23m04day20_8 and y23m07day13_8 missing !! for N3035_E100110\n",
    "2023: y23m04day20_8 !! for N3035_E110120\n",
    "\n",
    "\n",
    "2023 04: hours: 00 to 07   N3035_E110120           \n",
    "2023 07: hours: 00 to 07   N3035_E110120\n",
    "2024 04: hours: 00 to 07   N3035_E110120\n",
    "2024 07: hours: 00 to 07   N3035_E110120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract the region from lat_s,lat_e,lon_s,lon_e = 5, 10, 110, 120."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gems_loader_2023:          \n",
    "    def __init__(self, file_path,lat_s,lat_e,lon_s,lon_e):\n",
    "        self.file_path = file_path       \n",
    "        self.lat_s = lat_s \n",
    "        self.lat_e = lat_e  \n",
    "        self.lon_s = lon_s\n",
    "        self.lon_e = lon_e                         \n",
    "  \n",
    "    def extract_data(self,file_path):\n",
    "        location = xr.open_dataset(file_path, group='Geolocation Fields')\n",
    "        Z = xr.open_dataset(file_path, group='Data Fields')\n",
    "        \n",
    "        location_variables = ['Latitude', 'Longitude', 'Time']\n",
    "        tmp1 = location[location_variables]\n",
    "\n",
    "        # Convert xarray.Dataset to pandas DataFrame\n",
    "        location_df = tmp1.to_dataframe().reset_index()\n",
    "        location_df = location_df[location_variables]\n",
    "\n",
    "        Z_variables = ['ColumnAmountO3','FinalAlgorithmFlags']\n",
    "        tmp2 = Z[Z_variables]\n",
    "\n",
    "        Z_df = tmp2.to_dataframe().reset_index()\n",
    "        Z_df = Z_df[Z_variables]\n",
    "\n",
    "        mydata = pd.concat([location_df, Z_df], axis=1)\n",
    "        mydata = mydata[ location_variables + Z_variables ]\n",
    "        \n",
    "        # Close the NetCDF file\n",
    "        location.close()\n",
    "        Z.close()\n",
    "        return mydata\n",
    "    \n",
    "    def dropna(self):\n",
    "        mydata = self.extract_data(self.file_path)\n",
    "        mydata = mydata.dropna(subset=['Latitude', 'Longitude','Time','ColumnAmountO3','FinalAlgorithmFlags'])\n",
    "        \n",
    "        return mydata\n",
    "\n",
    "    def result(self):\n",
    "        # self.mydata =  self.extract_data(self.file_path)\n",
    "\n",
    "        df = self.dropna()\n",
    "  \n",
    "        df2 = df[ (df['Latitude']<= self.lat_e) & (df['Latitude']>= self.lat_s) & (df['Longitude']>= self.lon_s) & (df['Longitude']<= self.lon_e) ]\n",
    "        \n",
    "        df2 = df2[df2.iloc[:,3]<1000]    # Cut off missing values\n",
    "\n",
    "        df2['Time'] = np.mean(df2.iloc[:,2])\n",
    "\n",
    "        # Convert 'Time' column to datetime type\n",
    "        # print(df2['Time'])\n",
    "\n",
    "        df2['Time'] = pd.to_datetime(df2['Time'], unit='h')\n",
    "        df2['Time'] = df2['Time'].dt.floor('min')  \n",
    "        \n",
    "        return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create csv file by uing loop for days given month and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "data_23_09_0130_N510_E110120.csv\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings due to duplicated dimension names\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"xarray\")\n",
    "\n",
    "\n",
    "year = 2023\n",
    "for month in range(9, 10):  # From January to November\n",
    "\n",
    "\n",
    "    if month == 2:\n",
    "        day_str = \"0128\"  # Handle February specifically\n",
    "    else:\n",
    "        day_str = \"0131\" if (month in [1, 3, 5, 7, 8, 10, 12]) else \"0130\"\n",
    "\n",
    "    last_day_range = int(day_str[2:])+1\n",
    "    \n",
    "    def makefilenames(year,month): #year 2024 month 7 integer\n",
    "        base_directory = f'{year}{month:02d}{day_str}/'\n",
    "        \n",
    "        file_prefixes = []\n",
    "        for i in range(1,last_day_range):\n",
    "            file_prefixes.append(f'{year}{month:02d}{i:02d}_')\n",
    "        \n",
    "        filenames = [f\"{base_directory}{prefix}{hour:02d}45.nc\" for prefix in file_prefixes for hour in range(0, 8)] # 6 for january 8 for else\n",
    "\n",
    "        return filenames\n",
    "    filenames = makefilenames(year,month)\n",
    "\n",
    "\n",
    "    lat_s, lat_e, lon_s, lon_e = 5, 10, 110, 120\n",
    "\n",
    "    # Initialize an empty DataFrame to store all data\n",
    "    data = pd.DataFrame()\n",
    "    for i, filename in enumerate(filenames):\n",
    "        try:\n",
    "            # Attempt to load the data\n",
    "            my_loader = gems_loader_2023(filename, lat_s, lat_e, lon_s, lon_e)\n",
    "            cur_data = my_loader.result()\n",
    "\n",
    "            # Append the data to the main DataFrame\n",
    "            if data.empty:\n",
    "                data = cur_data\n",
    "            else:\n",
    "                data = pd.concat([data, cur_data], ignore_index=True)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            # Log a warning for the missing file and continue\n",
    "            print(f\"Warning: File not found - {filename}. Skipping this file.\")\n",
    "            continue\n",
    "\n",
    "    data['Hours_elapsed'] = data['Time'].astype('int64') // 10**9/3600\n",
    "    gqdata = data[data['FinalAlgorithmFlags']<=2]\n",
    "    # frequency_table3= gqdata['FinalAlgorithmFlags'].value_counts()\n",
    "    # print(frequency_table3)\n",
    "\n",
    "    tmp_path = f'data_{int(str(year)[2:4])}_{month:02d}_{day_str}_N{str(lat_s)+str(lat_e)}_E{str(lon_s)+str(lon_e)}.csv' \n",
    "    print(tmp_path)\n",
    "    csv_file_path = os.path.join(r\"C:\\\\Users\\\\joonw\\tco\\\\data_engineering\", tmp_path)\n",
    "    gqdata.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You may ignore belows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Belows are previous codes, which can be used for debugging errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change year, month and hour range\n",
    "year=2024\n",
    "month=12\n",
    "days = '0131'\n",
    "last_day_range = int(days[2:])+1\n",
    "print(last_day_range)\n",
    "def makefilenames(year,month): #year 2024 month 7 integer\n",
    "    base_directory = f'{year}{month:02d}{days}/'\n",
    "\n",
    "    file_prefixes = []\n",
    "    for i in range(1,last_day_range):\n",
    "        file_prefixes.append(f'{year}{month:02d}{i:02d}_')\n",
    "    \n",
    "    filenames = [f\"{base_directory}{prefix}{hour:02d}45.nc\" for prefix in file_prefixes for hour in range(0, 8)] # 6 for january 8 for else\n",
    "\n",
    "    return filenames\n",
    "filenames = makefilenames(year,month)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_s, lat_e, lon_s, lon_e = 5, 10, 110, 120\n",
    "\n",
    "# Initialize an empty DataFrame to store all data\n",
    "data = pd.DataFrame()\n",
    "for i, filename in enumerate(filenames):\n",
    "    try:\n",
    "        # Attempt to load the data\n",
    "        my_loader = gems_loader_2023(filename, lat_s, lat_e, lon_s, lon_e)\n",
    "        cur_data = my_loader.result()\n",
    "\n",
    "        # Append the data to the main DataFrame\n",
    "        if data.empty:\n",
    "            data = cur_data\n",
    "        else:\n",
    "            data = pd.concat([data, cur_data], ignore_index=True)\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        # Log a warning for the missing file and continue\n",
    "        print(f\"Warning: File not found - {filename}. Skipping this file.\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data quality control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Hours_elapsed'] = data['Time'].astype('int64') // 10**9/3600\n",
    "frequency_table = data['FinalAlgorithmFlags'].value_counts()\n",
    "print(frequency_table)\n",
    "\n",
    "# 0's are best but consider 2^1 + 2^2 + 2^7\n",
    "# OVER 2^1+2^2+2^7 = 134 is out.\n",
    "data = data[data['FinalAlgorithmFlags']<=134]\n",
    "\n",
    "frequency_table2 = data['FinalAlgorithmFlags'].value_counts()\n",
    "print(frequency_table2)\n",
    "\n",
    "gqdata = data[data['FinalAlgorithmFlags']!=3]\n",
    "frequency_table3= gqdata['FinalAlgorithmFlags'].value_counts()\n",
    "print(frequency_table3)\n",
    "\n",
    "gqdata = data[data['FinalAlgorithmFlags']<=2]\n",
    "frequency_table3= gqdata['FinalAlgorithmFlags'].value_counts()\n",
    "print(frequency_table3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verfiy the region (N05-N10 E110-E120) I chosed is consistently available over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GEMS_TCO import orbitmap\n",
    "# from GEMS_TCO import smoothspace\n",
    "from GEMS_TCO.smoothspace import space_average\n",
    "\n",
    "df = gqdata\n",
    "df['Time'] = df['Time'].astype(str)\n",
    "resolution = 0.4 \n",
    "\n",
    "instance = orbitmap.MakeOrbitdata(df,lat_s,lat_e,lon_s,lon_e,resolution,resolution)   # lat_s,lat_e, lon_s, lon_e\n",
    "orbit_map24_7 = instance.makeorbitmap()\n",
    "\n",
    "len(sorted(orbit_map24_7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df['Orbit'].unique() \n",
    "\n",
    "Shows that for data from 2024/07/01 to 2024/07/09 missing 8th orbit so we have\n",
    " \n",
    "240-9 = 231 orbits in total. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path = f'data_{int(str(year)[2:4])}_{month:02d}_{days}_N{str(lat_s)+str(lat_e)}_E{str(lon_s)+str(lon_e)}.csv'\n",
    "csv_file_path = os.path.join(r\"C:\\\\Users\\\\joonw\\tco\\\\data_engineering\", tmp_path)\n",
    "# Save the DataFrame to a CSV file in the specified directory\n",
    "gqdata.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if there is an error in opening a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joonw\\anaconda3\\envs\\jl2815\\Lib\\site-packages\\xarray\\namedarray\\core.py:514: UserWarning: Duplicate dimension names present: dimensions {'nlayer2'} appear more than once in dims=('nlayer2', 'nlayer2', 'spatial', 'image'). We do not yet support duplicate dimension names, but we do allow initial construction of the object. We recommend you rename the dims immediately to become distinct, as most xarray functionality is likely to fail silently if you do not. To rename the dimensions you will need to set the ``.dims`` attribute of each variable, ``e.g. var.dims=('x0', 'x1')``.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "file_path = 'D:\\\\GEMS_UNZIPPED\\\\2023080131\\\\20230818_0745.nc'\n",
    "location = xr.open_dataset(file_path, group='Geolocation Fields')\n",
    "Z = xr.open_dataset(file_path, group='Data Fields')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jl2815",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
