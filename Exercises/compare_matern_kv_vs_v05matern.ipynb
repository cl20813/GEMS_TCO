{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     longitude   latitude        time\n",
      "0   -41.042413  72.311482  128.066920\n",
      "1  -118.658691 -33.886564  123.977527\n",
      "2   -67.244260 -43.615308  107.831699\n",
      "3   -22.693072  32.253074  150.683464\n",
      "4   -54.507869  51.103011  135.731688\n",
      "..         ...        ...         ...\n",
      "95  -69.500039 -80.200911  164.189459\n",
      "96   89.977123  41.736301  151.724857\n",
      "97   97.012391  17.828390  171.285136\n",
      "98  162.892887  53.241542  178.931654\n",
      "99  117.575736  -2.605172  102.154213\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Special functions and optimizations\n",
    "from typing import Callable, Union, Tuple\n",
    "from scipy.spatial.distance import cdist  # For space and time distance\n",
    "from scipy.special import gamma, kv  # Bessel function and gamma function\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import basinhopping, minimize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a DataFrame with 3 columns: longitude, latitude, and time\n",
    "data = {\n",
    "    'longitude': np.random.uniform(-180, 180, 100),\n",
    "    'latitude': np.random.uniform(-90, 90, 100),\n",
    "    'time': np.random.uniform(90, 180, 100)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "df = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.01000000e+01 3.33031983e-19 1.95198265e-16 ... 9.36410262e-23\n",
      "  9.35396181e-30 5.38999394e-28]\n",
      " [3.33031983e-19 2.01000000e+01 3.26513188e-09 ... 6.07038134e-31\n",
      "  3.08349456e-41 3.25875428e-38]\n",
      " [1.95198265e-16 3.26513188e-09 2.01000000e+01 ... 2.02080667e-24\n",
      "  2.38265321e-34 4.84064237e-29]\n",
      " ...\n",
      " [9.36410262e-23 6.07038134e-31 2.02080667e-24 ... 2.01000000e+01\n",
      "  3.02474639e-10 8.53576657e-17]\n",
      " [9.35396181e-30 3.08349456e-41 2.38265321e-34 ... 3.02474639e-10\n",
      "  2.01000000e+01 2.40667027e-16]\n",
      " [5.38999394e-28 3.25875428e-38 4.84064237e-29 ... 8.53576657e-17\n",
      "  2.40667027e-16 2.01000000e+01]]\n"
     ]
    }
   ],
   "source": [
    "smooth = 0.5\n",
    "params = [20,8,8,0.5,0.5,0.1]\n",
    "\n",
    "range_lon, range_lat = params[1], params[2]\n",
    "sqrt_range_mat = np.diag([ 1/range_lon**0.5, 1/range_lat**0.5])\n",
    "sqrt_range_mat = sqrt_range_mat\n",
    "\n",
    "\n",
    "# Custom distance function for cdist\n",
    "def custom_distance(u, v):\n",
    "    d = np.dot(sqrt_range_mat, u[:2] - v[:2] ) # Distance between x1,x2 (2D)\n",
    "    spatial_diff = np.linalg.norm(d)  # Distance between x1,x2 (2D)\n",
    "    temporal_diff = np.abs(u[2] - v[2])           # Distance between y1 and y2\n",
    "    return np.sqrt(spatial_diff**2 + temporal_diff**2)\n",
    "\n",
    "\n",
    "def matern_cov_yx_test(params: Tuple[float,float,float,float,float,float], y: np.ndarray, x: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    sigmasq, range_lat, range_lon, advec, beta, nugget  = params\n",
    "    # Validate inputs\n",
    "    if y is None or x is None:\n",
    "        raise ValueError(\"Both y and x_df must be provided.\")\n",
    "    # Extract values\n",
    "    x1 = x[:, 0]\n",
    "    y1 = x[:, 1]\n",
    "    t1 = x[:, 2]\n",
    "\n",
    "    x2 = y[:, 0]\n",
    "    y2 = y[:, 1]\n",
    "    t2 = y[:, 2] # hour\n",
    "\n",
    "    spat_coord1 = np.stack((x1- advec*t1, y1 - advec*t1), axis=-1)\n",
    "    spat_coord2 = np.stack((x2- advec*t2, y2 - advec*t2), axis=-1)\n",
    "\n",
    "    coords1 = np.hstack ((spat_coord1, (beta * t1).reshape(-1,1) ))\n",
    "    coords2 = np.hstack ((spat_coord2, (beta * t2).reshape(-1,1) ))\n",
    "\n",
    "\n",
    "\n",
    "    distance = cdist(coords1,coords2, metric = custom_distance)\n",
    "\n",
    "    # Initialize the covariance matrix with zeros\n",
    "    out = distance\n",
    "    \n",
    "    # Compute the covariance for non-zero distances\n",
    "\n",
    "    # Compute the covariance for non-zero distances\n",
    "    non_zero_indices = distance != 0\n",
    "    if np.any(non_zero_indices):\n",
    "        out[non_zero_indices] = (sigmasq * (2**(1-smooth)) / gamma(smooth) *\n",
    "                                (distance[non_zero_indices])**smooth *\n",
    "                                kv(smooth, distance[non_zero_indices]))\n",
    "    out[~non_zero_indices] = sigmasq\n",
    "\n",
    "    # Add a small jitter term to the diagonal for numerical stability\n",
    "    out += np.eye(out.shape[0]) * nugget\n",
    "    return out\n",
    "\n",
    "\n",
    "b=matern_cov_yx_test(params,df,df)\n",
    "\n",
    "def matern2(params: Tuple[float,float,float,float,float,float], y: np.ndarray, x: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    sigmasq, range_lat, range_lon, advec, beta, nugget  = params\n",
    "    # Validate inputs\n",
    "    if y is None or x is None:\n",
    "        raise ValueError(\"Both y and x_df must be provided.\")\n",
    "    # Extract values\n",
    "    x1 = x[:, 0]\n",
    "    y1 = x[:, 1]\n",
    "    t1 = x[:, 2]\n",
    "\n",
    "    x2 = y[:, 0]\n",
    "    y2 = y[:, 1]\n",
    "    t2 = y[:, 2] # hour\n",
    "\n",
    "    spat_coord1 = np.stack((x1- advec*t1, y1 - advec*t1), axis=-1)\n",
    "    spat_coord2 = np.stack((x2- advec*t2, y2 - advec*t2), axis=-1)\n",
    "\n",
    "    coords1 = np.hstack ((spat_coord1, (beta * t1).reshape(-1,1) ))\n",
    "    coords2 = np.hstack ((spat_coord2, (beta * t2).reshape(-1,1) ))\n",
    "\n",
    "\n",
    "    distance = cdist(coords1,coords2, metric = custom_distance)\n",
    "\n",
    "    # Initialize the covariance matrix with zeros\n",
    "    out = distance\n",
    "    \n",
    "    # Compute the covariance for non-zero distances\n",
    "\n",
    "    # Compute the covariance for non-zero distances\n",
    "    non_zero_indices = distance != 0\n",
    "    if np.any(non_zero_indices):\n",
    "        out[non_zero_indices] = sigmasq* np.exp(-distance[non_zero_indices])\n",
    "    out[~non_zero_indices] = sigmasq\n",
    "    \n",
    "\n",
    "    # Add a small jitter term to the diagonal for numerical stability\n",
    "    out += np.eye(out.shape[0]) * nugget\n",
    "    return out\n",
    "a = matern2(params,df,df)\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-2.2237754044130276e-15)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18315638888734181\n",
      "0.1831563888873418\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def matern_cov(d,v):\n",
    "    abs_d = np.abs(d)\n",
    "    if abs_d ==0:\n",
    "        return 1\n",
    "    else:\n",
    "        sigmasq = 2\n",
    "        range = 1\n",
    "        out = sigmasq * (2**(1-v))/math.gamma(v) * (abs_d/range)**(v)*kv(v, abs_d/range)        \n",
    "        return out  \n",
    "    \n",
    "print(matern_cov(4,1.5))\n",
    "d = 4\n",
    "abs_d = np.abs(d)\n",
    "sigmasq = 2\n",
    "range_ = 1\n",
    "out = sigmasq * (1+ abs_d/range_)* np.exp(-abs_d/range_)\n",
    "print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vecchia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work environment: jl2815\n",
    "# Standard libraries\n",
    "import sys\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import concurrent\n",
    "from concurrent.futures import ThreadPoolExecutor  # Importing specific executor for clarity\n",
    "import time \n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Nearest neighbor search\n",
    "import sklearn\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# Special functions and optimizations\n",
    "from scipy.special import gamma, kv  # Bessel function and gamma function\n",
    "from scipy.stats import multivariate_normal  # Simulation\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import cdist  # For space and time distance\n",
    "from scipy.spatial import distance  # Find closest spatial point\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "# Plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Type hints\n",
    "from typing import Callable, Union, Tuple\n",
    "\n",
    "# Add your custom path\n",
    "sys.path.append(\"/cache/home/jl2815/tco\")\n",
    "\n",
    "# Custom imports\n",
    "from GEMS_TCO import orbitmap \n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import smoothspace\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'GEMS_TCO.kernels' has no attribute 'matern_spatio_temporal'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 68\u001b[0m\n\u001b[0;32m     64\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m tmp\u001b[38;5;241m.\u001b[39miloc[ord_mm, :\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m     65\u001b[0m     analysis_data_map[key_idx[i]] \u001b[38;5;241m=\u001b[39m tmp\n\u001b[1;32m---> 68\u001b[0m instance \u001b[38;5;241m=\u001b[39m kernels\u001b[38;5;241m.\u001b[39mmatern_spatio_temporal(smooth \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, input_map \u001b[38;5;241m=\u001b[39m analysis_data_map, nns_map \u001b[38;5;241m=\u001b[39m nns_map, mm_cond_number \u001b[38;5;241m=\u001b[39m mm_cond_number )\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# data = data.iloc[ord,:]\u001b[39;00m\n\u001b[0;32m     71\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'GEMS_TCO.kernels' has no attribute 'matern_spatio_temporal'"
     ]
    }
   ],
   "source": [
    "\n",
    "lat_lon_resolution = [20,20]\n",
    "key_for_dict = 4\n",
    "mm_cond_number=10\n",
    "# Load the one dictionary to set spaital coordinates\n",
    "filepath = \"C:/Users/joonw/TCO/data_engineering/data_2023/sparse_cen_map23_01.pkl\"\n",
    "\"\"\n",
    "with open(filepath, 'rb') as pickle_file:\n",
    "    coarse_dict_24_1 = pickle.load(pickle_file)\n",
    "\n",
    "sample_df = coarse_dict_24_1['y23m01day01_hm02:12']\n",
    "\n",
    "sample_key = coarse_dict_24_1.get('y23m01day01_hm02:12')\n",
    "if sample_key is None:\n",
    "    print(\"Key 'y23m01day01_hm02:12' not found in the dictionary.\")\n",
    "\n",
    "# { (20,20):(5,1), (5,5):(20,40) }\n",
    "rho_lat = lat_lon_resolution[0]          \n",
    "rho_lon = lat_lon_resolution[1]\n",
    "lat_n = sample_df['Latitude'].unique()[::rho_lat]\n",
    "lon_n = sample_df['Longitude'].unique()[::rho_lon]\n",
    "\n",
    "lat_number = len(lat_n)\n",
    "lon_number = len(lon_n)\n",
    "\n",
    "# Set spatial coordinates for each dataset\n",
    "coarse_dicts = {}\n",
    "\n",
    "years = ['2024']\n",
    "for year in years:\n",
    "    for month in range(7, 8):  # Iterate over all months\n",
    "        filepath = f\"C:/Users/joonw/TCO/data_engineering/data_{year}/sparse_cen_map{year[2:]}_{month:02d}.pkl\"\n",
    "        with open(filepath, 'rb') as pickle_file:\n",
    "            loaded_map = pickle.load(pickle_file)\n",
    "            for key in loaded_map:\n",
    "                tmp_df = loaded_map[key]\n",
    "                coarse_filter = (tmp_df['Latitude'].isin(lat_n)) & (tmp_df['Longitude'].isin(lon_n))\n",
    "                coarse_dicts[f\"{year}_{month:02d}_{key}\"] = tmp_df[coarse_filter].reset_index(drop=True)\n",
    "\n",
    "\n",
    "key_idx = sorted(coarse_dicts)\n",
    "if not key_idx:\n",
    "    raise ValueError(\"coarse_dicts is empty\")\n",
    "\n",
    "# extract first hour data because all data shares the same spatial grid\n",
    "data_for_coord = coarse_dicts[key_idx[0]]\n",
    "x1 = data_for_coord['Longitude'].values\n",
    "y1 = data_for_coord['Latitude'].values \n",
    "coords1 = np.stack((x1, y1), axis=-1)\n",
    "\n",
    "instance = orbitmap.MakeOrbitdata(df = data_for_coord, lat_s= 5, lat_e=10, lon_s=110, lon_e=120)\n",
    "s_dist = cdist(coords1, coords1, 'euclidean')\n",
    "ord_mm, _ = instance.maxmin_naive(s_dist, 0)\n",
    "\n",
    "data_for_coord = data_for_coord.iloc[ord_mm].reset_index(drop=True)\n",
    "coords1_reordered = np.stack((data_for_coord['Longitude'].values, data_for_coord['Latitude'].values), axis=-1)\n",
    "nns_map = instance.find_nns_naive(locs=coords1_reordered, dist_fun='euclidean', max_nn=mm_cond_number)\n",
    "\n",
    "\n",
    "\n",
    "analysis_data_map = {}\n",
    "for i in range(key_for_dict):\n",
    "    tmp = coarse_dicts[key_idx[i]]\n",
    "    # tmp = tmp.iloc[ord_mm].reset_index(drop=True)  \n",
    "    tmp = tmp.iloc[ord_mm, :4].to_numpy()\n",
    "    analysis_data_map[key_idx[i]] = tmp\n",
    "\n",
    "\n",
    "instance = kernels.matern_spatio_temporal(smooth =0.5, input_map = analysis_data_map, nns_map = nns_map, mm_cond_number = mm_cond_number )\n",
    "# data = data.iloc[ord,:]\n",
    "\n",
    "start_time = time.time()\n",
    "number_of_timestamps = key_for_dict\n",
    "input_map = analysis_data_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'key_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m neg_log_lik   \n\u001b[0;32m     84\u001b[0m params \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m5\u001b[39m ,\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m]\n\u001b[1;32m---> 85\u001b[0m vecchia_likelihood_test(params)\n",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m, in \u001b[0;36mvecchia_likelihood_test\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m      4\u001b[0m neg_log_lik \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m time_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(number_of_timestamps):\n\u001b[1;32m----> 7\u001b[0m     current_np \u001b[38;5;241m=\u001b[39m input_map[key_list[time_idx]]\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# cur_heads = current_df.iloc[:31,:]\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# neg_log_lik += full_likelihood(params,cur_heads, cur_heads[\"ColumnAmountO3\"])\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, size_per_hour):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'key_list' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def vecchia_likelihood_test( params: Tuple[float,float,float,float,float,float]):\n",
    "    neg_log_lik = 0\n",
    "\n",
    "    for time_idx in range(number_of_timestamps):\n",
    "        current_np = input_map[key_list[time_idx]]\n",
    "\n",
    "        # cur_heads = current_df.iloc[:31,:]\n",
    "        # neg_log_lik += full_likelihood(params,cur_heads, cur_heads[\"ColumnAmountO3\"])\n",
    "\n",
    "        for index in range(0, size_per_hour):\n",
    "\n",
    "            current_row = current_np[index]\n",
    "    \n",
    "            current_row = current_row.reshape(1,-1)\n",
    "            print(current_row.shape)\n",
    "            print(current_row[2])\n",
    "            current_y = current_row[2]\n",
    "\n",
    "            # construct conditioning set on time 0\n",
    "            \n",
    "            mm_neighbors = nns_map[index]\n",
    "            past = list(mm_neighbors)\n",
    "            data_list = []\n",
    "            if past:\n",
    "                data_list.append( current_np[past])\n",
    "        \n",
    "            if time_idx >0:\n",
    "                last_hour_np = input_map[key_list[time_idx-1]]\n",
    "                \n",
    "                past_conditioning_data = last_hour_np[ (past+[index]),: ]\n",
    "                data_list.append( past_conditioning_data)\n",
    "            \n",
    "            if data_list:\n",
    "                conditioning_data = np.vstack(data_list)\n",
    "            else:\n",
    "                conditioning_data = np.array([])\n",
    "    \n",
    "\n",
    "            y_xx = conditioning_data[:,2]\n",
    "            print(y_xx)\n",
    "            y_yy = current_y\n",
    "            # locs = np.array(df[['Latitude','Longitude']])\n",
    "            locs_xx = conditioning_data[:,:2]\n",
    "            print(locs_xx)\n",
    "            locs_yy = current_row[:2]\n",
    "\n",
    "            cov_xx = matern_cov_yx(params=params, y = conditioning_data, x = conditioning_data)\n",
    "            cov_yy = matern_cov_yx(params=params, y = current_row, x = current_row)\n",
    "\n",
    "            # get mean\n",
    "            \n",
    "            tmp_xx1 = np.dot(locs_xx.T, np.linalg.solve(cov_xx, locs_xx))\n",
    "            tmp_xx2 = np.dot(locs_xx.T, np.linalg.solve(cov_xx, y_xx))\n",
    "            beta_xx = np.linalg.solve(tmp_xx1, tmp_xx2)\n",
    "            mu_xx = np.dot(locs_xx, beta_xx)\n",
    "\n",
    "            tmp_yy1 = np.dot(locs_yy.T, np.linalg.solve(cov_yy, locs_yy))\n",
    "            tmp_yy2 = np.dot(locs_xx.T, np.linalg.solve(cov_yy, y_yy))\n",
    "            beta_yy = np.linalg.solve(tmp_xx1, tmp_xx2)\n",
    "            mu_yy = np.dot(locs_yy, beta_yy)\n",
    "\n",
    "            # mean and variance of y|x\n",
    "            sigma = cov_yy\n",
    "            cov_yx = matern_cov_yx(params=params, y_df = current_row, x_df = conditioning_data)\n",
    "            cov_ygivenx = sigma - np.dot(cov_yx.T,np.linalg.solve(cov_xx, cov_yx))\n",
    "            \n",
    "            # cov_ygivenx = max(cov_ygivenx, 7)\n",
    "            \n",
    "            cond_mean = mu_yy + np.dot(cov_yx.T, np.linalg.solve( cov_xx, (y_xx- mu_xx) ))   # adjust for bias, mean_xz should be 0 which is not true but we can't do same for y1 so just use mean_z almost 0\n",
    "            # print(f'cond_mean{mean_z}')\n",
    "\n",
    "            alpha = current_y - cond_mean\n",
    "            quad_form = alpha**2 *(1/cov_ygivenx)\n",
    "            log_det = np.log(cov_ygivenx)\n",
    "            # Compute the negative log-likelihood\n",
    "\n",
    "            neg_log_lik += 0.5 * (1 * np.log(2 * np.pi) + log_det + quad_form)\n",
    "        # prev_prev_df = prev_df\n",
    "        # prev_df = current_df\n",
    "    return neg_log_lik   \n",
    "\n",
    "params = [20,5 ,5, 0.5, 0.5, 0.5]\n",
    "vecchia_likelihood_test(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conditioning_data is empty\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "if conditioning_data.size == 0:\n",
    "    print(\"conditioning_data is empty\")\n",
    "    locs_xx = np.array([]).reshape(0, 2)  # Create an empty array with the appropriate shape\n",
    "else:\n",
    "    locs_xx = conditioning_data[:, :2]\n",
    "\n",
    "print(locs_xx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512000000\n",
      "1280000.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ab(n,m,k):\n",
    "    out = n**3/(n*m**3)\n",
    "    print(n**3)\n",
    "    print(n*m**3/k)\n",
    "    return out/k\n",
    "\n",
    "ab(800,20,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jl2815",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
