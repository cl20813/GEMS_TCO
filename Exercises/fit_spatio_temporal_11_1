log:
resolution 0.4 day 1 estimates : 300 8.92 7.6 8.923 0.1456  bounds on 0.05,300
Another resolution not known
day3: 100 28 100 100 0.8055 bounds on 0.05 100   
day2: 100 80.53 100 100 0.1756 bounds on 0.05 100   
day1: 100 100 100 100 0.15679 bounds on 0.05 100    
Another resolution not known
day1 100 100 100 100 0.12644857 bounds on 0.05 100

### copy ozone data and python file to Amarel HPC
scp "C:\\Users\\joonw\\TCO\\data_engineering\\data_24_07_0130_N510_E110120.csv" jl2815@amarel.rutgers.edu:/home/jl2815/tco/data/data_N510_E110120
scp "C:\Users\joonw\TCO\newpipeline\Exercises\fit_spatio_temporal_10_27.py" jl2815@amarel.rutgers.edu:/home/jl2815/tco/exercise

### Copy python from Amarel HPC to my computer
scp jl2815@amarel.rutgers.edu:/home/jl2815/tco/exercise/fit_spatio_temporal_10_27.py "C:\Users\joonw\TCO\newpipeline\Exercises"

### Quick run using srun

ssh jl2815@amarel.rutgers.edu
module use /projects/community/modulefiles            # # Ensure the module path is added. Without this, I can't load 2024.06-ts840
module load anaconda/2024.06-ts840 
conda activate gems_tco
srun --cpus-per-task=32 --mem=40G --time=05:00:00 python /home/jl2815/tco/exercise/fit_spatio_temporal_10_27.py --key 1 --resolution 0.4 --params 100 100 100 100 0.8 --mm_cond_number 30 --nugget 5 --bounds 0.05 300
## sbatch 

nano fit_st_model1.sh                  # open a new text editor

'''
#!/bin/bash
#SBATCH --job-name=fit_spatio_temporal        # Job name
#SBATCH --output=/home/jl2815/GEMS/fit_spatio_temporal_output_%j.out            # Standard output file (%j = JobID)
#SBATCH --error=/home/jl2815/GEMS/fit_spatio_temporal_error_%j.err              # Standard error file (%j = JobID)
#SBATCH --time=72:00:00                   # Maximum time 
#SBATCH --ntasks=1                        # Number of tasks
#SBATCH --cpus-per-task=32                 # Number of CPU cores per task
#SBATCH --mem=200G                          # Memory per node
#SBATCH --partition=main               # Partition to submit to

# Load the Anaconda module to use srun 

module purge                                     # unload every other environment to avoid conflict
module use /projects/community/modulefiles                  # without this, I can't load 2024.06-ts840
module load anaconda/2024.06-ts840 

# Initialize Conda
eval "$(conda shell.bash hook)"                  # Initialize Conda for SLURM environment
conda activate gems_tco
# Add the GEMS_TCO package directory to PYTHONPATH
export PYTHONPATH=$PYTHONPATH:/home/jl2815/tco/GEMS_TCO

# Verify environment variables and paths
echo "PYTHONPATH: $PYTHONPATH"                                                      # Ensures your directoreis are set. 
echo "Conda env: $(conda info --envs)"                                              # Lists your Conda environments and there should be * next to gems_tco because it is activated. 
echo 'fit_st_model1.sh --key 5 --resolution 0.2 --params 300 8.92 7.6 8.923 0.1456 --mm_cond_number 30 --nugget 5 --bounds 0.05 600'

# Run the Python script
srun python /home/jl2815/tco/exercise/fit_spatio_temporal_10_27.py --key 5 --resolution 0.2 --params 300 8.92 7.6 8.923 0.1456 --mm_cond_number 30 --nugget 5 --bounds 0.05 600
# Store the job ID in a variable
JOBID=$SLURM_JOBID

# Run sacct to get resource usage details
sacct --units=G --format=MaxRSS,MaxDiskRead,MaxDiskWrite,Elapsed,NodeList -j $JOBID
```
sbatch fit_st_model1.sh


---------------------------------------------------------------------------------------

nano fit_st_model_res0.4.sh                  # open a new text editor

'''
#!/bin/bash
#SBATCH --job-name=fit_st_model_res0.4        # Job name
#SBATCH --output=/home/jl2815/GEMS/fit_st_res0.4_output_%j.out            # Standard output file (%j = JobID)
#SBATCH --error=/home/jl2815/GEMS/fit_st_res0.4_error_%j.err              # Standard error file (%j = JobID)
#SBATCH --time=72:00:00                   # Maximum time 
#SBATCH --ntasks=1                        # Number of tasks
#SBATCH --cpus-per-task=32                 # Number of CPU cores per task
#SBATCH --mem=200G                          # Memory per node
#SBATCH --partition=main               # Partition to submit to

# Load the Anaconda module to use srun 

module purge                                     # unload every other environment to avoid conflict
module use /projects/community/modulefiles                  # without this, I can't load 2024.06-ts840
module load anaconda/2024.06-ts840 

# Initialize Conda
eval "$(conda shell.bash hook)"                  # Initialize Conda for SLURM environment
conda activate gems_tco
# Add the GEMS_TCO package directory to PYTHONPATH
export PYTHONPATH=$PYTHONPATH:/home/jl2815/tco/GEMS_TCO

# Verify environment variables and paths
echo "PYTHONPATH: $PYTHONPATH"                                                      # Ensures your directoreis are set. 
echo "Conda env: $(conda info --envs)"                                              # Lists your Conda environments and there should be * next to gems_tco because it is activated. 
echo 'fit_st_model_res0.4.sh --key 5 --resolution 0.4 --params 300 8.92 7.6 8.923 0.1456 --mm_cond_number 20 --nugget 5 --bounds 0.05 600'

# Run the Python script
srun python /home/jl2815/tco/exercise/fit_spatio_temporal_10_27.py --key 5 --resolution 0.4 --params 300 8.92 7.6 8.923 0.1456 --mm_cond_number 20 --nugget 5 --bounds 0.05 600
# Store the job ID in a variable
JOBID=$SLURM_JOBID

# Run sacct to get resource usage details
sacct --units=G --format=MaxRSS,MaxDiskRead,MaxDiskWrite,Elapsed,NodeList -j $JOBID
```
sbatch fit_st_model_res0.4.sh  





