{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631d9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# when python interpreter is different, add path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "from collections import defaultdict\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "# Special functions and optimizations\n",
    "from typing import Callable, Union, Tuple\n",
    "from scipy.spatial.distance import cdist  # For space and time distance\n",
    "from scipy.special import gamma, kv  # Bessel function and gamma function\n",
    "from scipy.interpolate import splrep, splev\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchcubicspline import natural_cubic_spline_coeffs, NaturalCubicSpline\n",
    "\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings\n",
    "from GEMS_TCO import load_data\n",
    "\n",
    "from GEMS_TCO import configuration as config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b491b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate faiss_env\n",
    "\n",
    "!/opt/anaconda3/envs/faiss_env/bin/python /Users/joonwonlee/Documents/GEMS_TCO-1/src/GEMS_TCO/mymac_config.py --space \"20,20\" --days \"0,31\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "982a5d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-2, data size per day: 200.0, smooth: 0.4\n",
      "mm_cond_number: 10,\n",
      "initial parameters: \n",
      " tensor([ 2.4793e+01,  1.5845e+00,  1.7182e+00,  9.0885e-03, -1.0730e-01,\n",
      "         1.3104e-01,  2.7172e+00], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "lat_lon_resolution = [10,10]\n",
    "years = ['2024']\n",
    "month_range =[7,8]\n",
    "nheads = 10\n",
    "mm_cond_number = 10 \n",
    "v= 0.4\n",
    "\n",
    "data_load_instance = load_data(config.mac_data_load_path)\n",
    "df = data_load_instance.read_pickle(config.mac_estimates_day_path,config.mac_full_day_v05_pickle)\n",
    "map, ord_mm, nns_map= data_load_instance.load_mm20k_data_bymonthyear( lat_lon_resolution= lat_lon_resolution, mm_cond_number=mm_cond_number,years_=years, months_=month_range)\n",
    "\n",
    "df.head()\n",
    "\n",
    "for day in range(1,2):\n",
    "    params = list(df.iloc[day-1][:-1])\n",
    "    params = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "    print(f'2024-07-{day+1}, data size per day: { (200/lat_lon_resolution[0])*(100/lat_lon_resolution[0]) }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params}')\n",
    "               \n",
    "    idx_for_datamap= [ 8*(day),8*(day+1)]\n",
    "    analysis_data_map, aggregated_data = data_load_instance.load_working_data_byday( map, ord_mm, nns_map, idx_for_datamap= idx_for_datamap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6291d565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2284.9292, dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarse_factor = 100\n",
    "spline_instance = kernels.spline(epsilon = 0, coarse_factor=coarse_factor, smooth = 0.5, input_map= analysis_data_map, aggregated_data= aggregated_data, nns_map=nns_map, mm_cond_number=10)\n",
    "distances, non_zero_indices = spline_instance.precompute_coords_anisotropy(params, spline_instance.aggregated_data, spline_instance.aggregated_data)\n",
    "# flat_distances = distances.flatten()\n",
    "# spline_instance.max_distance = torch.max(distances).clone().detach()\n",
    "# spline_instance.max_distance_len = len(flat_distances)\n",
    "# spline_instance.spline_object = spline_instance.fit_cubic_spline(params)\n",
    "\n",
    "## I made new_aggregated_data so that I don't have to initiate distances matrix every time.\n",
    "spline_instance.new_aggregated_data = aggregated_data[:,:4]\n",
    "spline_instance.new_aggregated_response = aggregated_data[:,2]\n",
    "spline_instance.new_data_analysis_map = analysis_data_map\n",
    "spline_instance.nheads= 1\n",
    "spline_instance.input_map = analysis_data_map\n",
    "\n",
    "cov_map = spline_instance.cov_structure_saver(params)\n",
    "out = spline_instance.vecchia_nll_using_spline(params, cov_map)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f69618bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2290.0550, dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## I made new_aggregated_data so that I don't have to initiate distances matrix every time.\n",
    "spline_instance.new_aggregated_data = aggregated_data[:,:4]\n",
    "spline_instance.new_aggregated_response = aggregated_data[:,2]\n",
    "spline_instance.nheads= 10\n",
    "spline_instance.input_map = analysis_data_map\n",
    "\n",
    "cov_map = spline_instance.cov_structure_saver(params)\n",
    "\n",
    "out1 = spline_instance.vecchia_nll_using_spline(params, cov_map)\n",
    "out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e2e0a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2283.8084, dtype=torch.float64, grad_fn=<MulBackward0>) tensor(2290.0550, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "instance_2 = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "excat_ll = instance_2.full_likelihood(params, aggregated_data[:,:4], aggregated_data[:,2], instance_2.matern_cov_anisotropy_v05)\n",
    "# 12663.4804\n",
    "print(excat_ll, out1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215fd326",
   "metadata": {},
   "source": [
    "likelihood "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c18688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2283.8084, dtype=torch.float64, grad_fn=<MulBackward0>) tensor(2281.6374, dtype=torch.float64, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "smooth = 0.5\n",
    "coarse_factor = 100\n",
    "spline_instance = kernels.spline(epsilon = 1e-17, coarse_factor=coarse_factor, smooth = smooth, input_map= analysis_data_map, aggregated_data= aggregated_data, nns_map=nns_map, mm_cond_number=10)\n",
    "distances, non_zero_indices = spline_instance.precompute_coords_anisotropy(params, spline_instance.aggregated_data, spline_instance.aggregated_data)\n",
    "# flat_distances = distances.flatten()\n",
    "# spline_instance.max_distance = torch.max(distances).clone().detach()\n",
    "# spline_instance.max_distance_len = len(flat_distances)\n",
    "# spline_instance.spline_object = spline_instance.fit_cubic_spline(params)\n",
    "\n",
    "## I made new_aggregated_data so that I don't have to initiate distances matrix every time.\n",
    "spline_instance.new_aggregated_data = aggregated_data[:,:4]\n",
    "spline_instance.new_aggregated_response = aggregated_data[:,2]\n",
    "\n",
    "full_ll = spline_instance.full_likelihood_using_spline( params, distances)\n",
    "full_ll\n",
    "\n",
    "instance_2 = kernels.vecchia_experiment(smooth, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "excat_ll = instance_2.full_likelihood(params, aggregated_data[:,:4], aggregated_data[:,2], instance_2.matern_cov_anisotropy_kv)\n",
    "# 12663.4804\n",
    "print(excat_ll, full_ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf7c3fb",
   "metadata": {},
   "source": [
    "# debug error when high resolution cov_1d returns nans\n",
    "# Summary I have to make the element wise different smaller than 2.53e-7 to make\n",
    "# likelihood difference smaller than 0.15\n",
    "\n",
    "## I suggest 1000 for resolution 1250(4,4) and 5000 for (2,2) and 50,000 for (1,1)\n",
    "\n",
    "\n",
    "resolution 3,3\n",
    "10,000:   total diff   1.66       5.01e-9\n",
    "100,000                868        2.6e-6\n",
    "\n",
    "resolution 4,4  (160000**2/(10000**2)  1/256 from original)\n",
    "#coarse factor 5 error coarse factor 10 okay\n",
    "coarse_factor 100 took 18 sec       sum diff 0.167   1.67e-9\n",
    "coarse_factor 1000 okay difference elementwise ( sum diff 0.2831, 2.83e-9 )\n",
    "coarse_factor 10,000        sum difference 1.45 (   1.45/10000**2= 1.5e-8  )\n",
    "\n",
    "resolution 6,6\n",
    "100:     sum:0.028  1.315e-9\n",
    "1000:   sum: 0.0314   1.47e-9\n",
    "10000:  sum: -124    5.82e-6\n",
    "\n",
    "resolution 10,10\n",
    "\n",
    "coarse_factor 100     sum diff 0.02     8.5e-9\n",
    "coarse_factor 1000    sum diff  -13.8154   -5.39 e-6\n",
    "\n",
    "coarse_factor 10,000  sum diff 3793\n",
    "coarse_factor 100,000 began to show difference at 10-4\n",
    "\n",
    "resolution 20,20\n",
    "coarse_facttor 100    sum diff 5.729   3.57e-5\n",
    "coarse_factor 1000    sum diff  200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a69bbdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_factor = 100\n",
    "spline_instance = kernels.spline(epsilon = 1e-17,  coarse_factor=coarse_factor, smooth = 1.0, input_map= analysis_data_map, aggregated_data= aggregated_data, nns_map=nns_map, mm_cond_number=10)\n",
    "distances, non_zero_indices = spline_instance.precompute_coords_anisotropy(params, spline_instance.aggregated_data, spline_instance.aggregated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "78bb228b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[27.5107,  2.2801,  2.8418,  ...,  3.3473, 14.3790, 15.2532],\n",
       "        [ 2.2801, 27.5107,  0.3088,  ...,  0.4272,  1.3129,  2.0119],\n",
       "        [ 2.8418,  0.3088, 27.5107,  ..., 14.7534,  2.9714,  2.0137],\n",
       "        ...,\n",
       "        [ 3.3473,  0.4272, 14.7534,  ..., 27.5107,  4.2969,  2.9851],\n",
       "        [14.3790,  1.3129,  2.9714,  ...,  4.2969, 27.5107, 19.5894],\n",
       "        [15.2532,  2.0119,  2.0137,  ...,  2.9851, 19.5894, 27.5107]],\n",
       "       dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_1d =spline_instance.spline_object.evaluate(distances)\n",
    "sigmasq, _, _, _, _, _, nugget = params\n",
    "cov_matrix = cov_1d.reshape(distances.shape)\n",
    "cov_matrix = cov_matrix * sigmasq\n",
    "cov_matrix = cov_matrix + torch.eye(cov_matrix.shape[0], dtype=torch.float64) * nugget \n",
    "cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e603fda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[27.5107,  2.3400,  2.5597,  ..., 10.3685,  6.3491,  5.5617],\n",
       "        [ 2.3400, 27.5107,  0.1307,  ...,  0.8240,  9.1023,  0.3373],\n",
       "        [ 2.5597,  0.1307, 27.5107,  ...,  5.0807,  0.4341, 10.8432],\n",
       "        ...,\n",
       "        [10.3685,  0.8240,  5.0807,  ..., 27.5107,  2.9851, 13.2747],\n",
       "        [ 6.3491,  9.1023,  0.4341,  ...,  2.9851, 27.5107,  1.2692],\n",
       "        [ 5.5617,  0.3373, 10.8432,  ..., 13.2747,  1.2692, 27.5107]],\n",
       "       dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance_2 = kernels.vecchia_experiment(1.0, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "out = instance_2.matern_cov_anisotropy_kv(params, instance_2.aggregated_data, instance_2.aggregated_data)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e153691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 10000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c4fa59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.5541, dtype=torch.float64, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum ( cov_matrix-out )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "570df445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0554e-07, dtype=torch.float64, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(cov_matrix-out)/ cov_matrix.shape[0]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8fa9fd",
   "metadata": {},
   "source": [
    "optimization for full likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9acc1f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.4793e+01,  1.5845e+00,  1.7182e+00,  9.0885e-03, -1.0730e-01,\n",
      "         1.3104e-01,  2.7172e+00], dtype=torch.float64, requires_grad=True)\n",
      "Epoch 1, Gradients: [  1.02435928 -10.95966353 -10.79886952  -9.32213875 -23.53334563\n",
      "  26.48364774   4.07843655]\n",
      " Loss: 617.9208199649293, Parameters: [ 2.47934437e+01  1.58452892e+00  1.71824777e+00  9.08850413e-03\n",
      " -1.07299447e-01  1.31037638e-01  2.71723866e+00]\n",
      "Epoch 11, Gradients: [  0.88616354  -7.91125793  -8.14658393 -30.76599035  -1.50333976\n",
      " -95.12353828   0.98808646]\n",
      " Loss: 612.1662288114492, Parameters: [24.59320242  1.78401844  1.91608201  0.09813627 -0.02729967  0.12307405\n",
      "  2.57402907]\n",
      "Epoch 21, Gradients: [  0.80310461  -6.56687474  -6.92065378 -16.57179696  -0.85058761\n",
      " -32.24166774   2.04916333]\n",
      " Loss: 607.3939070211577, Parameters: [24.39681831  1.97683983  2.10762147  0.1661918   0.04917295  0.1154253\n",
      "  2.40519322]\n",
      "Epoch 31, Gradients: [ 0.67219019 -5.70159305 -6.02041735  0.01925934  3.28534281 -9.3379126\n",
      "  2.41813432]\n",
      " Loss: 604.2687331541772, Parameters: [24.21105223  2.15456176  2.28932786  0.22488621  0.06156932  0.10850042\n",
      "  2.23590506]\n",
      "Epoch 41, Gradients: [ 0.58903469 -4.5492631  -4.94012984  6.26220936 -1.21519822  0.4141022\n",
      "  2.13279505]\n",
      " Loss: 602.0012147551506, Parameters: [24.04254426  2.31197517  2.46010301  0.23295592  0.03619412  0.11983738\n",
      "  2.07752725]\n",
      "Epoch 51, Gradients: [ 0.52734538 -4.47308085 -3.76253905  3.71446924 -1.65103907  2.98484449\n",
      "  1.7792113 ]\n",
      " Loss: 600.2508505501281, Parameters: [23.88780897  2.45262428  2.61525077  0.19901228  0.04200035  0.13727401\n",
      "  1.93640165]\n",
      "Epoch 61, Gradients: [ 0.44113553 -4.08454074 -2.93729182  3.35556924 -0.57820166  1.77014096\n",
      "  1.54309712]\n",
      " Loss: 598.7661186777442, Parameters: [23.74281537  2.58935401  2.75003262  0.1537324   0.06861521  0.15039241\n",
      "  1.80827261]\n",
      "Epoch 71, Gradients: [ 0.33668867 -3.72135165 -2.24402811  2.18846642  0.2636017   1.27835859\n",
      "  1.29213471]\n",
      " Loss: 597.5711444028238, Parameters: [23.61028551  2.72326141  2.86538532  0.09589885  0.08973015  0.16012037\n",
      "  1.68808184]\n",
      "Epoch 81, Gradients: [ 0.20749373 -3.71472759 -1.57101721  0.81305982  0.22389316  0.13773375\n",
      "  0.86720188]\n",
      " Loss: 596.678770286216, Parameters: [23.49677239  2.85480861  2.96197856  0.04358078  0.09576132  0.16569071\n",
      "  1.57783479]\n",
      "Epoch 91, Gradients: [ 0.06665814 -3.95503475 -0.99337007  0.1835022  -0.06162955 -0.80752077\n",
      "  0.30734875]\n",
      " Loss: 595.979029242071, Parameters: [2.34121604e+01 2.99002564e+00 3.03849985e+00 1.01785370e-02\n",
      " 9.46819868e-02 1.68011554e-01 1.48889810e+00]\n",
      "Epoch 101, Gradients: [-0.0539412  -3.3392275  -0.57528608 -0.33451997  0.0402838   0.07092428\n",
      " -0.16396421]\n",
      " Loss: 595.3916875655649, Parameters: [ 2.33666819e+01  3.13275035e+00  3.09469100e+00 -4.70995559e-03\n",
      "  9.40827745e-02  1.68619997e-01  1.43694041e+00]\n",
      "Epoch 111, Gradients: [-0.0704185  -3.07660451 -0.53424313 -0.27645394  0.05416295  0.10899445\n",
      " -0.23297278]\n",
      " Loss: 595.2990343430911, Parameters: [ 2.33654765e+01  3.16038357e+00  3.10266743e+00 -4.67449242e-03\n",
      "  9.38781901e-02  1.68526017e-01  1.43470413e+00]\n",
      "Epoch 121, Gradients: [-0.08140513 -2.80725133 -0.51958237 -0.12100756  0.05992731  0.24652677\n",
      " -0.27802323]\n",
      " Loss: 595.217038228687, Parameters: [ 2.33688999e+01  3.18665385e+00  3.10919937e+00 -3.52023884e-03\n",
      "  9.35268348e-02  1.68336854e-01  1.43736469e+00]\n",
      "Epoch 131, Gradients: [-0.08983439 -2.53847617 -0.51401014  0.02530949  0.03480635  0.11890813\n",
      " -0.31741932]\n",
      " Loss: 595.1445689904915, Parameters: [ 2.33749758e+01  3.21171584e+00  3.11533397e+00 -2.57675198e-03\n",
      "  9.31078170e-02  1.68057361e-01  1.44286058e+00]\n",
      "Epoch 141, Gradients: [-0.0958454  -2.27440756 -0.51005989  0.11024977  0.01119038  0.1806671\n",
      " -0.34233138]\n",
      " Loss: 595.0808278816328, Parameters: [ 2.33828657e+01  3.23553150e+00  3.12152669e+00 -2.37990147e-03\n",
      "  9.27567962e-02  1.67754144e-01  1.45032492e+00]\n",
      "Epoch 151, Gradients: [-0.1002906  -2.02414315 -0.50851005  0.13134358 -0.00871724  0.20264242\n",
      " -0.36114946]\n",
      " Loss: 595.0249926927296, Parameters: [ 2.33921822e+01  3.25802078e+00  3.12796737e+00 -2.90311275e-03\n",
      "  9.25683182e-02  1.67405726e-01  1.45932899e+00]\n",
      "Epoch 161, Gradients: [-0.10338039 -1.78938748 -0.50782457  0.1081258  -0.02137093  0.17692991\n",
      " -0.37524716]\n",
      " Loss: 594.9762283565876, Parameters: [ 2.34027207e+01  3.27912763e+00  3.13475145e+00 -3.83739590e-03\n",
      "  9.25650261e-02  1.67015551e-01  1.46964253e+00]\n",
      "Epoch 171, Gradients: [-0.1050874  -1.57031625 -0.50368152  0.07203159 -0.02317186  0.17508714\n",
      " -0.38338786]\n",
      " Loss: 594.933795573608, Parameters: [ 2.34143411e+01  3.29881653e+00  3.14192070e+00 -4.82868988e-03\n",
      "  9.27074719e-02  1.66598103e-01  1.48111151e+00]\n",
      "Epoch 181, Gradients: [-0.1055334  -1.36885137 -0.50098489  0.04601493 -0.01930807  0.17750404\n",
      " -0.38668745]\n",
      " Loss: 594.8969082838704, Parameters: [ 2.34269188e+01  3.31706736e+00  3.14947861e+00 -5.65680041e-03\n",
      "  9.29239607e-02  1.66158089e-01  1.49360250e+00]\n",
      "Epoch 191, Gradients: [-0.1048463  -1.1851263  -0.49380558  0.03393728 -0.01248163  0.17226386\n",
      " -0.38584254]\n",
      " Loss: 594.8647912188816, Parameters: [ 2.34403272e+01  3.33388046e+00  3.15743000e+00 -6.28189021e-03\n",
      "  9.31505377e-02  1.65700526e-01  1.50698697e+00]\n",
      "Epoch 201, Gradients: [-0.10316182 -1.01913634 -0.48565047  0.02980587 -0.0069729   0.1633603\n",
      " -0.38146682]\n",
      " Loss: 594.8367649445062, Parameters: [ 2.34544342e+01  3.34927250e+00  3.16574594e+00 -6.76458653e-03\n",
      "  9.33331776e-02  1.65229587e-01  1.52113351e+00]\n",
      "Converged at epoch 201\n",
      "Epoch 202, : Loss: 594.8362424132129, \n",
      " vecc Parameters: [ 2.34550129e+01  3.34985489e+00  3.16608692e+00 -6.78182664e-03\n",
      "  9.33390149e-02  1.65210524e-01  1.52171542e+00]\n",
      "FINAL STATE: Epoch 202, Loss: 594.836, \n",
      " vecc Parameters: [23.455, 3.35, 3.166, -0.007, 0.093, 0.165, 1.522]\n"
     ]
    }
   ],
   "source": [
    "coarse_factor =10\n",
    "spline_instance = kernels.spline(epsilon = 1e-17, coarse_factor=coarse_factor,  smooth = 0.5, input_map= analysis_data_map, aggregated_data= aggregated_data, nns_map=nns_map, mm_cond_number=10)\n",
    "spline_instance.new_aggregated_data = aggregated_data[:,:4]\n",
    "spline_instance.new_aggregated_response = aggregated_data[:,2]\n",
    "print(params)\n",
    "# spline_instance = kernels.spline(epsilon = 1e-17, coarse_factor=5, k=3, smooth = 0.5, input_map= analysis_data_map, aggregated_data= aggregated_data, nns_map=nns_map, mm_cond_number=10)\n",
    "# optimizer, scheduler =  instance.optimizer_fun(params, lr= 0.01 , betas=(0.9, 0.99), eps=1e-8, step_size= 5, gamma=0.1)    \n",
    "optimizer, scheduler = spline_instance.optimizer_fun(params, lr=0.02, betas=(0.9, 0.99), eps=1e-8, step_size=100, gamma=0.2)  \n",
    "out, epoch = spline_instance.run_full(params, optimizer,scheduler, epochs=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0acff27",
   "metadata": {},
   "source": [
    "optimization for vecchia approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd29203",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_factor = 10\n",
    "spline_instance = kernels.spline(epsilon = 1e-17, coarse_factor=coarse_factor, smooth = 0.5, input_map= analysis_data_map, aggregated_data= aggregated_data, nns_map=nns_map, mm_cond_number=10)\n",
    "distances, non_zero_indices = spline_instance.precompute_coords_anisotropy(params, spline_instance.aggregated_data, spline_instance.aggregated_data)\n",
    "# flat_distances = distances.flatten()\n",
    "# spline_instance.max_distance = torch.max(distances).clone().detach()\n",
    "# spline_instance.max_distance_len = len(flat_distances)\n",
    "# spline_instance.spline_object = spline_instance.fit_cubic_spline(params)\n",
    "\n",
    "## I made new_aggregated_data so that I don't have to initiate distances matrix every time.\n",
    "spline_instance.new_aggregated_data = aggregated_data[:,:4]\n",
    "spline_instance.new_aggregated_response = aggregated_data[:,2]\n",
    "spline_instance.nheads= 5\n",
    "spline_instance.input_map = analysis_data_map\n",
    "\n",
    "\n",
    "params = list(df.iloc[day-1][:-1])\n",
    "params = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "cov_map = spline_instance.cov_structure_saver(params)\n",
    "out = spline_instance.vecchia_nll_using_spline(params, cov_map)\n",
    "\n",
    "\n",
    "optimizer, scheduler = spline_instance.optimizer_fun(params, lr=0.02, betas=(0.9, 0.99), eps=1e-8, step_size=100, gamma=0.2)  \n",
    "out, epoch = spline_instance.fit_vecchia(params, optimizer,scheduler, epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce1e4e2",
   "metadata": {},
   "source": [
    "# Saved files below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "81cd5e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class spline:\n",
    "    def __init__(self, epsilon, coarse_factor, k, smooth):\n",
    "        self.smooth = torch.tensor(smooth, dtype= torch.float64)\n",
    "        self.k = k\n",
    "        self.coarse_factor = coarse_factor\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def compute_cov(self, params) :\n",
    "         # fit_distances and flat_distances both 1d\n",
    "        sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "        distances, non_zero_indices = instance_2.precompute_coords_anisotropy(params, aggregated_data[:,:4],aggregated_data[:,:4])\n",
    "        \n",
    "        flat_distances = distances.flatten()\n",
    "        fit_distances = torch.linspace(self.epsilon, torch.max(flat_distances), len(flat_distances) // self.coarse_factor)\n",
    "\n",
    "        # fit_distances = torch.zeros_like(distances)\n",
    "        # print(fit_distances.shape)\n",
    "        # Compute the covariance for non-zero distances\n",
    "        non_zero_indices = fit_distances != 0\n",
    "        out = torch.zeros_like(fit_distances, dtype= torch.float64)\n",
    "\n",
    "        if torch.any(non_zero_indices):\n",
    "            tmp = kv(self.smooth, torch.sqrt(fit_distances[non_zero_indices])).double().clone()\n",
    "            out[non_zero_indices] = (sigmasq * (2**(1-self.smooth)) / gamma(self.smooth) *\n",
    "                                    (torch.sqrt(fit_distances[non_zero_indices]) ) ** self.smooth *\n",
    "                                    tmp)\n",
    "        out[~non_zero_indices] = sigmasq\n",
    "\n",
    "        # print(out.shape)\n",
    "        #         \n",
    "        # Compute spline coefficients\n",
    "        coeffs = natural_cubic_spline_coeffs(fit_distances, out.unsqueeze(1))\n",
    "\n",
    "        # Create spline object\n",
    "        spline = NaturalCubicSpline(coeffs)\n",
    "        # Interpolate using the spline\n",
    "        out = spline.evaluate(distances)\n",
    "        out = out.reshape(distances.shape)\n",
    "        out += torch.eye(out.shape[0], dtype=torch.float64) * nugget \n",
    "        return out\n",
    "     \n",
    "    def full_likelihood(self,params: torch.Tensor, input_np: torch.Tensor, y: torch.Tensor, cov_matrix) -> torch.Tensor:\n",
    "        input_arr = input_np[:, :4]  ## input_np is aggregated data over a day.\n",
    "        y_arr = y\n",
    "\n",
    "        # Compute the covariance matrix\n",
    "        # cov_matrix = covariance_function(params=params, y=input_arr, x=input_arr)\n",
    "        \n",
    "        # Compute the log determinant of the covariance matrix\n",
    "        sign, log_det = torch.slogdet(cov_matrix)\n",
    "        # if sign <= 0:\n",
    "        #     raise ValueError(\"Covariance matrix is not positive definite\")\n",
    "        \n",
    "        # Extract locations\n",
    "        locs = input_arr[:, :2]\n",
    "\n",
    "        # Compute beta\n",
    "        tmp1 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, locs))\n",
    "        tmp2 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, y_arr))\n",
    "        beta = torch.linalg.solve(tmp1, tmp2)\n",
    "\n",
    "        # Compute the mean\n",
    "        mu = torch.matmul(locs, beta)\n",
    "        y_mu = y_arr - mu\n",
    "\n",
    "        # Compute the quadratic form\n",
    "        quad_form = torch.matmul(y_mu, torch.linalg.solve(cov_matrix, y_mu))\n",
    "\n",
    "        # Compute the negative log likelihood\n",
    "        neg_log_lik = 0.5 * (log_det + quad_form)\n",
    "     \n",
    "        return  neg_log_lik\n",
    "    \n",
    "    def compute_full_nll(self, params, covariance_function):\n",
    "        cov_mat = covariance_function(params) \n",
    "        nll = self.full_likelihood( params,aggregated_data[:,:4], aggregated_data[:,2], cov_mat)\n",
    "        return nll\n",
    "\n",
    "    def optimizer_fun(self, params, lr=0.01, betas=(0.9, 0.8), eps=1e-8, step_size=40, gamma=0.5):\n",
    "        optimizer = torch.optim.Adam([params], lr=lr, betas=betas, eps=eps)\n",
    "        scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)  # Decrease LR by a factor of 0.1 every 10 epochs\n",
    "        return optimizer, scheduler\n",
    "\n",
    "   # use adpating lr\n",
    "    def run_full(self, params, optimizer, scheduler,  covariance_function, epochs=10 ):\n",
    "        prev_loss= float('inf')\n",
    "\n",
    "        tol = 1e-4  # Convergence tolerance\n",
    "        for epoch in range(epochs):  # Number of epochs\n",
    "            optimizer.zero_grad()  # Zero the gradients \n",
    "            \n",
    "            loss = self.compute_full_nll(params, covariance_function)\n",
    "            loss.backward()  # Backpropagate the loss\n",
    "            \n",
    "            # Print gradients and parameters every 10th epoch\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "            \n",
    "            # if epoch % 500 == 0:\n",
    "            #     print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "            \n",
    "            optimizer.step()  # Update the parameters\n",
    "            scheduler.step()  # Update the learning rate\n",
    "            # Check for convergence\n",
    "            if abs(prev_loss - loss.item()) < tol:\n",
    "                print(f\"Converged at epoch {epoch}\")\n",
    "                print(f'Epoch {epoch+1}, : Loss: {loss.item()}, \\n vecc Parameters: {params.detach().numpy()}')\n",
    "                break\n",
    "\n",
    "            prev_loss = loss.item()\n",
    "        print(f'FINAL STATE: Epoch {epoch+1}, Loss: {loss.item()}, \\n vecc Parameters: {params.detach().numpy()}')\n",
    "        return params.detach().numpy().tolist() + [ loss.item()], epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bf7ade",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e5ea7",
   "metadata": {},
   "source": [
    "# Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426935a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params)\n",
    "\n",
    "instance_2 = kernels.vecchia_experiment(1.0, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "instance = spline( epsilon = 1e-8, coarse_factor = 4, k=3, smooth= 0.5)\n",
    "# optimizer, scheduler =  instance.optimizer_fun(params, lr= 0.01 , betas=(0.9, 0.99), eps=1e-8, step_size= 5, gamma=0.1)    \n",
    "optimizer, scheduler = instance.optimizer_fun(params, lr=0.03, betas=(0.9, 0.99), eps=1e-8, step_size=100, gamma=0.9)  \n",
    "out, epoch = instance.run_full(params, optimizer,scheduler, instance.compute_cov, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d69fb24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, 10.9571,  9.3482,  ...,  8.2249,  1.1083,  0.9382],\n",
       "        [10.9571,  0.0000, 30.5686,  ..., 26.7820, 15.4784, 11.9212],\n",
       "        [ 9.3482, 30.5686,  0.0000,  ...,  1.0330,  9.0359, 11.9142],\n",
       "        ...,\n",
       "        [ 8.2249, 26.7820,  1.0330,  ...,  0.0000,  6.6330,  9.0040],\n",
       "        [ 1.1083, 15.4784,  9.0359,  ...,  6.6330,  0.0000,  0.3387],\n",
       "        [ 0.9382, 11.9212, 11.9142,  ...,  9.0040,  0.3387,  0.0000]],\n",
       "       dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1 = splinenn.evaluate(distances)\n",
    "out1 = out1.reshape(distances.shape)\n",
    "\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "41c5a49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "tensor([[27.5107,  0.9052,  1.1654,  ...,  1.4087,  8.6523,  9.4119],\n",
      "        [ 0.9052, 27.5107,  0.0984,  ...,  0.1402,  0.4850,  0.7849],\n",
      "        [ 1.1654,  0.0984, 27.5107,  ...,  8.9731,  1.2270,  0.7858],\n",
      "        ...,\n",
      "        [ 1.4087,  0.1402,  8.9731,  ..., 27.5107,  1.8872,  1.2336],\n",
      "        [ 8.6523,  0.4850,  1.2270,  ...,  1.8872, 27.5107, 13.8542],\n",
      "        [ 9.4119,  0.7849,  0.7858,  ...,  1.2336, 13.8542, 27.5107]],\n",
      "       dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "tensor([[27.5107,  0.9052,  1.1654,  ...,  1.4087,  8.6523,  9.4119],\n",
      "        [ 0.9052, 27.5107,  0.0984,  ...,  0.1402,  0.4850,  0.7849],\n",
      "        [ 1.1654,  0.0984, 27.5107,  ...,  8.9731,  1.2270,  0.7858],\n",
      "        ...,\n",
      "        [ 1.4087,  0.1402,  8.9731,  ..., 27.5107,  1.8872,  1.2336],\n",
      "        [ 8.6523,  0.4850,  1.2270,  ...,  1.8872, 27.5107, 13.8542],\n",
      "        [ 9.4119,  0.7849,  0.7858,  ...,  1.2336, 13.8542, 27.5107]],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2283.8084, dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smooth = 0.5\n",
    "\n",
    "instance_2 = kernels.vecchia_experiment(smooth, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "instance = spline( epsilon = 1e-15, coarse_factor = 2, k=3, smooth= smooth)\n",
    "\n",
    "distances, non_zero_indices = instance_2.precompute_coords_anisotropy(params, aggregated_data[:,:4],aggregated_data[:,:4])\n",
    "\n",
    "flat_distances = distances.flatten()\n",
    "sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "epsilon = 0\n",
    "coarse_factor = 100\n",
    "\n",
    "fit_distances = torch.linspace(epsilon, torch.max(flat_distances), len(flat_distances) // coarse_factor)\n",
    "print(fit_distances.shape)\n",
    "# Compute the covariance for non-zero distances\n",
    "non_zero_indices = fit_distances != 0\n",
    "out = torch.zeros_like(fit_distances, dtype= torch.float64)\n",
    "\n",
    "if torch.any(non_zero_indices):\n",
    "    tmp = kv(smooth, torch.sqrt(fit_distances[non_zero_indices])).double().clone()\n",
    "    out[non_zero_indices] = (sigmasq * (2**(1-smooth)) / gamma(smooth) *\n",
    "                            (torch.sqrt(fit_distances[non_zero_indices]) ) ** smooth *\n",
    "                            tmp)\n",
    "    \n",
    "out[~non_zero_indices] = sigmasq\n",
    "\n",
    "print(out.shape)\n",
    "\n",
    "# Compute spline coefficients\n",
    "coeffs = natural_cubic_spline_coeffs(fit_distances, out.unsqueeze(1))\n",
    "\n",
    "# Create spline object\n",
    "splinenn = NaturalCubicSpline(coeffs)\n",
    "\n",
    "# Interpolate using the spline\n",
    "out1 = splinenn.evaluate(distances)\n",
    "out1 = out1.reshape(distances.shape)\n",
    "out1 += torch.eye(out1.shape[0], dtype=torch.float64) * nugget \n",
    "\n",
    "print(out1)\n",
    "out2 = instance_2.matern_cov_anisotropy_kv(params, aggregated_data[:,:4],aggregated_data[:,:4])\n",
    "\n",
    "\n",
    "print(out2)\n",
    "instance.full_likelihood( params,aggregated_data[:,:4], aggregated_data[:,2], out1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
