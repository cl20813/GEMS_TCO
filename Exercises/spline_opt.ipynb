{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631d9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# when python interpreter is different, add path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "from collections import defaultdict\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "# Special functions and optimizations\n",
    "from typing import Callable, Union, Tuple\n",
    "from scipy.spatial.distance import cdist  # For space and time distance\n",
    "from scipy.special import gamma, kv  # Bessel function and gamma function\n",
    "from scipy.interpolate import splrep, splev\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchcubicspline import natural_cubic_spline_coeffs, NaturalCubicSpline\n",
    "\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings\n",
    "from GEMS_TCO import load_data\n",
    "\n",
    "from GEMS_TCO import configuration as config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b491b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate faiss_env\n",
    "\n",
    "!/opt/anaconda3/envs/faiss_env/bin/python /Users/joonwonlee/Documents/GEMS_TCO-1/src/GEMS_TCO/mymac_config.py --space \"20,20\" --days \"0,31\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "982a5d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-2, data size per hour: 281, smooth: 0.5\n",
      "mm_cond_number: 10,\n",
      "initial parameters: \n",
      " tensor([ 2.4793e+01,  1.5845e+00,  1.7182e+00,  9.0885e-03, -1.0730e-01,\n",
      "         1.3104e-01,  2.7172e+00], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "lat_lon_resolution = [8,8]\n",
    "years = ['2024']\n",
    "month_range =[7,8]\n",
    "nheads = 10\n",
    "mm_cond_number = 10 \n",
    "v= 0.5\n",
    "\n",
    "data_load_instance = load_data(config.mac_data_load_path)\n",
    "df = data_load_instance.read_pickle(config.mac_estimates_day_path,config.mac_full_day_v05_pickle)\n",
    "map, ord_mm, nns_map= data_load_instance.load_mm20k_data_bymonthyear( lat_lon_resolution= lat_lon_resolution, mm_cond_number=mm_cond_number,years_=years, months_=month_range)\n",
    "\n",
    "df.head()\n",
    "\n",
    "for day in range(1,2):\n",
    "    params = list(df.iloc[day-1][:-1])\n",
    "    params = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "    print(f'2024-07-{day+1}, data size per hour: { (int(158.7 / lat_lon_resolution[0] * (113.63 / lat_lon_resolution[0]))) }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params}')\n",
    "               \n",
    "    idx_for_datamap= [ 8*(day),8*(day+1)]\n",
    "    analysis_data_map, aggregated_data = data_load_instance.load_working_data_byday( map, ord_mm, nns_map, idx_for_datamap= idx_for_datamap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4da1ea48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full: 4180.555600142574, vecc: 4252.477514877538\n"
     ]
    }
   ],
   "source": [
    "instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "excat_ll = instance.full_likelihood(params, aggregated_data[:,:4], aggregated_data[:,2], instance.matern_cov_anisotropy_v05)\n",
    "\n",
    "cov_map = instance.cov_structure_saver(params, instance.matern_cov_anisotropy_v05)\n",
    "vecc_ll = instance.vecchia_may9(params, instance.matern_cov_anisotropy_v05 ,cov_map)\n",
    "print(f'full: {excat_ll.item()}, vecc: {vecc_ll.item()}')   # (14435.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87f0863d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact full: 4180.555600142574, spline_full: 4180.186564100173\n"
     ]
    }
   ],
   "source": [
    "coarse_factor_head = 4 # 16:2 8:4  4:16, i expect 2:64, 1:128\n",
    "coarse_factor_cond = 1\n",
    "spline_instance = kernels.spline(epsilon = 0, coarse_factor_head=coarse_factor_head,coarse_factor_cond=coarse_factor_cond, smooth = v, input_map= analysis_data_map, aggregated_data= aggregated_data, nns_map=nns_map, mm_cond_number=10)\n",
    "distances, non_zero_indices = spline_instance.precompute_coords_anisotropy(params, spline_instance.aggregated_data, spline_instance.aggregated_data)\n",
    "# flat_distances = distances.flatten()\n",
    "# spline_instance.max_distance = torch.max(distances).clone().detach()\n",
    "# spline_instance.max_distance_len = len(flat_distances)\n",
    "# spline_instance.spline_object = spline_instance.fit_cubic_spline(params)\n",
    "\n",
    "spline_instance.nheads= 500\n",
    "\n",
    "distances, non_zero_indices = spline_instance.precompute_coords_anisotropy(params, aggregated_data, aggregated_data)\n",
    "spline_object_head = spline_instance.fit_cubic_spline( distances, spline_instance.coarse_factor_head)  # change here\n",
    "\n",
    "spline_full = spline_instance.full_likelihood_using_spline(params,aggregated_data[:,:4], aggregated_data[:,2], distances, spline_object_head)\n",
    "\n",
    "\n",
    "# 12663.4804\n",
    "print( f'exact full: {excat_ll.item()}, spline_full: {spline_full.item()}' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6291d565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " spline full likelihood: 4180.186564100173, spline vecchia: 4235.268833591424\n"
     ]
    }
   ],
   "source": [
    "coarse_factor_head = 4\n",
    "coarse_factor_cond = 1\n",
    "spline_instance = kernels.spline(epsilon = 0, coarse_factor_head=coarse_factor_head,coarse_factor_cond=coarse_factor_cond, smooth = 0.5, input_map= analysis_data_map, aggregated_data= aggregated_data, nns_map=nns_map, mm_cond_number=10)\n",
    "distances, non_zero_indices = spline_instance.precompute_coords_anisotropy(params, spline_instance.aggregated_data, spline_instance.aggregated_data)\n",
    "# flat_distances = distances.flatten()\n",
    "# spline_instance.max_distance = torch.max(distances).clone().detach()\n",
    "# spline_instance.max_distance_len = len(flat_distances)\n",
    "# spline_instance.spline_object = spline_instance.fit_cubic_spline(params)\n",
    "\n",
    "spline_instance.nheads= 50\n",
    "\n",
    "cov_map = spline_instance.cov_structure_saver_using_spline(params)\n",
    "vecc = spline_instance.vecchia_nll_using_spline(params, cov_map)\n",
    "print(f' spline full likelihood: {spline_full.item()}, spline vecchia: {vecc.item()}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ffaecb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14636.4830, dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarse_factor_head = 8 # 4:16 8:4\n",
    "coarse_factor_cond = 1\n",
    "spline_instance = kernels.spline(epsilon = 0, coarse_factor_head=coarse_factor_head,coarse_factor_cond=coarse_factor_cond, smooth = 0.5, input_map= analysis_data_map, aggregated_data= aggregated_data, nns_map=nns_map, mm_cond_number=10)\n",
    "\n",
    "spline_instance.nheads= 10\n",
    "\n",
    "cov_map = spline_instance.cov_structure_saver_using_spline(params)\n",
    "vecc = spline_instance.vecchia_nll_using_spline(params, cov_map)\n",
    "vecc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abacefb",
   "metadata": {},
   "source": [
    "### Note that coarse_factor_head in vecchia model relies on nheads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5e02775e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vecc: 4252.477514877538, spline vecc: 4233.940743985809\n"
     ]
    }
   ],
   "source": [
    "coarse_factor_head = 1 # 4:16 8(284):4, 16(70):1\n",
    "coarse_factor_cond = 1\n",
    "spline_instance = kernels.spline(epsilon = 0, coarse_factor_head=coarse_factor_head,coarse_factor_cond=coarse_factor_cond, smooth = 0.5, input_map= analysis_data_map, aggregated_data= aggregated_data, nns_map=nns_map, mm_cond_number=10)\n",
    "\n",
    "spline_instance.nheads= 70\n",
    "\n",
    "cov_map_spline = spline_instance.cov_structure_saver_using_spline(params)\n",
    "vecc = spline_instance.vecchia_nll_using_spline(params, cov_map_spline)\n",
    "\n",
    "\n",
    "instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "\n",
    "cov_map = instance.cov_structure_saver(params, instance.matern_cov_anisotropy_v05)\n",
    "vecc_ll = instance.vecchia_may9(params, instance.matern_cov_anisotropy_v05 ,cov_map)\n",
    "print(f'vecc: {vecc_ll.item()}, spline vecc: {vecc.item()}')   # (14435.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215fd326",
   "metadata": {},
   "source": [
    "likelihood "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf7c3fb",
   "metadata": {},
   "source": [
    "# debug error when high resolution cov_1d returns nans\n",
    "# Summary I have to make the element wise different smaller than 2.53e-7 to make\n",
    "# likelihood difference smaller than 0.15\n",
    "\n",
    "## I suggest 1000 for resolution 1250(4,4) and 5000 for (2,2) and 50,000 for (1,1)\n",
    "\n",
    "\n",
    "resolution 3,3\n",
    "10,000:   total diff   1.66       5.01e-9\n",
    "100,000                868        2.6e-6\n",
    "\n",
    "resolution 4,4  (160000**2/(10000**2)  1/256 from original)\n",
    "#coarse factor 5 error coarse factor 10 okay\n",
    "coarse_factor 100 took 18 sec       sum diff 0.167   1.67e-9\n",
    "coarse_factor 1000 okay difference elementwise ( sum diff 0.2831, 2.83e-9 )\n",
    "coarse_factor 10,000        sum difference 1.45 (   1.45/10000**2= 1.5e-8  )\n",
    "\n",
    "resolution 6,6\n",
    "100:     sum:0.028  1.315e-9\n",
    "1000:   sum: 0.0314   1.47e-9\n",
    "10000:  sum: -124    5.82e-6\n",
    "\n",
    "resolution 10,10\n",
    "\n",
    "coarse_factor 100     sum diff 0.02     8.5e-9\n",
    "coarse_factor 1000    sum diff  -13.8154   -5.39 e-6\n",
    "\n",
    "coarse_factor 10,000  sum diff 3793\n",
    "coarse_factor 100,000 began to show difference at 10-4\n",
    "\n",
    "resolution 20,20\n",
    "coarse_facttor 100    sum diff 5.729   3.57e-5\n",
    "coarse_factor 1000    sum diff  200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a69bbdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_factor_head = 4 # 16:2 8:4  4:16, i expect 2:64, 1:128\n",
    "coarse_factor_cond = 1\n",
    "spline_instance = kernels.spline(epsilon = 0, coarse_factor_head=coarse_factor_head,coarse_factor_cond=coarse_factor_cond, smooth = v, input_map= analysis_data_map, aggregated_data= aggregated_data, nns_map=nns_map, mm_cond_number=10)\n",
    "\n",
    "spline_instance.nheads= 500\n",
    "\n",
    "distances, non_zero_indices = spline_instance.precompute_coords_anisotropy(params, aggregated_data, aggregated_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79f703c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covarinace matrix using spline:\n",
      "\n",
      " tensor([[27.5107,  0.8990,  1.1548,  ...,  3.6048,  1.9665,  5.8334],\n",
      "        [ 0.8990, 27.5107,  0.0942,  ...,  3.6373,  0.1284,  2.0618],\n",
      "        [ 1.1548,  0.0942, 27.5107,  ...,  0.4144,  7.1918,  0.7017],\n",
      "        ...,\n",
      "        [ 3.6048,  3.6373,  0.4144,  ..., 27.5107,  0.7292, 13.2462],\n",
      "        [ 1.9665,  0.1284,  7.1918,  ...,  0.7292, 27.5107,  1.3196],\n",
      "        [ 5.8334,  2.0618,  0.7017,  ..., 13.2462,  1.3196, 27.5107]],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "Original covarinace matrix : \n",
      "\n",
      " tensor([[27.5107,  0.8990,  1.1548,  ...,  3.6048,  1.9665,  5.8334],\n",
      "        [ 0.8990, 27.5107,  0.0942,  ...,  3.6373,  0.1284,  2.0618],\n",
      "        [ 1.1548,  0.0942, 27.5107,  ...,  0.4144,  7.1919,  0.7017],\n",
      "        ...,\n",
      "        [ 3.6048,  3.6373,  0.4144,  ..., 27.5107,  0.7292, 13.2462],\n",
      "        [ 1.9665,  0.1284,  7.1919,  ...,  0.7292, 27.5107,  1.3196],\n",
      "        [ 5.8334,  2.0618,  0.7017,  ..., 13.2462,  1.3196, 27.5107]],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "shape of the matrices: torch.Size([2240, 2240])\n",
      "sum of differences: -15.737926207017424\n",
      "element-wise difference on average: -3.1365446043960107e-06\n"
     ]
    }
   ],
   "source": [
    "spline_object_head = spline_instance.fit_cubic_spline( distances, spline_instance.coarse_factor_head)  # change here\n",
    "cov_1d = spline_object_head.evaluate(distances)\n",
    "sigmasq, _, _, _, _, _, nugget = params\n",
    "cov_matrix = cov_1d.reshape(distances.shape)\n",
    "cov_matrix = cov_matrix * sigmasq\n",
    "cov_matrix = cov_matrix + torch.eye(cov_matrix.shape[0], dtype=torch.float64) * nugget \n",
    "print(f'Covarinace matrix using spline:\\n\\n {cov_matrix}\\n')\n",
    "\n",
    "instance_2 = kernels.vecchia_experiment(v, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "out = instance_2.matern_cov_anisotropy_kv(params, instance_2.aggregated_data, instance_2.aggregated_data)\n",
    "print(f'Original covarinace matrix : \\n\\n {out}')\n",
    "\n",
    "print(f'shape of the matrices: {cov_matrix.shape}')\n",
    "\n",
    "print(f'sum of differences: {torch.sum ( cov_matrix-out )}')\n",
    "print(f'element-wise difference on average: {torch.sum(cov_matrix-out)/ cov_matrix.shape[0]**2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8fa9fd",
   "metadata": {},
   "source": [
    "optimization for full likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acc1f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.4793e+01,  1.5845e+00,  1.7182e+00,  9.0885e-03, -1.0730e-01,\n",
      "         1.3104e-01,  2.7172e+00], dtype=torch.float64, requires_grad=True)\n",
      "Epoch 1, Gradients: [  -15.20851441   120.5063186      2.6793614    112.21640154\n",
      "   415.09729463 -1298.56608861  -131.57867753]\n",
      " Loss: 4180.186564100173, Parameters: [ 2.47934437e+01  1.58452892e+00  1.71824777e+00  9.08850413e-03\n",
      " -1.07299447e-01  1.31037638e-01  2.71723866e+00]\n",
      "Epoch 101, Gradients: [ -0.56453347  -1.35252707  -0.35120962  -0.17410995  -1.05352026\n",
      "   1.46938781 -11.13395343]\n",
      " Loss: 3995.81722199123, Parameters: [25.65032925  0.81789756  1.33432273 -0.03404876 -0.20534873  0.23931403\n",
      "  3.76331965]\n",
      "Epoch 201, Gradients: [-4.73988425e-01 -3.07447383e+00 -2.67035741e-01  6.36760257e-03\n",
      " -8.58732676e-02  2.16389788e+00 -1.06408411e+01]\n",
      " Loss: 3993.538017034656, Parameters: [25.72579407  0.84119467  1.38558457 -0.03311623 -0.20346034  0.2292493\n",
      "  3.96119062]\n",
      "Epoch 301, Gradients: [ -0.47940358  -2.91583638  -0.25999449  -0.05530284  -0.11661979\n",
      "   1.82192683 -10.63545899]\n",
      " Loss: 3992.889329665657, Parameters: [25.75026275  0.85092548  1.40281483 -0.03290206 -0.20318069  0.22593921\n",
      "  4.01737477]\n"
     ]
    }
   ],
   "source": [
    "coarse_factor_head = 4 # 16:2 8:4  4:16, i expect 2:64, 1:128\n",
    "coarse_factor_cond = 1\n",
    "spline_instance = kernels.spline(epsilon = 0, coarse_factor_head=coarse_factor_head,coarse_factor_cond=coarse_factor_cond, smooth = v, input_map= analysis_data_map, aggregated_data= aggregated_data, nns_map=nns_map, mm_cond_number=10)\n",
    "\n",
    "spline_instance.nheads= 50\n",
    "print(params)\n",
    "# spline_instance = kernels.spline(epsilon = 1e-17, coarse_factor=5, k=3, smooth = 0.5, input_map= analysis_data_map, aggregated_data= aggregated_data, nns_map=nns_map, mm_cond_number=10)\n",
    "# optimizer, scheduler =  instance.optimizer_fun(params, lr= 0.01 , betas=(0.9, 0.99), eps=1e-8, step_size= 5, gamma=0.1)    \n",
    "optimizer, scheduler = spline_instance.optimizer_fun(params, lr=0.02, betas=(0.9, 0.99), eps=1e-8, step_size=100, gamma=0.2)  \n",
    "out, epoch = spline_instance.run_full(params, aggregated_data,optimizer,scheduler, epochs=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0acff27",
   "metadata": {},
   "source": [
    "optimization for vecchia approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4dd29203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Gradients: [   -7.67722426     2.83025242    16.85498825     2.64241634\n",
      "   238.00750953 -1084.52291744   -61.3463364 ]\n",
      " Loss: 4233.940743985809, Parameters: [ 2.47934437e+01  1.58452892e+00  1.71824777e+00  9.08850413e-03\n",
      " -1.07299447e-01  1.31037638e-01  2.71723866e+00]\n",
      "Epoch 101, Gradients: [-1.12733587 -0.03197711 -0.15042502  0.13406993  0.02084264  2.49632787\n",
      " -3.03525171]\n",
      " Loss: 4084.463421299732, Parameters: [26.04144235  0.82541716  0.93562984 -0.10381089 -0.27430643  0.43281586\n",
      "  3.56584923]\n",
      "Epoch 201, Gradients: [-1.02647531e+00 -2.26693468e-01 -2.97412747e-01  7.69974465e-04\n",
      "  9.68885649e-03  2.45994888e+00 -2.89671405e+00]\n",
      " Loss: 4083.364952753561, Parameters: [26.28595707  0.83535842  0.95487828 -0.10441174 -0.27445142  0.42099665\n",
      "  3.72530959]\n",
      "Epoch 301, Gradients: [-1.00559833 -0.20466934 -0.26562644  0.01845811  0.03619645  2.29507976\n",
      " -2.88125286]\n",
      " Loss: 4081.932979925134, Parameters: [26.34813217  0.83956224  0.9621159  -0.10445452 -0.27459755  0.41696864\n",
      "  3.77339197]\n",
      "Epoch 401, Gradients: [-1.00205897 -0.18134335 -0.24165715  0.02114086  0.07483205  2.20441044\n",
      " -2.88283403]\n",
      " Loss: 4082.812033940194, Parameters: [26.36244407  0.84081968  0.96428183 -0.10449341 -0.27471182  0.41568518\n",
      "  3.78593161]\n",
      "FINAL STATE: Epoch 500, Loss: 4082.692, \n",
      " vecc Parameters: [26.366, 0.841, 0.965, -0.105, -0.275, 0.415, 3.789]\n"
     ]
    }
   ],
   "source": [
    "coarse_factor_head = 1 # 16:2 8:4  4:16, i expect 2:64, 1:128\n",
    "coarse_factor_cond = 1\n",
    "spline_instance = kernels.spline(epsilon = 0, coarse_factor_head=coarse_factor_head,coarse_factor_cond=coarse_factor_cond, smooth = v, input_map= analysis_data_map, aggregated_data= aggregated_data, nns_map=nns_map, mm_cond_number=10)\n",
    "\n",
    "\n",
    "spline_instance.nheads= 70\n",
    "\n",
    "optimizer, scheduler = spline_instance.optimizer_fun(params, lr=0.02, betas=(0.9, 0.99), eps=1e-8, step_size=100, gamma=0.2)  \n",
    "out, epoch = spline_instance.fit_vecchia(params, optimizer,scheduler, epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce1e4e2",
   "metadata": {},
   "source": [
    "# Saved spline class (May 26, 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cd5e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class spline(spatio_temporal_kernels):\n",
    "    '''\n",
    "    fit_cublic_spline() for each data shares the common locations. Even though the\n",
    "    'distances' matrix is a function of parameters, we can make a common upper bound\n",
    "    by putting range parameters 0.5, advections 0, beta 2.\n",
    "    and we fit cubic_spline() for fixed smooth Matern model with range=1 and sigmasq=1.\n",
    "    Essentially, we are approximating simple Matern model for v=1.\n",
    "    \n",
    "    Any change in parameters will be reflected through \"distances\" matrix. So,\n",
    "    we define \"distances\" matrix for each epoch.\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, epsilon:float, coarse_factor_head:int, coarse_factor_cond:int, smooth:float, input_map: Dict[str, Any], aggregated_data:torch.Tensor, nns_map: np.ndarray, mm_cond_number:int):\n",
    "        super().__init__(smooth, input_map, aggregated_data, nns_map, mm_cond_number)\n",
    "        self.smooth = torch.tensor(smooth, dtype= torch.float64)\n",
    "        \n",
    "        self.epsilon = epsilon  # starting point for the spline fitting\n",
    "        sample_params = [25, 0.5, 0.5, 0, 0, 2, 5] # just random nuumber to initialize spline\n",
    "        sample_params = torch.tensor(sample_params, dtype=torch.float64, requires_grad=True)\n",
    "        \n",
    "        self.coarse_factor_head = coarse_factor_head\n",
    " \n",
    "        self.coarse_factor_cond = coarse_factor_cond\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize the class with given parameters.\n",
    "        Args:\n",
    "            coarse_factor (int): Factor used for coarse-graining.\n",
    "            smooth (float): Smooth parameter in Matern model.\n",
    "            input_map (Dict[str, Any]): Dictionary containing input mappings.\n",
    "            aggregated_data (torch.Tensor): Tensor containing aggregated data.\n",
    "            nns_map (Dict[str, Any]): 2-d nd.array containing nearest neighbors mappings.\n",
    "            mm_cond_number (int): Condition number for Vecchia approximation\n",
    "        \"\"\"\n",
    "\n",
    "    def fit_cubic_spline(self, target_distances, coarse_factor:int=4):\n",
    "\n",
    "        \"\"\"\n",
    "        Fit a natural cubic spline coefficients.\n",
    "\n",
    "        Args:\n",
    "            params (tuple): Parameters for the spline fitting.\n",
    "\n",
    "        Returns:\n",
    "            NaturalCubicSpline: The fitted spline object with coefficients.\n",
    "        \"\"\"\n",
    "\n",
    "        def flat_distance_matrix(distances: torch.Tensor) -> torch.Tensor:\n",
    "            n = distances.size(0)\n",
    "            indices = torch.triu_indices(n, n, offset=1)\n",
    "            upper_tri = distances[indices[0], indices[1]]\n",
    "            unique_sorted = torch.unique(upper_tri, sorted=True)\n",
    "            flat_distances = torch.cat([torch.tensor([0.0], device=unique_sorted.device), unique_sorted])\n",
    "            max_distance = torch.max(flat_distances).clone().detach()\n",
    "            len_distance_arr = len(flat_distances)\n",
    "            \n",
    "            return max_distance, len_distance_arr\n",
    "        \n",
    "        max_distance, len_distance_arr = flat_distance_matrix(target_distances)\n",
    "\n",
    "        # fit_distances should be 1 d array to be used in natural_cubic_spline_coeffs\n",
    "        fit_distances = torch.linspace(0, max_distance + 1e-6 , len_distance_arr// coarse_factor)\n",
    "        non_zero_indices = fit_distances != 0\n",
    "        out = torch.zeros_like(fit_distances, dtype= torch.float64)\n",
    "\n",
    "        if torch.any(non_zero_indices):\n",
    "            tmp = kv(self.smooth, torch.sqrt(fit_distances[non_zero_indices])).double().clone()\n",
    "            out[non_zero_indices] = (1 * (2**(1-self.smooth)) / gamma(self.smooth) *\n",
    "                                    (torch.sqrt(fit_distances[non_zero_indices]) ) ** self.smooth *\n",
    "                                    tmp)\n",
    "        out[~non_zero_indices] = 1\n",
    "\n",
    "        # Compute spline coefficients. If input is tensor, so is output.\n",
    "        # natural_cubic_spline_coeffs(t,x), t should be 1-d array (n,) and x should be (n,channels)\n",
    "        # where channels reoresent number of features. out.unsquueze(1) makes (n,1).\n",
    "        coeffs = natural_cubic_spline_coeffs(fit_distances, out.unsqueeze(1))\n",
    "        # Create spline object\n",
    "        spline = NaturalCubicSpline(coeffs)\n",
    "        return spline\n",
    "\n",
    "\n",
    "    def interpolate_cubic_spline(self, params:torch.Tensor, target_distances:torch.Tensor, spline_object) -> torch.Tensor:\n",
    "\n",
    "        \"\"\"\n",
    "        Interpolate using the fitted cubic spline.\n",
    "        Args:\n",
    "            params (tuple): Parameters for the interpolation.\n",
    "            target_distances (torch.Tensor): Distances to interpolate.\n",
    "            spline_object (NaturalCubicSpline): The fitted spline object.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Interpolated values.\n",
    "        \"\"\"\n",
    "    \n",
    "        sigmasq, _, _, _, _, _, nugget = params\n",
    "        n = target_distances.size(0)\n",
    "        indices = torch.triu_indices(n, n, offset=0)  # offset=0 to include diagonal\n",
    "\n",
    "        # Evaluate spline only on upper triangle\n",
    "        cov_upper = spline_object.evaluate(target_distances[indices[0], indices[1]])\n",
    "\n",
    "        # Create empty matrix and fill upper triangle\n",
    "        cov_matrix = torch.zeros_like(target_distances)\n",
    "\n",
    "        # spline_object.evaluate return [N,1] \n",
    "        #print(cov_matrix.shape, cov_upper.shape, indices.shape, indices[0].shape)\n",
    "        #print(indices)\n",
    "        \n",
    "        cov_matrix[indices[0], indices[1]] = cov_upper.view(-1)\n",
    "\n",
    "        # Mirror to lower triangle\n",
    "        cov_matrix = cov_matrix + cov_matrix.T - torch.diag(torch.diag(cov_matrix))\n",
    "\n",
    "        # Apply scaling and nugget\n",
    "        cov_matrix = cov_matrix * sigmasq\n",
    "        cov_matrix = cov_matrix + torch.eye(n, dtype=torch.float64, device=cov_matrix.device) * nugget\n",
    "\n",
    "        ''' \n",
    "        Before May26\n",
    "        sigmasq, _, _, _, _, _, nugget = params\n",
    "        cov_1d = spline_object.evaluate(target_distances)\n",
    "        cov_matrix = cov_1d.reshape(target_distances.shape)\n",
    "        cov_matrix = cov_matrix * sigmasq\n",
    "        cov_matrix = cov_matrix + torch.eye(cov_matrix.shape[0], dtype=torch.float64) * nugget \n",
    "        '''\n",
    "        return cov_matrix\n",
    "\n",
    "\n",
    "    def full_likelihood_using_spline(self, params:torch.Tensor, input_data: torch.Tensor, y: torch.Tensor, target_distances:torch.Tensor, spline_object):\n",
    "    \n",
    "        cov_matrix = self.interpolate_cubic_spline(params, target_distances, spline_object)\n",
    "\n",
    "        sign, log_det = torch.slogdet(cov_matrix)\n",
    "\n",
    "        # if sign <= 0:\n",
    "        #     raise ValueError(\"Covariance matrix is not positive definite\")\n",
    "        # Compute beta\n",
    "\n",
    "        locs = input_data[:,:2]\n",
    "        response = input_data[:,2]\n",
    "\n",
    "        tmp1 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, locs))\n",
    "        tmp2 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, response))\n",
    "        beta = torch.linalg.solve(tmp1, tmp2)\n",
    "\n",
    "        mu = torch.matmul(locs, beta)\n",
    "        y_mu = response - mu\n",
    "        quad_form = torch.matmul(y_mu, torch.linalg.solve(cov_matrix, y_mu))\n",
    "        neg_log_lik = 0.5 * (log_det + quad_form)\n",
    "        return  neg_log_lik\n",
    "\n",
    "    def cov_structure_saver_using_spline(self, params: torch.Tensor) -> None:\n",
    "        \n",
    "        cov_map = defaultdict(lambda: defaultdict(dict))\n",
    "        cut_line= self.nheads\n",
    "        key_list = list(self.input_map.keys())\n",
    "\n",
    "        for time_idx in range(0,3):\n",
    "            current_array = self.input_map[key_list[time_idx]]\n",
    "\n",
    "            # Use below when working on local computer to avoid singular matrix\n",
    "            for index in range(cut_line, self.size_per_hour):\n",
    "                current_row = current_array[index].reshape(1, -1)\n",
    "                mm_neighbors = self.nns_map[index]\n",
    "                past = list(mm_neighbors) \n",
    "                data_list = []\n",
    "\n",
    "                if past:\n",
    "                    data_list.append(current_array[past])\n",
    "\n",
    "                if time_idx > 0:\n",
    "                    one_hour_lag = self.input_map[key_list[time_idx - 1]]\n",
    "                    data_list.append(one_hour_lag[past + [index], :])\n",
    "\n",
    "                if time_idx > 1:\n",
    "                    two_hour_lag = self.input_map[key_list[time_idx -2]]\n",
    "                    data_list.append(two_hour_lag [past + [index], :])\n",
    "                \n",
    "                conditioning_data = torch.vstack(data_list) if data_list else torch.empty((0, current_row.shape[1]), dtype=torch.float64)\n",
    "                aggregated_arr = torch.vstack((current_row, conditioning_data))\n",
    "                locs = aggregated_arr[:, :2]\n",
    "\n",
    "                target_distances_for_cond, non_zero_indices = self.precompute_coords_anisotropy(params, aggregated_arr,aggregated_arr)\n",
    "\n",
    "                cond_spline_object = self.fit_cubic_spline(target_distances_for_cond, self.coarse_factor_cond )  # change here  \n",
    "                cov_matrix = self.interpolate_cubic_spline(params, target_distances_for_cond, cond_spline_object)\n",
    "\n",
    "                # if sign <= 0:\n",
    "                #     raise ValueError(\"Covariance matrix is not positive definite\")\n",
    "\n",
    "                cov_yx = cov_matrix[0, 1:]\n",
    "                sign, log_det = torch.slogdet(cov_matrix)\n",
    "                tmp1 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, locs))\n",
    "            \n",
    "                # Mean and variance of y|x\n",
    "                sigma = cov_matrix[0, 0]\n",
    "                cov_xx = cov_matrix[1:, 1:]\n",
    "                cov_xx_inv = torch.linalg.inv(cov_xx)\n",
    "                cov_ygivenx = sigma - torch.matmul(cov_yx, torch.matmul(cov_xx_inv, cov_yx))\n",
    "                cond_mean_tmp = torch.matmul(cov_yx, cov_xx_inv)\n",
    "                log_det = torch.log(cov_ygivenx)\n",
    "            \n",
    "                cov_map[(time_idx,index)] = {\n",
    "                    'tmp1': tmp1.clone().detach(),\n",
    "                    'cov_xx_inv': cov_xx_inv.clone().detach(),\n",
    "                    'cov_matrix': cov_matrix.clone().detach(),\n",
    "                    'cov_ygivenx': cov_ygivenx.clone().detach(),\n",
    "                    'cond_mean_tmp': cond_mean_tmp.clone().detach(),\n",
    "                    'log_det': log_det.clone().detach(),\n",
    "                    'locs': locs.clone().detach()\n",
    "                }\n",
    "        return cov_map\n",
    "\n",
    "    def vecchia_nll_using_spline(self, params: torch.Tensor, cov_map:Dict[str,Any]) -> torch.Tensor:\n",
    "\n",
    "        cut_line= self.nheads\n",
    "        key_list = list(self.input_map.keys())\n",
    "        neg_log_lik = 0.0\n",
    "        heads = self.input_map[key_list[0]][:cut_line,:]\n",
    "\n",
    "        for time_idx in range(1, len(self.input_map)):\n",
    "            tmp = self.input_map[key_list[time_idx]][:cut_line,:]\n",
    "            heads = torch.cat( (heads,tmp), dim=0)\n",
    "\n",
    "        print(heads.shape )\n",
    "        distances_heads, _ = self.precompute_coords_anisotropy(params, heads, heads)\n",
    "        spline_object_head = self.fit_cubic_spline( distances_heads, self.coarse_factor_head)  # change here\n",
    "\n",
    "        neg_log_lik += self.full_likelihood_using_spline(params, heads[:,:4], heads[:,2], distances_heads, spline_object_head)\n",
    "    \n",
    "        for time_idx in range(0,len(self.input_map)):\n",
    "            current_np = self.input_map[key_list[time_idx]]\n",
    "\n",
    "            for index in range(cut_line, self.size_per_hour):\n",
    "                current_row = current_np[index].reshape(1, -1)\n",
    "                current_y = current_row[0, 2]\n",
    "\n",
    "                mm_neighbors = self.nns_map[index]\n",
    "                past = list(mm_neighbors) \n",
    "                data_list = []\n",
    "\n",
    "                if past:\n",
    "                    data_list.append(current_np[past])  \n",
    "                if time_idx < 2:\n",
    "                    cov_matrix = cov_map[(time_idx,index)]['cov_matrix']\n",
    "                    tmp1 = cov_map[(time_idx,index)]['tmp1']\n",
    "                    cov_xx_inv = cov_map[(time_idx,index)]['cov_xx_inv']\n",
    "                    cov_ygivenx = cov_map[(time_idx,index)]['cov_ygivenx']\n",
    "                    cond_mean_tmp = cov_map[(time_idx,index)]['cond_mean_tmp']\n",
    "                    log_det = cov_map[(time_idx,index)]['log_det']\n",
    "                    locs = cov_map[(time_idx,index)]['locs']\n",
    "                else:\n",
    "                    cov_matrix = cov_map[(2,index)]['cov_matrix']\n",
    "                    tmp1 = cov_map[(2,index)]['tmp1']\n",
    "                    cov_xx_inv = cov_map[(2,index)]['cov_xx_inv']\n",
    "                    cov_ygivenx = cov_map[(2,index)]['cov_ygivenx']\n",
    "                    cond_mean_tmp = cov_map[(2,index)]['cond_mean_tmp']\n",
    "                    log_det = cov_map[(2,index)]['log_det']\n",
    "                    locs = cov_map[(2,index)]['locs']\n",
    "\n",
    "                if time_idx >= 1:\n",
    "                    one_hour_lag = self.input_map[key_list[time_idx - 1]]\n",
    "                    past_conditioning_data = one_hour_lag[past + [index], :]\n",
    "                    data_list.append(past_conditioning_data)\n",
    "                \n",
    "                if time_idx > 1:\n",
    "                    two_hour_lag = self.input_map[key_list[time_idx - 2]]\n",
    "                    past_conditioning_data = two_hour_lag[past + [index], :]\n",
    "                    data_list.append(past_conditioning_data)\n",
    "    \n",
    "                if data_list:\n",
    "                    conditioning_data = torch.vstack(data_list)\n",
    "                else:\n",
    "                    conditioning_data = torch.empty((0, current_row.shape[1]), dtype=torch.float64)\n",
    "\n",
    "                aggregated_arr = torch.vstack((current_row, conditioning_data))\n",
    "                aggregated_y = aggregated_arr[:, 2]\n",
    "\n",
    "                cov_yx = cov_matrix[0, 1:]\n",
    "                tmp2 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, aggregated_y))\n",
    "                beta = torch.linalg.solve(tmp1, tmp2)\n",
    "                mu = torch.matmul(locs, beta)\n",
    "                mu_current = mu[0]\n",
    "                mu_neighbors = mu[1:]\n",
    "                \n",
    "                # Mean and variance of y|x\n",
    "                cond_mean = mu_current + torch.matmul(cond_mean_tmp, (aggregated_y[1:] - mu_neighbors))\n",
    "                alpha = current_y - cond_mean\n",
    "                quad_form = alpha**2 * (1 / cov_ygivenx)\n",
    "                neg_log_lik += 0.5 * (log_det + quad_form)\n",
    "        return neg_log_lik\n",
    "\n",
    "    def compute_full_nll(self, params:torch.Tensor, aggregated_data, distances:torch.Tensor, spline_object): \n",
    "        nll = self.full_likelihood_using_spline( params, aggregated_data[:,:4], aggregated_data[:,2], distances, spline_object)\n",
    "        return nll\n",
    "\n",
    "    def compute_vecchia_nll(self, params:torch.Tensor): \n",
    "        cov_map = self.cov_structure_saver_using_spline(params)\n",
    "        nll = self.vecchia_nll_using_spline(params, cov_map)\n",
    "        return nll\n",
    "\n",
    "    def optimizer_fun(self, params:torch.Tensor, lr:float =0.01, betas: tuple=(0.9, 0.8), eps:float=1e-8, step_size:int=40, gamma:float=0.5):\n",
    "        optimizer = torch.optim.Adam([params], lr=lr, betas=betas, eps=eps)\n",
    "        scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)  # Decrease LR by a factor of 0.1 every 10 epochs\n",
    "        return optimizer, scheduler\n",
    "\n",
    "    def run_full(self, params:torch.Tensor, aggregated_data, optimizer:torch.optim.Optimizer, scheduler:torch.optim.lr_scheduler, epochs:int=10 ):\n",
    "\n",
    "        \"\"\"\n",
    "        Run the training loop for the full likelihood model.\n",
    "\n",
    "        Args:\n",
    "            params (torch.Tensor): Model parameters.\n",
    "            optimizer (torch.optim.Optimizer): Optimizer for updating parameters.\n",
    "            scheduler (torch.optim.lr_scheduler): Learning rate scheduler.\n",
    "            epochs (int): Number of epochs to train.\n",
    "\n",
    "        Returns:\n",
    "            list: Final parameters and loss.\n",
    "            int: Number of epochs run.\n",
    "        \"\"\"\n",
    "\n",
    "        prev_loss= float('inf')\n",
    "        # 1e-3: Faster convergence, slightly lower accuracy than 1e-4\n",
    "        tol = 1e-3  # Convergence tolerance\n",
    "        for epoch in range(epochs):  \n",
    "            optimizer.zero_grad()  # Zero the gradients \n",
    "            distances, non_zero_indices = self.precompute_coords_anisotropy(params, aggregated_data[:,:4], aggregated_data[:,:4])\n",
    "            spline_object = self.fit_cubic_spline( distances, self.coarse_factor_head)  # change here\n",
    "\n",
    "            loss = self.compute_full_nll(params, aggregated_data, distances, spline_object)\n",
    "            loss.backward()  # Backpropagate the loss\n",
    "\n",
    "            # Gradient and Parameter Logging for every 10th epoch\n",
    "            #if epoch % 10 == 0:\n",
    "            #    print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "            \n",
    "            optimizer.step()  # Update the parameters\n",
    "            scheduler.step()  # Update the learning rate\n",
    "\n",
    "            # Convergence Check\n",
    "            if abs(prev_loss - loss.item()) < tol:\n",
    "                print(f\"Converged at epoch {epoch}\")\n",
    "                print(f'Epoch {epoch+1}, : Loss: {loss.item()}, \\n vecc Parameters: {params.detach().numpy()}')\n",
    "                break\n",
    "\n",
    "            prev_loss = loss.item()\n",
    "        params = [torch.round(x*1000).detach().numpy()/1000 for x in params]\n",
    "        loss = (torch.round(loss*1000)/1000).item()\n",
    "        print(f'FINAL STATE: Epoch {epoch+1}, Loss: {loss}, \\n vecc Parameters: {params}')\n",
    "        return params + [loss], epoch\n",
    "\n",
    "    def fit_vecchia(self, params:torch.Tensor, optimizer:torch.optim.Optimizer, scheduler:torch.optim.lr_scheduler, epochs:int=10 ):\n",
    "\n",
    "        \"\"\"\n",
    "        Run the training loop for the full likelihood model.\n",
    "\n",
    "        Args:\n",
    "            params (torch.Tensor): Model parameters.\n",
    "            optimizer (torch.optim.Optimizer): Optimizer for updating parameters.\n",
    "            scheduler (torch.optim.lr_scheduler): Learning rate scheduler.\n",
    "            epochs (int): Number of epochs to train.\n",
    "\n",
    "        Returns:\n",
    "            list: Final parameters and loss.\n",
    "            int: Number of epochs run.\n",
    "        \"\"\"\n",
    "\n",
    "        prev_loss= float('inf')\n",
    "        # 1e-3: Faster convergence, slightly lower accuracy than 1e-4\n",
    "        tol = 1e-3  # Convergence tolerance\n",
    "\n",
    "        for epoch in range(epochs):  \n",
    "            optimizer.zero_grad()  # Zero the gradients \n",
    "            # distance is a function of parameters\n",
    "            # distances, non_zero_indices = self.precompute_coords_anisotropy(params, self.new_aggregated_data[:,:4], self.new_aggregated_data[:,:4])\n",
    "            \n",
    "            loss = self.compute_vecchia_nll(params)\n",
    "            loss.backward()  # Backpropagate the loss\n",
    "\n",
    "            # Gradient and Parameter Logging for every 10th epoch\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "            \n",
    "            # if epoch % 500 == 0:\n",
    "            #     print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "            \n",
    "            optimizer.step()  # Update the parameters\n",
    "            scheduler.step()  # Update the learning rate\n",
    "\n",
    "            # Convergence Check\n",
    "            if abs(prev_loss - loss.item()) < tol:\n",
    "                print(f\"Converged at epoch {epoch}\")\n",
    "                print(f'Epoch {epoch+1}, : Loss: {loss.item()}, \\n vecc Parameters: {params.detach().numpy()}')\n",
    "                break\n",
    "\n",
    "            prev_loss = loss.item()\n",
    "        params = [torch.round(x*1000).detach().numpy()/1000 for x in params]\n",
    "        loss = (torch.round(loss*1000)/1000).item()\n",
    "        print(f'FINAL STATE: Epoch {epoch+1}, Loss: {loss}, \\n vecc Parameters: {params}')\n",
    "        return params + [loss], epoch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
