{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631d9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# when python interpreter is different, add path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "from collections import defaultdict\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "# Special functions and optimizations\n",
    "from typing import Callable, Union, Tuple\n",
    "from scipy.spatial.distance import cdist  # For space and time distance\n",
    "from scipy.special import gamma, kv  # Bessel function and gamma function\n",
    "from scipy.interpolate import splrep, splev\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchcubicspline import natural_cubic_spline_coeffs, NaturalCubicSpline\n",
    "\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings\n",
    "from GEMS_TCO import load_data\n",
    "\n",
    "from GEMS_TCO import configuration as config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b491b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate faiss_env\n",
    "\n",
    "!/opt/anaconda3/envs/faiss_env/bin/python /Users/joonwonlee/Documents/GEMS_TCO-1/src/GEMS_TCO/mymac_config.py --space \"20,20\" --days \"0,31\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "982a5d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-2, data size per day: 312.5, smooth: 0.5\n",
      "mm_cond_number: 10,\n",
      "initial parameters: \n",
      " tensor([ 2.4793e+01,  1.5845e+00,  1.7182e+00,  9.0885e-03, -1.0730e-01,\n",
      "         1.3104e-01,  2.7172e+00], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "lat_lon_resolution = [8,8]\n",
    "years = ['2024']\n",
    "month_range =[7,8]\n",
    "nheads = 10\n",
    "mm_cond_number = 10 \n",
    "v= 0.5\n",
    "\n",
    "data_load_instance = load_data(config.mac_data_load_path)\n",
    "df = data_load_instance.read_pickle(config.mac_estimates_day_path,config.mac_full_day_v05_pickle)\n",
    "map, ord_mm, nns_map= data_load_instance.load_mm20k_data_bymonthyear( lat_lon_resolution= lat_lon_resolution, mm_cond_number=mm_cond_number,years_=years, months_=month_range)\n",
    "\n",
    "df.head()\n",
    "\n",
    "for day in range(1,2):\n",
    "    params = list(df.iloc[day-1][:-1])\n",
    "    params = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "    print(f'2024-07-{day+1}, data size per day: { (200/lat_lon_resolution[0])*(100/lat_lon_resolution[0]) }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params}')\n",
    "               \n",
    "    idx_for_datamap= [ 8*(day),8*(day+1)]\n",
    "    analysis_data_map, aggregated_data = data_load_instance.load_working_data_byday( map, ord_mm, nns_map, idx_for_datamap= idx_for_datamap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4da1ea48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full: 4180.555600142574, vecc: 4252.477514877538\n"
     ]
    }
   ],
   "source": [
    "instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "excat_ll = instance.full_likelihood(params, aggregated_data[:,:4], aggregated_data[:,2], instance.matern_cov_anisotropy_v05)\n",
    "\n",
    "cov_map = instance.cov_structure_saver(params, instance.matern_cov_anisotropy_v05)\n",
    "vecc_ll = instance.vecchia_may9(params, instance.matern_cov_anisotropy_v05 ,cov_map)\n",
    "print(f'full: {excat_ll.item()}, vecc: {vecc_ll.item()}')   # (14435.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87f0863d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact full: 4180.555600142574, spline_full: 4180.186564100173\n"
     ]
    }
   ],
   "source": [
    "coarse_factor_head = 4 # 16:2 8:4  4:16, i expect 2:64, 1:128\n",
    "coarse_factor_cond = 1\n",
    "spline_instance = kernels.spline(epsilon = 0, coarse_factor_head=coarse_factor_head,coarse_factor_cond=coarse_factor_cond, smooth = v, input_map= analysis_data_map, aggregated_data= aggregated_data, nns_map=nns_map, mm_cond_number=10)\n",
    "distances, non_zero_indices = spline_instance.precompute_coords_anisotropy(params, spline_instance.aggregated_data, spline_instance.aggregated_data)\n",
    "# flat_distances = distances.flatten()\n",
    "# spline_instance.max_distance = torch.max(distances).clone().detach()\n",
    "# spline_instance.max_distance_len = len(flat_distances)\n",
    "# spline_instance.spline_object = spline_instance.fit_cubic_spline(params)\n",
    "\n",
    "spline_instance.nheads= 500\n",
    "\n",
    "distances, non_zero_indices = spline_instance.precompute_coords_anisotropy(params, aggregated_data, aggregated_data)\n",
    "spline_object_head = spline_instance.fit_cubic_spline( distances, spline_instance.coarse_factor_head)  # change here\n",
    "\n",
    "spline_full = spline_instance.full_likelihood_using_spline(params,aggregated_data[:,:4], aggregated_data[:,2], distances, spline_object_head)\n",
    "\n",
    "\n",
    "# 12663.4804\n",
    "print( f'exact full: {excat_ll.item()}, spline_full: {spline_full.item()}' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6291d565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 4])\n",
      " spline full likelihood: 4180.186564100173, spline vecchia: 4235.268833591424\n"
     ]
    }
   ],
   "source": [
    "coarse_factor_head = 4\n",
    "coarse_factor_cond = 1\n",
    "spline_instance = kernels.spline(epsilon = 0, coarse_factor_head=coarse_factor_head,coarse_factor_cond=coarse_factor_cond, smooth = 0.5, input_map= analysis_data_map, aggregated_data= aggregated_data, nns_map=nns_map, mm_cond_number=10)\n",
    "distances, non_zero_indices = spline_instance.precompute_coords_anisotropy(params, spline_instance.aggregated_data, spline_instance.aggregated_data)\n",
    "# flat_distances = distances.flatten()\n",
    "# spline_instance.max_distance = torch.max(distances).clone().detach()\n",
    "# spline_instance.max_distance_len = len(flat_distances)\n",
    "# spline_instance.spline_object = spline_instance.fit_cubic_spline(params)\n",
    "\n",
    "spline_instance.nheads= 50\n",
    "\n",
    "cov_map = spline_instance.cov_structure_saver_using_spline(params)\n",
    "vecc = spline_instance.vecchia_nll_using_spline(params, cov_map)\n",
    "print(f' spline full likelihood: {spline_full.item()}, spline vecchia: {vecc.item()}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215fd326",
   "metadata": {},
   "source": [
    "likelihood "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf7c3fb",
   "metadata": {},
   "source": [
    "# debug error when high resolution cov_1d returns nans\n",
    "# Summary I have to make the element wise different smaller than 2.53e-7 to make\n",
    "# likelihood difference smaller than 0.15\n",
    "\n",
    "## I suggest 1000 for resolution 1250(4,4) and 5000 for (2,2) and 50,000 for (1,1)\n",
    "\n",
    "\n",
    "resolution 3,3\n",
    "10,000:   total diff   1.66       5.01e-9\n",
    "100,000                868        2.6e-6\n",
    "\n",
    "resolution 4,4  (160000**2/(10000**2)  1/256 from original)\n",
    "#coarse factor 5 error coarse factor 10 okay\n",
    "coarse_factor 100 took 18 sec       sum diff 0.167   1.67e-9\n",
    "coarse_factor 1000 okay difference elementwise ( sum diff 0.2831, 2.83e-9 )\n",
    "coarse_factor 10,000        sum difference 1.45 (   1.45/10000**2= 1.5e-8  )\n",
    "\n",
    "resolution 6,6\n",
    "100:     sum:0.028  1.315e-9\n",
    "1000:   sum: 0.0314   1.47e-9\n",
    "10000:  sum: -124    5.82e-6\n",
    "\n",
    "resolution 10,10\n",
    "\n",
    "coarse_factor 100     sum diff 0.02     8.5e-9\n",
    "coarse_factor 1000    sum diff  -13.8154   -5.39 e-6\n",
    "\n",
    "coarse_factor 10,000  sum diff 3793\n",
    "coarse_factor 100,000 began to show difference at 10-4\n",
    "\n",
    "resolution 20,20\n",
    "coarse_facttor 100    sum diff 5.729   3.57e-5\n",
    "coarse_factor 1000    sum diff  200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a69bbdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_factor_head = 4 # 16:2 8:4  4:16, i expect 2:64, 1:128\n",
    "coarse_factor_cond = 1\n",
    "spline_instance = kernels.spline(epsilon = 0, coarse_factor_head=coarse_factor_head,coarse_factor_cond=coarse_factor_cond, smooth = v, input_map= analysis_data_map, aggregated_data= aggregated_data, nns_map=nns_map, mm_cond_number=10)\n",
    "\n",
    "spline_instance.nheads= 500\n",
    "\n",
    "distances, non_zero_indices = spline_instance.precompute_coords_anisotropy(params, aggregated_data, aggregated_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79f703c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covarinace matrix using spline:\n",
      "\n",
      " tensor([[27.5107,  0.8990,  1.1548,  ...,  3.6048,  1.9665,  5.8334],\n",
      "        [ 0.8990, 27.5107,  0.0942,  ...,  3.6373,  0.1284,  2.0618],\n",
      "        [ 1.1548,  0.0942, 27.5107,  ...,  0.4144,  7.1918,  0.7017],\n",
      "        ...,\n",
      "        [ 3.6048,  3.6373,  0.4144,  ..., 27.5107,  0.7292, 13.2462],\n",
      "        [ 1.9665,  0.1284,  7.1918,  ...,  0.7292, 27.5107,  1.3196],\n",
      "        [ 5.8334,  2.0618,  0.7017,  ..., 13.2462,  1.3196, 27.5107]],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "Original covarinace matrix : \n",
      "\n",
      " tensor([[27.5107,  0.8990,  1.1548,  ...,  3.6048,  1.9665,  5.8334],\n",
      "        [ 0.8990, 27.5107,  0.0942,  ...,  3.6373,  0.1284,  2.0618],\n",
      "        [ 1.1548,  0.0942, 27.5107,  ...,  0.4144,  7.1919,  0.7017],\n",
      "        ...,\n",
      "        [ 3.6048,  3.6373,  0.4144,  ..., 27.5107,  0.7292, 13.2462],\n",
      "        [ 1.9665,  0.1284,  7.1919,  ...,  0.7292, 27.5107,  1.3196],\n",
      "        [ 5.8334,  2.0618,  0.7017,  ..., 13.2462,  1.3196, 27.5107]],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "shape of the matrices: torch.Size([2240, 2240])\n",
      "sum of differences: -15.737926207017424\n",
      "element-wise difference on average: -3.1365446043960107e-06\n"
     ]
    }
   ],
   "source": [
    "spline_object_head = spline_instance.fit_cubic_spline( distances, spline_instance.coarse_factor_head)  # change here\n",
    "cov_1d = spline_object_head.evaluate(distances)\n",
    "sigmasq, _, _, _, _, _, nugget = params\n",
    "cov_matrix = cov_1d.reshape(distances.shape)\n",
    "cov_matrix = cov_matrix * sigmasq\n",
    "cov_matrix = cov_matrix + torch.eye(cov_matrix.shape[0], dtype=torch.float64) * nugget \n",
    "print(f'Covarinace matrix using spline:\\n\\n {cov_matrix}\\n')\n",
    "\n",
    "instance_2 = kernels.vecchia_experiment(v, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "out = instance_2.matern_cov_anisotropy_kv(params, instance_2.aggregated_data, instance_2.aggregated_data)\n",
    "print(f'Original covarinace matrix : \\n\\n {out}')\n",
    "\n",
    "print(f'shape of the matrices: {cov_matrix.shape}')\n",
    "\n",
    "print(f'sum of differences: {torch.sum ( cov_matrix-out )}')\n",
    "print(f'element-wise difference on average: {torch.sum(cov_matrix-out)/ cov_matrix.shape[0]**2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8fa9fd",
   "metadata": {},
   "source": [
    "optimization for full likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acc1f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.4793e+01,  1.5845e+00,  1.7182e+00,  9.0885e-03, -1.0730e-01,\n",
      "         1.3104e-01,  2.7172e+00], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "coarse_factor_head = 4 # 16:2 8:4  4:16, i expect 2:64, 1:128\n",
    "coarse_factor_cond = 1\n",
    "spline_instance = kernels.spline(epsilon = 0, coarse_factor_head=coarse_factor_head,coarse_factor_cond=coarse_factor_cond, smooth = v, input_map= analysis_data_map, aggregated_data= aggregated_data, nns_map=nns_map, mm_cond_number=10)\n",
    "\n",
    "spline_instance.nheads= 50\n",
    "print(params)\n",
    "# spline_instance = kernels.spline(epsilon = 1e-17, coarse_factor=5, k=3, smooth = 0.5, input_map= analysis_data_map, aggregated_data= aggregated_data, nns_map=nns_map, mm_cond_number=10)\n",
    "# optimizer, scheduler =  instance.optimizer_fun(params, lr= 0.01 , betas=(0.9, 0.99), eps=1e-8, step_size= 5, gamma=0.1)    \n",
    "optimizer, scheduler = spline_instance.optimizer_fun(params, lr=0.02, betas=(0.9, 0.99), eps=1e-8, step_size=100, gamma=0.2)  \n",
    "out, epoch = spline_instance.run_full(params, aggregated_data,optimizer,scheduler, epochs=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0acff27",
   "metadata": {},
   "source": [
    "optimization for vecchia approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd29203",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_factor = 10\n",
    "spline_instance = kernels.spline(epsilon = 1e-17, coarse_factor=coarse_factor, smooth = 0.5, input_map= analysis_data_map, aggregated_data= aggregated_data, nns_map=nns_map, mm_cond_number=10)\n",
    "distances, non_zero_indices = spline_instance.precompute_coords_anisotropy(params, spline_instance.aggregated_data, spline_instance.aggregated_data)\n",
    "# flat_distances = distances.flatten()\n",
    "# spline_instance.max_distance = torch.max(distances).clone().detach()\n",
    "# spline_instance.max_distance_len = len(flat_distances)\n",
    "# spline_instance.spline_object = spline_instance.fit_cubic_spline(params)\n",
    "\n",
    "## I made new_aggregated_data so that I don't have to initiate distances matrix every time.\n",
    "spline_instance.new_aggregated_data = aggregated_data[:,:4]\n",
    "spline_instance.new_aggregated_response = aggregated_data[:,2]\n",
    "spline_instance.nheads= 5\n",
    "spline_instance.input_map = analysis_data_map\n",
    "\n",
    "\n",
    "params = list(df.iloc[day-1][:-1])\n",
    "params = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "cov_map = spline_instance.cov_structure_saver(params)\n",
    "out = spline_instance.vecchia_nll_using_spline(params, cov_map)\n",
    "\n",
    "\n",
    "optimizer, scheduler = spline_instance.optimizer_fun(params, lr=0.02, betas=(0.9, 0.99), eps=1e-8, step_size=100, gamma=0.2)  \n",
    "out, epoch = spline_instance.fit_vecchia(params, optimizer,scheduler, epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce1e4e2",
   "metadata": {},
   "source": [
    "# Saved files below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "81cd5e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class spline:\n",
    "    def __init__(self, epsilon, coarse_factor, k, smooth):\n",
    "        self.smooth = torch.tensor(smooth, dtype= torch.float64)\n",
    "        self.k = k\n",
    "        self.coarse_factor = coarse_factor\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def compute_cov(self, params) :\n",
    "         # fit_distances and flat_distances both 1d\n",
    "        sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "        distances, non_zero_indices = instance_2.precompute_coords_anisotropy(params, aggregated_data[:,:4],aggregated_data[:,:4])\n",
    "        \n",
    "        flat_distances = distances.flatten()\n",
    "        fit_distances = torch.linspace(self.epsilon, torch.max(flat_distances), len(flat_distances) // self.coarse_factor)\n",
    "\n",
    "        # fit_distances = torch.zeros_like(distances)\n",
    "        # print(fit_distances.shape)\n",
    "        # Compute the covariance for non-zero distances\n",
    "        non_zero_indices = fit_distances != 0\n",
    "        out = torch.zeros_like(fit_distances, dtype= torch.float64)\n",
    "\n",
    "        if torch.any(non_zero_indices):\n",
    "            tmp = kv(self.smooth, torch.sqrt(fit_distances[non_zero_indices])).double().clone()\n",
    "            out[non_zero_indices] = (sigmasq * (2**(1-self.smooth)) / gamma(self.smooth) *\n",
    "                                    (torch.sqrt(fit_distances[non_zero_indices]) ) ** self.smooth *\n",
    "                                    tmp)\n",
    "        out[~non_zero_indices] = sigmasq\n",
    "\n",
    "        # print(out.shape)\n",
    "        #         \n",
    "        # Compute spline coefficients\n",
    "        coeffs = natural_cubic_spline_coeffs(fit_distances, out.unsqueeze(1))\n",
    "\n",
    "        # Create spline object\n",
    "        spline = NaturalCubicSpline(coeffs)\n",
    "        # Interpolate using the spline\n",
    "        out = spline.evaluate(distances)\n",
    "        out = out.reshape(distances.shape)\n",
    "        out += torch.eye(out.shape[0], dtype=torch.float64) * nugget \n",
    "        return out\n",
    "     \n",
    "    def full_likelihood(self,params: torch.Tensor, input_np: torch.Tensor, y: torch.Tensor, cov_matrix) -> torch.Tensor:\n",
    "        input_arr = input_np[:, :4]  ## input_np is aggregated data over a day.\n",
    "        y_arr = y\n",
    "\n",
    "        # Compute the covariance matrix\n",
    "        # cov_matrix = covariance_function(params=params, y=input_arr, x=input_arr)\n",
    "        \n",
    "        # Compute the log determinant of the covariance matrix\n",
    "        sign, log_det = torch.slogdet(cov_matrix)\n",
    "        # if sign <= 0:\n",
    "        #     raise ValueError(\"Covariance matrix is not positive definite\")\n",
    "        \n",
    "        # Extract locations\n",
    "        locs = input_arr[:, :2]\n",
    "\n",
    "        # Compute beta\n",
    "        tmp1 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, locs))\n",
    "        tmp2 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, y_arr))\n",
    "        beta = torch.linalg.solve(tmp1, tmp2)\n",
    "\n",
    "        # Compute the mean\n",
    "        mu = torch.matmul(locs, beta)\n",
    "        y_mu = y_arr - mu\n",
    "\n",
    "        # Compute the quadratic form\n",
    "        quad_form = torch.matmul(y_mu, torch.linalg.solve(cov_matrix, y_mu))\n",
    "\n",
    "        # Compute the negative log likelihood\n",
    "        neg_log_lik = 0.5 * (log_det + quad_form)\n",
    "     \n",
    "        return  neg_log_lik\n",
    "    \n",
    "    def compute_full_nll(self, params, covariance_function):\n",
    "        cov_mat = covariance_function(params) \n",
    "        nll = self.full_likelihood( params,aggregated_data[:,:4], aggregated_data[:,2], cov_mat)\n",
    "        return nll\n",
    "\n",
    "    def optimizer_fun(self, params, lr=0.01, betas=(0.9, 0.8), eps=1e-8, step_size=40, gamma=0.5):\n",
    "        optimizer = torch.optim.Adam([params], lr=lr, betas=betas, eps=eps)\n",
    "        scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)  # Decrease LR by a factor of 0.1 every 10 epochs\n",
    "        return optimizer, scheduler\n",
    "\n",
    "   # use adpating lr\n",
    "    def run_full(self, params, optimizer, scheduler,  covariance_function, epochs=10 ):\n",
    "        prev_loss= float('inf')\n",
    "\n",
    "        tol = 1e-4  # Convergence tolerance\n",
    "        for epoch in range(epochs):  # Number of epochs\n",
    "            optimizer.zero_grad()  # Zero the gradients \n",
    "            \n",
    "            loss = self.compute_full_nll(params, covariance_function)\n",
    "            loss.backward()  # Backpropagate the loss\n",
    "            \n",
    "            # Print gradients and parameters every 10th epoch\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "            \n",
    "            # if epoch % 500 == 0:\n",
    "            #     print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "            \n",
    "            optimizer.step()  # Update the parameters\n",
    "            scheduler.step()  # Update the learning rate\n",
    "            # Check for convergence\n",
    "            if abs(prev_loss - loss.item()) < tol:\n",
    "                print(f\"Converged at epoch {epoch}\")\n",
    "                print(f'Epoch {epoch+1}, : Loss: {loss.item()}, \\n vecc Parameters: {params.detach().numpy()}')\n",
    "                break\n",
    "\n",
    "            prev_loss = loss.item()\n",
    "        print(f'FINAL STATE: Epoch {epoch+1}, Loss: {loss.item()}, \\n vecc Parameters: {params.detach().numpy()}')\n",
    "        return params.detach().numpy().tolist() + [ loss.item()], epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bf7ade",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e5ea7",
   "metadata": {},
   "source": [
    "# Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426935a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params)\n",
    "\n",
    "instance_2 = kernels.vecchia_experiment(1.0, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "instance = spline( epsilon = 1e-8, coarse_factor = 4, k=3, smooth= 0.5)\n",
    "# optimizer, scheduler =  instance.optimizer_fun(params, lr= 0.01 , betas=(0.9, 0.99), eps=1e-8, step_size= 5, gamma=0.1)    \n",
    "optimizer, scheduler = instance.optimizer_fun(params, lr=0.03, betas=(0.9, 0.99), eps=1e-8, step_size=100, gamma=0.9)  \n",
    "out, epoch = instance.run_full(params, optimizer,scheduler, instance.compute_cov, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d69fb24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, 10.9571,  9.3482,  ...,  8.2249,  1.1083,  0.9382],\n",
       "        [10.9571,  0.0000, 30.5686,  ..., 26.7820, 15.4784, 11.9212],\n",
       "        [ 9.3482, 30.5686,  0.0000,  ...,  1.0330,  9.0359, 11.9142],\n",
       "        ...,\n",
       "        [ 8.2249, 26.7820,  1.0330,  ...,  0.0000,  6.6330,  9.0040],\n",
       "        [ 1.1083, 15.4784,  9.0359,  ...,  6.6330,  0.0000,  0.3387],\n",
       "        [ 0.9382, 11.9212, 11.9142,  ...,  9.0040,  0.3387,  0.0000]],\n",
       "       dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1 = splinenn.evaluate(distances)\n",
    "out1 = out1.reshape(distances.shape)\n",
    "\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "41c5a49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25600])\n",
      "torch.Size([25600])\n",
      "tensor([[27.5107,  0.9052,  1.1654,  ...,  1.4087,  8.6523,  9.4119],\n",
      "        [ 0.9052, 27.5107,  0.0984,  ...,  0.1402,  0.4850,  0.7849],\n",
      "        [ 1.1654,  0.0984, 27.5107,  ...,  8.9731,  1.2270,  0.7858],\n",
      "        ...,\n",
      "        [ 1.4087,  0.1402,  8.9731,  ..., 27.5107,  1.8872,  1.2336],\n",
      "        [ 8.6523,  0.4850,  1.2270,  ...,  1.8872, 27.5107, 13.8542],\n",
      "        [ 9.4119,  0.7849,  0.7858,  ...,  1.2336, 13.8542, 27.5107]],\n",
      "       dtype=torch.float64, grad_fn=<AsStridedBackward0>)\n",
      "tensor([[27.5107,  0.9052,  1.1654,  ...,  1.4087,  8.6523,  9.4119],\n",
      "        [ 0.9052, 27.5107,  0.0984,  ...,  0.1402,  0.4850,  0.7849],\n",
      "        [ 1.1654,  0.0984, 27.5107,  ...,  8.9731,  1.2270,  0.7858],\n",
      "        ...,\n",
      "        [ 1.4087,  0.1402,  8.9731,  ..., 27.5107,  1.8872,  1.2336],\n",
      "        [ 8.6523,  0.4850,  1.2270,  ...,  1.8872, 27.5107, 13.8542],\n",
      "        [ 9.4119,  0.7849,  0.7858,  ...,  1.2336, 13.8542, 27.5107]],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2283.8084, dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smooth = 0.5\n",
    "\n",
    "instance_2 = kernels.vecchia_experiment(smooth, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "instance = spline( epsilon = 1e-15, coarse_factor = 2, k=3, smooth= smooth)\n",
    "\n",
    "distances, non_zero_indices = instance_2.precompute_coords_anisotropy(params, aggregated_data[:,:4],aggregated_data[:,:4])\n",
    "\n",
    "flat_distances = distances.flatten()\n",
    "sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "epsilon = 0\n",
    "coarse_factor = 100\n",
    "\n",
    "fit_distances = torch.linspace(epsilon, torch.max(flat_distances), len(flat_distances) // coarse_factor)\n",
    "print(fit_distances.shape)\n",
    "# Compute the covariance for non-zero distances\n",
    "non_zero_indices = fit_distances != 0\n",
    "out = torch.zeros_like(fit_distances, dtype= torch.float64)\n",
    "\n",
    "if torch.any(non_zero_indices):\n",
    "    tmp = kv(smooth, torch.sqrt(fit_distances[non_zero_indices])).double().clone()\n",
    "    out[non_zero_indices] = (sigmasq * (2**(1-smooth)) / gamma(smooth) *\n",
    "                            (torch.sqrt(fit_distances[non_zero_indices]) ) ** smooth *\n",
    "                            tmp)\n",
    "    \n",
    "out[~non_zero_indices] = sigmasq\n",
    "\n",
    "print(out.shape)\n",
    "\n",
    "# Compute spline coefficients\n",
    "coeffs = natural_cubic_spline_coeffs(fit_distances, out.unsqueeze(1))\n",
    "\n",
    "# Create spline object\n",
    "splinenn = NaturalCubicSpline(coeffs)\n",
    "\n",
    "# Interpolate using the spline\n",
    "out1 = splinenn.evaluate(distances)\n",
    "out1 = out1.reshape(distances.shape)\n",
    "out1 += torch.eye(out1.shape[0], dtype=torch.float64) * nugget \n",
    "\n",
    "print(out1)\n",
    "out2 = instance_2.matern_cov_anisotropy_kv(params, aggregated_data[:,:4],aggregated_data[:,:4])\n",
    "\n",
    "\n",
    "print(out2)\n",
    "instance.full_likelihood( params,aggregated_data[:,:4], aggregated_data[:,2], out1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
