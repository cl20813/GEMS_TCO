{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "# sys.path.append(gems_tco_path)\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings\n",
    "from GEMS_TCO import load_data_local_computer\n",
    "\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.func import grad, hessian, jacfwd, jacrev\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import copy                    # clone tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Two options: 1. torch.autograd 2. torch.func (recommended for both gradients and hessians)\n",
    "\n",
    "Observations:\n",
    "- In order to track gradients, ```sqrt()``` in distance function has to be removed and put ```sqrt(distance function output)``` in covariance function.   \n",
    "\n",
    "- If dtypes don't match, both autograd and torch.func cannot track hessians, so consider ```.to(torch.float64)``` so ``` aggregated_data[:,:4].torch.float64()```   \n",
    "for the consistency.\n",
    "Actually, it turns out that if I use ```float32```, then autograd derivative can be different from analytical derivative by ```0.001 ~ 0.004```. \n",
    "\n",
    "the difference is on the order of one-thousandth \n",
    "\n",
    "- For hessians, torch.func is recommended. ``` torch.autograd.functional.hessian(compute_loss, params)``` this doesn't work.   \n",
    "\n",
    "- It seems there is nontrivial difference between float32 and float64 settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD estimates for July 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sigmasq</th>\n",
       "      <th>range_lat</th>\n",
       "      <th>range_lon</th>\n",
       "      <th>advec_lat</th>\n",
       "      <th>advec_lon</th>\n",
       "      <th>beta</th>\n",
       "      <th>nugget</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.793444</td>\n",
       "      <td>1.584529</td>\n",
       "      <td>1.718248</td>\n",
       "      <td>0.009089</td>\n",
       "      <td>-0.107299</td>\n",
       "      <td>0.131038</td>\n",
       "      <td>2.717239</td>\n",
       "      <td>14068.529297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.424301</td>\n",
       "      <td>1.997055</td>\n",
       "      <td>1.942683</td>\n",
       "      <td>0.043588</td>\n",
       "      <td>-0.072679</td>\n",
       "      <td>0.137124</td>\n",
       "      <td>1.513148</td>\n",
       "      <td>12357.715820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.009497</td>\n",
       "      <td>1.215236</td>\n",
       "      <td>1.558868</td>\n",
       "      <td>0.023392</td>\n",
       "      <td>-0.150548</td>\n",
       "      <td>0.199850</td>\n",
       "      <td>2.890678</td>\n",
       "      <td>14948.140625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.701347</td>\n",
       "      <td>1.612308</td>\n",
       "      <td>1.822960</td>\n",
       "      <td>-0.164069</td>\n",
       "      <td>-0.237443</td>\n",
       "      <td>0.131595</td>\n",
       "      <td>3.636499</td>\n",
       "      <td>14786.204102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.598671</td>\n",
       "      <td>2.901185</td>\n",
       "      <td>3.722327</td>\n",
       "      <td>-0.011729</td>\n",
       "      <td>-0.152072</td>\n",
       "      <td>0.072866</td>\n",
       "      <td>2.397249</td>\n",
       "      <td>12096.261719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sigmasq  range_lat  range_lon  advec_lat  advec_lon      beta    nugget  \\\n",
       "0  24.793444   1.584529   1.718248   0.009089  -0.107299  0.131038  2.717239   \n",
       "1  24.424301   1.997055   1.942683   0.043588  -0.072679  0.137124  1.513148   \n",
       "2  26.009497   1.215236   1.558868   0.023392  -0.150548  0.199850  2.890678   \n",
       "3  24.701347   1.612308   1.822960  -0.164069  -0.237443  0.131595  3.636499   \n",
       "4  22.598671   2.901185   3.722327  -0.011729  -0.152072  0.072866  2.397249   \n",
       "\n",
       "           loss  \n",
       "0  14068.529297  \n",
       "1  12357.715820  \n",
       "2  14948.140625  \n",
       "3  14786.204102  \n",
       "4  12096.261719  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_lon_resolution = [8,8]\n",
    "day = 1\n",
    "mm_cond_number = 20\n",
    "\n",
    "years = ['2024']\n",
    "month_range =[7,8]\n",
    "idx_for_datamap= [ 8*(day-1),8*day]\n",
    "\n",
    "instance = load_data_local_computer()\n",
    "month_map, ord_mm, nns_map= instance.load_mm20k_data_bymonthyear( lat_lon_resolution= lat_lon_resolution, mm_cond_number=mm_cond_number,years_=years, months_=month_range)\n",
    "analysis_data_map, aggregated_data = instance.load_working_data_byday( month_map, ord_mm, nns_map, idx_for_datamap=idx_for_datamap)\n",
    "\n",
    "input_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/Exercises/st_model/estimates\"\n",
    "output_filename = 'vecchia_inter_estimates_1250_july24.csv'\n",
    "output_csv_path = os.path.join(input_path, output_filename)\n",
    "\n",
    "df = pd.read_csv(output_csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients and hessians sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input parameters: tensor([ 2.4793e+01,  1.5845e+00,  1.7182e+00,  9.0885e-03, -1.0730e-01,\n",
      "         1.3104e-01,  2.7172e+00], dtype=torch.float64, requires_grad=True)\n",
      " the gradient: (tensor([  -3.5293,    3.5674,   -3.6402,   16.3591,   81.4576, -450.7034,\n",
      "         -23.6871], dtype=torch.float64),)\n",
      " the gradient: tensor([  -3.5293,    3.5674,   -3.6402,   16.3591,   81.4576, -450.7034,\n",
      "         -23.6871], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nheads =10\n",
    "instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "\n",
    "# Convert parameters to a tensor with requires_grad=True\n",
    "params = torch.tensor(df.iloc[0, :-1].values, dtype=torch.float64, requires_grad=True)\n",
    "print(f'input parameters: {params}')\n",
    "\n",
    "# Define the function to compute the loss\n",
    "def compute_loss(params):\n",
    "    return instance.full_likelihood(params, aggregated_data[:, :4].to(torch.float64), aggregated_data[:, 2].to(torch.float64), instance.matern_cov_anisotropy_v05)\n",
    "    # return instance.vecchia_interpolation_1to6(params, instance.matern_cov_ani, 35)\n",
    "    \n",
    "# Compute the first derivative using torch.func.grad\n",
    "grad_f = torch.autograd.grad(compute_loss(params), params)\n",
    "print(f' the gradient: {grad_f}')\n",
    "\n",
    "grad_function = torch.func.grad(compute_loss)\n",
    "gradient = grad_function(params)\n",
    "print(f' the gradient: {gradient}')\n",
    "\n",
    "#[  0.9324, -43.9642, -35.9082,  59.9937, -17.1091, -76.0932,  -0.6668]\n",
    "torch.autograd.gradcheck(compute_loss, params, atol=1e-9, rtol=1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradient(vecc) * hessian (full) * gradient (vecc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.2136, dtype=torch.float64, grad_fn=<DotBackward0>)\n",
      "tensor(18.0869, dtype=torch.float64, grad_fn=<DotBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(19.9679, dtype=torch.float64, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "\n",
    "# Convert parameters to a tensor with requires_grad=True\n",
    "params = torch.tensor(df.iloc[0, :-1].values, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "params = [ 27.25, 2.18, 2.294, 4.099e-4, -0.07915, 0.0999, 3.65]   #200\n",
    "params = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "  \n",
    "\n",
    "nheads =10\n",
    "instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "\n",
    "o1 = instance.vecc_ghg_statistic(params,instance.full_likelihood, instance.vecchia_b1,instance.matern_cov_anisotropy_v05, aggregated_data[:, :4],aggregated_data[:, 2])\n",
    "print(o1)\n",
    "o2 = instance.vecc_ghg_statistic(params,instance.full_likelihood, instance.vecchia_b2,instance.matern_cov_anisotropy_v05, aggregated_data[:, :4],aggregated_data[:, 2])\n",
    "print(o2)\n",
    "\n",
    "mm_cond_number = 10\n",
    "instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "instance.vecc_ghg_statistic(params,instance.full_likelihood, instance.vecchia_b2,instance.matern_cov_anisotropy_v05, aggregated_data[:, :4],aggregated_data[:, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now compare statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cond_number of hessian 2816516.366634098\n",
      "3940.2817598903225\n",
      "-59.56466834318\n"
     ]
    }
   ],
   "source": [
    "copy_analysis_map = copy.deepcopy(analysis_data_map)\n",
    "key_order = [0,1,2,4,3,5,7,6]\n",
    "keys = list(analysis_data_map.keys())\n",
    "reordered_dict = {keys[key]: copy_analysis_map[keys[key]] for key in key_order}\n",
    "\n",
    "\n",
    "params = [ 27.25, 2.18, 2.294, 4.099e-4, -0.07915, 0.0999, 3.65]   #200\n",
    "params = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "nheads =200\n",
    "instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "fl= instance.full_likelihood(params, aggregated_data[:, :4],aggregated_data[:, 2], instance.matern_cov_anisotropy_v05)\n",
    "fs = instance.full_ghg_statistic(params,instance.full_likelihood, instance.matern_cov_anisotropy_v05, aggregated_data[:, :4],aggregated_data[:, 2])\n",
    "\n",
    "print(fl.item())\n",
    "\n",
    "print(fs.item())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm_cond_number: 20 likelihood: 3961.533734087265\n",
      "mm_cond_number: 20 likelihood: 3946.0588701555866\n",
      "mm_cond_number: 20 likelihood: 3957.1965567747948\n"
     ]
    }
   ],
   "source": [
    "nheads = 200\n",
    "params = [ 27.25, 2.18, 2.294, 4.099e-4, -0.07915, 0.0999, 3.65]   #200\n",
    "params = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "\n",
    "\n",
    "ll = instance.vecchia_b2(params, instance.matern_cov_anisotropy_v05 )\n",
    "print(f'mm_cond_number: {mm_cond_number} likelihood: {ll}')\n",
    "\n",
    "ll2 = instance.vecchia_interpolation_1to6(params, instance.matern_cov_anisotropy_v05 )\n",
    "print(f'mm_cond_number: {mm_cond_number} likelihood: {ll2}')\n",
    "\n",
    "key_order = [0,1,2,4,3,5,7,6]\n",
    "keys = list(analysis_data_map.keys())\n",
    "reordered_dict = {keys[key]: copy_analysis_map[keys[key]] for key in key_order}\n",
    "\n",
    "instance = kernels.vecchia_experiment(0.5, reordered_dict, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "ll = instance.vecchia_b2(params, instance.matern_cov_anisotropy_v05 )\n",
    "print(f'mm_cond_number: {mm_cond_number} likelihood: {ll}')\n",
    "\n",
    "key_order = [0,1,2,4,3,7,5,6]\n",
    "keys = list(copy_analysis_map.keys())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm_cond_number: 20 likelihood: 3938.502954147818, statistic:-58.767466763382814\n",
      "mm_cond_number: 20 likelihood: 3936.1105450710866, statistic:-58.611760282523576\n",
      "mm_cond_number: 20 likelihood: 3940.224792674919, statistic:-58.56296216905197\n"
     ]
    }
   ],
   "source": [
    "nheads = 300\n",
    "instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "\n",
    "o1= instance.vecc_ghg_statistic(params,instance.full_likelihood, instance.vecchia_b2,instance.matern_cov_anisotropy_v05, aggregated_data[:, :4],aggregated_data[:, 2])\n",
    "\n",
    "ll = instance.vecchia_b2(params, instance.matern_cov_anisotropy_v05 )\n",
    "print(f'mm_cond_number: {mm_cond_number} likelihood: {ll}, statistic:{o1}')\n",
    "\n",
    "o1= instance.vecc_ghg_statistic(params,instance.full_likelihood, instance.vecchia_interpolation_1to6,instance.matern_cov_anisotropy_v05, aggregated_data[:, :4],aggregated_data[:, 2])\n",
    "\n",
    "ll = instance.vecchia_interpolation_1to6(params, instance.matern_cov_anisotropy_v05 )\n",
    "print(f'mm_cond_number: {mm_cond_number} likelihood: {ll}, statistic:{o1}')\n",
    "\n",
    "\n",
    "\n",
    "key_order = [0,1,2,4,3,5,7,6]\n",
    "keys = list(copy_analysis_map.keys())\n",
    "reordered_dict = {keys[key]: copy_analysis_map[keys[key]] for key in key_order}\n",
    "\n",
    "instance = kernels.vecchia_experiment(0.5, reordered_dict, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "\n",
    "o1= instance.vecc_ghg_statistic(params,instance.full_likelihood, instance.vecchia_b2, instance.matern_cov_anisotropy_v05, aggregated_data[:, :4],aggregated_data[:, 2])\n",
    "ll = instance.vecchia_b2(params, instance.matern_cov_anisotropy_v05 )\n",
    "print(f'mm_cond_number: {mm_cond_number} likelihood: {ll}, statistic:{o1}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "fl= instance.full_likelihood(params, aggregated_data[:, :4],aggregated_data[:, 2], instance.matern_cov_anisotropy_v05)\n",
    "fs = instance.full_ghg_statistic(params,instance.full_likelihood, instance.matern_cov_anisotropy_v05, aggregated_data[:, :4],aggregated_data[:, 2])\n",
    "\n",
    "mm_cond_number = 10\n",
    "\n",
    "instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "\n",
    "o1= instance.vecc_ghg_statistic(params,instance.full_likelihood, instance.vecchia_b2,instance.matern_cov_anisotropy_v05, aggregated_data[:, :4],aggregated_data[:, 2])\n",
    "ll = instance.vecchia_b2(params, instance.matern_cov_anisotropy_v05 )\n",
    "print(f'mm_cond_number: {mm_cond_number} likelihood: {ll}, statistic:{o1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vary the size of conditioning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cond_number of hessian 189194.83871267262\n",
      "full likelihood: 2547.258276245673, full statistic: 5.100717330882949\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 5 likelihood: 2571.7202011676554, statistic:4.810536260238044\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 6 likelihood: 2569.9784416212933, statistic:4.106628202046433\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 7 likelihood: 2567.640680723155, statistic:4.602141260135241\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 8 likelihood: 2567.6782831405317, statistic:5.079131623977824\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 9 likelihood: 2568.0001926920804, statistic:5.343090709089482\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 10 likelihood: 2566.523313914881, statistic:5.518904441534436\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 11 likelihood: 2566.7857736093847, statistic:5.4051722056787765\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 12 likelihood: 2568.124282603543, statistic:5.0296126600381275\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 13 likelihood: 2568.233047121573, statistic:5.250201889848884\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 14 likelihood: 2567.7039694302566, statistic:5.450737014135071\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 15 likelihood: 2568.0358535771416, statistic:5.417689812997492\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 16 likelihood: 2568.219484059257, statistic:5.164314269337584\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 17 likelihood: 2568.4759060207284, statistic:5.039571475176778\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 18 likelihood: 2568.5694376562374, statistic:4.8627871727010685\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 19 likelihood: 2568.953714275638, statistic:5.170509997209056\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 20 likelihood: 2569.1609961219333, statistic:5.325902938501164\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 21 likelihood: 2569.1609961219333, statistic:5.325902938501164\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 22 likelihood: 2569.1609961219333, statistic:5.325902938501164\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 23 likelihood: 2569.1609961219333, statistic:5.325902938501164\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 24 likelihood: 2569.1609961219333, statistic:5.325902938501164\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 25 likelihood: 2569.1609961219333, statistic:5.325902938501164\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 26 likelihood: 2569.1609961219333, statistic:5.325902938501164\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 27 likelihood: 2569.1609961219333, statistic:5.325902938501164\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 28 likelihood: 2569.1609961219333, statistic:5.325902938501164\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 29 likelihood: 2569.1609961219333, statistic:5.325902938501164\n"
     ]
    }
   ],
   "source": [
    "instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "fl= instance.full_likelihood(params, aggregated_data[:, :4],aggregated_data[:, 2], instance.matern_cov_anisotropy_v05)\n",
    "fs = instance.full_ghg_statistic(params,instance.full_likelihood, instance.matern_cov_anisotropy_v05, aggregated_data[:, :4],aggregated_data[:, 2])\n",
    "\n",
    "print(f'full likelihood: {fl}, full statistic: {fs}')\n",
    "for i in range(5,30):\n",
    "    mm_cond_number = i\n",
    "    instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "    \n",
    "    o1= instance.vecc_ghg_statistic(params,instance.full_likelihood, instance.vecchia_b2,instance.matern_cov_anisotropy_v05, aggregated_data[:, :4],aggregated_data[:, 2])\n",
    "    ll = instance.vecchia_b2(params, instance.matern_cov_anisotropy_v05 )\n",
    "    print(f'mm_cond_number: {i} likelihood: {ll}, statistic:{o1}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
