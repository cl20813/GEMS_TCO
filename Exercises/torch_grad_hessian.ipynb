{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "# sys.path.append(gems_tco_path)\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings\n",
    "from GEMS_TCO import load_data_local_computer\n",
    "\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.func import grad, hessian, jacfwd, jacrev\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import copy                    # clone tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Two options: 1. torch.autograd 2. torch.func (recommended for both gradients and hessians)\n",
    "\n",
    "Observations:\n",
    "- In order to track gradients, ```sqrt()``` in distance function has to be removed and put ```sqrt(distance function output)``` in covariance function.   \n",
    "\n",
    "- If dtypes don't match, both autograd and torch.func cannot track hessians, so consider ```.to(torch.float64)``` so ``` aggregated_data[:,:4].torch.float64()```   \n",
    "for the consistency.\n",
    "Actually, it turns out that if I use ```float32```, then autograd derivative can be different from analytical derivative by ```0.001 ~ 0.004```. \n",
    "\n",
    "the difference is on the order of one-thousandth \n",
    "\n",
    "- For hessians, torch.func is recommended. ``` torch.autograd.functional.hessian(compute_loss, params)``` this doesn't work.   \n",
    "\n",
    "- It seems there is nontrivial difference between float32 and float64 settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD estimates for July 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sigmasq</th>\n",
       "      <th>range_lat</th>\n",
       "      <th>range_lon</th>\n",
       "      <th>advec_lat</th>\n",
       "      <th>advec_lon</th>\n",
       "      <th>beta</th>\n",
       "      <th>nugget</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.793444</td>\n",
       "      <td>1.584529</td>\n",
       "      <td>1.718248</td>\n",
       "      <td>0.009089</td>\n",
       "      <td>-0.107299</td>\n",
       "      <td>0.131038</td>\n",
       "      <td>2.717239</td>\n",
       "      <td>14068.529297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.424301</td>\n",
       "      <td>1.997055</td>\n",
       "      <td>1.942683</td>\n",
       "      <td>0.043588</td>\n",
       "      <td>-0.072679</td>\n",
       "      <td>0.137124</td>\n",
       "      <td>1.513148</td>\n",
       "      <td>12357.715820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.009497</td>\n",
       "      <td>1.215236</td>\n",
       "      <td>1.558868</td>\n",
       "      <td>0.023392</td>\n",
       "      <td>-0.150548</td>\n",
       "      <td>0.199850</td>\n",
       "      <td>2.890678</td>\n",
       "      <td>14948.140625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.701347</td>\n",
       "      <td>1.612308</td>\n",
       "      <td>1.822960</td>\n",
       "      <td>-0.164069</td>\n",
       "      <td>-0.237443</td>\n",
       "      <td>0.131595</td>\n",
       "      <td>3.636499</td>\n",
       "      <td>14786.204102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.598671</td>\n",
       "      <td>2.901185</td>\n",
       "      <td>3.722327</td>\n",
       "      <td>-0.011729</td>\n",
       "      <td>-0.152072</td>\n",
       "      <td>0.072866</td>\n",
       "      <td>2.397249</td>\n",
       "      <td>12096.261719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sigmasq  range_lat  range_lon  advec_lat  advec_lon      beta    nugget  \\\n",
       "0  24.793444   1.584529   1.718248   0.009089  -0.107299  0.131038  2.717239   \n",
       "1  24.424301   1.997055   1.942683   0.043588  -0.072679  0.137124  1.513148   \n",
       "2  26.009497   1.215236   1.558868   0.023392  -0.150548  0.199850  2.890678   \n",
       "3  24.701347   1.612308   1.822960  -0.164069  -0.237443  0.131595  3.636499   \n",
       "4  22.598671   2.901185   3.722327  -0.011729  -0.152072  0.072866  2.397249   \n",
       "\n",
       "           loss  \n",
       "0  14068.529297  \n",
       "1  12357.715820  \n",
       "2  14948.140625  \n",
       "3  14786.204102  \n",
       "4  12096.261719  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_lon_resolution = [2,2]\n",
    "day = 2\n",
    "mm_cond_number = 20\n",
    "\n",
    "years = ['2024']\n",
    "month_range =[7,8]\n",
    "idx_for_datamap= [ 8*(day-1),8*day]\n",
    "\n",
    "instance = load_data_local_computer()\n",
    "month_map, ord_mm, nns_map= instance.load_mm20k_data_bymonthyear( lat_lon_resolution= lat_lon_resolution, mm_cond_number=mm_cond_number,years_=years, months_=month_range)\n",
    "analysis_data_map, aggregated_data = instance.load_working_data_byday( month_map, ord_mm, nns_map, idx_for_datamap=idx_for_datamap)\n",
    "\n",
    "input_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/Exercises/st_model/estimates\"\n",
    "output_filename = 'vecchia_inter_estimates_1250_july24.csv'\n",
    "output_csv_path = os.path.join(input_path, output_filename)\n",
    "\n",
    "df = pd.read_csv(output_csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients and hessians sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input parameters: tensor([ 2.4793e+01,  1.5845e+00,  1.7182e+00,  9.0885e-03, -1.0730e-01,\n",
      "         1.3104e-01,  2.7172e+00], dtype=torch.float64, requires_grad=True)\n",
      " the gradient: (tensor([ -1.1415,  -8.8638,  -0.3739,  24.1656, -43.6310, -94.4988,  -6.3544],\n",
      "       dtype=torch.float64),)\n",
      " the gradient: tensor([ -1.1415,  -8.8638,  -0.3739,  24.1656, -43.6310, -94.4988,  -6.3544],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nheads =10\n",
    "instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "\n",
    "# Convert parameters to a tensor with requires_grad=True\n",
    "params = torch.tensor(df.iloc[0, :-1].values, dtype=torch.float64, requires_grad=True)\n",
    "print(f'input parameters: {params}')\n",
    "\n",
    "# Define the function to compute the loss\n",
    "def compute_loss(params):\n",
    "    return instance.full_likelihood(params, aggregated_data[:, :4].to(torch.float64), aggregated_data[:, 2].to(torch.float64), instance.matern_cov_anisotropy_v05)\n",
    "    # return instance.vecchia_interpolation_1to6(params, instance.matern_cov_ani, 35)\n",
    "    \n",
    "# Compute the first derivative using torch.func.grad\n",
    "grad_f = torch.autograd.grad(compute_loss(params), params)\n",
    "print(f' the gradient: {grad_f}')\n",
    "\n",
    "grad_function = torch.func.grad(compute_loss)\n",
    "gradient = grad_function(params)\n",
    "print(f' the gradient: {gradient}')\n",
    "\n",
    "#[  0.9324, -43.9642, -35.9082,  59.9937, -17.1091, -76.0932,  -0.6668]\n",
    "torch.autograd.gradcheck(compute_loss, params, atol=1e-9, rtol=1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradient(vecc) * hessian (full) * gradient (vecc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.9017, dtype=torch.float64, grad_fn=<DotBackward0>)\n",
      "tensor(5.3259, dtype=torch.float64, grad_fn=<DotBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(5.5189, dtype=torch.float64, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "\n",
    "# Convert parameters to a tensor with requires_grad=True\n",
    "params = torch.tensor(df.iloc[0, :-1].values, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "params = [ 27.25, 2.18, 2.294, 4.099e-4, -0.07915, 0.0999, 3.65]   #200\n",
    "params = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "  \n",
    "\n",
    "nheads =10\n",
    "instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "\n",
    "o1 = instance.vecc_ghg_statistic(params,instance.full_likelihood, instance.vecchia_b1,instance.matern_cov_anisotropy_v05, aggregated_data[:, :4],aggregated_data[:, 2])\n",
    "print(o1)\n",
    "o2 = instance.vecc_ghg_statistic(params,instance.full_likelihood, instance.vecchia_b2,instance.matern_cov_anisotropy_v05, aggregated_data[:, :4],aggregated_data[:, 2])\n",
    "print(o2)\n",
    "\n",
    "mm_cond_number = 10\n",
    "instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "instance.vecc_ghg_statistic(params,instance.full_likelihood, instance.vecchia_b2,instance.matern_cov_anisotropy_v05, aggregated_data[:, :4],aggregated_data[:, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now compare statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_analysis_map = copy.deepcopy(analysis_data_map)\n",
    "key_order = [0,1,2,4,3,5,7,6]\n",
    "keys = list(copy_analysis_map.keys())\n",
    "reordered_dict = {keys[key]: copy_analysis_map[keys[key]] for key in key_order}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12811.132541513696\n"
     ]
    }
   ],
   "source": [
    "params = [ 27.25, 2.18, 2.294, 4.099e-4, -0.07915, 0.0999, 3.65]   #200\n",
    "params = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "  \n",
    "\n",
    "nheads =200\n",
    "instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "fl= instance.full_likelihood(params, aggregated_data[:, :4],aggregated_data[:, 2], instance.matern_cov_anisotropy_v05)\n",
    "# fs = instance.full_ghg_statistic(params,instance.full_likelihood, instance.matern_cov_anisotropy_v05, aggregated_data[:, :4],aggregated_data[:, 2])\n",
    "\n",
    "print(fl.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm_cond_number: 20 likelihood: 46431.319637458175\n",
      "mm_cond_number: 20 likelihood: 46236.93969215202\n",
      "mm_cond_number: 20 likelihood: 46381.45128941246\n"
     ]
    }
   ],
   "source": [
    "instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "\n",
    "nheads = 400\n",
    "ll = instance.vecchia_b2(params, instance.matern_cov_anisotropy_v05 )\n",
    "print(f'mm_cond_number: {mm_cond_number} likelihood: {ll}')\n",
    "\n",
    "ll = instance.vecchia_interpolation_1to6(params, instance.matern_cov_anisotropy_v05 )\n",
    "print(f'mm_cond_number: {mm_cond_number} likelihood: {ll}')\n",
    "\n",
    "key_order = [0,1,2,4,3,5,7,6]\n",
    "keys = list(copy_analysis_map.keys())\n",
    "reordered_dict = {keys[key]: copy_analysis_map[keys[key]] for key in key_order}\n",
    "\n",
    "instance = kernels.vecchia_experiment(0.5, reordered_dict, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "ll = instance.vecchia_b2(params, instance.matern_cov_anisotropy_v05 )\n",
    "print(f'mm_cond_number: {mm_cond_number} likelihood: {ll}')\n",
    "\n",
    "key_order = [0,1,2,4,3,7,5,6]\n",
    "keys = list(copy_analysis_map.keys())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m instance = kernels.vecchia_experiment(\u001b[32m0.5\u001b[39m, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m o1= \u001b[43minstance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvecc_ghg_statistic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfull_likelihood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvecchia_b2\u001b[49m\u001b[43m,\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatern_cov_anisotropy_v05\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregated_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43maggregated_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m ll = instance.vecchia_b2(params, instance.matern_cov_anisotropy_v05 )\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmm_cond_number: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmm_cond_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m likelihood: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mll\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, statistic:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/kernels.py:495\u001b[39m, in \u001b[36mvecchia_experiment.vecc_ghg_statistic\u001b[39m\u001b[34m(self, full_params, full_ll, vecc_ll, cov_function, aggregated_data, aggregated_y)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;66;03m# print(f'Gradient: {g1}')\u001b[39;00m\n\u001b[32m    493\u001b[39m \u001b[38;5;66;03m# Compute the Hessian matrix using torch.func.hessian\u001b[39;00m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     hessian_matrix =  \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhessian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_loss_full\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    496\u001b[39m     cond_number = torch.linalg.cond(hessian_matrix)\n\u001b[32m    497\u001b[39m     \u001b[38;5;66;03m# print(f'cond_number of hessian {cond_number}')\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/_functorch/eager_transforms.py:1310\u001b[39m, in \u001b[36mjacfwd.<locals>.wrapper_fn\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m   1307\u001b[39m     _, jvp_out = output\n\u001b[32m   1308\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m jvp_out\n\u001b[32m-> \u001b[39m\u001b[32m1310\u001b[39m results = \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpush_jvp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[32m   1312\u001b[39m     results, aux = results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/_functorch/apis.py:203\u001b[39m, in \u001b[36mvmap.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/_functorch/vmap.py:331\u001b[39m, in \u001b[36mvmap_impl\u001b[39m\u001b[34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[32m    321\u001b[39m         func,\n\u001b[32m    322\u001b[39m         flat_in_dims,\n\u001b[32m   (...)\u001b[39m\u001b[32m    327\u001b[39m         **kwargs,\n\u001b[32m    328\u001b[39m     )\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/_functorch/vmap.py:479\u001b[39m, in \u001b[36m_flat_vmap\u001b[39m\u001b[34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[32m    476\u001b[39m     batched_inputs = _create_batched_inputs(\n\u001b[32m    477\u001b[39m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[32m    478\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     batched_outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/_functorch/eager_transforms.py:1299\u001b[39m, in \u001b[36mjacfwd.<locals>.wrapper_fn.<locals>.push_jvp\u001b[39m\u001b[34m(basis)\u001b[39m\n\u001b[32m   1298\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpush_jvp\u001b[39m(basis):\n\u001b[32m-> \u001b[39m\u001b[32m1299\u001b[39m     output = \u001b[43m_jvp_with_argnums\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m=\u001b[49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_aux\u001b[49m\n\u001b[32m   1301\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1302\u001b[39m     \u001b[38;5;66;03m# output[0] is the output of `func(*args)`\u001b[39;00m\n\u001b[32m   1303\u001b[39m     error_if_complex(\u001b[33m\"\u001b[39m\u001b[33mjacfwd\u001b[39m\u001b[33m\"\u001b[39m, output[\u001b[32m0\u001b[39m], is_input=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/_functorch/eager_transforms.py:1139\u001b[39m, in \u001b[36m_jvp_with_argnums\u001b[39m\u001b[34m(func, primals, tangents, argnums, strict, has_aux)\u001b[39m\n\u001b[32m   1137\u001b[39m     primals = _wrap_all_tensors(primals, level)\n\u001b[32m   1138\u001b[39m     duals = _replace_args(primals, duals, argnums)\n\u001b[32m-> \u001b[39m\u001b[32m1139\u001b[39m result_duals = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mduals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[32m   1141\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(result_duals, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result_duals) == \u001b[32m2\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/_functorch/eager_transforms.py:604\u001b[39m, in \u001b[36mjacrev.<locals>.wrapper_fn\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    602\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper_fn\u001b[39m(*args):\n\u001b[32m    603\u001b[39m     error_if_complex(\u001b[33m\"\u001b[39m\u001b[33mjacrev\u001b[39m\u001b[33m\"\u001b[39m, args, is_input=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     vjp_out = \u001b[43m_vjp_with_argnums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m=\u001b[49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    605\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[32m    606\u001b[39m         output, vjp_fn, aux = vjp_out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/_functorch/vmap.py:48\u001b[39m, in \u001b[36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(f)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfn\u001b[39m(*args, **kwargs):\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.graph.disable_saved_tensors_hooks(message):\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/_functorch/eager_transforms.py:399\u001b[39m, in \u001b[36m_vjp_with_argnums\u001b[39m\u001b[34m(func, argnums, has_aux, *primals)\u001b[39m\n\u001b[32m    397\u001b[39m     diff_primals = _slice_argnums(primals, argnums, as_tuple=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    398\u001b[39m     tree_map_(partial(_create_differentiable, level=level), diff_primals)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m primals_out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprimals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(primals_out, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(primals_out) == \u001b[32m2\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/kernels.py:484\u001b[39m, in \u001b[36mvecchia_experiment.vecc_ghg_statistic.<locals>.compute_loss_full\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_loss_full\u001b[39m(params):\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfull_ll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregated_data\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregated_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov_function\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/kernels.py:120\u001b[39m, in \u001b[36mlikelihood_function.full_likelihood\u001b[39m\u001b[34m(self, params, input_np, y, covariance_function)\u001b[39m\n\u001b[32m    117\u001b[39m y_arr = y\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# Compute the covariance matrix\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m cov_matrix = \u001b[43mcovariance_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_arr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# Compute the log determinant of the covariance matrix\u001b[39;00m\n\u001b[32m    123\u001b[39m sign, log_det = torch.slogdet(cov_matrix)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/kernels.py:97\u001b[39m, in \u001b[36mspatio_temporal_kernels.matern_cov_anisotropy_v05\u001b[39m\u001b[34m(self, params, x, y)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmatern_cov_anisotropy_v05\u001b[39m(\u001b[38;5;28mself\u001b[39m,params: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n\u001b[32m     94\u001b[39m     sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     distance, non_zero_indices = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecompute_coords_anisotropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m     out = torch.zeros_like(distance)\n\u001b[32m    100\u001b[39m     non_zero_indices = distance != \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/kernels.py:88\u001b[39m, in \u001b[36mspatio_temporal_kernels.precompute_coords_anisotropy\u001b[39m\u001b[34m(self, params, y, x)\u001b[39m\n\u001b[32m     85\u001b[39m U = torch.cat((spat_coord1, (beta * t1).reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)), dim=\u001b[32m1\u001b[39m)\n\u001b[32m     86\u001b[39m V = torch.cat((spat_coord2, (beta * t2).reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)), dim=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m distance = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcustom_distance_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m non_zero_indices = distance != \u001b[32m0\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m distance, non_zero_indices\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/kernels.py:60\u001b[39m, in \u001b[36mspatio_temporal_kernels.custom_distance_matrix\u001b[39m\u001b[34m(self, U, V)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcustom_distance_matrix\u001b[39m(\u001b[38;5;28mself\u001b[39m, U, V):\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# Efficient distance computation with broadcasting\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     spatial_diff = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m     temporal_diff = torch.abs(U[:, \u001b[32m2\u001b[39m].unsqueeze(\u001b[32m1\u001b[39m) - V[:, \u001b[32m2\u001b[39m].unsqueeze(\u001b[32m0\u001b[39m))\n\u001b[32m     63\u001b[39m     distance = (spatial_diff**\u001b[32m2\u001b[39m + temporal_diff**\u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# move torch.sqrt to covariance function to track gradients of beta and avec\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/functional.py:1800\u001b[39m, in \u001b[36mnorm\u001b[39m\u001b[34m(input, p, dim, keepdim, out, dtype)\u001b[39m\n\u001b[32m   1796\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p == \u001b[33m\"\u001b[39m\u001b[33mfro\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m   1797\u001b[39m     dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dim, (\u001b[38;5;28mint\u001b[39m, torch.SymInt)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dim) <= \u001b[32m2\u001b[39m\n\u001b[32m   1798\u001b[39m ):\n\u001b[32m   1799\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvector_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1801\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\n\u001b[32m   1802\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1803\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1804\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m torch.linalg.vector_norm(\n\u001b[32m   1805\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[32m2\u001b[39m, _dim, keepdim, dtype=dtype, out=out\n\u001b[32m   1806\u001b[39m         )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "\n",
    "o1= instance.vecc_ghg_statistic(params,instance.full_likelihood, instance.vecchia_b2,instance.matern_cov_anisotropy_v05, aggregated_data[:, :4],aggregated_data[:, 2])\n",
    "ll = instance.vecchia_b2(params, instance.matern_cov_anisotropy_v05 )\n",
    "print(f'mm_cond_number: {mm_cond_number} likelihood: {ll}, statistic:{o1}')\n",
    "\n",
    "\n",
    "\n",
    "instance = kernels.vecchia_experiment(0.5, reordered_dict, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "\n",
    "ll = instance.vecchia_b2(params, instance.matern_cov_anisotropy_v05 )\n",
    "print(f'mm_cond_number: {mm_cond_number} likelihood: {ll}, statistic:{o1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "fl= instance.full_likelihood(params, aggregated_data[:, :4],aggregated_data[:, 2], instance.matern_cov_anisotropy_v05)\n",
    "fs = instance.full_ghg_statistic(params,instance.full_likelihood, instance.matern_cov_anisotropy_v05, aggregated_data[:, :4],aggregated_data[:, 2])\n",
    "\n",
    "mm_cond_number = 10\n",
    "\n",
    "instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "\n",
    "o1= instance.vecc_ghg_statistic(params,instance.full_likelihood, instance.vecchia_b2,instance.matern_cov_anisotropy_v05, aggregated_data[:, :4],aggregated_data[:, 2])\n",
    "ll = instance.vecchia_b2(params, instance.matern_cov_anisotropy_v05 )\n",
    "print(f'mm_cond_number: {mm_cond_number} likelihood: {ll}, statistic:{o1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vary the size of conditioning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cond_number of hessian 189194.83871267262\n",
      "full likelihood: 2547.258276245673, full statistic: 5.100717330882949\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 5 likelihood: 2571.7202011676554, statistic:4.810536260238044\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 6 likelihood: 2569.9784416212933, statistic:4.106628202046433\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 7 likelihood: 2567.640680723155, statistic:4.602141260135241\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 8 likelihood: 2567.6782831405317, statistic:5.079131623977824\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 9 likelihood: 2568.0001926920804, statistic:5.343090709089482\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 10 likelihood: 2566.523313914881, statistic:5.518904441534436\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 11 likelihood: 2566.7857736093847, statistic:5.4051722056787765\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 12 likelihood: 2568.124282603543, statistic:5.0296126600381275\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 13 likelihood: 2568.233047121573, statistic:5.250201889848884\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 14 likelihood: 2567.7039694302566, statistic:5.450737014135071\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 15 likelihood: 2568.0358535771416, statistic:5.417689812997492\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 16 likelihood: 2568.219484059257, statistic:5.164314269337584\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 17 likelihood: 2568.4759060207284, statistic:5.039571475176778\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 18 likelihood: 2568.5694376562374, statistic:4.8627871727010685\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 19 likelihood: 2568.953714275638, statistic:5.170509997209056\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 20 likelihood: 2569.1609961219333, statistic:5.325902938501164\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 21 likelihood: 2569.1609961219333, statistic:5.325902938501164\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 22 likelihood: 2569.1609961219333, statistic:5.325902938501164\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 23 likelihood: 2569.1609961219333, statistic:5.325902938501164\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 24 likelihood: 2569.1609961219333, statistic:5.325902938501164\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 25 likelihood: 2569.1609961219333, statistic:5.325902938501164\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 26 likelihood: 2569.1609961219333, statistic:5.325902938501164\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 27 likelihood: 2569.1609961219333, statistic:5.325902938501164\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 28 likelihood: 2569.1609961219333, statistic:5.325902938501164\n",
      "cond_number of hessian 189194.83871267262\n",
      "mm_cond_number: 29 likelihood: 2569.1609961219333, statistic:5.325902938501164\n"
     ]
    }
   ],
   "source": [
    "instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "fl= instance.full_likelihood(params, aggregated_data[:, :4],aggregated_data[:, 2], instance.matern_cov_anisotropy_v05)\n",
    "fs = instance.full_ghg_statistic(params,instance.full_likelihood, instance.matern_cov_anisotropy_v05, aggregated_data[:, :4],aggregated_data[:, 2])\n",
    "\n",
    "print(f'full likelihood: {fl}, full statistic: {fs}')\n",
    "for i in range(5,30):\n",
    "    mm_cond_number = i\n",
    "    instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "    \n",
    "    o1= instance.vecc_ghg_statistic(params,instance.full_likelihood, instance.vecchia_b2,instance.matern_cov_anisotropy_v05, aggregated_data[:, :4],aggregated_data[:, 2])\n",
    "    ll = instance.vecchia_b2(params, instance.matern_cov_anisotropy_v05 )\n",
    "    print(f'mm_cond_number: {i} likelihood: {ll}, statistic:{o1}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
