{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings\n",
    "from GEMS_TCO import load_data_local_computer\n",
    "\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.func import grad, hessian, jacfwd, jacrev\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import copy                    # clone tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Two options: 1. torch.autograd 2. torch.func (recommended for both gradients and hessians)\n",
    "\n",
    "Observations:\n",
    "- In order to track gradients, ```sqrt()``` in distance function has to be removed and put ```sqrt(distance function output)``` in covariance function.   \n",
    "- If dtypes don't match, both autograd and torch.func cannot track hessians, so consider ```.to(torch.float64)``` so ``` aggregated_data[:,:4].torch.float64()```   \n",
    "\n",
    "- For hessians, torch.func is recommended. ``` torch.autograd.functional.hessian(compute_loss, params)``` this doesn't work.   \n",
    "- It seems there is nontrivial difference between float32 and float64 settings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([25.8229,  1.0230,  1.1314,  0.0733, -0.0958,  0.1777,  1.5697],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "Gradient: tensor([  -2.3781,  -20.0865,   -9.3401,  -61.2322,   37.6362, -191.4623,\n",
      "         -14.3487], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "<function compute_loss at 0x3cf3e5260>\n"
     ]
    }
   ],
   "source": [
    "# CODE EXAMPLE\n",
    "\n",
    "# Define the function to compute the loss\n",
    "def compute_loss(params):\n",
    "    return instance.full_likelihood(params, aggregated_data[:, :4].to(torch.float64), aggregated_data[:, 2].to(torch.float64), instance.matern_cov_ani)\n",
    "    # return instance.vecchia_interpolation_1to6(params, instance.matern_cov_ani, 35)\n",
    "\n",
    "# Convert parameters to a tensor with requires_grad=True\n",
    "params = torch.tensor(df.iloc[0, :-1].values, dtype=torch.float64, requires_grad=True)\n",
    "print(params)\n",
    "\n",
    "# Compute the first derivative using torch.func.grad\n",
    "grad_f = grad(compute_loss)\n",
    "g1 = grad_f(params)\n",
    "print(f'Gradient: {g1}')\n",
    "\n",
    "# Compute the Hessian matrix using torch.func.hessian\n",
    "try:\n",
    "    hessian_matrix = hessian(compute_loss, params)      ## this is equivalent to jacfwd(jacrev(compute_loss))(params)\n",
    "    print(hessian_matrix)\n",
    "except Exception as e:\n",
    "    print(f'Error computing Hessian: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sigmasq</th>\n",
       "      <th>range_lat</th>\n",
       "      <th>range_lon</th>\n",
       "      <th>advec_lat</th>\n",
       "      <th>advec_lon</th>\n",
       "      <th>beta</th>\n",
       "      <th>nugget</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.822897</td>\n",
       "      <td>1.023014</td>\n",
       "      <td>1.131423</td>\n",
       "      <td>0.073286</td>\n",
       "      <td>-0.095810</td>\n",
       "      <td>0.177670</td>\n",
       "      <td>1.569700</td>\n",
       "      <td>50396.417969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.242847</td>\n",
       "      <td>2.147953</td>\n",
       "      <td>1.945775</td>\n",
       "      <td>0.050045</td>\n",
       "      <td>-0.070032</td>\n",
       "      <td>0.140178</td>\n",
       "      <td>1.442902</td>\n",
       "      <td>43290.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.156120</td>\n",
       "      <td>0.818184</td>\n",
       "      <td>1.061921</td>\n",
       "      <td>0.081691</td>\n",
       "      <td>-0.119475</td>\n",
       "      <td>0.241490</td>\n",
       "      <td>2.123204</td>\n",
       "      <td>54808.964844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.001230</td>\n",
       "      <td>1.194200</td>\n",
       "      <td>1.434477</td>\n",
       "      <td>-0.206528</td>\n",
       "      <td>-0.174575</td>\n",
       "      <td>0.041952</td>\n",
       "      <td>2.593778</td>\n",
       "      <td>52697.542969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.724660</td>\n",
       "      <td>1.819339</td>\n",
       "      <td>2.713623</td>\n",
       "      <td>-0.166419</td>\n",
       "      <td>-0.037361</td>\n",
       "      <td>-0.014584</td>\n",
       "      <td>1.945128</td>\n",
       "      <td>43821.054688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sigmasq  range_lat  range_lon  advec_lat  advec_lon      beta    nugget  \\\n",
       "0  25.822897   1.023014   1.131423   0.073286  -0.095810  0.177670  1.569700   \n",
       "1  24.242847   2.147953   1.945775   0.050045  -0.070032  0.140178  1.442902   \n",
       "2  26.156120   0.818184   1.061921   0.081691  -0.119475  0.241490  2.123204   \n",
       "3  26.001230   1.194200   1.434477  -0.206528  -0.174575  0.041952  2.593778   \n",
       "4  23.724660   1.819339   2.713623  -0.166419  -0.037361 -0.014584  1.945128   \n",
       "\n",
       "           loss  \n",
       "0  50396.417969  \n",
       "1  43290.984375  \n",
       "2  54808.964844  \n",
       "3  52697.542969  \n",
       "4  43821.054688  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_lon_resolution = [10,10]\n",
    "day = 1\n",
    "mm_cond_number = 20\n",
    "\n",
    "years = ['2024']\n",
    "month_range =[7,8]\n",
    "idx_for_datamap= [ 8*(day-1),8*day]\n",
    "\n",
    "instance = load_data_local_computer()\n",
    "map, ord_mm, nns_map= instance.load_mm20k_data_bymonthyear( lat_lon_resolution= lat_lon_resolution, mm_cond_number=mm_cond_number,years_=years, months_=month_range)\n",
    "analysis_data_map, aggregated_data = instance.load_working_data_byday( map, ord_mm, nns_map, idx_for_datamap=[0,8])\n",
    "\n",
    "input_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/Exercises/st_model/estimates\"\n",
    "output_filename = 'vecchia_inter_estimates_1250_july24.csv'\n",
    "output_csv_path = os.path.join(input_path, output_filename)\n",
    "\n",
    "df = pd.read_csv(output_csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class matern_advec_beta_torch_vecchia:\n",
    "    def __init__(self, analaysis_data_map: torch.Tensor, params: torch.Tensor, nns_map=nns_map, mm_cond_number=mm_cond_number):\n",
    "        \n",
    "        self.key_list = sorted(analysis_data_map)\n",
    "        self.input_map = analysis_data_map\n",
    "\n",
    "        self.mm_cond_number = mm_cond_number\n",
    "        self.nns_map = nns_map \n",
    "        self.input_map = analaysis_data_map\n",
    "        self.smooth = 0.5  \n",
    "        sample_df = analaysis_data_map[self.key_list[0]]\n",
    "\n",
    "        self.size_per_hour = len(sample_df)\n",
    "\n",
    "    def custom_distance_matrix(self, U, V):\n",
    "        # Efficient distance computation with broadcasting\n",
    "        spatial_diff = torch.norm(U[:, :2].unsqueeze(1) - V[:, :2].unsqueeze(0), dim=2)\n",
    "\n",
    "        temporal_diff = torch.abs(U[:, 2].unsqueeze(1) - V[:, 2].unsqueeze(0))\n",
    "        distance = (spatial_diff**2 + temporal_diff**2)  # move torch.sqrt to covariance function to track gradients of beta and avec\n",
    "        return distance\n",
    "    \n",
    "    def precompute_coords_ani(self, params, y: torch.Tensor, x: torch.Tensor)-> torch.Tensor:\n",
    "        sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "\n",
    "        if y is None or x is None:\n",
    "            raise ValueError(\"Both y and x_df must be provided.\")\n",
    "\n",
    "        x1 = x[:, 0]\n",
    "        y1 = x[:, 1]\n",
    "        t1 = x[:, 3]\n",
    "\n",
    "        x2 = y[:, 0]\n",
    "        y2 = y[:, 1]\n",
    "        t2 = y[:, 3]\n",
    "\n",
    "        # spat_coord1 = torch.stack((self.x1 , self.y1 - advec * self.t1), dim=-1)\n",
    "        spat_coord1 = torch.stack(( (x1 - advec_lat * t1)/range_lat, (y1 - advec_lon * t1)/range_lon ), dim=-1)\n",
    "        spat_coord2 = torch.stack(( (x2 - advec_lat * t2)/range_lat, (y2 - advec_lon * t2)/range_lon ), dim=-1)\n",
    "\n",
    "        U = torch.cat((spat_coord1, (beta * t1).reshape(-1, 1)), dim=1)\n",
    "        V = torch.cat((spat_coord2, (beta * t2).reshape(-1, 1)), dim=1)\n",
    "\n",
    "        distance = self.custom_distance_matrix(U,V)\n",
    "        non_zero_indices = distance != 0\n",
    "        return distance, non_zero_indices\n",
    "    \n",
    "    # anisotropic in three \n",
    "    def matern_cov_ani(self,params: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "        \n",
    "\n",
    "        distance, non_zero_indices = self.precompute_coords_ani(params, x,y)\n",
    "        out = torch.zeros_like(distance)\n",
    "\n",
    "        non_zero_indices = distance != 0\n",
    "        if torch.any(non_zero_indices):\n",
    "            out[non_zero_indices] = sigmasq * torch.exp(- torch.sqrt(distance[non_zero_indices]))\n",
    "        out[~non_zero_indices] = sigmasq\n",
    "\n",
    "        # Add a small jitter term to the diagonal for numerical stability\n",
    "        out += torch.eye(out.shape[0]) * nugget \n",
    "\n",
    "        return out\n",
    "    \n",
    "    def full_likelihood(self,params: torch.Tensor, input_np: torch.Tensor, y: torch.Tensor, covariance_function) -> torch.Tensor:\n",
    "        input_arr = input_np[:, :4]\n",
    "        y_arr = y\n",
    "\n",
    "        # Compute the covariance matrix\n",
    "        cov_matrix = covariance_function(params=params, y=input_arr, x=input_arr)\n",
    "        \n",
    "        # Compute the log determinant of the covariance matrix\n",
    "        sign, log_det = torch.slogdet(cov_matrix)\n",
    "        #log_det = torch.log(torch.linalg.det(cov_matrix))\n",
    "        #if sign <= 0:\n",
    "        #    raise ValueError(\"Covariance matrix is not positive definite\")\n",
    "        \n",
    "        # Extract locations\n",
    "        locs = input_arr[:, :2]\n",
    "\n",
    "        # Compute beta\n",
    "        tmp1 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, locs))\n",
    "        tmp2 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, y_arr))\n",
    "        beta = torch.linalg.solve(tmp1, tmp2)\n",
    "\n",
    "        # Compute the mean\n",
    "        mu = torch.matmul(locs, beta)\n",
    "        y_mu = y_arr - mu\n",
    "\n",
    "        # Compute the quadratic form\n",
    "        quad_form = torch.matmul(y_mu, torch.linalg.solve(cov_matrix, y_mu))\n",
    "\n",
    "        # Compute the negative log likelihood\n",
    "        neg_log_lik = 0.5 * (log_det + quad_form)\n",
    "        # neg_log_lik = 0.5 * ( log_det )\n",
    "        return  neg_log_lik \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2588.8357, grad_fn=<MulBackward0>)\n",
      "day 1 finished\n"
     ]
    }
   ],
   "source": [
    "lat_lon_resolution = [10,10]\n",
    "\n",
    "head100map = defaultdict(list)\n",
    "\n",
    "headn = 10\n",
    "b = [0]*7\n",
    "b1=b2=b3=b4=b5=b6=0\n",
    "for day in range(1,2):\n",
    "    mm_cond_number = 10\n",
    "\n",
    "    years = ['2024']\n",
    "    month_range =[7,8]\n",
    "    idx_for_datamap= [ 8*(day-1),8*day]\n",
    "\n",
    "    instance = load_data_local_computer()\n",
    "    map, ord_mm, nns_map= instance.load_mm20k_data_bymonthyear( lat_lon_resolution= lat_lon_resolution, mm_cond_number=mm_cond_number,years_=years, months_=month_range)\n",
    "    analysis_data_map, aggregated_data = instance.load_working_data_byday( map, ord_mm, nns_map, idx_for_datamap= idx_for_datamap)\n",
    "\n",
    "    params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "    params = torch.tensor(params, requires_grad=True)\n",
    "    instance = matern_advec_beta_torch_vecchia(analysis_data_map, params, nns_map, mm_cond_number)\n",
    "    out = instance.full_likelihood(params, aggregated_data[:,:4],aggregated_data[:,2], instance.matern_cov_ani)\n",
    "    print(out)  # 15105\n",
    "\n",
    "    print(f'day {day} finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients and hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  0.5812,  19.9819,  11.6205,   2.1511,  18.5098, 447.8913,   5.8960],\n",
       "        dtype=torch.float64),)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "params = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "# Define the function to compute the loss\n",
    "def compute_loss(params):\n",
    "    return instance.full_likelihood(params, aggregated_data[:, :4].to(torch.float64), aggregated_data[:, 2].to(torch.float64), instance.matern_cov_ani)\n",
    "    # return instance.vecchia_interpolation_1to6(params, instance.matern_cov_ani, 35)\n",
    "    \n",
    "# Compute the first derivative using torch.func.grad\n",
    "grad_f = torch.autograd.grad(compute_loss(params), params)\n",
    "\n",
    "grad_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([25.8229,  1.0230,  1.1314,  0.0733, -0.0958,  0.1777,  1.5697],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "Gradient: tensor([  0.9324, -43.9642, -35.9082,  59.9937, -17.1091, -76.0932,  -0.6668],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "<function compute_loss at 0x3cf321800>\n",
      "tensor([[ 6.8449e-01, -3.0525e+00, -4.1062e+00,  5.7238e+00, -5.5499e+00,\n",
      "          3.8050e+01,  3.0690e+00],\n",
      "        [-3.0525e+00,  1.5700e+02, -8.9039e+00,  1.1649e+02,  5.2665e+01,\n",
      "         -3.1752e+01, -1.3032e+01],\n",
      "        [-4.1062e+00, -8.9039e+00,  1.5732e+02, -4.8812e+01, -6.4641e+01,\n",
      "          5.5250e+01, -1.4956e+01],\n",
      "        [ 5.7238e+00,  1.1649e+02, -4.8812e+01,  1.6052e+03, -9.2467e+02,\n",
      "          1.2072e+03,  7.3658e+01],\n",
      "        [-5.5499e+00,  5.2665e+01, -6.4641e+01, -9.2467e+02,  2.2599e+03,\n",
      "         -1.1357e+03, -7.7113e+01],\n",
      "        [ 3.8050e+01, -3.1752e+01,  5.5250e+01,  1.2072e+03, -1.1357e+03,\n",
      "          6.4662e+03,  2.6074e+02],\n",
      "        [ 3.0690e+00, -1.3032e+01, -1.4956e+01,  7.3658e+01, -7.7113e+01,\n",
      "          2.6074e+02,  1.9769e+01]], dtype=torch.float64,\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "params = torch.tensor(params, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Convert parameters to a tensor with requires_grad=True\n",
    "params = torch.tensor(df.iloc[0, :-1].values, dtype=torch.float64, requires_grad=True)\n",
    "print(params)\n",
    "\n",
    "# Define the function to compute the loss\n",
    "def compute_loss(params):\n",
    "    return instance.full_likelihood(params, aggregated_data[:, :4].to(torch.float64), aggregated_data[:, 2].to(torch.float64), instance.matern_cov_ani)\n",
    "    # return instance.vecchia_interpolation_1to6(params, instance.matern_cov_ani, 35)\n",
    "\n",
    "\n",
    "\n",
    "# Compute the first derivative using torch.func.grad\n",
    "grad_f = grad(compute_loss)\n",
    "g1 = grad_f(params)\n",
    "print(f'Gradient: {g1}')\n",
    "\n",
    "# Compute the Hessian matrix using torch.func.hessian\n",
    "try:\n",
    "    hessian_matrix = hessian(compute_loss, params)\n",
    "    print(hessian_matrix)\n",
    "except Exception as e:\n",
    "    print(f'Error computing Hessian: {e}')\n",
    "\n",
    "\n",
    "print(hessian(compute_loss)(params))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
