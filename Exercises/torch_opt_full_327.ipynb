{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path in sys.path:\n",
    "#   print(path)\n",
    "\n",
    "import sys\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import concurrent\n",
    "from concurrent.futures import ThreadPoolExecutor  # Importing specific executor for clarity\n",
    "import time\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Nearest neighbor search\n",
    "import sklearn\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# Special functions and optimizations\n",
    "from scipy.special import gamma, kv  # Bessel function and gamma function\n",
    "from scipy.stats import multivariate_normal  # Simulation\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import cdist  # For space and time distance\n",
    "from scipy.spatial import distance  # Find closest spatial point\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "# Plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Type hints\n",
    "from typing import Callable, Union, Tuple\n",
    "\n",
    "# Add your custom path\n",
    "# sys.path.append(\"/cache/home/jl2815/tco\")\n",
    "\n",
    "# Custom imports\n",
    "\n",
    "from GEMS_TCO import orbitmap \n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import evaluate\n",
    "from GEMS_TCO import orderings as _orderings\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy                    # clone tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_resolution = [10,10]\n",
    "mm_cond_number = 10\n",
    "params= [20, 8.25, 5.25, 0.2, 0.5, 5]\n",
    "idx_for_datamap= [0,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the one dictionary to set spaital coordinates\n",
    "# filepath = \"C:/Users/joonw/TCO/GEMS_data/data_2023/sparse_cen_map23_01.pkl\"\n",
    "filepath = \"/Users/joonwonlee/Documents/GEMS_DATA/pickle_2023/coarse_cen_map23_01.pkl\"\n",
    "with open(filepath, 'rb') as pickle_file:\n",
    "    coarse_dict_24_1 = pickle.load(pickle_file)\n",
    "\n",
    "sample_df = coarse_dict_24_1['y23m01day01_hm02:12']\n",
    "\n",
    "sample_key = coarse_dict_24_1.get('y23m01day01_hm02:12')\n",
    "if sample_key is None:\n",
    "    print(\"Key 'y23m01day01_hm02:12' not found in the dictionary.\")\n",
    "\n",
    "# { (20,20):(5,1), (5,5):(20,40) }\n",
    "rho_lat = lat_lon_resolution[0]          \n",
    "rho_lon = lat_lon_resolution[1]\n",
    "lat_n = sample_df['Latitude'].unique()[::rho_lat]\n",
    "lon_n = sample_df['Longitude'].unique()[::rho_lon]\n",
    "\n",
    "lat_number = len(lat_n)\n",
    "lon_number = len(lon_n)\n",
    "\n",
    "# Set spatial coordinates for each dataset\n",
    "coarse_dicts = {}\n",
    "\n",
    "years = ['2024']\n",
    "for year in years:\n",
    "    for month in range(7, 8):  # Iterate over all months\n",
    "        # filepath = f\"C:/Users/joonw/TCO/GEMS_data/data_{year}/sparse_cen_map{year[2:]}_{month:02d}.pkl\"\n",
    "        filepath = f\"/Users/joonwonlee/Documents/GEMS_DATA/pickle_{year}/coarse_cen_map{year[2:]}_{month:02d}.pkl\"\n",
    "        with open(filepath, 'rb') as pickle_file:\n",
    "            loaded_map = pickle.load(pickle_file)\n",
    "            for key in loaded_map:\n",
    "                tmp_df = loaded_map[key]\n",
    "                coarse_filter = (tmp_df['Latitude'].isin(lat_n)) & (tmp_df['Longitude'].isin(lon_n))\n",
    "                coarse_dicts[f\"{year}_{month:02d}_{key}\"] = tmp_df[coarse_filter].reset_index(drop=True)\n",
    "\n",
    "\n",
    "key_idx = sorted(coarse_dicts)\n",
    "if not key_idx:\n",
    "    raise ValueError(\"coarse_dicts is empty\")\n",
    "\n",
    "# extract first hour data because all data shares the same spatial grid\n",
    "data_for_coord = coarse_dicts[key_idx[0]]\n",
    "x1 = data_for_coord['Longitude'].values\n",
    "y1 = data_for_coord['Latitude'].values \n",
    "coords1 = np.stack((x1, y1), axis=-1)\n",
    "\n",
    "\n",
    "# instance = orbitmap.MakeOrbitdata(data_for_coord, lat_s=5, lat_e=10, lon_s=110, lon_e=120)\n",
    "# s_dist = cdist(coords1, coords1, 'euclidean')\n",
    "# ord_mm, _ = instance.maxmin_naive(s_dist, 0)\n",
    "\n",
    "ord_mm = _orderings.maxmin_cpp(coords1)\n",
    "data_for_coord = data_for_coord.iloc[ord_mm].reset_index(drop=True)\n",
    "coords1_reordered = np.stack((data_for_coord['Longitude'].values, data_for_coord['Latitude'].values), axis=-1)\n",
    "# nns_map = instance.find_nns_naive(locs=coords1_reordered, dist_fun='euclidean', max_nn=mm_cond_number)\n",
    "nns_map=_orderings.find_nns_l2(locs= coords1_reordered  ,max_nn = mm_cond_number)\n",
    "\n",
    "\n",
    "analysis_data_map = {}\n",
    "for i in range(idx_for_datamap[0],idx_for_datamap[1]):\n",
    "    tmp = coarse_dicts[key_idx[i]].copy()\n",
    "    tmp['Hours_elapsed'] = np.round(tmp['Hours_elapsed']-477700)\n",
    "\n",
    "    tmp = tmp.iloc[ord_mm, :4].to_numpy()\n",
    "    tmp = torch.from_numpy(tmp).float()  # Convert NumPy to Tensor\n",
    "    # tmp = tmp.clone().detach().requires_grad_(True)  # Enable gradients\n",
    "    \n",
    "    analysis_data_map[key_idx[i]] = tmp\n",
    "\n",
    "aggregated_data = pd.DataFrame()\n",
    "for i in range(idx_for_datamap[0],idx_for_datamap[1]):\n",
    "    tmp = coarse_dicts[key_idx[i]].copy()\n",
    "    tmp['Hours_elapsed'] = np.round(tmp['Hours_elapsed']-477700)\n",
    "    tmp = tmp.iloc[ord_mm].reset_index(drop=True)  \n",
    "    aggregated_data = pd.concat((aggregated_data, tmp), axis=0)\n",
    "\n",
    "aggregated_data = aggregated_data.iloc[:, :4].to_numpy()\n",
    "\n",
    "aggregated_data = torch.from_numpy(aggregated_data).float()  # Convert NumPy to Tensor\n",
    "# aggregated_np = aggregated_np.clone().detach().requires_grad_(True)  # Enable gradients\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Gradients: [  0.5812483  19.981602   11.618286    2.1514587  18.508026  447.89014\n",
      "   5.896081 ]\n",
      " Loss: 2588.835693359375, Parameters: [ 2.442e+01  1.920e+00  1.920e+00  1.000e-03 -4.500e-02  2.370e-01\n",
      "  3.340e+00]\n",
      "Epoch 101, Gradients: [-0.84959674 -0.43359375 -0.36999512 -0.29135132 -3.8077698  -6.9865723\n",
      "  0.06155443]\n",
      " Loss: 2548.97900390625, Parameters: [ 2.49888325e+01  2.04243159e+00  2.14122796e+00  1.22757768e-03\n",
      " -8.13331604e-02  1.05529346e-01  3.72364473e+00]\n",
      "Epoch 201, Gradients: [-0.8188605   0.02231026  0.02679443  0.00822449 -0.38000488  1.8916016\n",
      "  0.03283119]\n",
      " Loss: 2548.55615234375, Parameters: [ 2.5473244e+01  2.0911043e+00  2.1936471e+00  1.8002884e-03\n",
      " -7.9336733e-02  1.0402137e-01  3.7235968e+00]\n",
      "Epoch 301, Gradients: [-0.7940925  -0.02832031 -0.019104   -0.11378479  0.20098877 -1.6644287\n",
      " -0.02568793]\n",
      " Loss: 2548.3251953125, Parameters: [ 2.5759418e+01  2.1081748e+00  2.2122281e+00  1.7022894e-03\n",
      " -7.9268247e-02  1.0296774e-01  3.7139628e+00]\n",
      "Epoch 401, Gradients: [-0.7732562   0.00411797 -0.00164795  0.09942627  0.14627075  0.3297119\n",
      "  0.0049901 ]\n",
      " Loss: 2548.193115234375, Parameters: [ 2.59282646e+01  2.11855030e+00  2.22339797e+00  1.75939058e-03\n",
      " -7.92920068e-02  1.02613755e-01  3.70893717e+00]\n",
      "Converged at epoch 446\n",
      "Epoch 447, Gradients: [-0.7697732  -0.01019478 -0.00305176 -0.04103088  0.03244019 -0.5115967\n",
      " -0.00895929]\n",
      " Loss: 2548.15283203125, full Parameters: [ 2.59814014e+01  2.12175608e+00  2.22699022e+00  1.73025124e-03\n",
      " -7.93599486e-02  1.02427535e-01  3.70715070e+00]\n",
      "FINAL STATE: Epoch 447, Gradients: [-0.7697732  -0.01019478 -0.00305176 -0.04103088  0.03244019 -0.5115967\n",
      " -0.00895929]\n",
      " Loss: 2548.15283203125, full Parameters: [ 2.59814014e+01  2.12175608e+00  2.22699022e+00  1.73025124e-03\n",
      " -7.93599486e-02  1.02427535e-01  3.70715070e+00]\n",
      "Epoch 1, Gradients: [  7.113331 -18.575752 -24.994537  -4.22068  -20.734985 434.53107\n",
      "  37.038548]\n",
      " Loss: 2325.30615234375, Parameters: [ 2.442e+01  1.920e+00  1.920e+00  1.000e-03 -4.500e-02  2.370e-01\n",
      "  3.340e+00]\n",
      "Epoch 101, Gradients: [  0.640502   -1.819318   -8.543015    3.1418533  -1.1879425 -11.643036\n",
      "   6.2221184]\n",
      " Loss: 2222.59326171875, Parameters: [2.3537296e+01 2.8002629e+00 2.7720702e+00 4.9534809e-02 1.7577816e-02\n",
      " 9.2159480e-02 2.4757919e+00]\n",
      "Epoch 201, Gradients: [-0.28645396 -0.9790766  -0.7342377   0.36849213 -0.1151886   0.10168457\n",
      "  0.01825523]\n",
      " Loss: 2219.92431640625, Parameters: [2.3630571e+01 3.0386078e+00 3.3340566e+00 4.6616144e-02 1.7296955e-02\n",
      " 8.5786238e-02 2.4894879e+00]\n",
      "Epoch 301, Gradients: [-0.4686408  -0.02002549  0.01689148  0.205307    0.29000854  2.8422241\n",
      "  0.05115867]\n",
      " Loss: 2219.58642578125, Parameters: [2.3908236e+01 3.3091543e+00 3.6325336e+00 4.8700403e-02 1.9848086e-02\n",
      " 7.9145402e-02 2.6265030e+00]\n",
      "Converged at epoch 385\n",
      "Epoch 386, Gradients: [-4.6643066e-01  9.5937252e-03  7.4920654e-03  1.6323090e-01\n",
      " -4.5776367e-04  1.3996582e+00  1.7481446e-02]\n",
      " Loss: 2219.515625, full Parameters: [2.4057718e+01 3.3334122e+00 3.6577389e+00 4.8503298e-02 1.9488851e-02\n",
      " 7.8446291e-02 2.6273870e+00]\n",
      "FINAL STATE: Epoch 386, Gradients: [-4.6643066e-01  9.5937252e-03  7.4920654e-03  1.6323090e-01\n",
      " -4.5776367e-04  1.3996582e+00  1.7481446e-02]\n",
      " Loss: 2219.515625, full Parameters: [2.4057718e+01 3.3334122e+00 3.6577389e+00 4.8503298e-02 1.9488851e-02\n",
      " 7.8446291e-02 2.6273870e+00]\n",
      "Epoch 1, Gradients: [ -1.4837532  34.451126   14.306274  -27.760681   55.023315  375.6548\n",
      "  -5.6640596]\n",
      " Loss: 2677.8740234375, Parameters: [ 2.442e+01  1.920e+00  1.920e+00  1.000e-03 -4.500e-02  2.370e-01\n",
      "  3.340e+00]\n",
      "Epoch 101, Gradients: [-0.91619337  0.3423233  -0.8708496  -1.9966431  -0.30316162 -2.6568604\n",
      " -0.42619425]\n",
      " Loss: 2644.093994140625, Parameters: [ 2.5267197e+01  1.8115035e+00  2.0850852e+00 -1.2238592e-02\n",
      " -1.1835557e-01  1.2599245e-01  4.1429038e+00]\n",
      "Epoch 201, Gradients: [-0.8639611   0.02591133  0.01159668 -0.17996216  0.44934082  1.2405396\n",
      "  0.0081979 ]\n",
      " Loss: 2643.60888671875, Parameters: [ 2.5753086e+01  1.8812029e+00  2.1897309e+00 -1.0610226e-02\n",
      " -1.1545650e-01  1.2074962e-01  4.2582421e+00]\n",
      "Epoch 301, Gradients: [-0.83463764  0.01272964  0.0234375  -0.1685791  -0.04553223  1.0878296\n",
      "  0.01434839]\n",
      " Loss: 2643.364990234375, Parameters: [ 2.6039230e+01  1.8961506e+00  2.2084749e+00 -1.0845099e-02\n",
      " -1.1626082e-01  1.1978296e-01  4.2472949e+00]\n",
      "Epoch 401, Gradients: [-0.819458   -0.01177025 -0.01605225  0.05450439  0.10870361 -0.61505127\n",
      " -0.00753057]\n",
      " Loss: 2643.225830078125, Parameters: [ 2.6208069e+01  1.9049569e+00  2.2190673e+00 -1.0735317e-02\n",
      " -1.1633556e-01  1.1910749e-01  4.2407093e+00]\n",
      "Converged at epoch 488\n",
      "Epoch 489, Gradients: [-8.1002963e-01 -2.4242401e-03  2.4414062e-04  2.6245117e-02\n",
      " -5.4809570e-02  3.4179688e-02 -1.7440319e-04]\n",
      " Loss: 2643.15283203125, full Parameters: [ 2.6298939e+01  1.9097911e+00  2.2250631e+00 -1.0885691e-02\n",
      " -1.1659390e-01  1.1882248e-01  4.2370915e+00]\n",
      "FINAL STATE: Epoch 489, Gradients: [-8.1002963e-01 -2.4242401e-03  2.4414062e-04  2.6245117e-02\n",
      " -5.4809570e-02  3.4179688e-02 -1.7440319e-04]\n",
      " Loss: 2643.15283203125, full Parameters: [ 2.6298939e+01  1.9097911e+00  2.2250631e+00 -1.0885691e-02\n",
      " -1.1659390e-01  1.1882248e-01  4.2370915e+00]\n",
      "Epoch 1, Gradients: [  1.231359   20.102432   -0.477417   23.869202   24.144897  275.09216\n",
      "  -4.4810686]\n",
      " Loss: 2607.619873046875, Parameters: [ 2.442e+01  1.920e+00  1.920e+00  1.000e-03 -4.500e-02  2.370e-01\n",
      "  3.340e+00]\n",
      "Epoch 101, Gradients: [-0.07951355 -0.39974213 -0.70947266  3.3319702   2.3253174  -4.8779297\n",
      " -5.984004  ]\n",
      " Loss: 2571.326416015625, Parameters: [24.188946    2.0488732   2.5043147  -0.14637609 -0.18407561  0.09291023\n",
      "  4.1628013 ]\n",
      "Epoch 201, Gradients: [-0.06158829 -0.00585556 -0.73947144  0.53149414 -0.03881836 -0.6748047\n",
      " -4.2346325 ]\n",
      " Loss: 2568.38916015625, Parameters: [24.651592    2.4172068   3.0165708  -0.1402092  -0.17671837  0.08161087\n",
      "  4.6512303 ]\n",
      "Epoch 301, Gradients: [ 0.01066852 -0.04388046 -1.1026917   0.05279541 -0.18066406  0.9194336\n",
      " -2.4650922 ]\n",
      " Loss: 2567.13525390625, Parameters: [24.828886    2.6218815   3.292914   -0.13666695 -0.17205264  0.07644527\n",
      "  4.945056  ]\n",
      "Epoch 401, Gradients: [ 0.00980842  0.00788879 -1.1087036  -0.03149414  0.06652832  0.06347656\n",
      " -1.6726332 ]\n",
      " Loss: 2566.58740234375, Parameters: [24.665974    2.7369885   3.4615846  -0.1343143  -0.16855936  0.07402891\n",
      "  5.1171927 ]\n",
      "Epoch 501, Gradients: [ 0.01505125 -0.00462341 -1.0906372  -0.00732422 -0.05938721  0.7553711\n",
      " -1.1492224 ]\n",
      " Loss: 2566.332763671875, Parameters: [24.567808    2.8014534   3.56117    -0.13303591 -0.16683896  0.07275873\n",
      "  5.2186775 ]\n",
      "Converged at epoch 561\n",
      "Epoch 562, Gradients: [ 0.01568973 -0.00541115 -1.0945435  -0.00628662 -0.00238037  0.2734375\n",
      " -0.9454777 ]\n",
      " Loss: 2566.24755859375, full Parameters: [24.528248    2.8282459   3.6011322  -0.132559   -0.16611877  0.0721757\n",
      "  5.25932   ]\n",
      "FINAL STATE: Epoch 562, Gradients: [ 0.01568973 -0.00541115 -1.0945435  -0.00628662 -0.00238037  0.2734375\n",
      " -0.9454777 ]\n",
      " Loss: 2566.24755859375, full Parameters: [24.528248    2.8282459   3.6011322  -0.132559   -0.16611877  0.0721757\n",
      "  5.25932   ]\n",
      "Epoch 1, Gradients: [  8.118851 -22.829233 -22.05867   25.472961  29.545013 465.70422\n",
      "  37.95094 ]\n",
      " Loss: 2297.7041015625, Parameters: [ 2.442e+01  1.920e+00  1.920e+00  1.000e-03 -4.500e-02  2.370e-01\n",
      "  3.340e+00]\n",
      "Epoch 101, Gradients: [ 1.6056671  -6.58325    -8.410614    0.15118408  1.3903809  26.384521\n",
      " 11.287935  ]\n",
      " Loss: 2165.14501953125, Parameters: [23.552412    2.7741923   2.766488   -0.06421755 -0.13995832  0.07170037\n",
      "  2.4872828 ]\n",
      "Epoch 201, Gradients: [ 0.03675413 -0.49240875 -1.6211853   0.1383667   0.7463379  -2.3840332\n",
      " -0.9699429 ]\n",
      " Loss: 2160.052734375, Parameters: [23.189585    3.3509133   3.3254838  -0.06369708 -0.1401024   0.0647203\n",
      "  2.4505296 ]\n",
      "Epoch 301, Gradients: [-0.10649621 -0.09096718 -0.85369873  0.20013428 -0.1583252   0.60498047\n",
      "  0.03323126]\n",
      " Loss: 2159.5625, Parameters: [23.236225    3.6064115   3.6145415  -0.06130186 -0.13747613  0.06044158\n",
      "  2.5981975 ]\n",
      "Converged at epoch 329\n",
      "Epoch 330, Gradients: [-0.14408529 -0.02300072 -0.66000366 -0.21679688  0.18804932  0.22607422\n",
      " -0.02300596]\n",
      " Loss: 2159.507080078125, full Parameters: [23.292744    3.6576366   3.6765862  -0.06118661 -0.13705723  0.0597056\n",
      "  2.6174376 ]\n",
      "FINAL STATE: Epoch 330, Gradients: [-0.14408529 -0.02300072 -0.66000366 -0.21679688  0.18804932  0.22607422\n",
      " -0.02300596]\n",
      " Loss: 2159.507080078125, full Parameters: [23.292744    3.6576366   3.6765862  -0.06118661 -0.13705723  0.0597056\n",
      "  2.6174376 ]\n",
      "Epoch 1, Gradients: [  0.71019864  11.881107     5.586914    -6.918457    56.961914\n",
      " 358.03882      2.9255602 ]\n",
      " Loss: 2595.6083984375, Parameters: [ 2.442e+01  1.920e+00  1.920e+00  1.000e-03 -4.500e-02  2.370e-01\n",
      "  3.340e+00]\n",
      "Epoch 101, Gradients: [-0.86158276  0.04751968 -0.29309082 -0.20751953 -0.03686523 19.098633\n",
      " -2.1622288 ]\n",
      " Loss: 2562.4873046875, Parameters: [ 2.4976994e+01  2.1995878e+00  2.4182043e+00  3.5204794e-03\n",
      " -1.3657494e-01  1.1184246e-01  3.8725345e+00]\n",
      "Epoch 201, Gradients: [-0.82142246 -0.07392502  0.18481445 -0.02734375  0.42578125 -0.9230957\n",
      " -0.17818356]\n",
      " Loss: 2561.310302734375, Parameters: [ 2.5464869e+01  2.5104365e+00  2.8374248e+00  3.3808239e-03\n",
      " -1.3708222e-01  9.5309563e-02  4.3707428e+00]\n",
      "Epoch 301, Gradients: [-0.7790892  -0.0166378  -0.02819824  0.12792969 -0.18286133 -1.9077148\n",
      " -0.01483226]\n",
      " Loss: 2561.0830078125, Parameters: [ 2.5750593e+01  2.5348623e+00  2.8568130e+00  3.4391950e-03\n",
      " -1.3758567e-01  9.4358996e-02  4.3741689e+00]\n",
      "Converged at epoch 356\n",
      "Epoch 357, Gradients: [-0.76646364  0.00114632 -0.0020752  -0.06811523 -0.08789062  0.22265625\n",
      "  0.00324464]\n",
      " Loss: 2561.00244140625, full Parameters: [ 2.5857313e+01  2.5422144e+00  2.8654017e+00  3.1708130e-03\n",
      " -1.3760866e-01  9.4123915e-02  4.3707428e+00]\n",
      "FINAL STATE: Epoch 357, Gradients: [-0.76646364  0.00114632 -0.0020752  -0.06811523 -0.08789062  0.22265625\n",
      "  0.00324464]\n",
      " Loss: 2561.00244140625, full Parameters: [ 2.5857313e+01  2.5422144e+00  2.8654017e+00  3.1708130e-03\n",
      " -1.3760866e-01  9.4123915e-02  4.3707428e+00]\n",
      "Epoch 1, Gradients: [ -2.2238035  41.01631    -9.174072  -65.535645  132.46729   215.1643\n",
      " -12.932717 ]\n",
      " Loss: 2720.223388671875, Parameters: [ 2.442e+01  1.920e+00  1.920e+00  1.000e-03 -4.500e-02  2.370e-01\n",
      "  3.340e+00]\n",
      "Epoch 101, Gradients: [-1.3683219  -0.7709732  -1.3688354  -0.56933594  5.8465576  -3.5512695\n",
      " -4.776184  ]\n",
      " Loss: 2661.382080078125, Parameters: [25.26286     1.6728208   2.4418852   0.05261557 -0.28658882  0.11634145\n",
      "  4.203118  ]\n",
      "Epoch 201, Gradients: [-1.2841265  -0.08716202 -0.16319275 -0.38012695 -0.24572754 -2.3950195\n",
      " -3.037966  ]\n",
      " Loss: 2658.449951171875, Parameters: [25.746977    1.8936138   2.8530939   0.0510966  -0.29700482  0.10060778\n",
      "  4.6952586 ]\n",
      "Epoch 301, Gradients: [-1.181303   -0.03887177 -0.09825134 -0.02563477  0.07232666 -1.5771484\n",
      " -1.4634976 ]\n",
      " Loss: 2657.40771484375, Parameters: [26.03379     2.0199494   3.0823512   0.05104545 -0.29937744  0.09319378\n",
      "  4.991139  ]\n",
      "Epoch 401, Gradients: [-1.1282287  -0.01230812 -0.06010437 -0.09936523 -0.10150146 -1.230957\n",
      " -0.53003716]\n",
      " Loss: 2657.02880859375, Parameters: [26.202845    2.0995588   3.2256243   0.05087816 -0.30106544  0.08912724\n",
      "  5.168024  ]\n",
      "Converged at epoch 407\n",
      "Epoch 408, Gradients: [-1.1217948  -0.01058197 -0.05465698 -0.14233398 -0.05273438  0.06396484\n",
      " -0.45789623]\n",
      " Loss: 2657.016357421875, full Parameters: [26.212595    2.1033912   3.2323694   0.05097214 -0.30102852  0.0890457\n",
      "  5.1785574 ]\n",
      "FINAL STATE: Epoch 408, Gradients: [-1.1217948  -0.01058197 -0.05465698 -0.14233398 -0.05273438  0.06396484\n",
      " -0.45789623]\n",
      " Loss: 2657.016357421875, full Parameters: [26.212595    2.1033912   3.2323694   0.05097214 -0.30102852  0.0890457\n",
      "  5.1785574 ]\n",
      "Epoch 1, Gradients: [ -1.8155222  35.338993  -14.963623  -47.95166   179.53784   123.01758\n",
      " -14.725693 ]\n",
      " Loss: 2716.2421875, Parameters: [ 2.442e+01  1.920e+00  1.920e+00  1.000e-03 -4.500e-02  2.370e-01\n",
      "  3.340e+00]\n",
      "Epoch 101, Gradients: [-1.3764623   0.727478   -0.14038086  2.4726562   2.645752   -1.2871094\n",
      " -0.64488035]\n",
      " Loss: 2634.37744140625, Parameters: [25.254719    1.750956    2.321513    0.02887192 -0.39193866  0.11548088\n",
      "  4.109277  ]\n",
      "Epoch 201, Gradients: [-1.2480284  -0.01747513 -0.01306152  0.11474609 -0.3857422  -0.31445312\n",
      "  0.00743842]\n",
      " Loss: 2633.7236328125, Parameters: [25.740051    1.782276    2.386419    0.02747834 -0.39417028  0.11250418\n",
      "  4.1738615 ]\n",
      "Epoch 301, Gradients: [-1.2106712  -0.02206421 -0.02593994 -0.265625    0.01391602 -1.3828125\n",
      " -0.01358619]\n",
      " Loss: 2633.373779296875, Parameters: [26.026136    1.7948846   2.40302     0.0272529  -0.39425403  0.11158556\n",
      "  4.159984  ]\n",
      "Converged at epoch 328\n",
      "Epoch 329, Gradients: [-1.1982613   0.02076244  0.015625    0.17382812  0.14135742  1.1679688\n",
      "  0.01105678]\n",
      " Loss: 2633.3076171875, full Parameters: [26.084076    1.797581    2.4063692   0.02719593 -0.39443997  0.11148611\n",
      "  4.157628  ]\n",
      "FINAL STATE: Epoch 329, Gradients: [-1.1982613   0.02076244  0.015625    0.17382812  0.14135742  1.1679688\n",
      "  0.01105678]\n",
      " Loss: 2633.3076171875, full Parameters: [26.084076    1.797581    2.4063692   0.02719593 -0.39443997  0.11148611\n",
      "  4.157628  ]\n"
     ]
    }
   ],
   "source": [
    "result = {}\n",
    "\n",
    "for day in range(8):\n",
    "    idx_for_datamap= [8*day,8*(day+1)]\n",
    "    analysis_data_map = {}\n",
    "    for i in range(idx_for_datamap[0],idx_for_datamap[1]):\n",
    "        tmp = coarse_dicts[key_idx[i]].copy()\n",
    "        tmp['Hours_elapsed'] = np.round(tmp['Hours_elapsed']-477700)\n",
    "\n",
    "        tmp = tmp.iloc[ord_mm, :4].to_numpy()\n",
    "        tmp = torch.from_numpy(tmp).float()  # Convert NumPy to Tensor\n",
    "        # tmp = tmp.clone().detach().requires_grad_(True)  # Enable gradients\n",
    "        \n",
    "        analysis_data_map[key_idx[i]] = tmp\n",
    "    aggregated_data = pd.DataFrame()\n",
    "    for i in range(idx_for_datamap[0],idx_for_datamap[1]):\n",
    "        tmp = coarse_dicts[key_idx[i]].copy()\n",
    "        tmp['Hours_elapsed'] = np.round(tmp['Hours_elapsed']-477700)\n",
    "        tmp = tmp.iloc[ord_mm].reset_index(drop=True)  \n",
    "        aggregated_data = pd.concat((aggregated_data, tmp), axis=0)\n",
    "    \n",
    "    aggregated_data = aggregated_data.iloc[:, :4].to_numpy()\n",
    "\n",
    "    aggregated_data = torch.from_numpy(aggregated_data).float()  # Convert NumPy to Tensor\n",
    "\n",
    "    params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "    params = torch.tensor(params, requires_grad=True)\n",
    "\n",
    "    torch_smooth = torch.tensor(0.5, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    instance = kernels.model_fitting(\n",
    "    smooth=0.5,\n",
    "    input_map=analysis_data_map,\n",
    "    aggregated_data=aggregated_data,\n",
    "    nns_map=nns_map,\n",
    "    mm_cond_number=mm_cond_number\n",
    "    )\n",
    "\n",
    "    # optimizer = optim.Adam([params], lr=0.01)  # For Adam\n",
    "    optimizer, scheduler = instance.optimizer_fun2( params, lr=0.01, betas=(0.9, 0.8), eps=1e-8, step_size=20, gamma=0.9)    \n",
    "    out = instance.run_full2(params, optimizer,scheduler, epochs=3000)\n",
    "    result[day+1] = out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [array([ 2.59814014e+01,  2.12175608e+00,  2.22699022e+00,  1.73025124e-03,\n",
       "         -7.93599486e-02,  1.02427535e-01,  3.70715070e+00], dtype=float32),\n",
       "  2548.15283203125],\n",
       " 2: [array([2.4057718e+01, 3.3334122e+00, 3.6577389e+00, 4.8503298e-02,\n",
       "         1.9488851e-02, 7.8446291e-02, 2.6273870e+00], dtype=float32),\n",
       "  2219.515625],\n",
       " 3: [array([ 2.6298939e+01,  1.9097911e+00,  2.2250631e+00, -1.0885691e-02,\n",
       "         -1.1659390e-01,  1.1882248e-01,  4.2370915e+00], dtype=float32),\n",
       "  2643.15283203125],\n",
       " 4: [array([24.528248  ,  2.8282459 ,  3.6011322 , -0.132559  , -0.16611877,\n",
       "          0.0721757 ,  5.25932   ], dtype=float32),\n",
       "  2566.24755859375],\n",
       " 5: [array([23.292744  ,  3.6576366 ,  3.6765862 , -0.06118661, -0.13705723,\n",
       "          0.0597056 ,  2.6174376 ], dtype=float32),\n",
       "  2159.507080078125],\n",
       " 6: [array([ 2.5857313e+01,  2.5422144e+00,  2.8654017e+00,  3.1708130e-03,\n",
       "         -1.3760866e-01,  9.4123915e-02,  4.3707428e+00], dtype=float32),\n",
       "  2561.00244140625],\n",
       " 7: [array([26.212595  ,  2.1033912 ,  3.2323694 ,  0.05097214, -0.30102852,\n",
       "          0.0890457 ,  5.1785574 ], dtype=float32),\n",
       "  2657.016357421875],\n",
       " 8: [array([26.084076  ,  1.797581  ,  2.4063692 ,  0.02719593, -0.39443997,\n",
       "          0.11148611,  4.157628  ], dtype=float32),\n",
       "  2633.3076171875]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
