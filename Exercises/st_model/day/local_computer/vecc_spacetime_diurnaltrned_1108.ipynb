{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b0b21b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from GEMS_TCO import kernels\n",
    "from GEMS_TCO import data_preprocess \n",
    "from GEMS_TCO import kernels_new, kernels_reparam_space_time_temporal_tend as kernels_reparam_space_time\n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO.data_loader import load_data2\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4394f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting data to lat: [0.0, 5.0], lon: [123.0, 133.0]\n"
     ]
    }
   ],
   "source": [
    "space: List[str] = ['4', '4']\n",
    "lat_lon_resolution = [int(s) for s in space]\n",
    "mm_cond_number: int = 20\n",
    "years = ['2024']\n",
    "month_range = [7] \n",
    "\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "data_load_instance = load_data2(config.mac_data_load_path)\n",
    "\n",
    "\n",
    "df_map, ord_mm, nns_map = data_load_instance.load_maxmin_ordered_data_bymonthyear(\n",
    "lat_lon_resolution=lat_lon_resolution, \n",
    "mm_cond_number=mm_cond_number,\n",
    "years_=years, \n",
    "months_=month_range,\n",
    "lat_range=[0.0, 5.0],      \n",
    "lon_range=[123.0, 133.0] \n",
    ")\n",
    "\n",
    "#days: List[str] = ['0', '31']\n",
    "#days_s_e = [int(d) for d in days]\n",
    "#days_list = list(range(days_s_e[0], days_s_e[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e7038f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8960, 4])\n"
     ]
    }
   ],
   "source": [
    "daily_aggregated_tensors = [] \n",
    "daily_hourly_maps = []        \n",
    "\n",
    "for day_index in range(31):\n",
    "  \n",
    "    hour_start_index = day_index * 8\n",
    "    hour_end_index = (day_index + 1) * 8\n",
    "    #hour_end_index = day_index*8 + 1\n",
    "    hour_indices = [hour_start_index, hour_end_index]\n",
    "    \n",
    "    # Load the data for the current day\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "        df_map, \n",
    "        hour_indices, \n",
    "        ord_mm=ord_mm,  \n",
    "        dtype=torch.float \n",
    "    )\n",
    "    # Append the day's data to their respective lists\n",
    "    daily_aggregated_tensors.append(day_aggregated_tensor)\n",
    "    daily_hourly_maps.append(day_hourly_map) \n",
    "\n",
    "print(daily_aggregated_tensors[0].shape)\n",
    "#print(daily_hourly_maps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40a5d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0.5 # smooth\n",
    "mm_cond_number = 8\n",
    "nheads = 300\n",
    "#nheads = 1230\n",
    "#lr = 0.01\n",
    "#step = 80\n",
    "#gamma_par = 0.5\n",
    "\n",
    "# --- Placeholder Global Variables ---\n",
    "# ðŸ’¥ REVISED: Added lr, patience, factor. Removed step, gamma_par\n",
    "lr=0.1\n",
    "patience = 5       # Scheduler: Epochs to wait for improvement\n",
    "factor = 0.5         # Scheduler: Factor to reduce LR by (e.g., 0.5 = 50% cut)\n",
    "epochs=200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8667a0e9",
   "metadata": {},
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be1a7d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 1092, smooth: 0.5\n",
      "mm_cond_number: 8,\n",
      "initial parameters (log/linear): \n",
      "\n",
      "  Param 0: 3.0647\n",
      "  Param 1: 0.3567\n",
      "  Param 2: 0.1177\n",
      "  Param 3: -4.6052\n",
      "  Param 4: 0.0200\n",
      "  Param 5: -0.0800\n",
      "  Param 6: 0.4055\n",
      "--- Starting L-BFGS Optimization ---\n",
      "--- Step 1/50 / Loss: 1.707625 ---\n",
      "  Param 0: Value=3.3641, Grad=-3.6065079810739385e-07\n",
      "  Param 1: Value=1.1622, Grad=3.815536762749096e-07\n",
      "  Param 2: Value=0.5686, Grad=1.199137323775934e-06\n",
      "  Param 3: Value=-5.0795, Grad=9.321988249903849e-08\n",
      "  Param 4: Value=0.0449, Grad=7.749080442334461e-06\n",
      "  Param 5: Value=-0.1769, Grad=-2.360538889848797e-06\n",
      "  Param 6: Value=1.2163, Grad=-9.427895988013095e-07\n",
      "  Max Abs Grad: 7.749080e-06\n",
      "------------------------------\n",
      "\n",
      "Converged on gradient norm (max|grad| < 1e-05) at step 1\n",
      "FINAL STATE: Step 1, Loss: 1.7076250567674514\n",
      "  Raw (vecc) Parameters: [3.3641479562311023, 1.162156501413251, 0.568629277137048, -5.079499596490625, 0.044855306366610656, -0.17692755472959093, 1.2163296747382855]\n",
      "  Interpretable Parameters:\n",
      "    sigma_sq  : 9.043004\n",
      "    range_lon : 0.312811\n",
      "    range_lat : 0.235400\n",
      "    beta      : 0.078886\n",
      "    advec_lat : 0.044855\n",
      "    advec_lon : -0.176928\n",
      "    nugget    : 3.374778\n",
      "Day 1 optimization finished in 208.63s over 1 steps.\n",
      "Day 1 final results (raw params + loss): [3.3641479562311023, 1.162156501413251, 0.568629277137048, -5.079499596490625, 0.044855306366610656, -0.17692755472959093, 1.2163296747382855, 1.7076250567674514]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "# --- Assume global variables are set: ---\n",
    "# daily_hourly_maps, daily_aggregated_tensors, nns_map\n",
    "# lat_lon_resolution, v, mm_cond_number, nheads\n",
    "# kernels_reparam_space_time (module containing fit_vecchia_lbfgs_ST)\n",
    "\n",
    "# --- L-BFGS SPECIFIC GLOBAL PARAMETERS ---\n",
    "LBFGS_LR = 1.0\n",
    "LBFGS_MAX_STEPS = 50       # Number of outer optimization steps\n",
    "LBFGS_HISTORY_SIZE = 100   # Memory for Hessian approximation\n",
    "LBFGS_MAX_EVAL = 50        # Max evaluations (line search) per step\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "day_indices = [0] \n",
    "for day_idx in day_indices:  \n",
    "\n",
    "    daily_hourly_map = daily_hourly_maps[day_idx]\n",
    "    daily_aggregated_tensor = daily_aggregated_tensors[day_idx]\n",
    "\n",
    "    # --- ðŸ’¥ Correct Parameter Initialization (SPATIO-TEMPORAL) ---\n",
    "    init_sigmasq   = 15.0\n",
    "    init_range_lat = 0.66 \n",
    "    init_range_lon = 0.7 \n",
    "    init_nugget    = 1.5\n",
    "    init_beta      = 0.1  # <-- Spatio-temporal parameter\n",
    "    init_advec_lat = 0.02  # <-- Spatio-temporal parameter\n",
    "    init_advec_lon = -0.08  # <-- Spatio-temporal parameter\n",
    "    \n",
    "    # Map model parameters to the 'phi' reparameterization\n",
    "    init_phi2 = 1.0 / init_range_lon                # [1] 1/range_lon\n",
    "    init_phi1 = init_sigmasq * init_phi2            # [0] sigmasq / range_lon\n",
    "    init_phi3 = (init_range_lon / init_range_lat)**2  # [2] (range_lon / range_lat)^2\n",
    "    init_phi4 = init_beta**2                        # [3] beta^2\n",
    "    \n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # ðŸ’¥ 7-parameter spatio-temporal list\n",
    "    params_list = [\n",
    "        torch.tensor([np.log(init_phi1)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [0] log(phi1)\n",
    "        torch.tensor([np.log(init_phi2)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [1] log(phi2)\n",
    "        torch.tensor([np.log(init_phi3)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [2] log(phi3)\n",
    "        torch.tensor([np.log(init_phi4)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [3] log(phi4)\n",
    "        torch.tensor([init_advec_lat],         requires_grad=True, dtype=torch.float64, device=device_str ), # [4] advec_lat (linear)\n",
    "        torch.tensor([init_advec_lon],         requires_grad=True, dtype=torch.float64, device=device_str ), # [5] advec_lon (linear)\n",
    "        torch.tensor([np.log(init_nugget)],    requires_grad=True, dtype=torch.float64, device=device_str )  # [6] log(nugget)\n",
    "    ]\n",
    "\n",
    "    # --- Define parameter groups ---\n",
    "    lr_all = LBFGS_LR\n",
    "    all_indices = [0, 1, 2, 3, 4, 5, 6] \n",
    "    \n",
    "    # L-BFGS requires the parameters to be iterable in a single list or group\n",
    "    param_groups = [\n",
    "        {'params': [params_list[idx] for idx in all_indices], 'lr': lr_all, 'name': 'all_params'}\n",
    "    ]\n",
    "\n",
    "    # --- Print Job Info ---\n",
    "    res_calc = (113 // lat_lon_resolution[0]) * (158 // lat_lon_resolution[0]) \n",
    "    print(f'\\n--- Starting Day {day_idx+1} (2024-07-{day_idx+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters (log/linear): \\n')\n",
    "    for i, p in enumerate(params_list):\n",
    "        print(f\"  Param {i}: {p.item():.4f}\")\n",
    "            \n",
    "    # --- ðŸ’¥ REVISED: Instantiate the LBFGS Spatio-Temporal Class ---\n",
    "    # (This assumes the class 'fit_vecchia_lbfgs_ST' exists in the module\n",
    "    # and inherits from 'VecchiaLikelihoodPeriodicST' as we discussed)\n",
    "    model_instance = kernels_reparam_space_time.fit_vecchia_lbfgs_ST(\n",
    "            smooth = v,\n",
    "            input_map = daily_hourly_map,\n",
    "            aggregated_data = daily_aggregated_tensor,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- ðŸ’¥ REVISED: Call the LBFGS Optimizer Method ---\n",
    "    optimizer = model_instance.set_optimizer(\n",
    "            param_groups,     \n",
    "            lr=LBFGS_LR,            \n",
    "            max_iter=LBFGS_MAX_EVAL,        # max_iter in LBFGS is the line search limit\n",
    "            history_size=LBFGS_HISTORY_SIZE \n",
    "        )\n",
    "\n",
    "    # --- ðŸ’¥ REVISED: Call the LBFGS Fit Method ---\n",
    "    out, steps_ran = model_instance.fit_vecc_lbfgs(\n",
    "            params_list,\n",
    "            optimizer,\n",
    "            model_instance.matern_cov_aniso_STABLE_log_reparam, # Pass the ST covariance method\n",
    "            max_steps=LBFGS_MAX_STEPS # Total number of L-BFGS steps\n",
    "        )\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Day {day_idx+1} optimization finished in {epoch_time:.2f}s over {steps_ran+1} steps.\")\n",
    "    print(f\"Day {day_idx+1} final results (raw params + loss): {out}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
