{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f33ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reflected location error in ozone data simulation\n",
    "\n",
    "import torch\n",
    "import torch.fft\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import argparse \n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "from sklearn.neighbors import BallTree\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CUSTOM PATHS ---\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "# (ÌïÑÏöî Ïãú Ïã§Ï†ú GEMS_TCO ÎùºÏù¥Î∏åÎü¨Î¶¨ import)\n",
    "try:\n",
    "    from GEMS_TCO import kernels_for_simulation_no_trend_020626 as kernels_simulation\n",
    "    \n",
    "    \n",
    "    \n",
    "    from GEMS_TCO import orderings as _orderings\n",
    "    from GEMS_TCO import alg_optimization, BaseLogger\n",
    "\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Warning: GEMS_TCO modules not found. Ensure paths are correct.\")\n",
    "from GEMS_TCO import kernels_reparam_space_time_gpu as kernels_gpu\n",
    "\n",
    "from GEMS_TCO import configuration as config\n",
    "from GEMS_TCO.data_loader import load_data2, exact_location_filter\n",
    "from GEMS_TCO import debiased_whittle\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d6ba3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [AR(1) Analysis for Mean Function] ---\n",
      "  Global Monthly Mean (a_y): 257.0667\n",
      "  Fitted AR(1) Phi: 0.6765\n",
      "Global Monthly Mean: {'2024_07_y24m07day01': 257.06668737814357, '2024_07_y24m07day02': 256.21248030343077, '2024_07_y24m07day03': 254.65984163444872, '2024_07_y24m07day04': 257.0527308280941, '2024_07_y24m07day05': 257.2790983012689, '2024_07_y24m07day06': 255.7956560813988, '2024_07_y24m07day07': 257.6881021700275, '2024_07_y24m07day08': 257.56118987013565, '2024_07_y24m07day09': 256.43433212608755, '2024_07_y24m07day10': 259.1536434348535, '2024_07_y24m07day11': 260.3522919246311, '2024_07_y24m07day12': 259.7040303955381, '2024_07_y24m07day13': 257.0934930401064, '2024_07_y24m07day14': 256.8174594747026, '2024_07_y24m07day15': 257.17490616699297, '2024_07_y24m07day16': 257.3539992400824, '2024_07_y24m07day17': 257.9564063039137, '2024_07_y24m07day18': 256.2079378650727, '2024_07_y24m07day19': 256.77166028450097, '2024_07_y24m07day20': 255.62018125292764, '2024_07_y24m07day21': 256.0174673291513, '2024_07_y24m07day22': 257.31759986311505, '2024_07_y24m07day23': 257.40056772285413, '2024_07_y24m07day24': 260.31735350112405, '2024_07_y24m07day25': 260.4355249777581, '2024_07_y24m07day26': 258.0906990075358, '2024_07_y24m07day27': 257.0043933607775, '2024_07_y24m07day28': 255.87734771877032, '2024_07_y24m07day29': 256.89638925162114, '2024_07_y24m07day30': 255.31197331342963, '2024_07_y24m07day31': 254.61949831519252}\n",
      "torch.Size([22680, 12])\n"
     ]
    }
   ],
   "source": [
    "space: List[str] = ['1', '1']\n",
    "lat_lon_resolution = [int(s) for s in space]\n",
    "mm_cond_number: int = 8\n",
    "years = ['2024']\n",
    "month_range = [7] \n",
    "\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "data_load_instance = load_data2(config.mac_data_load_path)\n",
    "\n",
    "#lat_range_input = [1, 3]\n",
    "#lon_range_input = [125.0, 129.0]\n",
    "\n",
    "lat_range_input=[-3,-1]      \n",
    "lon_range_input=[121, 125] \n",
    "\n",
    "#lat_range_input=[-3,2]      \n",
    "#lon_range_input=[121, 131] \n",
    "\n",
    "# Í∏∞Ï°¥: df_map, ord_mm, nns_map, day_offsets = ...\n",
    "# ÏàòÏ†ï ÌõÑ: Î≥ÄÏàòÎ™ÖÏùÑ monthly_meanÏúºÎ°ú Î≥ÄÍ≤Ω\n",
    "\n",
    "df_map, ord_mm, nns_map, monthly_mean = data_load_instance.load_maxmin_ordered_data_bymonthyear(\n",
    "    lat_lon_resolution=lat_lon_resolution, \n",
    "    mm_cond_number=mm_cond_number,\n",
    "    years_=years, \n",
    "    months_=month_range,\n",
    "    lat_range=lat_range_input,   \n",
    "    lon_range=lon_range_input\n",
    ")\n",
    "\n",
    "print(f\"Global Monthly Mean: {monthly_mean}\") # ÌôïÏù∏Ïö© Ï∂úÎ†•\n",
    "\n",
    "\n",
    "daily_aggregated_reg_vecc = [] \n",
    "daily_hourly_maps_reg_vecc = []      \n",
    "\n",
    "daily_aggregated_irr_vecc = [] \n",
    "daily_hourly_maps_irr_vecc = []   \n",
    "\n",
    "\n",
    "for day_index in range(31):\n",
    "    hour_start_index = day_index * 8\n",
    "    \n",
    "    hour_end_index = (day_index + 1) * 8\n",
    "    #hour_end_index = day_index*8 + 1\n",
    "    hour_indices = [hour_start_index, hour_end_index]\n",
    "\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "    df_map, \n",
    "    monthly_mean,  # <--- Ïù¥ Î∂ÄÎ∂ÑÏù¥ Ï∂îÍ∞ÄÎêòÏñ¥Ïïº Ìï©ÎãàÎã§\n",
    "    hour_indices, \n",
    "    ord_mm=ord_mm,\n",
    "    dtype=torch.float64, \n",
    "    keep_ori=False\n",
    "    )\n",
    "\n",
    "    daily_aggregated_reg_vecc.append( day_aggregated_tensor )\n",
    "    daily_hourly_maps_reg_vecc.append( day_hourly_map )\n",
    "\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "    df_map, \n",
    "    monthly_mean,  # <--- Ïù¥ Î∂ÄÎ∂ÑÏù¥ Ï∂îÍ∞ÄÎêòÏñ¥Ïïº Ìï©ÎãàÎã§\n",
    "    hour_indices, \n",
    "    ord_mm=ord_mm,\n",
    "    dtype=torch.float64, \n",
    "    keep_ori= True\n",
    "    )\n",
    "\n",
    "    daily_aggregated_irr_vecc.append( day_aggregated_tensor )\n",
    "    daily_hourly_maps_irr_vecc.append( day_hourly_map )\n",
    "print(daily_aggregated_irr_vecc[0].shape)\n",
    "\n",
    "nn = daily_aggregated_irr_vecc[0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ec4a7e",
   "metadata": {},
   "source": [
    "ÏàòÏ†ïÎêú ÏΩîÎìúÏùò Íµ¨Ï°∞\n",
    "\n",
    "Vecc_Irr (Original): Ïã§Ï†ú ÏúÑÏÑ± Ï¢åÌëú ÏÇ¨Ïö© (Ïã§ÏàòÌòï).\n",
    "\n",
    "Vecc_Reg (Snapped): Ïã§Ï†ú Ï¢åÌëúÎ•º Í∞ÄÏû• Í∞ÄÍπåÏö¥ Í≤©ÏûêÏ†êÏúºÎ°ú Ïù¥ÎèôÏãúÌÇ® Îç∞Ïù¥ÌÑ∞ (Í∞úÏàòÎäî IrrÏôÄ ÎèôÏùº, Ï¢åÌëúÎßå Ï†ïÏàòÌòï Í≤©Ïûê).\n",
    "\n",
    "DW_Full (Complete): ÏúÑÏÑ± Í≤ΩÎ°úÏôÄ Î¨¥Í¥ÄÌïòÍ≤å ÏßÅÏÇ¨Í∞ÅÌòï Ï†ÑÏ≤¥Î•º ÍΩâ Ï±ÑÏö¥ Îç∞Ïù¥ÌÑ∞ (Í∞úÏàòÍ∞Ä ÎßéÏùå)\n",
    "\n",
    "   [Simulation Info]\n",
    "    - Daily Base (Offset): 265.0\n",
    "    - Hourly Trend (Target for Dummies): [15. 10.  5.  0.  0.  5. 10. 15.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e65dbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Inputs:\n",
      " -> Target SigmaSq: 13.059\n",
      "\n",
      "üöÄ Generating 3 Distinct Datasets (Irr, Reg, DW)...\n",
      "\n",
      "[Validation]\n",
      "1. Vecc Irr (Swath, Float Coords): torch.Size([22680, 12])\n",
      "2. Vecc Reg (Swath, Grid Coords):  torch.Size([22680, 12])\n",
      "3. DW Full  (Rectangular Grid):    torch.Size([23552, 12])\n",
      "   -> DW Count Match: 23552 == 23552 (Nx46*Ny64*8)\n",
      "Generating NNS Map for Full Grid (Size: 23552)...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "import sys\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.float64\n",
    "\n",
    "# --- 2. GRID & FFT HELPERS ---\n",
    "def make_target_grid(lat_start, lat_end, lat_step, lon_start, lon_end, lon_step, device, dtype):\n",
    "    if lat_start > lat_end and lat_step > 0: lat_step = -lat_step\n",
    "    if lon_start > lon_end and lon_step > 0: lon_step = -lon_step\n",
    "    lats = torch.arange(lat_start, lat_end - 0.0001, lat_step, device=device, dtype=dtype)\n",
    "    lons = torch.arange(lon_start, lon_end + 0.0001, lon_step, device=device, dtype=dtype)\n",
    "    lats = torch.round(lats * 10000) / 10000\n",
    "    lons = torch.round(lons * 10000) / 10000\n",
    "    grid_lat, grid_lon = torch.meshgrid(lats, lons, indexing='ij')\n",
    "    center_points = torch.stack([grid_lat.flatten(), grid_lon.flatten()], dim=1)\n",
    "    return center_points, len(lats), len(lons)\n",
    "\n",
    "def generate_high_res_field(target_lat_range, target_lon_range, t_steps, params, device, dtype):\n",
    "    lat_res_factor, lon_res_factor = 200.0, 4.0\n",
    "    lat_res_high = 0.044 / lat_res_factor\n",
    "    lon_res_high = 0.063 / lon_res_factor\n",
    "    \n",
    "    t_lat_max, t_lat_min = max(target_lat_range), min(target_lat_range)\n",
    "    lats_high = torch.arange(t_lat_max + 0.1, t_lat_min - 0.1, -lat_res_high, device=device, dtype=dtype)\n",
    "    lons_high = torch.arange(target_lon_range[0] - 0.1, target_lon_range[1] + 0.1, lon_res_high, device=device, dtype=dtype)\n",
    "    \n",
    "    Nx, Ny, Nt = len(lats_high), len(lons_high), t_steps\n",
    "    dlat, dlon, dt = lat_res_high, lon_res_high, 1.0\n",
    "    \n",
    "    Px, Py, Pt = 2*Nx, 2*Ny, 2*Nt\n",
    "    lags_x = torch.arange(Px, device=device, dtype=dtype) * dlat; lags_x[Px//2:] -= (Px * dlat)\n",
    "    lags_y = torch.arange(Py, device=device, dtype=dtype) * dlon; lags_y[Py//2:] -= (Py * dlon)\n",
    "    lags_t = torch.arange(Pt, device=device, dtype=dtype) * dt;   lags_t[Pt//2:] -= (Pt * dt)\n",
    "\n",
    "    L_x, L_y, L_t = torch.meshgrid(lags_x, lags_y, lags_t, indexing='ij')\n",
    "    phi1, phi2, phi3, phi4 = torch.exp(params[0]), torch.exp(params[1]), torch.exp(params[2]), torch.exp(params[3])\n",
    "    adv_lat, adv_lon = params[4], params[5]\n",
    "    sigma_sq = phi1 / phi2 \n",
    "    \n",
    "    u_x = L_x - adv_lat * L_t\n",
    "    u_y = L_y - adv_lon * L_t\n",
    "    dist_sq = (u_x * torch.sqrt(phi3) * phi2)**2 + (u_y * phi2)**2 + (L_t * torch.sqrt(phi4) * phi2)**2\n",
    "    C_vals = sigma_sq * torch.exp(-torch.sqrt(dist_sq + 1e-12))\n",
    "\n",
    "    S = torch.fft.fftn(C_vals); S.real = torch.clamp(S.real, min=0)\n",
    "    random_phase = torch.fft.fftn(torch.randn(Px, Py, Pt, device=device, dtype=dtype))\n",
    "    field_sim_raw = torch.fft.ifftn(torch.sqrt(S.real) * random_phase).real\n",
    "    \n",
    "    field_sim = field_sim_raw[:Nx, :Ny, :Nt]\n",
    "    target_std = torch.sqrt(sigma_sq)\n",
    "    field_calibrated = (field_sim - field_sim.mean()) * (target_std / (field_sim.std() + 1e-9))\n",
    "    \n",
    "    return field_calibrated, lats_high, lons_high\n",
    "\n",
    "# --- [ÏàòÏ†ïÎêú ÌïµÏã¨ Ìï®Ïàò] 3Í∞ÄÏßÄ Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ± ---\n",
    "def generate_three_datasets(daily_maps_real, true_params_tensor, target_grid_info, device, dtype):\n",
    "    \n",
    "    lat_s, lat_e, lat_step, lon_s, lon_e, lon_step = target_grid_info\n",
    "    \n",
    "    # 1. Target Grid (Full)\n",
    "    target_grid_coords, Nx, Ny = make_target_grid(lat_s, lat_e, lat_step, lon_s, lon_e, lon_step, device, dtype)\n",
    "    full_grid_locs_np = target_grid_coords.cpu().numpy()\n",
    "    target_tree = BallTree(np.radians(full_grid_locs_np), metric='haversine') \n",
    "    \n",
    "    # 2. High-Res GP Field\n",
    "    high_res_field, lats_high, lons_high = generate_high_res_field((lat_s, lat_e), (lon_s, lon_e), 8, true_params_tensor, device, dtype)\n",
    "    \n",
    "    hr_mesh_lat, hr_mesh_lon = torch.meshgrid(lats_high, lons_high, indexing='ij')\n",
    "    hr_tree = BallTree(np.radians(torch.stack([hr_mesh_lat.flatten(), hr_mesh_lon.flatten()], dim=1).cpu().numpy()), metric='haversine')\n",
    "    high_res_flat = high_res_field.reshape(-1, 8) \n",
    "\n",
    "    # Ï†ÄÏû•ÏÜå (3Ï¢ÖÎ•ò)\n",
    "    # (1) Irr: Real Coordinates\n",
    "    # (2) Reg: Snapped Coordinates (Subset of DW)\n",
    "    # (3) DW : Full Grid Coordinates\n",
    "    list_irr, list_reg, list_dw = [], [], []\n",
    "    map_irr, map_reg, map_dw = {}, {}, {}\n",
    "\n",
    "    noise_std = torch.sqrt(torch.exp(true_params_tensor[6]))\n",
    "    DAILY_BASE = 260.0 + 5.0 \n",
    "    HOURLY_TREND = torch.tensor([15.0, 10.0, 5.0, 0.0, 0.0, 5.0, 10.0, 15.0], device=device, dtype=dtype)\n",
    "\n",
    "    day0_dict = daily_maps_real[0]\n",
    "    sorted_keys = sorted([k for k in day0_dict.keys() if 'hm' in k or 'time' in k])\n",
    "    \n",
    "    # -------------------------------------------------------------\n",
    "    # Step A: Generate [DW Full] & Cache Values for Reg\n",
    "    # -------------------------------------------------------------\n",
    "    # Full GridÏóê ÎåÄÌïú High-Res Îß§Ìïë Ïù∏Îç±Ïä§ ÎØ∏Î¶¨ Í≥ÑÏÇ∞\n",
    "    _, hr_indices_full = hr_tree.query(np.radians(full_grid_locs_np), k=1)\n",
    "    hr_indices_full = torch.tensor(hr_indices_full.flatten(), device=device)\n",
    "    \n",
    "    # ÎÇòÏ§ëÏóê Reg Îç∞Ïù¥ÌÑ∞ ÎßåÎì§ Îïå Ïì∏ Í∞íÎì§ÏùÑ ÏûÑÏãú Ï†ÄÏû•\n",
    "    full_sim_values_cache = [] \n",
    "\n",
    "    for t_idx in range(8):\n",
    "        key = f\"time_{t_idx}\"\n",
    "        \n",
    "        # GP + Noise + Trend + Base (Full Grid)\n",
    "        gp_signal = high_res_flat[hr_indices_full, t_idx]\n",
    "        current_trend = HOURLY_TREND[t_idx]\n",
    "        sim_vals = gp_signal + (torch.randn_like(gp_signal) * noise_std) + DAILY_BASE + current_trend\n",
    "        \n",
    "        # Cache for Reg\n",
    "        full_sim_values_cache.append(sim_vals)\n",
    "        \n",
    "        # Features for DW\n",
    "        N_full = len(target_grid_coords)\n",
    "        time_col = torch.full((N_full, 1), float(t_idx), device=device, dtype=dtype)\n",
    "        dummies = F.one_hot(torch.tensor([t_idx]), num_classes=8).repeat(N_full, 1)[:, 1:].to(device=device, dtype=dtype)\n",
    "        offset_col = torch.full((N_full, 1), DAILY_BASE, device=device, dtype=dtype)\n",
    "        \n",
    "        dw_row = torch.cat([target_grid_coords, sim_vals.unsqueeze(-1), time_col, dummies, offset_col], dim=1)\n",
    "        list_dw.append(dw_row)\n",
    "        map_dw[key] = dw_row\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Step B: Generate [Vecc Irr] & [Vecc Reg] (Path-based)\n",
    "    # -------------------------------------------------------------\n",
    "    for t_idx, key in enumerate(sorted_keys):\n",
    "        if t_idx >= 8: break\n",
    "        \n",
    "        real_tensor = day0_dict[key].to(device)\n",
    "        real_locs = real_tensor[:, :2]\n",
    "        N_points = len(real_locs)\n",
    "        real_locs_np = real_locs.cpu().numpy()\n",
    "        \n",
    "        # 1. Vecc Irr (Original Coords)\n",
    "        _, hr_indices_irr = hr_tree.query(np.radians(real_locs_np), k=1)\n",
    "        gp_signal_irr = high_res_flat[torch.tensor(hr_indices_irr.flatten(), device=device), t_idx]\n",
    "        \n",
    "        current_trend = HOURLY_TREND[t_idx]\n",
    "        sim_vals_irr = gp_signal_irr + (torch.randn_like(gp_signal_irr) * noise_std) + DAILY_BASE + current_trend\n",
    "        \n",
    "        dummies = F.one_hot(torch.tensor([t_idx]), num_classes=8).repeat(N_points, 1)[:, 1:].to(device=device, dtype=dtype)\n",
    "        offset_col = torch.full((N_points, 1), DAILY_BASE, device=device, dtype=dtype)\n",
    "        \n",
    "        irr_row = torch.cat([real_locs, sim_vals_irr.unsqueeze(-1), real_tensor[:, 3].unsqueeze(-1), dummies, offset_col], dim=1)\n",
    "        list_irr.append(irr_row)\n",
    "        map_irr[key] = irr_row\n",
    "        \n",
    "        # 2. Vecc Reg (Snapped Coords)\n",
    "        # Ïã§Ï†ú ÏúÑÏπòÏóêÏÑú Í∞ÄÏû• Í∞ÄÍπåÏö¥ \"Í≤©Ïûê Ïù∏Îç±Ïä§\" Ï∞æÍ∏∞\n",
    "        _, grid_indices = target_tree.query(np.radians(real_locs_np), k=1)\n",
    "        grid_indices = grid_indices.flatten()\n",
    "        \n",
    "        # DWÏóêÏÑú ÏÉùÏÑ±Ìï¥Îëî Í∞í Í∞ÄÏ†∏Ïò§Í∏∞ (Consistency Î≥¥Ïû•)\n",
    "        sim_vals_reg = full_sim_values_cache[t_idx][grid_indices]\n",
    "        mapped_locs = target_grid_coords[torch.tensor(grid_indices, device=device)]\n",
    "        \n",
    "        reg_row = torch.cat([mapped_locs, sim_vals_reg.unsqueeze(-1), real_tensor[:, 3].unsqueeze(-1), dummies, offset_col], dim=1)\n",
    "        list_reg.append(reg_row)\n",
    "        map_reg[key] = reg_row\n",
    "\n",
    "    # Returns 3 Tuples\n",
    "    return (\n",
    "        (torch.cat(list_irr), map_irr),  # 1. Irr\n",
    "        (torch.cat(list_reg), map_reg),  # 2. Reg\n",
    "        (torch.cat(list_dw), map_dw)     # 3. DW\n",
    "    )\n",
    "\n",
    "# --- 4. EXECUTION ---\n",
    "# ... (ÏÑ§Ï†ï Î∂ÄÎ∂Ñ ÎèôÏùº) ...\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.float64\n",
    "target_grid_info = (-1.0, -3.0, 0.044, 121.0, 125.0, 0.063)\n",
    "\n",
    "# True Params\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lon = 0.195 \n",
    "init_range_lat = 0.154 \n",
    "init_range_time = 1.0\n",
    "init_nugget    = 0.247\n",
    "init_advec_lat = 0.0418\n",
    "init_advec_lon = -0.1689\n",
    "\n",
    "true_phi2 = 1.0 / init_range_lon              \n",
    "true_phi1 = init_sigmasq * true_phi2          \n",
    "true_phi3 = (init_range_lon / init_range_lat)**2\n",
    "true_phi4 = (init_range_lon / init_range_time)**2\n",
    "\n",
    "true_params_tensor = [\n",
    "    torch.tensor([np.log(true_phi1)], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([np.log(true_phi2)], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([np.log(true_phi3)], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([np.log(true_phi4)], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([init_advec_lat],    device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([init_advec_lon],    device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([np.log(init_nugget)], device=DEVICE, dtype=DTYPE)\n",
    "]\n",
    "\n",
    "print(\"Checking Inputs:\")\n",
    "print(f\" -> Target SigmaSq: {init_sigmasq}\")\n",
    "\n",
    "if 'daily_hourly_maps_irr_vecc' in locals() and len(daily_hourly_maps_irr_vecc) > 0:\n",
    "    print(\"\\nüöÄ Generating 3 Distinct Datasets (Irr, Reg, DW)...\")\n",
    "    \n",
    "    # [ÏàòÏ†ï] 3Í∞úÎ•º Î™ÖÌôïÌûà Î∞õÏäµÎãàÎã§.\n",
    "    (vecc_irr_data, vecc_reg_data, dw_full_data) = generate_three_datasets(\n",
    "        daily_hourly_maps_irr_vecc, true_params_tensor, target_grid_info, DEVICE, DTYPE\n",
    "    )\n",
    "    \n",
    "    # Unpack\n",
    "    agg_irr, map_irr = vecc_irr_data\n",
    "    agg_reg, map_reg = vecc_reg_data\n",
    "    agg_dw,  map_dw  = dw_full_data\n",
    "    \n",
    "    # Global Variables for Models\n",
    "    daily_aggregated_irr_vecc_sim = [agg_irr]\n",
    "    daily_hourly_maps_irr_vecc_sim = [map_irr]\n",
    "    \n",
    "    daily_aggregated_reg_vecc_sim = [agg_reg] # Snapped Swath\n",
    "    daily_hourly_maps_reg_vecc_sim = [map_reg]\n",
    "    \n",
    "    \n",
    "    daily_aggregated_dw_sim = [agg_dw] # Full Grid\n",
    "    daily_hourly_maps_dw_sim = [map_dw]\n",
    "    \n",
    "    print(f\"\\n[Validation]\")\n",
    "    print(f\"1. Vecc Irr (Swath, Float Coords): {agg_irr.shape}\")\n",
    "    print(f\"2. Vecc Reg (Swath, Grid Coords):  {agg_reg.shape}\")\n",
    "    print(f\"3. DW Full  (Rectangular Grid):    {agg_dw.shape}\")\n",
    "    \n",
    "    # Grid Size Check\n",
    "    _, Nx, Ny = make_target_grid(-1.0, -3.0, 0.044, 121.0, 125.0, 0.063, DEVICE, DTYPE)\n",
    "    expected_dw = Nx * Ny * 8\n",
    "    print(f\"   -> DW Count Match: {agg_dw.shape[0]} == {expected_dw} (Nx{Nx}*Ny{Ny}*8)\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: Load data first.\")\n",
    "\n",
    "\n",
    "# 1. DW Îç∞Ïù¥ÌÑ∞ Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "dw_tensor = daily_aggregated_dw_sim[0]\n",
    "coords_dw = dw_tensor[:, :2].cpu().numpy()\n",
    "\n",
    "print(f\"Generating NNS Map for Full Grid (Size: {len(coords_dw)})...\")\n",
    "\n",
    "# 2. MaxMin Ordering ÏàòÌñâ (Îç∞Ïù¥ÌÑ∞ Ïû¨Ï†ïÎ†¨)\n",
    "# VecchiaÎäî ÏàúÏÑúÍ∞Ä Ï§ëÏöîÌïòÎØÄÎ°ú, Î¨¥ÏûëÏúÑ Í≤©Ïûê ÏàúÏÑúÎ≥¥Îã§Îäî MaxMinÏù¥ ÏÑ±Îä•Ïù¥ Ï¢ãÏäµÎãàÎã§.\n",
    "ord_mm_dw = _orderings.maxmin_cpp(coords_dw)\n",
    "dw_tensor_ordered = dw_tensor[ord_mm_dw]  # ‚òÖ Ï§ëÏöî: Îç∞Ïù¥ÌÑ∞ ÏàúÏÑú Î≥ÄÍ≤Ω\n",
    "\n",
    "# 3. DWÏö© NNS Map ÏÉùÏÑ±\n",
    "coords_dw_ordered = dw_tensor_ordered[:, :2].cpu().numpy()\n",
    "nns_map_dw = _orderings.find_nns_l2(locs=coords_dw_ordered, max_nn=mm_cond_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7191bead",
   "metadata": {},
   "source": [
    "# Fit vecchia max min time 2 \n",
    "\n",
    "reg sim  ÏóêÎü¨ÎÇòÎäî Ïù¥Ïú†Îäî Îç∞Ïù¥ÌÑ∞ Ï§ëÎ≥µ snap ÎïåÎ¨∏\n",
    "\n",
    "Ï§ëÏöî! reg sim ÏùÄ Îç∞Ïù¥ÌÑ∞ Ï§ëÏã¨, Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ ÏúÑÏπòÏóêÏÑú Í∞ÄÏû• Í∞ÄÍπåÏö¥ Í∑∏Î¶¨ÎìúÎ°ú Ïù¥Îèô Ïù¥ÎûòÏÑú Ï§ëÎ≥µ Í∞ÄÎä•, ÏΩúÎ†àÏä§ÌÇ§ Íπ®ÏßÄÍ∏∞ÎèÑ \n",
    "\n",
    "dw Îäî Í∑∏Î¶¨Îìú Ï§ëÏã¨, ÌÉÄÍ≤ü Î†àÍ∑§Îü¨ Í∑∏Î¶¨ÎìúÏóêÏÑú Í∞ÄÏû• Í∞ÄÍπåÏö¥ Ï†ê Í∞ÄÏ†∏Ïò§Í∏∞ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b800f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "========================================\n",
      "--- Initializing VecchiaBatched Model ---\n",
      "========================================\n",
      "\n",
      "========================================\n",
      "--- Running L-BFGS Optimization ---\n",
      "========================================\n",
      "üöÄ Pre-computing (Time Dummies & Latitude Centering, NO Offset)... [Mean Lat: -1.9900] ‚úÖ Done. (Heads: 2400, Tails: 21152)\n",
      "--- Starting Batched L-BFGS Optimization (GPU) ---\n",
      "--- Step 1/2 / Loss: 1.681878 ---\n",
      "  Param 0: Value=4.2081, Grad=-1.8621361352240146e-06\n",
      "  Param 1: Value=1.6610, Grad=-1.1731858640516178e-06\n",
      "  Param 2: Value=0.4206, Grad=-1.5414708073244188e-06\n",
      "  Param 3: Value=-3.4774, Grad=-4.087361791672145e-07\n",
      "  Param 4: Value=-0.0183, Grad=-6.413948392368363e-08\n",
      "  Param 5: Value=-0.1815, Grad=4.270714601444002e-06\n",
      "  Param 6: Value=-1.3861, Grad=-1.0950438231585916e-05\n",
      "  Max Abs Grad: 1.095044e-05\n",
      "------------------------------\n",
      "--- Step 2/2 / Loss: 1.681606 ---\n",
      "  Param 0: Value=4.2081, Grad=-1.8621361352240146e-06\n",
      "  Param 1: Value=1.6610, Grad=-1.1731858640516178e-06\n",
      "  Param 2: Value=0.4206, Grad=-1.5414708073244188e-06\n",
      "  Param 3: Value=-3.4774, Grad=-4.087361791672145e-07\n",
      "  Param 4: Value=-0.0183, Grad=-6.413948392368363e-08\n",
      "  Param 5: Value=-0.1815, Grad=4.270714601444002e-06\n",
      "  Param 6: Value=-1.3861, Grad=-1.0950438231585916e-05\n",
      "  Max Abs Grad: 1.095044e-05\n",
      "------------------------------\n",
      "Final Interpretable Params: {'sigma_sq': 12.769596103987634, 'range_lon': 0.18994956189017806, 'range_lat': 0.1539205661855349, 'range_time': 1.0808176483809082, 'advec_lat': -0.018327408486246168, 'advec_lon': -0.18151784072220092, 'nugget': 0.2500480576450665}\n",
      "\n",
      "Optimization finished in 11.48s.\n",
      "Results after 1 steps: [4.208063746838271, 1.6609967057992905, 0.4206438156637803, -3.477429084655411, -0.018327408486246168, -0.18151784072220092, -1.3861021490135552, 1.681606150165216]\n",
      "Final Params: [ 4.20806375  1.66099671  0.42064382 -3.47742908 -0.01832741 -0.18151784\n",
      " -1.38610215]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "v = 0.5              # Smoothness\n",
    "mm_cond_number = 8   # Neighbors\n",
    "#mm_cond_number = 16   # Neighbors\n",
    "nheads = 300           # 0 = Pure Vecchia\n",
    "lr = 1.0             # LBFGS learning rate\n",
    "LBFGS_MAX_STEPS = 2\n",
    "LBFGS_HISTORY_SIZE = 100 # 100\n",
    "LBFGS_LR = 1.0\n",
    "LBFGS_MAX_EVAL = 30    \n",
    "\n",
    "#DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- 1. SETUP PARAMETERS (List of Scalars) ---\n",
    "# Truth: [4.18, 1.94, 0.24, -3.97, 0.014, -0.20, -0.85]\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lat = 0.154 \n",
    "init_range_lon = 0.195\n",
    "init_advec_lat = 0.0218\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "# Map model parameters to the 'phi' reparameterization\n",
    "init_phi2 = 1.0 / init_range_lon                # 1/range_lon\n",
    "init_phi1 = init_sigmasq * init_phi2            # sigmasq / range_lon\n",
    "init_phi3 = (init_range_lon / init_range_lat)**2  # (range_lon / range_lat)^2\n",
    "init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "# Create Initial Parameters (Float64, Requires Grad)\n",
    "initial_vals = [np.log(init_phi1), np.log(init_phi2), np.log(init_phi3), \n",
    "                np.log(init_phi4), init_advec_lat, init_advec_lon, np.log(init_nugget)]\n",
    "\n",
    "# [4.2042, 1.6348, 0.4721, -3.2695, 0.0218, -0.1689, -1.3984]\n",
    "params_list = [\n",
    "    torch.tensor([val], requires_grad=True, dtype=torch.float64, device=DEVICE)\n",
    "    for val in initial_vals\n",
    "]\n",
    "\n",
    "# --- 2. INSTANTIATE MODEL ---\n",
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'--- Initializing VecchiaBatched Model ---')\n",
    "print(f'{\"=\"*40}')\n",
    "\n",
    "if isinstance(daily_aggregated_irr_vecc_sim, torch.Tensor):\n",
    "    daily_aggregated_irr_vecc_sim = daily_aggregated_irr_vecc_sim.to(DEVICE)\n",
    "\n",
    "# Instantiate\n",
    "model_instance = kernels_gpu.fit_vecchia_lbfgs(\n",
    "    smooth=v,\n",
    "\n",
    "    #input_map=daily_hourly_maps_reg_vecc_sim[0],\n",
    "    #aggregated_data= daily_aggregated_reg_vecc_sim[0],\n",
    "    #input_map=daily_hourly_maps_irr_vecc_sim[0],\n",
    "    #aggregated_data= daily_aggregated_irr_vecc_sim[0],\n",
    "    input_map=daily_hourly_maps_dw_sim[0],\n",
    "    aggregated_data= daily_aggregated_dw_sim[0],\n",
    "    nns_map=nns_map_dw,      # or nns_map for the swath data\n",
    "    mm_cond_number=mm_cond_number,\n",
    "    nheads=nheads\n",
    ")\n",
    "\n",
    "'''\n",
    "model_instance = kernels_reparam_space_time_gpu_col.fit_vecchia_lbfgs(\n",
    "    smooth=v,\n",
    "    #input_map=daily_hourly_maps_vecc_sim[0],\n",
    "    #aggregated_data= daily_aggregated_tensors_vecc_sim[0],\n",
    "\n",
    "    input_map=daily_hourly_maps_irr_vecc_sim[0],\n",
    "    aggregated_data= daily_aggregated_irr_vecc_sim[0],\n",
    "\n",
    "    nns_map=None,\n",
    "    mm_cond_number=mm_cond_number\n",
    ")\n",
    "''' \n",
    "\n",
    "# --- 3. OPTIMIZATION LOOP ---\n",
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'--- Running L-BFGS Optimization ---')\n",
    "print(f'{\"=\"*40}')\n",
    "\n",
    "# Optimizer takes the LIST of scalars\n",
    "optimizer_vecc = model_instance.set_optimizer(\n",
    "            params_list,     \n",
    "            lr=LBFGS_LR,            \n",
    "            max_iter=LBFGS_MAX_EVAL,        \n",
    "            history_size=LBFGS_HISTORY_SIZE \n",
    "        )\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "out, steps_ran = model_instance.fit_vecc_lbfgs(\n",
    "        params_list,\n",
    "        optimizer_vecc,\n",
    "        # covariance_function argument is GONE\n",
    "        max_steps=LBFGS_MAX_STEPS, \n",
    "        grad_tol=1e-7\n",
    "    )\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "epoch_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nOptimization finished in {epoch_time:.2f}s.\")\n",
    "print(f\"Results after {steps_ran} steps: {out}\")\n",
    "print(f\"Final Params: {torch.cat(params_list).detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d6a00",
   "metadata": {},
   "source": [
    "irr\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 13.251375803999347, 'range_lon': 0.17490332632183242, 'range_lat': 0.14574358781113017, 'range_time': 0.8728788765946908, 'advec_lat': -0.05107314841222374, 'advec_lon': -0.17666227434803639, 'nugget': 0.24422111544308162}\n",
    "\n",
    "Optimization finished in 11.47s.\n",
    "Results after 1 steps: [4.32762326002336, 1.7435218787174314, 0.3647691408733439, -3.2151268041425722, -0.05107314841222374, -0.17666227434803639, -1.4096812532177727, 1.1799846924366073]\n",
    "Final Params: [ 4.32762326  1.74352188  0.36476914 -3.2151268  -0.05107315 -0.17666227\n",
    " -1.40968125]\n",
    "\n",
    "\n",
    "reg\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 13.411637991321767, 'range_lon': 0.19734738984885475, 'range_lat': 0.16507470051912865, 'range_time': 0.9563503788327048, 'advec_lat': -0.049054522467388444, 'advec_lon': -0.17787447986717292, 'nugget': 0.5824048225534659}\n",
    "\n",
    "Optimization finished in 12.16s.\n",
    "Results after 1 steps: [4.218912539799084, 1.6227897029609746, 0.35713494830623127, -3.156317549877595, -0.049054522467388444, -0.17787447986717292, -0.5405895016506577, 1.2122487753198843]\n",
    "Final Params: [ 4.21891254  1.6227897   0.35713495 -3.15631755 -0.04905452 -0.17787448\n",
    " -0.5405895 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c961fa4",
   "metadata": {},
   "source": [
    "# fit dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9b3e3",
   "metadata": {},
   "source": [
    "difference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0b29571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2835, 12])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day=0\n",
    "daily_hourly_maps_reg_vecc_sim[day]['2024_07_y24m07day01_hm00:53'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d43435c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22680, 4])\n",
      "22680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ -2.9800, 121.0000,  -2.3671,   0.0000],\n",
       "        [ -2.9800, 121.0630,   0.3398,   0.0000],\n",
       "        [ -2.9800, 121.1260,   3.9098,   0.0000],\n",
       "        [ -2.9800, 121.1890,   6.2922,   0.0000],\n",
       "        [ -2.9800, 121.2520,   2.1264,   0.0000],\n",
       "        [ -2.9800, 121.3150,   2.4780,   0.0000],\n",
       "        [ -2.9800, 121.3780,  -3.4540,   0.0000],\n",
       "        [ -2.9800, 121.4410,  -2.2484,   0.0000],\n",
       "        [ -2.9800, 121.5040,   4.0353,   0.0000],\n",
       "        [ -2.9800, 121.5670,   1.5458,   0.0000],\n",
       "        [ -2.9800, 121.6300,  -3.3241,   0.0000],\n",
       "        [ -2.9800, 121.6930,   1.8169,   0.0000],\n",
       "        [ -2.9800, 121.7560,  -0.5641,   0.0000],\n",
       "        [ -2.9800, 121.8190,   3.7625,   0.0000],\n",
       "        [ -2.9800, 121.8820,  -6.0315,   0.0000],\n",
       "        [ -2.9800, 121.9450,  -2.7170,   0.0000],\n",
       "        [ -2.9800, 122.0080,   2.3653,   0.0000],\n",
       "        [ -2.9800, 122.0710,  -3.4617,   0.0000],\n",
       "        [ -2.9800, 122.1340,   7.0681,   0.0000],\n",
       "        [ -2.9800, 122.1970,  -1.8939,   0.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [11.0474, 0.0623, 0.2445, 1.0972, 0.0101, -0.1671, 1.1825]\n",
    "day = 0 # 0 index\n",
    "lat_range= [-3, -1]\n",
    "lon_range= [121, 125]\n",
    "#lat_range= [1,3]\n",
    "#lon_range= [125, 129.0]\n",
    "\n",
    "daily_aggregated_tensors_dw = [daily_aggregated_dw_sim[day]]\n",
    "daily_hourly_maps_dw = [daily_hourly_maps_dw_sim[day]]\n",
    "\n",
    "db = debiased_whittle.debiased_whittle_preprocess(daily_aggregated_tensors_dw, daily_hourly_maps_dw, day_idx=day, params_list=a, lat_range=lat_range, lon_range=lon_range)\n",
    "\n",
    "\n",
    "subsetted_aggregated_day = db.generate_spatially_filtered_days(-3,-1,121,125)\n",
    "print(subsetted_aggregated_day.shape)\n",
    "N2= subsetted_aggregated_day.shape[0]\n",
    "print(N2)\n",
    "subsetted_aggregated_day[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2497b8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Pre-computing J-vector (Hamming taper)...\n",
      "Pre-computing sample periodogram...\n",
      "Pre-computing Hamming taper autocorrelation...\n",
      "Data grid: 45x63, 8 time points. J-vector, Periodogram, Taper Autocorr on cpu.\n",
      "\n",
      "============================== Initialization Run 1/1 ==============================\n",
      "Starting with FIXED params (raw log-scale): [4.2042, 1.6348, 0.4721, -2.5562, 0.0218, -0.1689, -1.3984]\n",
      "Starting optimization run 1 on device cpu (Hamming, 7-param ST kernel, L-BFGS)...\n",
      "--- Step 1/20 ---\n",
      " Loss: 1.069512 | Max Grad: 1.137849e-02\n",
      "  Params (Raw Log): log_phi1: 4.1809, log_phi2: 1.5701, log_phi3: 0.4999, log_phi4: -3.2947, advec_lat: 0.0773, advec_lon: -0.1356, log_nugget: -2.7566\n",
      "--- Step 2/20 ---\n",
      " Loss: 1.028813 | Max Grad: 2.466505e-05\n",
      "  Params (Raw Log): log_phi1: 4.1963, log_phi2: 1.5950, log_phi3: 0.5039, log_phi4: -3.2986, advec_lat: 0.0776, advec_lon: -0.1368, log_nugget: -4.1302\n",
      "--- Step 3/20 ---\n",
      " Loss: 1.028628 | Max Grad: 2.466505e-05\n",
      "  Params (Raw Log): log_phi1: 4.1963, log_phi2: 1.5950, log_phi3: 0.5039, log_phi4: -3.2986, advec_lat: 0.0776, advec_lon: -0.1368, log_nugget: -4.1302\n",
      "--- Step 4/20 ---\n",
      " Loss: 1.028628 | Max Grad: 2.466505e-05\n",
      "  Params (Raw Log): log_phi1: 4.1963, log_phi2: 1.5950, log_phi3: 0.5039, log_phi4: -3.2986, advec_lat: 0.0776, advec_lon: -0.1368, log_nugget: -4.1302\n",
      "\n",
      "--- Converged on loss change (change < 1e-12) at step 4 ---\n",
      "\n",
      "--- Training Complete ---\n",
      "\n",
      "FINAL BEST STATE ACHIEVED (during training):\n",
      "Best Loss: 1.029\n",
      "\n",
      "\n",
      "========================= Overall Result from Run ========================= =========================\n",
      "Best Run Loss: 1.029 (after 4 steps)\n",
      "Final Parameters (Natural Scale): sigmasq: 13.4821, range_lat: 0.1577, range_lon: 0.2029, range_time: 1.0559, advec_lat: 0.0776, advec_lon: -0.1368, nugget: 0.0161\n",
      "Final Parameters (Phi Scale)    : phi1: 66.4420, phi2: 4.9282, phi3: 1.6552, phi4: 0.0369, advec_lat: 0.0776, advec_lon: -0.1368, nugget: 0.0161\n",
      "Final Parameters (Raw Log Scale): log_phi1: 4.1963, log_phi2: 1.5950, log_phi3: 0.5039, log_phi4: -3.2986, advec_lat: 0.0776, advec_lon: -0.1368, log_nugget: -4.1302\n",
      "\n",
      "Total execution time: 3.52 seconds\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Parameter\n",
    "dwl = debiased_whittle.debiased_whittle_likelihood()\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    DAY_TO_RUN = 3 # data is decided above\n",
    "    TAPERING_FUNC = dwl.cgn_hamming # Use Hamming taper\n",
    "    NUM_RUNS = 1\n",
    "    MAX_STEPS = 20 # L-BFGS usually converges in far fewer steps\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063 \n",
    "\n",
    "    LAT_COL, LON_COL = 0, 1\n",
    "    VAL_COL = 2 # Spatially differenced value\n",
    "    TIME_COL = 3\n",
    "\n",
    "\n",
    "    cur_df =subsetted_aggregated_day\n",
    "    \n",
    "    if cur_df.numel() == 0 or cur_df.shape[1] <= max(LAT_COL, LON_COL, VAL_COL, TIME_COL):\n",
    "        print(f\"Error: Data for Day {DAY_TO_RUN} is empty or invalid.\")\n",
    "        exit()\n",
    "\n",
    "    unique_times = torch.unique(cur_df[:, TIME_COL])\n",
    "    time_slices_list = [cur_df[cur_df[:, TIME_COL] == t_val] for t_val in unique_times]\n",
    "\n",
    "    # --- 1. Pre-compute J-vector, Taper Grid, and Taper Autocorrelation ---\n",
    "    print(\"Pre-computing J-vector (Hamming taper)...\")\n",
    "    \n",
    "    # --- üí• REVISED: Renamed 'p' to 'p_time' üí• ---\n",
    "    J_vec, n1, n2, p_time, taper_grid = dwl.generate_Jvector_tapered( \n",
    "        time_slices_list,\n",
    "        tapering_func=TAPERING_FUNC, \n",
    "        lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if J_vec is None or J_vec.numel() == 0 or n1 == 0 or n2 == 0 or p_time == 0:\n",
    "       print(f\"Error: J-vector generation failed for Day {DAY_TO_RUN}.\")\n",
    "       exit()\n",
    "       \n",
    "    print(\"Pre-computing sample periodogram...\")\n",
    "    I_sample = dwl.calculate_sample_periodogram_vectorized(J_vec)\n",
    "\n",
    "    print(\"Pre-computing Hamming taper autocorrelation...\")\n",
    "    taper_autocorr_grid = dwl.calculate_taper_autocorrelation_fft(taper_grid, n1, n2, DEVICE)\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Error: NaN/Inf in sample periodogram.\")\n",
    "        exit()\n",
    "    if torch.isnan(taper_autocorr_grid).any() or torch.isinf(taper_autocorr_grid).any():\n",
    "        print(\"Error: NaN/Inf in taper autocorrelation.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Data grid: {n1}x{n2}, {p_time} time points. J-vector, Periodogram, Taper Autocorr on {DEVICE}.\")\n",
    "    # --- END REVISION ---\n",
    "\n",
    "    # --- 2. Optimization Loop ---\n",
    "    all_final_results = []\n",
    "    all_final_losses = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        print(f\"\\n{'='*30} Initialization Run {i+1}/{NUM_RUNS} {'='*30}\")\n",
    "\n",
    "        # --- 7-PARAMETER initialization ---\n",
    "        ''' \n",
    "        init_sigmasq   = 15.0\n",
    "        init_range_lat = 0.66 \n",
    "        init_range_lon = 0.7 \n",
    "        init_nugget    = 1.5\n",
    "        init_beta      = 0.1  # Temporal range ratio\n",
    "        init_advec_lat = 0.02\n",
    "        init_advec_lon = -0.08\n",
    "        '''\n",
    "        init_sigmasq   = 13.059\n",
    "        init_range_lat = 0.154 \n",
    "        init_range_lon = 0.195\n",
    "        init_advec_lat = 0.0218\n",
    "        init_range_time = 0.7\n",
    "        init_advec_lon = -0.1689\n",
    "        init_nugget    = 0.247\n",
    "\n",
    "        init_phi2 = 1.0 / init_range_lon\n",
    "        init_phi1 = init_sigmasq * init_phi2\n",
    "        init_phi3 = (init_range_lon / init_range_lat)**2\n",
    "        # Change needed to match the spatial-temporal distance formula:\n",
    "        init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "        initial_params_values = [\n",
    "            np.log(init_phi1),    # [0] log_phi1\n",
    "            np.log(init_phi2),    # [1] log_phi2\n",
    "            np.log(init_phi3),    # [2] log_phi3\n",
    "            np.log(init_phi4),    # [3] log_phi4\n",
    "            init_advec_lat,       # [4] advec_lat (NOT log)\n",
    "            init_advec_lon,       # [5] advec_lon (NOT log)\n",
    "            np.log(init_nugget)   # [6] log_nugget\n",
    "        ]\n",
    "        \n",
    "        print(f\"Starting with FIXED params (raw log-scale): {[round(p, 4) for p in initial_params_values]}\")\n",
    "\n",
    "        params_list = [\n",
    "            Parameter(torch.tensor([val], dtype=torch.float64))\n",
    "            for val in initial_params_values\n",
    "        ]\n",
    "\n",
    "        # Helper to define the boundary globally for clarity\n",
    "        NUGGET_LOWER_BOUND = 0.05\n",
    "        LOG_NUGGET_LOWER_BOUND = np.log(NUGGET_LOWER_BOUND) # Approx -2.9957\n",
    "\n",
    "        # --- üí• REVISED: Use L-BFGS Optimizer üí• ---\n",
    "        optimizer = torch.optim.LBFGS(\n",
    "            params_list,\n",
    "            lr=1.0,           # Initial step length for line search\n",
    "            max_iter=20,      # Iterations per step\n",
    "            history_size=100,\n",
    "            line_search_fn=\"strong_wolfe\", # Often more robust\n",
    "            tolerance_grad=1e-5\n",
    "        )\n",
    "        # --- END REVISION ---\n",
    "\n",
    "        print(f\"Starting optimization run {i+1} on device {DEVICE} (Hamming, 7-param ST kernel, L-BFGS)...\")\n",
    "        \n",
    "        # --- üí• REVISED: Call L-BFGS trainer, pass p_time üí• ---\n",
    "        nat_params_str, phi_params_str, raw_params_str, loss, steps_run = dwl.run_lbfgs_tapered(\n",
    "            params_list=params_list,\n",
    "            optimizer=optimizer,\n",
    "            I_sample=I_sample,\n",
    "            n1=n1, n2=n2, p_time=p_time,\n",
    "            taper_autocorr_grid=taper_autocorr_grid, \n",
    "            max_steps=MAX_STEPS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        # --- END REVISION ---\n",
    "        \n",
    "        if loss is not None:\n",
    "            all_final_results.append((nat_params_str, phi_params_str, raw_params_str))\n",
    "            all_final_losses.append(loss)\n",
    "        else:\n",
    "            all_final_losses.append(float('inf'))\n",
    "\n",
    "    print(f\"\\n\\n{'='*25} Overall Result from Run {'='*25} {'='*25}\")\n",
    "    \n",
    "    valid_losses = [l for l in all_final_losses if l is not None and l != float('inf')]\n",
    "\n",
    "    if not valid_losses:\n",
    "        print(f\"The run failed or resulted in an invalid loss for Day {DAY_TO_RUN}.\")\n",
    "    else:\n",
    "        best_loss = min(valid_losses)\n",
    "        best_run_index = all_final_losses.index(best_loss)\n",
    "        best_results = all_final_results[best_run_index]\n",
    "        \n",
    "        print(f\"Best Run Loss: {best_loss} (after {steps_run} steps)\")\n",
    "        print(f\"Final Parameters (Natural Scale): {best_results[0]}\")\n",
    "        print(f\"Final Parameters (Phi Scale)    : {best_results[1]}\")\n",
    "        print(f\"Final Parameters (Raw Log Scale): {best_results[2]}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
