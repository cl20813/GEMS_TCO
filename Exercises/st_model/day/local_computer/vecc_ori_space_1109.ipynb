{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d8d314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from GEMS_TCO import kernels\n",
    "from GEMS_TCO import data_preprocess \n",
    "from GEMS_TCO import kernels_new, kernels_reparametrization_space as kernels_repar_space\n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO.data_loader import load_data2\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7bd05c",
   "metadata": {},
   "source": [
    "Load monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf44623d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting data to lat: [0.0, 5.0], lon: [123.0, 133.0]\n"
     ]
    }
   ],
   "source": [
    "space: List[str] = ['1', '1']\n",
    "lat_lon_resolution = [int(s) for s in space]\n",
    "mm_cond_number: int = 20\n",
    "years = ['2024']\n",
    "month_range = [7] \n",
    "\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "data_load_instance = load_data2(config.mac_data_load_path)\n",
    "\n",
    "\n",
    "df_map, ord_mm, nns_map = data_load_instance.load_maxmin_ordered_data_bymonthyear(\n",
    "lat_lon_resolution=lat_lon_resolution, \n",
    "mm_cond_number=mm_cond_number,\n",
    "years_=years, \n",
    "months_=month_range,\n",
    "lat_range=[0.0, 5.0],      \n",
    "lon_range=[123.0, 133.0] \n",
    ")\n",
    "\n",
    "#days: List[str] = ['0', '31']\n",
    "#days_s_e = [int(d) for d in days]\n",
    "#days_list = list(range(days_s_e[0], days_s_e[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d2e14",
   "metadata": {},
   "source": [
    "Load daily data applying max-min ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2425932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18126, 4])\n",
      "tensor([[  4.9760, 132.9840, 267.0307,  45.0000],\n",
      "        [  4.9760, 132.9210, 264.0250,  45.0000],\n",
      "        [  4.9760, 132.8580, 258.8635,  45.0000],\n",
      "        [  4.9760, 132.7950, 263.3982,  45.0000],\n",
      "        [  4.9760, 132.7320, 272.0078,  45.0000],\n",
      "        [  4.9760, 132.6690, 268.6431,  45.0000],\n",
      "        [  4.9760, 132.6060, 267.8075,  45.0000],\n",
      "        [  4.9760, 132.5430, 267.0912,  45.0000],\n",
      "        [  4.9760, 132.4800, 266.1405,  45.0000],\n",
      "        [  4.9760, 132.4170, 260.4594,  45.0000]])\n"
     ]
    }
   ],
   "source": [
    "daily_aggregated_tensors = [] \n",
    "daily_hourly_maps = []        \n",
    "\n",
    "for day_index in range(31):\n",
    "  \n",
    "    hour_start_index = day_index * 8\n",
    "    #hour_end_index = (day_index + 1) * 8\n",
    "    hour_end_index = day_index*8 + 1\n",
    "    hour_indices = [hour_start_index, hour_end_index]\n",
    "    \n",
    "    # Load the data for the current day\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "        df_map, \n",
    "        hour_indices, \n",
    "        ord_mm=None,  \n",
    "        dtype=torch.float \n",
    "    )\n",
    "    # Append the day's data to their respective lists\n",
    "    daily_aggregated_tensors.append(day_aggregated_tensor)\n",
    "    daily_hourly_maps.append(day_hourly_map) \n",
    "\n",
    "print(daily_aggregated_tensors[0].shape)\n",
    "print(daily_aggregated_tensors[1][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4629e6b5",
   "metadata": {},
   "source": [
    "# swap columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c11fbf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  4.9764, 132.9858, 267.0307,  45.0000,   4.9764, 132.9858],\n",
      "        [  4.9765, 132.9230, 264.0250,  45.0000,   4.9765, 132.9230],\n",
      "        [  4.9766, 132.8600, 258.8635,  45.0000,   4.9766, 132.8600],\n",
      "        [  4.9768, 132.7975, 263.3982,  45.0000,   4.9768, 132.7975],\n",
      "        [  4.9769, 132.7346, 272.0078,  45.0000,   4.9769, 132.7346],\n",
      "        [  4.9769, 132.6719, 268.6431,  45.0000,   4.9769, 132.6719],\n",
      "        [  4.9772, 132.6088, 267.8075,  45.0000,   4.9772, 132.6088],\n",
      "        [  4.9774, 132.5458, 267.0912,  45.0000,   4.9774, 132.5458],\n",
      "        [  4.9776, 132.4833, 266.1405,  45.0000,   4.9776, 132.4833],\n",
      "        [  4.9776, 132.4204, 260.4594,  45.0000,   4.9776, 132.4204]])\n"
     ]
    }
   ],
   "source": [
    "daily_aggregated_tensors2 = [] \n",
    "daily_hourly_maps2 = []      \n",
    "for day_index in range(31):\n",
    "    hour_start_index = day_index * 8\n",
    "    hour_end_index = (day_index + 1) * 8\n",
    "    hour_end_index = day_index*8 + 1\n",
    "    hour_indices = [hour_start_index, hour_end_index]\n",
    "    \n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data_keep_ori(\n",
    "    df_map, \n",
    "    hour_indices, \n",
    "    ord_mm= None,  # or just omit it\n",
    "    dtype=torch.float # or just omit it \n",
    "    )\n",
    "    day_aggregated_tensor[:,0], day_aggregated_tensor[:,1] = day_aggregated_tensor[:,4] , day_aggregated_tensor[:,5]\n",
    "\n",
    "\n",
    "    for map in day_hourly_map.values():\n",
    "        # This single line performs the swap safely and efficiently\n",
    "        map[:,0], map[:,1], map[:,4], map[:,5] = map[:,4], map[:,5], map[:,0], map[:,1]\n",
    "        \n",
    "    daily_aggregated_tensors2.append( day_aggregated_tensor )\n",
    "    daily_hourly_maps2.append( day_hourly_map )\n",
    "\n",
    "print(daily_aggregated_tensors2[1][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9ddaf4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_idx = 8\n",
    "\n",
    "lat = [daily_aggregated_tensors[da_idx][:,0], daily_aggregated_tensors2[da_idx][:,0]]\n",
    "lon = [daily_aggregated_tensors[da_idx][:,1], daily_aggregated_tensors2[da_idx][:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ec329526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fine-Scale Deviation Report ---\n",
      "Threshold: > 0.022\n",
      "Total data points: 18126\n",
      "Points with large deviation: 147\n",
      "Percentage with large deviation: 0.81%\n",
      "\n",
      "Values of the large deviations:\n",
      "[-0.04604769 -0.04634285 -0.02498245 -0.02498245 -0.0251441  -0.04697132\n",
      " -0.0689826  -0.0689826   0.06156063 -0.04745197  0.08293152 -0.02643919\n",
      " -0.02643919 -0.04833174 -0.04833174  0.03893137  0.03893137 -0.09233189\n",
      "  0.03818488  0.05994868  0.08144546 -0.04960227 -0.02844429  0.03744555\n",
      "  0.03744555 -0.02810502 -0.05088878 -0.05098557  0.058815    0.03657699\n",
      "  0.05772376  0.03563738  0.03563738 -0.0297122   0.03652239  0.0363369\n",
      "  0.0363369   0.03527045  0.0358212  -0.02970552 -0.05319834 -0.02962255\n",
      "  0.03337431 -0.03007603 -0.05232406 -0.03097391 -0.03164864  0.03480363\n",
      "  0.03441644  0.03441644 -0.0529213  -0.03138566 -0.03138566 -0.03175521\n",
      " -0.07564878 -0.03213763 -0.09692121 -0.07538557  0.05510449 -0.07613754\n",
      "  0.09855103 -0.03304458 -0.03304458 -0.05473518 -0.05473518 -0.05473518\n",
      " -0.03324294 -0.03324294  0.05455089  0.07594562 -0.07704473 -0.09873533\n",
      " -0.09873533 -0.09873533 -0.07724309 -0.07724309 -0.03378749  0.03217602\n",
      "  0.03194571  0.11822915  0.11822915  0.11822915 -0.121243   -0.0777874\n",
      " -0.05613089 -0.05613089  0.074229    0.074229    0.074229    0.09576416\n",
      "  0.09576416  0.07396173  0.07373071  0.07373071  0.03022909  0.03022909\n",
      "  0.03022909  0.05176425  0.05176425  0.02996182  0.0297308   0.0297308\n",
      "  0.05153513  0.07313704  0.02915072  0.0291369  -0.0624845   0.02407837\n",
      " -0.04255962 -0.06445789 -0.06445789 -0.08655977 -0.06491733  0.08709979\n",
      "  0.08692932  0.08692932 -0.0225625   0.04229188  0.04309988  0.04309988\n",
      "  0.04292941  0.04292941 -0.02429342 -0.02474546 -0.0474236  -0.0476644\n",
      " -0.02604461 -0.02604461  0.03911042  0.03911042 -0.09166431  0.03855073\n",
      "  0.05980635  0.0594883   0.03774571  0.0374186  -0.02907908  0.05704689\n",
      " -0.05171776 -0.03024471  0.03456545 -0.03028238 -0.05211496 -0.07428229\n",
      "  0.0335871   0.0335871   0.0296762 ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Assuming this is your data based on the example ---\n",
    "tensor_original = lat[0]\n",
    "\n",
    "tensor_grid = lat[1]\n",
    "\n",
    "\n",
    "# --- Analysis ---\n",
    "original_np = tensor_original.detach().cpu().numpy()\n",
    "grid_np = tensor_grid.detach().cpu().numpy()\n",
    "\n",
    "# 1. Calculate the deviation\n",
    "deviation = original_np - grid_np\n",
    "\n",
    "# 2. Define your threshold\n",
    "threshold = 0.022\n",
    "\n",
    "# 3. Find all absolute deviations greater than the threshold\n",
    "# np.abs() is crucial to catch both large positive and large negative diffs\n",
    "large_deviations_mask = np.abs(deviation) > threshold\n",
    "\n",
    "# 4. Count them\n",
    "count = np.sum(large_deviations_mask)\n",
    "\n",
    "# --- Report Results ---\n",
    "total_points = len(deviation)\n",
    "percentage = (count / total_points) * 100\n",
    "\n",
    "print(f\"--- Fine-Scale Deviation Report ---\")\n",
    "print(f\"Threshold: > {threshold}\")\n",
    "print(f\"Total data points: {total_points}\")\n",
    "print(f\"Points with large deviation: {count}\")\n",
    "print(f\"Percentage with large deviation: {percentage:.2f}%\")\n",
    "\n",
    "# Optional: Show the actual large deviation values\n",
    "if count > 0:\n",
    "    print(\"\\nValues of the large deviations:\")\n",
    "    print(deviation[large_deviations_mask])\n",
    "else:\n",
    "    print(\"\\nNo deviations found above the threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4eaf6d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fine-Scale Deviation Report ---\n",
      "Threshold: > 0.031\n",
      "Total data points: 18126\n",
      "Points with large deviation: 130\n",
      "Percentage with large deviation: 0.72%\n",
      "\n",
      "Values of the large deviations:\n",
      "[-0.08123779  0.04386902  0.04299927 -0.03128052 -0.08010864  0.04386902\n",
      "  0.04354858 -0.08010864  0.04386902 -0.03125    -0.07954407  0.04441833\n",
      "  0.03120422  0.04727173  0.03114319  0.04759216  0.04727173  0.04782104\n",
      " -0.07011414  0.05357361 -0.0697937   0.05451965  0.03118896 -0.06820679\n",
      "  0.05514526  0.03126526  0.05545044 -0.03102112  0.05670166  0.05371094\n",
      "  0.05700684  0.05371094  0.03111267 -0.06781006 -0.07009888  0.05548096\n",
      " -0.03100586 -0.06718445 -0.0697937   0.05548096 -0.06761169  0.05607605\n",
      " -0.06700134  0.03120422 -0.03100586  0.06021118 -0.03103638  0.03128052\n",
      "  0.06112671 -0.03137207  0.03133392 -0.03125     0.03108978 -0.03117371\n",
      "  0.03124237 -0.03121185  0.03101349 -0.03103638 -0.03101349  0.03126526\n",
      " -0.03134155  0.03125    -0.03137207 -0.03127289  0.03152466  0.03112793\n",
      " -0.03102112 -0.03143311 -0.03100586  0.03126526 -0.03102112 -0.03111267\n",
      " -0.03118896 -0.03102875 -0.03120422 -0.03102875  0.03120422 -0.03111267\n",
      " -0.03115845  0.03112793  0.03121948 -0.03115845 -0.03115845 -0.03102112\n",
      " -0.03102112  0.03132629  0.03125    -0.03105164 -0.03125     0.03120422\n",
      "  0.03109741 -0.03149414 -0.03101349  0.03109741  0.03140259  0.03100586\n",
      "  0.03103638  0.03139496 -0.03152466 -0.03117371  0.03114319  0.03108215\n",
      " -0.03106689 -0.03103638 -0.03111267  0.03128052  0.03105927 -0.03112793\n",
      " -0.0311203   0.03112793  0.03131104 -0.03173828 -0.03121948 -0.03121948\n",
      "  0.03116608  0.03129578 -0.03114319  0.0311203   0.03131104 -0.03126526\n",
      "  0.03121185 -0.03160858 -0.03116608 -0.03126526  0.03108215 -0.03114319\n",
      " -0.03101349 -0.03132629  0.03114319  0.0311203 ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Assuming this is your data based on the example ---\n",
    "tensor_original = lon[0]\n",
    "\n",
    "tensor_grid = lon[1]\n",
    "\n",
    "\n",
    "# --- Analysis ---\n",
    "original_np = tensor_original.detach().cpu().numpy()\n",
    "grid_np = tensor_grid.detach().cpu().numpy()\n",
    "\n",
    "# 1. Calculate the deviation\n",
    "deviation = original_np - grid_np\n",
    "\n",
    "# 2. Define your threshold\n",
    "threshold = 0.031\n",
    "\n",
    "# 3. Find all absolute deviations greater than the threshold\n",
    "# np.abs() is crucial to catch both large positive and large negative diffs\n",
    "large_deviations_mask = np.abs(deviation) > threshold\n",
    "\n",
    "# 4. Count them\n",
    "count = np.sum(large_deviations_mask)\n",
    "\n",
    "# --- Report Results ---\n",
    "total_points = len(deviation)\n",
    "percentage = (count / total_points) * 100\n",
    "\n",
    "print(f\"--- Fine-Scale Deviation Report ---\")\n",
    "print(f\"Threshold: > {threshold}\")\n",
    "print(f\"Total data points: {total_points}\")\n",
    "print(f\"Points with large deviation: {count}\")\n",
    "print(f\"Percentage with large deviation: {percentage:.2f}%\")\n",
    "\n",
    "# Optional: Show the actual large deviation values\n",
    "if count > 0:\n",
    "    print(\"\\nValues of the large deviations:\")\n",
    "    print(deviation[large_deviations_mask])\n",
    "else:\n",
    "    print(\"\\nNo deviations found above the threshold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a3fa5",
   "metadata": {},
   "source": [
    "Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf11885",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0.5 # smooth\n",
    "mm_cond_number = 8\n",
    "nheads = 300\n",
    "#nheads = 1230\n",
    "#lr = 0.01\n",
    "#step = 80\n",
    "#gamma_par = 0.5\n",
    "\n",
    "# --- Placeholder Global Variables ---\n",
    "# ðŸ’¥ REVISED: Added lr, patience, factor. Removed step, gamma_par\n",
    "lr=0.1\n",
    "patience = 5       # Scheduler: Epochs to wait for improvement\n",
    "factor = 0.5         # Scheduler: Factor to reduce LR by (e.g., 0.5 = 50% cut)\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5946f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- End Placeholders ---\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "day_indices = [0] \n",
    "for day_idx in day_indices:  \n",
    "\n",
    "    daily_hourly_map = daily_hourly_maps[day_idx]\n",
    "    daily_aggregated_tensor = daily_aggregated_tensors[day_idx]\n",
    "\n",
    "    # --- Correct Parameter Initialization (SPATIAL-ONLY) ---\n",
    "    init_sigmasq   = 15.0\n",
    "    init_range_lat = 0.66 \n",
    "    init_range_lon = 0.7 \n",
    "    init_nugget    = 1.5\n",
    "    \n",
    "    # Map model parameters to the 'phi' reparameterization\n",
    "    init_phi2 = 1.0 / init_range_lon                \n",
    "    init_phi1 = init_sigmasq * init_phi2            \n",
    "    init_phi3 = (init_range_lon / init_range_lat)**2  \n",
    "    \n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # 4-parameter spatial-only list\n",
    "    params_list = [\n",
    "        torch.tensor([np.log(init_phi1)],      requires_grad=True, dtype=torch.float64, device=device_str ),\n",
    "        torch.tensor([np.log(init_phi2)],      requires_grad=True, dtype=torch.float64, device=device_str ),\n",
    "        torch.tensor([np.log(init_phi3)],      requires_grad=True, dtype=torch.float64, device=device_str ),\n",
    "        torch.tensor([np.log(init_nugget)],    requires_grad=True, dtype=torch.float64, device=device_str )\n",
    "    ]\n",
    "\n",
    "    # --- Define learning rates and parameter groups ---\n",
    "    # ðŸ’¥ REVISED: Set lr_fast from the global lr\n",
    "    lr_fast = lr \n",
    "    fast_indices = [0, 1, 2, 3] # All parameters\n",
    "    \n",
    "    param_groups = [\n",
    "        {'params': [params_list[idx] for idx in fast_indices], 'lr': lr_fast, 'name': 'fast_group'}\n",
    "    ]\n",
    "\n",
    "    # --- Print Job Info ---\n",
    "    # Assuming lat_lon_resolution, v, mm_cond_number are defined globally\n",
    "    res_calc = (113 // lat_lon_resolution[0]) * (158 // lat_lon_resolution[0]) \n",
    "    print(f'\\n--- Starting Day {day_idx+1} (2024-07-{day_idx+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params_list}')\n",
    "            \n",
    "    # --- Instantiate the Correct Class ---\n",
    "    model_instance = kernels_repar_space.fit_vecchia_adams_fullbatch(\n",
    "            smooth = v,\n",
    "            input_map = daily_hourly_map,\n",
    "            aggregated_data = daily_aggregated_tensor,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- ðŸ’¥ REVISED: Call the Optimizer Method with Plateau settings ---\n",
    "    optimizer, scheduler = model_instance.set_optimizer(\n",
    "            param_groups,     \n",
    "            lr=lr,            \n",
    "            betas=(0.9, 0.99), \n",
    "            eps=1e-5, \n",
    "            scheduler_type='plateau', # Explicitly set to plateau\n",
    "            patience=patience,        # Pass patience\n",
    "            factor=factor             # Pass factor\n",
    "        )\n",
    "\n",
    "    # --- Call the Correct Fit Method ---\n",
    "    out, epoch_ran = model_instance.fit_vecc_scheduler_fullbatch(\n",
    "            params_list,\n",
    "            optimizer,\n",
    "            scheduler, \n",
    "            model_instance.matern_cov_SPATIAL_log_reparam, \n",
    "            epochs=epochs\n",
    "        )\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Day {day_idx+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day_idx+1} final results: {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79e7fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Negative Log-Likelihood: (22936.561132578532, 22882.89402724899)\n",
      "Full Negative Log-Likelihood: (22940.44941805631, 22893.58097275657)\n",
      "Full Negative Log-Likelihood: (22959.63309778168, 22897.341313474775)\n"
     ]
    }
   ],
   "source": [
    "def cal(a):\n",
    "    day_indices = [0] \n",
    "    for day_idx in day_indices:  \n",
    "\n",
    "        daily_hourly_map = daily_hourly_maps[day_idx]\n",
    "        daily_aggregated_tensor = daily_aggregated_tensors[day_idx]\n",
    "\n",
    "        # Convert initial parameters to a list of 1-element tensors\n",
    "        params_list = [\n",
    "            torch.tensor([val], dtype=torch.float64, requires_grad=True, device=device_str) for val in a\n",
    "        ]\n",
    "        \n",
    "        # ðŸ’¡ CRITICAL: Concatenate the list into a single tensor\n",
    "        # The full_likelihood function expects a single tensor, not a list\n",
    "        params_tensor = torch.cat(params_list)\n",
    "\n",
    "        # Assuming 'kernels_repar_space' has your 'fit_vecchia_adams' class\n",
    "        model_instance = kernels_repar_space.fit_vecchia_adams_fullbatch(\n",
    "                smooth = v,\n",
    "                input_map = daily_hourly_map,\n",
    "                aggregated_data = daily_aggregated_tensor,\n",
    "                nns_map = nns_map,\n",
    "                mm_cond_number = mm_cond_number,\n",
    "                nheads = nheads\n",
    "            )\n",
    "        \n",
    "        # ðŸ’¡ Pass the single 'params_tensor' and the correct 4-parameter spatial covariance function\n",
    "        bb = model_instance.full_likelihood_avg(\n",
    "            params = params_tensor, \n",
    "            input_data = daily_aggregated_tensor, \n",
    "            y = daily_aggregated_tensor[:,2], \n",
    "            covariance_function = model_instance.matern_cov_SPATIAL_log_reparam\n",
    "        )\n",
    "     \n",
    "        cov_map = model_instance.cov_structure_saver(params_tensor, model_instance.matern_cov_SPATIAL_log_reparam)\n",
    "        vecchia_nll = model_instance.vecchia_space_fullbatch( # Change this to your chosen Vecchia implementation\n",
    "            params = params_tensor, \n",
    "            covariance_function = model_instance.matern_cov_SPATIAL_log_reparam, \n",
    "            cov_map = cov_map # Assuming cov_map is precomputed or computed internally\n",
    "        )\n",
    " \n",
    "    return bb, vecchia_nll\n",
    "\n",
    "\n",
    "\n",
    "a = [3.5764, 0.7940, 0.3087, 0.6157]\n",
    "bb, nll_value = cal(a)\n",
    "print(f\"Full Negative Log-Likelihood: {bb.item()*18162, nll_value.item()*18162}\")\n",
    "\n",
    "a = [3.6764, 0.7940, 0.3087, 0.6157]\n",
    "bb, nll_value = cal(a)\n",
    "print(f\"Full Negative Log-Likelihood: {bb.item()*18162, nll_value.item()*18162}\")\n",
    "\n",
    "\n",
    "a = [3.4764, 0.7940, 0.3087, 0.6157]\n",
    "bb, nll_value = cal(a)\n",
    "print(f\"Full Negative Log-Likelihood: {bb.item()*18162, nll_value.item()*18162}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
