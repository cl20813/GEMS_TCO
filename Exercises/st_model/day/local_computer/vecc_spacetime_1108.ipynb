{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3584595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from GEMS_TCO import data_preprocess \n",
    "from GEMS_TCO import kernels_reparam_space_time as kernels_reparam_space_time\n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO.data_loader import load_data2\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f7acc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11042706734093927 0.16431006433040046\n"
     ]
    }
   ],
   "source": [
    "a,b = np.exp(1.8060), np.exp(0.7948)\n",
    "\n",
    "range_lon_inv = 1/a\n",
    "range_lat_inv = 1/(np.sqrt(b)*a) \n",
    "\n",
    "print(range_lat_inv, range_lon_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4551a",
   "metadata": {},
   "source": [
    "Load monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "403f1dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting data to lat: [0, 5], lon: [123, 133.0]\n"
     ]
    }
   ],
   "source": [
    "space: List[str] = ['1', '1']\n",
    "lat_lon_resolution = [int(s) for s in space]\n",
    "mm_cond_number: int = 8\n",
    "years = ['2024']\n",
    "month_range = [7] \n",
    "\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "data_load_instance = load_data2(config.mac_data_load_path)\n",
    "\n",
    "#lat_range_input = [1, 3]\n",
    "#lon_range_input = [125.0, 129.0]\n",
    "\n",
    "lat_range_input=[0,5]      \n",
    "lon_range_input=[123, 133.0] \n",
    "\n",
    "df_map, ord_mm, nns_map = data_load_instance.load_maxmin_ordered_data_bymonthyear(\n",
    "lat_lon_resolution=lat_lon_resolution, \n",
    "mm_cond_number=mm_cond_number,\n",
    "years_=years, \n",
    "months_=month_range,\n",
    "\n",
    "lat_range=lat_range_input,   \n",
    "lon_range=lon_range_input\n",
    "\n",
    ")\n",
    "\n",
    "#days: List[str] = ['0', '31']\n",
    "#days_s_e = [int(d) for d in days]\n",
    "#days_list = list(range(days_s_e[0], days_s_e[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0420c93",
   "metadata": {},
   "source": [
    "Load daily data applying max-min ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9d80a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([145008, 4])\n"
     ]
    }
   ],
   "source": [
    "daily_aggregated_tensors_dw = [] \n",
    "daily_hourly_maps_dw = []      \n",
    "\n",
    "daily_aggregated_tensors_vecc = [] \n",
    "daily_hourly_maps_vecc = []   \n",
    "\n",
    "\n",
    "for day_index in range(31):\n",
    "    hour_start_index = day_index * 8\n",
    "    \n",
    "    hour_end_index = (day_index + 1) * 8\n",
    "    #hour_end_index = day_index*8 + 1\n",
    "    hour_indices = [hour_start_index, hour_end_index]\n",
    "\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "    df_map, \n",
    "    hour_indices, \n",
    "    ord_mm= None,  # or just omit it\n",
    "    dtype=torch.float64, # or just omit it \n",
    "    keep_ori=False  #keep_exact_loc\n",
    "    )\n",
    "\n",
    "    daily_aggregated_tensors_dw.append( day_aggregated_tensor )\n",
    "    daily_hourly_maps_dw.append( day_hourly_map )\n",
    "\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "    df_map, \n",
    "    hour_indices, \n",
    "    ord_mm= ord_mm,  # or just omit it\n",
    "    dtype=torch.float64, # or just omit it \n",
    "    keep_ori=False  #keep_exact_loc\n",
    "    )\n",
    "\n",
    "    daily_aggregated_tensors_vecc.append( day_aggregated_tensor )\n",
    "    daily_hourly_maps_vecc.append( day_hourly_map )\n",
    "print(daily_aggregated_tensors_vecc[0].shape)\n",
    "#print(daily_hourly_maps[0])\n",
    "nn = daily_aggregated_tensors_vecc[0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd8939d",
   "metadata": {},
   "source": [
    "Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d63a7419",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0.5 # smooth\n",
    "mm_cond_number = 8\n",
    "nheads = 400\n",
    "#nheads = 1230\n",
    "#lr = 0.01\n",
    "#step = 80\n",
    "#gamma_par = 0.5\n",
    "\n",
    "# --- Placeholder Global Variables ---\n",
    "# ðŸ’¥ REVISED: Added lr, patience, factor. Removed step, gamma_par\n",
    "lr=0.1\n",
    "patience = 5       # Scheduler: Epochs to wait for improvement\n",
    "factor = 0.5         # Scheduler: Factor to reduce LR by (e.g., 0.5 = 50% cut)\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0491248b",
   "metadata": {},
   "source": [
    "Admas full batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab212aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 17854, smooth: 0.5\n",
      "mm_cond_number: 8,\n",
      "initial parameters: \n",
      "\n",
      "  Param 0: 4.2041\n",
      "  Param 1: 1.6389\n",
      "  Param 2: 0.4730\n",
      "  Param 3: -3.7638\n",
      "  Param 4: 0.0254\n",
      "  Param 5: -0.1641\n",
      "  Param 6: -1.4645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1 / Loss: 1.242094 ---\n",
      "  Param 0: Value=4.2041, Grad=-0.0019605295156889624\n",
      "  Param 1: Value=1.6389, Grad=0.00016578177948395437\n",
      "  Param 2: Value=0.4730, Grad=-0.00047301335143176324\n",
      "  Param 3: Value=-3.7638, Grad=-7.368123993227674e-05\n",
      "  Param 4: Value=0.0254, Grad=0.0015114695661382896\n",
      "  Param 5: Value=-0.1641, Grad=0.001130199229753584\n",
      "  Param 6: Value=-1.4645, Grad=0.0001448594927842602\n",
      "  Max Abs Grad: 1.960530e-03\n",
      "------------------------------\n",
      "--- Epoch 11 / Loss: 1.242767 ---\n",
      "  Param 0: Value=4.2558, Grad=0.020533918042005113\n",
      "  Param 1: Value=1.6332, Grad=-0.002348116442590908\n",
      "  Param 2: Value=0.5138, Grad=0.006276503570925667\n",
      "  Param 3: Value=-3.5709, Grad=0.0006742105277630215\n",
      "  Param 4: Value=-0.0048, Grad=-0.010196574750007526\n",
      "  Param 5: Value=-0.1739, Grad=-0.0033543686534067605\n",
      "  Param 6: Value=-1.6550, Grad=0.001350780128044063\n",
      "  Max Abs Grad: 2.053392e-02\n",
      "------------------------------\n",
      "--- Epoch 21 / Loss: 1.242143 ---\n",
      "  Param 0: Value=4.2116, Grad=-0.00853468822338965\n",
      "  Param 1: Value=1.6841, Grad=0.0011236418717309952\n",
      "  Param 2: Value=0.4651, Grad=-0.002869981985563667\n",
      "  Param 3: Value=-3.6654, Grad=0.00029750883508176793\n",
      "  Param 4: Value=0.0239, Grad=0.0005194957317436753\n",
      "  Param 5: Value=-0.1735, Grad=-0.002294626349483337\n",
      "  Param 6: Value=-1.7303, Grad=-0.0003114373691394113\n",
      "  Max Abs Grad: 8.534688e-03\n",
      "------------------------------\n",
      "--- Epoch 31 / Loss: 1.242048 ---\n",
      "  Param 0: Value=4.2281, Grad=0.0014519270296634332\n",
      "  Param 1: Value=1.6649, Grad=-0.0001492394389176147\n",
      "  Param 2: Value=0.4817, Grad=0.0003607702315994243\n",
      "  Param 3: Value=-3.7002, Grad=0.0002712458824776053\n",
      "  Param 4: Value=0.0168, Grad=-0.002174187258102249\n",
      "  Param 5: Value=-0.1722, Grad=-0.0020686344097550612\n",
      "  Param 6: Value=-1.7291, Grad=0.0002470765360514927\n",
      "  Max Abs Grad: 2.174187e-03\n",
      "------------------------------\n",
      "--- Epoch 41 / Loss: 1.242033 ---\n",
      "  Param 0: Value=4.2294, Grad=0.0019931341887878093\n",
      "  Param 1: Value=1.6630, Grad=-0.00021606316706304784\n",
      "  Param 2: Value=0.4834, Grad=0.0007040812360497133\n",
      "  Param 3: Value=-3.7160, Grad=0.00014518185336073503\n",
      "  Param 4: Value=0.0214, Grad=-8.360278049898167e-05\n",
      "  Param 5: Value=-0.1641, Grad=6.271521783539714e-05\n",
      "  Param 6: Value=-1.7333, Grad=0.00028160993113338843\n",
      "  Max Abs Grad: 1.993134e-03\n",
      "------------------------------\n",
      "--- Epoch 51 / Loss: 1.242030 ---\n",
      "  Param 0: Value=4.2287, Grad=0.0014259355457035311\n",
      "  Param 1: Value=1.6636, Grad=-0.00015210364642032493\n",
      "  Param 2: Value=0.4825, Grad=0.0005533374766367333\n",
      "  Param 3: Value=-3.7204, Grad=0.00011087072724346029\n",
      "  Param 4: Value=0.0222, Grad=0.00029935273334315716\n",
      "  Param 5: Value=-0.1630, Grad=0.00044536683687128325\n",
      "  Param 6: Value=-1.7360, Grad=0.00025070305952285015\n",
      "  Max Abs Grad: 1.425936e-03\n",
      "------------------------------\n",
      "--- Epoch 61 / Loss: 1.242029 ---\n",
      "  Param 0: Value=4.2283, Grad=0.0011953438098252025\n",
      "  Param 1: Value=1.6639, Grad=-0.00012618409454770668\n",
      "  Param 2: Value=0.4821, Grad=0.00047928341413367305\n",
      "  Param 3: Value=-3.7216, Grad=0.00010622480026270894\n",
      "  Param 4: Value=0.0222, Grad=0.0002953449057459663\n",
      "  Param 5: Value=-0.1630, Grad=0.0004431887283836625\n",
      "  Param 6: Value=-1.7370, Grad=0.00023772242458911715\n",
      "  Max Abs Grad: 1.195344e-03\n",
      "------------------------------\n",
      "--- Epoch 71 / Loss: 1.242029 ---\n",
      "  Param 0: Value=4.2282, Grad=0.0011186433386784613\n",
      "  Param 1: Value=1.6640, Grad=-0.00011763354101490477\n",
      "  Param 2: Value=0.4820, Grad=0.00045324177600483\n",
      "  Param 3: Value=-3.7219, Grad=0.00010561339334325075\n",
      "  Param 4: Value=0.0222, Grad=0.0002770196462223452\n",
      "  Param 5: Value=-0.1631, Grad=0.00042529672491209533\n",
      "  Param 6: Value=-1.7373, Grad=0.00023334575801613997\n",
      "  Max Abs Grad: 1.118643e-03\n",
      "------------------------------\n",
      "--- Epoch 81 / Loss: 1.242029 ---\n",
      "  Param 0: Value=4.2282, Grad=0.0010925323243564994\n",
      "  Param 1: Value=1.6640, Grad=-0.00011474008753443099\n",
      "  Param 2: Value=0.4819, Grad=0.0004441783329704941\n",
      "  Param 3: Value=-3.7220, Grad=0.0001055255044748356\n",
      "  Param 4: Value=0.0221, Grad=0.00026852552882978273\n",
      "  Param 5: Value=-0.1632, Grad=0.00041682883843483735\n",
      "  Param 6: Value=-1.7374, Grad=0.00023184606358370705\n",
      "  Max Abs Grad: 1.092532e-03\n",
      "------------------------------\n",
      "--- Epoch 91 / Loss: 1.242029 ---\n",
      "  Param 0: Value=4.2282, Grad=0.0010842820735711693\n",
      "  Param 1: Value=1.6640, Grad=-0.00011382852452722277\n",
      "  Param 2: Value=0.4819, Grad=0.0004412897290158443\n",
      "  Param 3: Value=-3.7220, Grad=0.00010551187795759374\n",
      "  Param 4: Value=0.0221, Grad=0.0002655809781830745\n",
      "  Param 5: Value=-0.1632, Grad=0.00041386293346011745\n",
      "  Param 6: Value=-1.7375, Grad=0.00023137092967404513\n",
      "  Max Abs Grad: 1.084282e-03\n",
      "------------------------------\n",
      "FINAL STATE: Epoch 100, Loss: 1.2420286917874348\n",
      "  Raw (vecc) Parameters: [4.228173480343444, 1.664023723844888, 0.48191787643950385, -3.722043393518716, 0.02213580128641457, -0.16318025798246621, -1.7374870626220842]\n",
      "  Interpretable Parameters:\n",
      "    sigma_sq  : 12.989609\n",
      "    range_lon : 0.189375\n",
      "    range_lat : 0.148825\n",
      "    range_time: 1.217742\n",
      "    advec_lat : 0.022136\n",
      "    advec_lon : -0.163180\n",
      "    nugget    : 0.175962\n",
      "Day 1 optimization finished in 10426.20s over 100 epochs.\n",
      "Day 1 final results (raw params + loss): [4.228173480343444, 1.664023723844888, 0.48191787643950385, -3.722043393518716, 0.02213580128641457, -0.16318025798246621, -1.7374870626220842, 1.2420286917874348]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- Assume global variables are set: ---\n",
    "# daily_hourly_maps, daily_aggregated_tensors, nns_map\n",
    "# lat_lon_resolution, v, mm_cond_number, nheads\n",
    "# lr, patience, factor, epochs\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "day_indices = [0] # 0 index \n",
    "for day_idx in day_indices:  \n",
    "\n",
    "    daily_hourly_map = daily_hourly_maps_vecc[day_idx]\n",
    "    daily_aggregated_tensor = daily_aggregated_tensors_vecc[day_idx]\n",
    "\n",
    "    # --- ðŸ’¥ Correct Parameter Initialization (SPATIO-TEMPORAL) ---\n",
    "    \n",
    "    init_sigmasq   = 13.0031\n",
    "    init_range_lat = 0.1533\n",
    "    init_range_lon = 0.1942\n",
    "    init_nugget    = 0.2312 \n",
    "    init_range_time  = 1.2751  # <-- NEW (Temporal range)\n",
    "    init_advec_lat = 0.0254  # <-- NEW (Start with no advection)\n",
    "    init_advec_lon = -0.1641  # <-- NEW (Start with no advection)\n",
    "    '''  \n",
    "    init_sigmasq   = 11.7969\n",
    "    init_range_lat = 0.1104\n",
    "    init_range_lon = 0.1643\n",
    "    init_nugget    = 0.0001 \n",
    "    init_range_time  = 1.28  # <-- NEW (Temporal range)\n",
    "    init_advec_lat = 0.0223  # <-- NEW (Start with no advection)\n",
    "    init_advec_lon = -0.1672  # <-- NEW (Start with no advection)\n",
    "    '''\n",
    "\n",
    "    # Map model parameters to the 'phi' reparameterization\n",
    "    init_phi2 = 1.0 / init_range_lon                # 1/range_lon\n",
    "    init_phi1 = init_sigmasq * init_phi2            # sigmasq / range_lon\n",
    "    init_phi3 = (init_range_lon / init_range_lat)**2  # (range_lon / range_lat)^2\n",
    "    init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / beta)^2\n",
    "    \n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # ðŸ’¥ 7-parameter spatio-temporal list\n",
    "    params_list = [\n",
    "        torch.tensor([np.log(init_phi1)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [0] log_phi1\n",
    "        torch.tensor([np.log(init_phi2)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [1] log_phi2\n",
    "        torch.tensor([np.log(init_phi3)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [2] log_phi3\n",
    "        torch.tensor([np.log(init_phi4)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [3] log_phi4\n",
    "        torch.tensor([init_advec_lat],         requires_grad=True, dtype=torch.float64, device=device_str ), # [4] advec_lat (NOT log)\n",
    "        torch.tensor([init_advec_lon],         requires_grad=True, dtype=torch.float64, device=device_str ), # [5] advec_lon (NOT log)\n",
    "        torch.tensor([np.log(init_nugget)],    requires_grad=True, dtype=torch.float64, device=device_str )  # [6] log_nugget\n",
    "    ]\n",
    "\n",
    "    # --- Define learning rates and parameter groups ---\n",
    "    # ðŸ’¥ Group all 7 parameters.\n",
    "    # For a more advanced setup, you could create a separate group\n",
    "    # for advection params [4, 5] with a different (e.g., smaller) LR.\n",
    "    lr_all = lr \n",
    "    all_indices = [0, 1, 2, 3, 4, 5, 6] \n",
    "    \n",
    "    param_groups = [\n",
    "        {'params': [params_list[idx] for idx in all_indices], 'lr': lr_all, 'name': 'all_params'}\n",
    "    ]\n",
    "\n",
    "    # --- Print Job Info ---\n",
    "    res_calc = (113 // lat_lon_resolution[0]) * (158 // lat_lon_resolution[0]) \n",
    "    print(f'\\n--- Starting Day {day_idx+1} (2024-07-{day_idx+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n')\n",
    "    for i, p in enumerate(params_list):\n",
    "        print(f\"  Param {i}: {p.item():.4f}\")\n",
    "            \n",
    "    # --- ðŸ’¥ Instantiate the Correct Class ---\n",
    "    # Assumes fit_vecchia_adams is defined in your current scope\n",
    "    model_instance = kernels_reparam_space_time.fit_vecchia_adams(\n",
    "            smooth = v,\n",
    "            input_map = daily_hourly_map,\n",
    "            aggregated_data = daily_aggregated_tensor,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- Set Optimizer (This part is unchanged) ---\n",
    "    optimizer, scheduler = model_instance.set_optimizer(\n",
    "            param_groups,     \n",
    "            lr=lr,            \n",
    "            betas=(0.9, 0.99), \n",
    "            eps=1e-5, \n",
    "            scheduler_type='plateau', # Explicitly set to plateau\n",
    "            patience=patience,        \n",
    "            factor=factor             \n",
    "        )\n",
    "\n",
    "    # --- ðŸ’¥ Call the Correct Fit Method ---\n",
    "    out, epoch_ran = model_instance.fit_model(  # <-- Changed method name\n",
    "            params_list,\n",
    "            optimizer,\n",
    "            scheduler, \n",
    "            model_instance.matern_cov_aniso_STABLE_log_reparam, # <-- Changed covariance function\n",
    "            epochs=epochs\n",
    "        )\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Day {day_idx+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day_idx+1} final results (raw params + loss): {out}\")\n",
    "\n",
    "    # 30m per 10. #epoch 120: 210 m. 331m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1540034",
   "metadata": {},
   "source": [
    "--- Starting Day 1 (2024-07-1) ---\n",
    "Data size per day: 17854, smooth: 0.5\n",
    "mm_cond_number: 8,\n",
    "initial parameters: \n",
    "\n",
    "  Param 0: 4.2739\n",
    "  Param 1: 1.8061\n",
    "  Param 2: 0.7952\n",
    "  Param 3: -4.1058\n",
    "  Param 4: 0.0223\n",
    "  Param 5: -0.1672\n",
    "  Param 6: -9.2103\n",
    "/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
    "  warnings.warn(\n",
    "--- Epoch 1 / Loss: 1.245499 ---\n",
    "  Param 0: Value=4.2739, Grad=0.017745675058296936\n",
    "  Param 1: Value=1.8061, Grad=0.0008265844799274488\n",
    "  Param 2: Value=0.7952, Grad=0.021193197189964156\n",
    "  Param 3: Value=-4.1058, Grad=-0.0016905664152934525\n",
    "  Param 4: Value=0.0223, Grad=0.001970792580072817\n",
    "  Param 5: Value=-0.1672, Grad=0.012366998773789932\n",
    "  Param 6: Value=-9.2103, Grad=6.652335220761932e-07\n",
    "  Max Abs Grad: 2.119320e-02\n",
    "------------------------------\n",
    "--- Epoch 11 / Loss: 1.243057 ---\n",
    "  Param 0: Value=4.3370, Grad=0.005532713651162728\n",
    "  Param 1: Value=1.8698, Grad=0.0015965380205475465\n",
    "  Param 2: Value=0.3906, Grad=-0.0031288813856308797\n",
    "  Param 3: Value=-3.5079, Grad=0.00012640251644008914\n",
    "  Param 4: Value=-0.0323, Grad=-0.019579353112110447\n",
    "  Param 5: Value=-0.1245, Grad=0.005058925449702462\n",
    "  Param 6: Value=-9.2022, Grad=2.4863423479562597e-07\n",
    "  Max Abs Grad: 1.957935e-02\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26799703",
   "metadata": {},
   "source": [
    "--- Starting Day 1 (2024-07-1) ---\n",
    "Data size per day: 17854, smooth: 0.5\n",
    "mm_cond_number: 8,\n",
    "initial parameters: \n",
    "\n",
    "  Param 0: 3.0647\n",
    "  Param 1: 0.3567\n",
    "  Param 2: 0.1177\n",
    "  Param 3: -4.6052\n",
    "  Param 4: 0.0200\n",
    "  Param 5: -0.0800\n",
    "  Param 6: 0.4055\n",
    "\n",
    "  Max Abs Grad: 3.261639e-01\n",
    "------------------------------\n",
    "--- Epoch 11 / Loss: 1.264441 ---\n",
    "  Param 0: Value=3.6519, Grad=0.033102462085258695\n",
    "  Param 1: Value=0.0235, Grad=-0.0007389134000692495\n",
    "  Param 2: Value=0.6861, Grad=0.012528458699466947\n",
    "  Param 3: Value=-4.0819, Grad=-0.0006021767918773808\n",
    "  Param 4: Value=0.0095, Grad=-0.0008643903938066648\n",
    "  Param 5: Value=-0.4955, Grad=-0.011065596143078432\n",
    "  Param 6: Value=0.7710, Grad=0.048430986604446245\n",
    "  Max Abs Grad: 4.843099e-02\n",
    "------------------------------\n",
    "--- Epoch 21 / Loss: 1.254908 ---\n",
    "  Param 0: Value=3.5690, Grad=-0.017414686875716533\n",
    "  Param 1: Value=0.3038, Grad=-0.00031863693271167153\n",
    "  Param 2: Value=0.4878, Grad=-0.0031871705493084403\n",
    "  Param 3: Value=-3.9547, Grad=-0.0009551805522446299\n",
    "  Param 4: Value=-0.0349, Grad=-0.003536394381183513\n",
    "  Param 5: Value=-0.5049, Grad=-0.009332212642619617\n",
    "  Param 6: Value=0.4779, Grad=0.0035137438635113425\n",
    "  Max Abs Grad: 1.741469e-02\n",
    "------------------------------\n",
    "--- Epoch 31 / Loss: 1.251845 ---\n",
    "  Param 0: Value=3.6745, Grad=-0.01555689658857398\n",
    "  Param 1: Value=0.5720, Grad=-0.0005128375911674786\n",
    "  Param 2: Value=0.5004, Grad=-0.002206009817795905\n",
    "  Param 3: Value=-3.8553, Grad=-0.0007826904517286726\n",
    "  Param 4: Value=0.0247, Grad=-0.00032502390531596485\n",
    "  Param 5: Value=-0.4514, Grad=-0.011680323865716345\n",
    "  Param 6: Value=0.3269, Grad=0.002009819891799335\n",
    "  Max Abs Grad: 1.555690e-02\n",
    "------------------------------\n",
    "--- Epoch 41 / Loss: 1.248747 ---\n",
    "  Param 0: Value=3.8277, Grad=0.006262703404085881\n",
    "  Param 1: Value=0.9707, Grad=-0.0011425011020983831\n",
    "  Param 2: Value=0.5399, Grad=0.004287867156494423\n",
    "  Param 3: Value=-3.7662, Grad=-0.00028585190495268624\n",
    "  Param 4: Value=0.0469, Grad=0.0025113508266898796\n",
    "  Param 5: Value=-0.3520, Grad=-0.014885347933420185\n",
    "  Param 6: Value=0.2267, Grad=0.011262560746062734\n",
    "  Max Abs Grad: 1.488535e-02\n",
    "------------------------------\n",
    "--- Epoch 51 / Loss: 1.245564 ---\n",
    "  Param 0: Value=3.8707, Grad=-0.009092823529823066\n",
    "  Param 1: Value=1.3319, Grad=0.0015285008232184647\n",
    "  Param 2: Value=0.4458, Grad=-0.0017986285273207225\n",
    "  Param 3: Value=-3.7123, Grad=5.181327027115963e-05\n",
    "  Param 4: Value=0.0220, Grad=-0.0026806427888267865\n",
    "  Param 5: Value=-0.2095, Grad=-0.0054568132706633\n",
    "  Param 6: Value=0.0793, Grad=0.0038554147044682343\n",
    "  Max Abs Grad: 9.092824e-03\n",
    "------------------------------\n",
    "--- Epoch 61 / Loss: 1.244809 ---\n",
    "  Param 0: Value=3.9545, Grad=0.0025311473891214244\n",
    "  Param 1: Value=1.1901, Grad=-0.0009350653303357313\n",
    "  Param 2: Value=0.4366, Grad=0.0010677078901593694\n",
    "  Param 3: Value=-3.6776, Grad=-0.00048638118522927384\n",
    "  Param 4: Value=0.0279, Grad=-0.000609149651126906\n",
    "  Param 5: Value=-0.1399, Grad=0.012523378927349654\n",
    "  Param 6: Value=-0.0443, Grad=0.006097463420966644\n",
    "  Max Abs Grad: 1.252338e-02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dedca71",
   "metadata": {},
   "source": [
    "# L bfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04483492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 17854, smooth: 0.5\n",
      "mm_cond_number: 8,\n",
      "initial parameters: \n",
      "\n",
      "  Param 0: 4.2042\n",
      "  Param 1: 1.6348\n",
      "  Param 2: 0.4721\n",
      "  Param 3: -2.5562\n",
      "  Param 4: 0.0218\n",
      "  Param 5: -0.1689\n",
      "  Param 6: -2.0025\n",
      "--- Starting L-BFGS Optimization ---\n",
      "--- Step 1/20 / Loss: 1.243818 ---\n",
      "  Param 0: Value=4.2824, Grad=3.7316651559889873e-07\n",
      "  Param 1: Value=1.7108, Grad=-4.9941013494580584e-08\n",
      "  Param 2: Value=0.4883, Grad=2.2757637405767504e-07\n",
      "  Param 3: Value=-3.7686, Grad=7.4054935999273844e-09\n",
      "  Param 4: Value=0.0201, Grad=-1.2531451323882498e-07\n",
      "  Param 5: Value=-0.1611, Grad=-8.692719141656638e-08\n",
      "  Param 6: Value=-5.1617, Grad=7.410251173775746e-10\n",
      "  Max Abs Grad: 3.731665e-07\n",
      "------------------------------\n",
      "--- Step 2/20 / Loss: 1.241995 ---\n",
      "  Param 0: Value=4.2824, Grad=3.7316651559889873e-07\n",
      "  Param 1: Value=1.7108, Grad=-4.9941013494580584e-08\n",
      "  Param 2: Value=0.4883, Grad=2.2757637405767504e-07\n",
      "  Param 3: Value=-3.7686, Grad=7.4054935999273844e-09\n",
      "  Param 4: Value=0.0201, Grad=-1.2531451323882498e-07\n",
      "  Param 5: Value=-0.1611, Grad=-8.692719141656638e-08\n",
      "  Param 6: Value=-5.1617, Grad=7.410251173775746e-10\n",
      "  Max Abs Grad: 3.731665e-07\n",
      "------------------------------\n",
      "--- Step 3/20 / Loss: 1.241995 ---\n",
      "  Param 0: Value=4.2824, Grad=3.7316651559889873e-07\n",
      "  Param 1: Value=1.7108, Grad=-4.9941013494580584e-08\n",
      "  Param 2: Value=0.4883, Grad=2.2757637405767504e-07\n",
      "  Param 3: Value=-3.7686, Grad=7.4054935999273844e-09\n",
      "  Param 4: Value=0.0201, Grad=-1.2531451323882498e-07\n",
      "  Param 5: Value=-0.1611, Grad=-8.692719141656638e-08\n",
      "  Param 6: Value=-5.1617, Grad=7.410251173775746e-10\n",
      "  Max Abs Grad: 3.731665e-07\n",
      "------------------------------\n",
      "--- Step 4/20 / Loss: 1.241995 ---\n",
      "  Param 0: Value=4.2824, Grad=3.7316651559889873e-07\n",
      "  Param 1: Value=1.7108, Grad=-4.9941013494580584e-08\n",
      "  Param 2: Value=0.4883, Grad=2.2757637405767504e-07\n",
      "  Param 3: Value=-3.7686, Grad=7.4054935999273844e-09\n",
      "  Param 4: Value=0.0201, Grad=-1.2531451323882498e-07\n",
      "  Param 5: Value=-0.1611, Grad=-8.692719141656638e-08\n",
      "  Param 6: Value=-5.1617, Grad=7.410251173775746e-10\n",
      "  Max Abs Grad: 3.731665e-07\n",
      "------------------------------\n",
      "--- Step 5/20 / Loss: 1.241995 ---\n",
      "  Param 0: Value=4.2824, Grad=3.7316651559889873e-07\n",
      "  Param 1: Value=1.7108, Grad=-4.9941013494580584e-08\n",
      "  Param 2: Value=0.4883, Grad=2.2757637405767504e-07\n",
      "  Param 3: Value=-3.7686, Grad=7.4054935999273844e-09\n",
      "  Param 4: Value=0.0201, Grad=-1.2531451323882498e-07\n",
      "  Param 5: Value=-0.1611, Grad=-8.692719141656638e-08\n",
      "  Param 6: Value=-5.1617, Grad=7.410251173775746e-10\n",
      "  Max Abs Grad: 3.731665e-07\n",
      "------------------------------\n",
      "--- Step 6/20 / Loss: 1.241995 ---\n",
      "  Param 0: Value=4.2824, Grad=3.7316651559889873e-07\n",
      "  Param 1: Value=1.7108, Grad=-4.9941013494580584e-08\n",
      "  Param 2: Value=0.4883, Grad=2.2757637405767504e-07\n",
      "  Param 3: Value=-3.7686, Grad=7.4054935999273844e-09\n",
      "  Param 4: Value=0.0201, Grad=-1.2531451323882498e-07\n",
      "  Param 5: Value=-0.1611, Grad=-8.692719141656638e-08\n",
      "  Param 6: Value=-5.1617, Grad=7.410251173775746e-10\n",
      "  Max Abs Grad: 3.731665e-07\n",
      "------------------------------\n",
      "--- Step 7/20 / Loss: 1.241995 ---\n",
      "  Param 0: Value=4.2824, Grad=3.7316651559889873e-07\n",
      "  Param 1: Value=1.7108, Grad=-4.9941013494580584e-08\n",
      "  Param 2: Value=0.4883, Grad=2.2757637405767504e-07\n",
      "  Param 3: Value=-3.7686, Grad=7.4054935999273844e-09\n",
      "  Param 4: Value=0.0201, Grad=-1.2531451323882498e-07\n",
      "  Param 5: Value=-0.1611, Grad=-8.692719141656638e-08\n",
      "  Param 6: Value=-5.1617, Grad=7.410251173775746e-10\n",
      "  Max Abs Grad: 3.731665e-07\n",
      "------------------------------\n",
      "--- Step 8/20 / Loss: 1.241995 ---\n",
      "  Param 0: Value=4.2824, Grad=3.7316651559889873e-07\n",
      "  Param 1: Value=1.7108, Grad=-4.9941013494580584e-08\n",
      "  Param 2: Value=0.4883, Grad=2.2757637405767504e-07\n",
      "  Param 3: Value=-3.7686, Grad=7.4054935999273844e-09\n",
      "  Param 4: Value=0.0201, Grad=-1.2531451323882498e-07\n",
      "  Param 5: Value=-0.1611, Grad=-8.692719141656638e-08\n",
      "  Param 6: Value=-5.1617, Grad=7.410251173775746e-10\n",
      "  Max Abs Grad: 3.731665e-07\n",
      "------------------------------\n",
      "--- Step 9/20 / Loss: 1.241995 ---\n",
      "  Param 0: Value=4.2824, Grad=3.7316651559889873e-07\n",
      "  Param 1: Value=1.7108, Grad=-4.9941013494580584e-08\n",
      "  Param 2: Value=0.4883, Grad=2.2757637405767504e-07\n",
      "  Param 3: Value=-3.7686, Grad=7.4054935999273844e-09\n",
      "  Param 4: Value=0.0201, Grad=-1.2531451323882498e-07\n",
      "  Param 5: Value=-0.1611, Grad=-8.692719141656638e-08\n",
      "  Param 6: Value=-5.1617, Grad=7.410251173775746e-10\n",
      "  Max Abs Grad: 3.731665e-07\n",
      "------------------------------\n",
      "--- Step 10/20 / Loss: 1.241995 ---\n",
      "  Param 0: Value=4.2824, Grad=3.7316651559889873e-07\n",
      "  Param 1: Value=1.7108, Grad=-4.9941013494580584e-08\n",
      "  Param 2: Value=0.4883, Grad=2.2757637405767504e-07\n",
      "  Param 3: Value=-3.7686, Grad=7.4054935999273844e-09\n",
      "  Param 4: Value=0.0201, Grad=-1.2531451323882498e-07\n",
      "  Param 5: Value=-0.1611, Grad=-8.692719141656638e-08\n",
      "  Param 6: Value=-5.1617, Grad=7.410251173775746e-10\n",
      "  Max Abs Grad: 3.731665e-07\n",
      "------------------------------\n",
      "--- Step 11/20 / Loss: 1.241995 ---\n",
      "  Param 0: Value=4.2824, Grad=3.7316651559889873e-07\n",
      "  Param 1: Value=1.7108, Grad=-4.9941013494580584e-08\n",
      "  Param 2: Value=0.4883, Grad=2.2757637405767504e-07\n",
      "  Param 3: Value=-3.7686, Grad=7.4054935999273844e-09\n",
      "  Param 4: Value=0.0201, Grad=-1.2531451323882498e-07\n",
      "  Param 5: Value=-0.1611, Grad=-8.692719141656638e-08\n",
      "  Param 6: Value=-5.1617, Grad=7.410251173775746e-10\n",
      "  Max Abs Grad: 3.731665e-07\n",
      "------------------------------\n",
      "--- Step 12/20 / Loss: 1.241995 ---\n",
      "  Param 0: Value=4.2824, Grad=3.7316651559889873e-07\n",
      "  Param 1: Value=1.7108, Grad=-4.9941013494580584e-08\n",
      "  Param 2: Value=0.4883, Grad=2.2757637405767504e-07\n",
      "  Param 3: Value=-3.7686, Grad=7.4054935999273844e-09\n",
      "  Param 4: Value=0.0201, Grad=-1.2531451323882498e-07\n",
      "  Param 5: Value=-0.1611, Grad=-8.692719141656638e-08\n",
      "  Param 6: Value=-5.1617, Grad=7.410251173775746e-10\n",
      "  Max Abs Grad: 3.731665e-07\n",
      "------------------------------\n",
      "--- Step 13/20 / Loss: 1.241995 ---\n",
      "  Param 0: Value=4.2824, Grad=3.7316651559889873e-07\n",
      "  Param 1: Value=1.7108, Grad=-4.9941013494580584e-08\n",
      "  Param 2: Value=0.4883, Grad=2.2757637405767504e-07\n",
      "  Param 3: Value=-3.7686, Grad=7.4054935999273844e-09\n",
      "  Param 4: Value=0.0201, Grad=-1.2531451323882498e-07\n",
      "  Param 5: Value=-0.1611, Grad=-8.692719141656638e-08\n",
      "  Param 6: Value=-5.1617, Grad=7.410251173775746e-10\n",
      "  Max Abs Grad: 3.731665e-07\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 107\u001b[39m\n\u001b[32m     97\u001b[39m optimizer = model_instance.set_optimizer(\n\u001b[32m     98\u001b[39m         param_groups,     \n\u001b[32m     99\u001b[39m         lr=LBFGS_LR,            \n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m         tolerance_change = \u001b[32m1e-12\u001b[39m\n\u001b[32m    104\u001b[39m     )\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# --- ðŸ’¥ Call the L-BFGS Fit Method ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m out, steps_ran = \u001b[43mmodel_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_vecc_lbfgs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatern_cov_aniso_STABLE_log_reparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLBFGS_MAX_STEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Outer loop steps\u001b[39;49;00m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_tol\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-7\u001b[39;49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m end_time = time.time()\n\u001b[32m    116\u001b[39m epoch_time = end_time - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/kernels_reparam_space_time.py:704\u001b[39m, in \u001b[36mfit_vecchia_lbfgs.fit_vecc_lbfgs\u001b[39m\u001b[34m(self, params_list, optimizer, covariance_function, max_steps, grad_tol)\u001b[39m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[32m--> \u001b[39m\u001b[32m704\u001b[39m     loss = \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    706\u001b[39m     max_abs_grad = \u001b[32m0.0\u001b[39m\n\u001b[32m    707\u001b[39m     grad_values = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    482\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    483\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    484\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    485\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/optim/lbfgs.py:330\u001b[39m, in \u001b[36mLBFGS.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    327\u001b[39m state.setdefault(\u001b[33m\"\u001b[39m\u001b[33mn_iter\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m)\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# evaluate initial f(x) and df/dx\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m orig_loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m loss = \u001b[38;5;28mfloat\u001b[39m(orig_loss)\n\u001b[32m    332\u001b[39m current_evals = \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/kernels_reparam_space_time.py:699\u001b[39m, in \u001b[36mfit_vecchia_lbfgs.fit_vecc_lbfgs.<locals>.closure\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    696\u001b[39m cov_map = \u001b[38;5;28mself\u001b[39m.cov_structure_saver(params, covariance_function)\n\u001b[32m    697\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.compute_vecc_nll(params, covariance_function, cov_map)\n\u001b[32m--> \u001b[39m\u001b[32m699\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import time\n",
    "# NOTE: Assuming the necessary classes and global variables are defined elsewhere, \n",
    "# specifically the referenced 'fit_vecchia_lbfgs' class and 'kernels_reparam_space_time' module.\n",
    "\n",
    "# --- Assume global variables are set: ---\n",
    "# daily_hourly_maps, daily_aggregated_tensors, nns_map\n",
    "# lat_lon_resolution, v, mm_cond_number, nheads\n",
    "# lr, patience, factor, epochs\n",
    "\n",
    "# --- L-BFGS SPECIFIC GLOBAL PARAMETERS ---\n",
    "LBFGS_LR = 1.0\n",
    "LBFGS_MAX_STEPS = 20       # Number of outer optimization steps 10 to 20\n",
    "LBFGS_HISTORY_SIZE = 100   # Memory for Hessian approximation\n",
    "LBFGS_MAX_EVAL = 80        # Max evaluations (line search) per step 50 to 80\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "day_indices = [0] # 0 index \n",
    "for day_idx in day_indices:  \n",
    "\n",
    "    # Assuming data access is correct\n",
    "    daily_hourly_map = daily_hourly_maps_vecc[day_idx]\n",
    "    daily_aggregated_tensor = daily_aggregated_tensors_vecc[day_idx]\n",
    "\n",
    "    # --- Parameter Initialization (SPATIO-TEMPORAL) ---\n",
    "    '''  \n",
    "    init_sigmasq   = 15.0\n",
    "    init_range_lat = 0.66 \n",
    "    init_range_lon = 0.7 \n",
    "    init_nugget    = 1.5\n",
    "    init_range_time = 0.1\n",
    "    init_advec_lat = 0.02\n",
    "    init_advec_lon = -0.08\n",
    "    '''\n",
    "    init_sigmasq   = 13.059\n",
    "    init_range_lat = 0.154 \n",
    "    init_range_lon = 0.195 \n",
    "    init_nugget    = 0.135\n",
    "    init_range_time = 0.7\n",
    "    init_advec_lat = 0.0218\n",
    "    init_advec_lon = -0.1689\n",
    "\n",
    "    \n",
    "    # Map model parameters to the 'phi' reparameterization\n",
    "    init_phi2 = 1.0 / init_range_lon                # 1/range_lon\n",
    "    init_phi1 = init_sigmasq * init_phi2            # sigmasq / range_lon\n",
    "    init_phi3 = (init_range_lon / init_range_lat)**2  # (range_lon / range_lat)^2\n",
    "    init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # 7-parameter spatio-temporal list (Log/Linear)\n",
    "    params_list = [\n",
    "        torch.tensor([np.log(init_phi1)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [0] log(phi1)\n",
    "        torch.tensor([np.log(init_phi2)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [1] log(phi2)\n",
    "        torch.tensor([np.log(init_phi3)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [2] log(phi3)\n",
    "        torch.tensor([np.log(init_phi4)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [3] log(phi4)\n",
    "        torch.tensor([init_advec_lat],         requires_grad=True, dtype=torch.float64, device=device_str ), # [4] advec_lat (linear)\n",
    "        torch.tensor([init_advec_lon],         requires_grad=True, dtype=torch.float64, device=device_str ), # [5] advec_lon (linear)\n",
    "        torch.tensor([np.log(init_nugget)],    requires_grad=True, dtype=torch.float64, device=device_str )  # [6] log(nugget)\n",
    "    ]\n",
    "\n",
    "    # --- Define parameter groups ---\n",
    "    lr_all = LBFGS_LR\n",
    "    all_indices = [0, 1, 2, 3, 4, 5, 6] \n",
    "    \n",
    "    # L-BFGS requires the parameters to be iterable in a single list or group\n",
    "    param_groups = [\n",
    "        {'params': [params_list[idx] for idx in all_indices], 'lr': lr_all, 'name': 'all_params'}\n",
    "    ]\n",
    "\n",
    "    # --- Print Job Info (using placeholder print variables) ---\n",
    "    res_calc = (113 // lat_lon_resolution[0]) * (158 // lat_lon_resolution[0]) \n",
    "    print(f'\\n--- Starting Day {day_idx+1} (2024-07-{day_idx+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n')\n",
    "    for i, p in enumerate(params_list):\n",
    "        print(f\"  Param {i}: {p.item():.4f}\")\n",
    "            \n",
    "    # --- ðŸ’¥ Instantiate the L-BFGS Class ---\n",
    "    # NOTE: Assuming fit_vecchia_lbfgs is available via kernels_reparam_space_time\n",
    "    model_instance = kernels_reparam_space_time.fit_vecchia_lbfgs(\n",
    "            smooth = v,\n",
    "            input_map = daily_hourly_map,\n",
    "            aggregated_data = daily_aggregated_tensor,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- ðŸ’¥ Set L-BFGS Optimizer ---\n",
    "    # L-BFGS specific arguments are passed here\n",
    "    optimizer = model_instance.set_optimizer(\n",
    "            param_groups,     \n",
    "            lr=LBFGS_LR,            \n",
    "            max_iter=LBFGS_MAX_EVAL,        # max_iter in LBFGS is the line search limit\n",
    "            history_size=LBFGS_HISTORY_SIZE,\n",
    "            tolerance_grad = 1e-7,\n",
    "            tolerance_change = 1e-12\n",
    "        )\n",
    "\n",
    "    # --- ðŸ’¥ Call the L-BFGS Fit Method ---\n",
    "    out, steps_ran = model_instance.fit_vecc_lbfgs(\n",
    "            params_list,\n",
    "            optimizer,\n",
    "            model_instance.matern_cov_aniso_STABLE_log_reparam, \n",
    "            max_steps=LBFGS_MAX_STEPS, # Outer loop steps\n",
    "            grad_tol = 1e-7\n",
    "        )\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Day {day_idx+1} optimization finished in {epoch_time:.2f}s over {steps_ran+1} L-BFGS steps.\")\n",
    "    print(f\"Day {day_idx+1} final results (raw params + loss): {out}\")\n",
    "\n",
    "    # 85 min 1 step 96m steo 2 finished!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae19f086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7020"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "60*117"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5896147",
   "metadata": {},
   "source": [
    "--- Starting Day 1 (2024-07-1) ---\n",
    "Data size per day: 17854, smooth: 0.5\n",
    "mm_cond_number: 8,\n",
    "initial parameters: \n",
    "\n",
    "  Param 0: 4.2042\n",
    "  Param 1: 1.6348\n",
    "  Param 2: 0.4721\n",
    "  Param 3: -3.7677\n",
    "  Param 4: 0.0218\n",
    "  Param 5: -0.1689\n",
    "  Param 6: -1.3984\n",
    "--- Starting L-BFGS Optimization ---\n",
    "--- Step 1/10 / Loss: 1.242106 ---\n",
    "  Param 0: Value=4.2866, Grad=6.404439975316003e-07\n",
    "  Param 1: Value=1.7396, Grad=-5.182434301931914e-07\n",
    "  Param 2: Value=0.4891, Grad=6.42059345455452e-07\n",
    "  Param 3: Value=-3.7771, Grad=-1.810516477001804e-08\n",
    "  Param 4: Value=0.0205, Grad=1.0994677672789547e-06\n",
    "  Param 5: Value=-0.1641, Grad=1.23514462006375e-06\n",
    "  Param 6: Value=-11.8133, Grad=1.01352131413913e-09\n",
    "  Max Abs Grad: 1.235145e-06\n",
    "------------------------------\n",
    "\n",
    "Converged on gradient norm (max|grad| < 1e-05) at step 1\n",
    "FINAL STATE: Step 1, Loss: 1.2421059448955856\n",
    "  Raw (vecc) Parameters: [4.286642426458099, 1.7396166546213205, 0.48908102342543236, -3.777078889337886, 0.020486826416164613, -0.16410986090084237, -11.813252497216158]\n",
    "  Interpretable Parameters:\n",
    "    sigma_sq  : 12.769069\n",
    "    range_lon : 0.175588\n",
    "    range_lat : 0.137496\n",
    "    beta      : 0.151293\n",
    "    advec_lat : 0.020487\n",
    "    advec_lon : -0.164110\n",
    "    nugget    : 0.000007\n",
    "Day 1 optimization finished in 5777.95s over 1 L-BFGS steps.\n",
    "Day 1 final results (raw params + loss): [4.286642426458099, 1.7396166546213205, 0.48908102342543236, -3.777078889337886, 0.020486826416164613, -0.16410986090084237, -11.813252497216158, 1.2421059448955856]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b3f0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "--- Starting Day 1 (2024-07-1) ---\n",
    "Data size per day: 17854, smooth: 0.5\n",
    "mm_cond_number: 8,\n",
    "initial parameters: \n",
    "\n",
    "  Param 0: 4.2042\n",
    "  Param 1: 1.6348\n",
    "  Param 2: 0.4721\n",
    "  Param 3: -3.7677\n",
    "  Param 4: 0.0218\n",
    "  Param 5: -0.1689\n",
    "  Param 6: -1.3984\n",
    "--- Starting L-BFGS Optimization ---\n",
    "--- Step 1/10 / Loss: 1.242106 ---\n",
    "  Param 0: Value=4.2866, Grad=6.404439975316003e-07\n",
    "  Param 1: Value=1.7396, Grad=-5.182434301931914e-07\n",
    "  Param 2: Value=0.4891, Grad=6.42059345455452e-07\n",
    "  Param 3: Value=-3.7771, Grad=-1.810516477001804e-08\n",
    "  Param 4: Value=0.0205, Grad=1.0994677672789547e-06\n",
    "  Param 5: Value=-0.1641, Grad=1.23514462006375e-06\n",
    "  Param 6: Value=-11.8133, Grad=1.01352131413913e-09\n",
    "  Max Abs Grad: 1.235145e-06\n",
    "------------------------------\n",
    "\n",
    "Converged on gradient norm (max|grad| < 1e-05) at step 1\n",
    "FINAL STATE: Step 1, Loss: 1.2421059448955856\n",
    "  Raw (vecc) Parameters: [4.286642426458099, 1.7396166546213205, 0.48908102342543236, -3.777078889337886, 0.020486826416164613, -0.16410986090084237, -11.813252497216158]\n",
    "  Interpretable Parameters:\n",
    "    sigma_sq  : 12.769069\n",
    "    range_lon : 0.175588\n",
    "    range_lat : 0.137496\n",
    "    beta      : 0.151293\n",
    "    advec_lat : 0.020487\n",
    "    advec_lon : -0.164110\n",
    "    nugget    : 0.000007\n",
    "Day 1 optimization finished in 5777.95s over 1 L-BFGS steps.\n",
    "Day 1 final results (raw params + loss): [4.286642426458099, 1.7396166546213205, 0.48908102342543236, -3.777078889337886, 0.020486826416164613, -0.16410986090084237, -11.813252497216158, 1.2421059448955856]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
