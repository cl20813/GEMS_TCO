{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3584595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from GEMS_TCO import kernels\n",
    "from GEMS_TCO import data_preprocess \n",
    "from GEMS_TCO import kernels_new, kernels_reparam_space_time as kernels_reparam_space_time\n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO.data_loader import load_data2\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4551a",
   "metadata": {},
   "source": [
    "Load monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "403f1dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting data to lat: [0.0, 5.0], lon: [123.0, 133.0]\n"
     ]
    }
   ],
   "source": [
    "space: List[str] = ['4', '4']\n",
    "lat_lon_resolution = [int(s) for s in space]\n",
    "mm_cond_number: int = 20\n",
    "years = ['2024']\n",
    "month_range = [7] \n",
    "\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "data_load_instance = load_data2(config.mac_data_load_path)\n",
    "\n",
    "\n",
    "df_map, ord_mm, nns_map = data_load_instance.load_maxmin_ordered_data_bymonthyear(\n",
    "lat_lon_resolution=lat_lon_resolution, \n",
    "mm_cond_number=mm_cond_number,\n",
    "years_=years, \n",
    "months_=month_range,\n",
    "lat_range=[0.0, 5.0],      \n",
    "lon_range=[123.0, 133.0] \n",
    ")\n",
    "\n",
    "#days: List[str] = ['0', '31']\n",
    "#days_s_e = [int(d) for d in days]\n",
    "#days_list = list(range(days_s_e[0], days_s_e[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0420c93",
   "metadata": {},
   "source": [
    "Load daily data applying max-min ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d80a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8960, 4])\n"
     ]
    }
   ],
   "source": [
    "daily_aggregated_tensors = [] \n",
    "daily_hourly_maps = []        \n",
    "\n",
    "for day_index in range(31):\n",
    "  \n",
    "    hour_start_index = day_index * 8\n",
    "    hour_end_index = (day_index + 1) * 8\n",
    "    #hour_end_index = day_index*8 + 1\n",
    "    hour_indices = [hour_start_index, hour_end_index]\n",
    "    \n",
    "    # Load the data for the current day\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "        df_map, \n",
    "        hour_indices, \n",
    "        ord_mm=ord_mm,  \n",
    "        dtype=torch.float \n",
    "    )\n",
    "    # Append the day's data to their respective lists\n",
    "    daily_aggregated_tensors.append(day_aggregated_tensor)\n",
    "    daily_hourly_maps.append(day_hourly_map) \n",
    "\n",
    "print(daily_aggregated_tensors[0].shape)\n",
    "#print(daily_hourly_maps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd8939d",
   "metadata": {},
   "source": [
    "Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d63a7419",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0.5 # smooth\n",
    "mm_cond_number = 8\n",
    "nheads = 300\n",
    "#nheads = 1230\n",
    "#lr = 0.01\n",
    "#step = 80\n",
    "#gamma_par = 0.5\n",
    "\n",
    "# --- Placeholder Global Variables ---\n",
    "# ðŸ’¥ REVISED: Added lr, patience, factor. Removed step, gamma_par\n",
    "lr=0.1\n",
    "patience = 5       # Scheduler: Epochs to wait for improvement\n",
    "factor = 0.5         # Scheduler: Factor to reduce LR by (e.g., 0.5 = 50% cut)\n",
    "epochs=200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0491248b",
   "metadata": {},
   "source": [
    "Admas full batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf8a1c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 1092, smooth: 0.5\n",
      "mm_cond_number: 8,\n",
      "initial parameters: \n",
      "\n",
      "  Param 0: 3.0647\n",
      "  Param 1: 0.3567\n",
      "  Param 2: 0.1177\n",
      "  Param 3: -4.6052\n",
      "  Param 4: 0.0200\n",
      "  Param 5: -0.0800\n",
      "  Param 6: 0.4055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1 / Loss: 1.753064 ---\n",
      "  Param 0: Value=3.0647, Grad=-0.23896844967440986\n",
      "  Param 1: Value=0.3567, Grad=0.009605592633231486\n",
      "  Param 2: Value=0.1177, Grad=-0.01976948523425638\n",
      "  Param 3: Value=-4.6052, Grad=-0.07504262649484936\n",
      "  Param 4: Value=0.0200, Grad=-0.32693794149868843\n",
      "  Param 5: Value=-0.0800, Grad=1.1559085418106856\n",
      "  Param 6: Value=0.4055, Grad=-0.17389288113657042\n",
      "  Max Abs Grad: 1.155909e+00\n",
      "------------------------------\n",
      "--- Epoch 11 / Loss: 1.627234 ---\n",
      "  Param 0: Value=3.6419, Grad=0.07594998746544755\n",
      "  Param 1: Value=0.5097, Grad=-0.014314080436500561\n",
      "  Param 2: Value=0.7000, Grad=0.015354697007560519\n",
      "  Param 3: Value=-3.8439, Grad=-0.0022427608301070195\n",
      "  Param 4: Value=-0.0555, Grad=-0.1015536503669124\n",
      "  Param 5: Value=-0.3215, Grad=-0.09193484371903574\n",
      "  Param 6: Value=1.0768, Grad=0.026480414662068595\n",
      "  Max Abs Grad: 1.015537e-01\n",
      "------------------------------\n",
      "--- Epoch 21 / Loss: 1.602822 ---\n",
      "  Param 0: Value=3.4599, Grad=0.009474985191501072\n",
      "  Param 1: Value=1.0211, Grad=0.005285376861359964\n",
      "  Param 2: Value=0.4594, Grad=0.0015324204201914542\n",
      "  Param 3: Value=-3.6387, Grad=-0.0028829762830090604\n",
      "  Param 4: Value=0.0593, Grad=0.04022833469137356\n",
      "  Param 5: Value=-0.2303, Grad=-0.030820140045220854\n",
      "  Param 6: Value=1.0522, Grad=0.005160812941596081\n",
      "  Max Abs Grad: 4.022833e-02\n",
      "------------------------------\n",
      "--- Epoch 31 / Loss: 1.603317 ---\n",
      "  Param 0: Value=3.3596, Grad=-0.018568511116536117\n",
      "  Param 1: Value=1.0126, Grad=0.012349913120267402\n",
      "  Param 2: Value=0.3368, Grad=-0.004048419010208873\n",
      "  Param 3: Value=-3.5275, Grad=-0.004281514011924508\n",
      "  Param 4: Value=0.0515, Grad=-1.6464664797186735e-05\n",
      "  Param 5: Value=-0.1736, Grad=0.05134413569004778\n",
      "  Param 6: Value=1.0117, Grad=-0.010397872814729576\n",
      "  Max Abs Grad: 5.134414e-02\n",
      "------------------------------\n",
      "--- Epoch 41 / Loss: 1.601347 ---\n",
      "  Param 0: Value=3.3724, Grad=-0.00412220802072185\n",
      "  Param 1: Value=0.8995, Grad=0.003696348265673378\n",
      "  Param 2: Value=0.3566, Grad=-0.0035530089243750855\n",
      "  Param 3: Value=-3.4816, Grad=-0.002360744725055609\n",
      "  Param 4: Value=0.0455, Grad=0.0017801070091002873\n",
      "  Param 5: Value=-0.1880, Grad=0.02896512439388156\n",
      "  Param 6: Value=1.0314, Grad=-0.0041559670672300575\n",
      "  Max Abs Grad: 2.896512e-02\n",
      "------------------------------\n",
      "--- Epoch 51 / Loss: 1.600738 ---\n",
      "  Param 0: Value=3.3821, Grad=0.005317755588696692\n",
      "  Param 1: Value=0.8374, Grad=-4.760005131234652e-05\n",
      "  Param 2: Value=0.3957, Grad=-0.002400251608034322\n",
      "  Param 3: Value=-3.4529, Grad=-0.0011853422720261467\n",
      "  Param 4: Value=0.0419, Grad=0.005160552083359127\n",
      "  Param 5: Value=-0.2062, Grad=0.00431721459502412\n",
      "  Param 6: Value=1.0476, Grad=0.0004956666344222326\n",
      "  Max Abs Grad: 5.317756e-03\n",
      "------------------------------\n",
      "--- Epoch 61 / Loss: 1.600623 ---\n",
      "  Param 0: Value=3.3780, Grad=0.007628348773225341\n",
      "  Param 1: Value=0.8149, Grad=-0.0008516193809820057\n",
      "  Param 2: Value=0.4298, Grad=-0.0016645465530588347\n",
      "  Param 3: Value=-3.4353, Grad=-0.0009924239025134722\n",
      "  Param 4: Value=0.0370, Grad=0.001636361447101156\n",
      "  Param 5: Value=-0.2148, Grad=-0.0043654321532961985\n",
      "  Param 6: Value=1.0524, Grad=0.0017723384674358802\n",
      "  Max Abs Grad: 7.628349e-03\n",
      "------------------------------\n",
      "--- Epoch 71 / Loss: 1.600560 ---\n",
      "  Param 0: Value=3.3727, Grad=0.006891657593784216\n",
      "  Param 1: Value=0.8141, Grad=-0.0006166914563133718\n",
      "  Param 2: Value=0.4412, Grad=-0.0015374850500191027\n",
      "  Param 3: Value=-3.4300, Grad=-0.0010886797367199625\n",
      "  Param 4: Value=0.0357, Grad=-0.00010359998768393364\n",
      "  Param 5: Value=-0.2153, Grad=-0.00421951881911019\n",
      "  Param 6: Value=1.0514, Grad=0.0014703177563590874\n",
      "  Max Abs Grad: 6.891658e-03\n",
      "------------------------------\n",
      "--- Epoch 81 / Loss: 1.600520 ---\n",
      "  Param 0: Value=3.3689, Grad=0.006156174150760665\n",
      "  Param 1: Value=0.8151, Grad=-0.0003803556322039698\n",
      "  Param 2: Value=0.4469, Grad=-0.0015067396498064576\n",
      "  Param 3: Value=-3.4272, Grad=-0.0011695160616081429\n",
      "  Param 4: Value=0.0354, Grad=-0.0006886360825786285\n",
      "  Param 5: Value=-0.2150, Grad=-0.003410174063492627\n",
      "  Param 6: Value=1.0502, Grad=0.001138025574109602\n",
      "  Max Abs Grad: 6.156174e-03\n",
      "------------------------------\n",
      "--- Epoch 91 / Loss: 1.600508 ---\n",
      "  Param 0: Value=3.3677, Grad=0.005924139841293294\n",
      "  Param 1: Value=0.8154, Grad=-0.0003060585171776486\n",
      "  Param 2: Value=0.4486, Grad=-0.0015003149179290195\n",
      "  Param 3: Value=-3.4264, Grad=-0.0011935882778248193\n",
      "  Param 4: Value=0.0354, Grad=-0.0007601440870436417\n",
      "  Param 5: Value=-0.2148, Grad=-0.003126110370696912\n",
      "  Param 6: Value=1.0499, Grad=0.0010316246150282645\n",
      "  Max Abs Grad: 5.924140e-03\n",
      "------------------------------\n",
      "--- Epoch 101 / Loss: 1.600504 ---\n",
      "  Param 0: Value=3.3673, Grad=0.00584196089388525\n",
      "  Param 1: Value=0.8156, Grad=-0.00027987049226515587\n",
      "  Param 2: Value=0.4492, Grad=-0.001498040494539081\n",
      "  Param 3: Value=-3.4260, Grad=-0.0012018767967191947\n",
      "  Param 4: Value=0.0354, Grad=-0.0007679029749931557\n",
      "  Param 5: Value=-0.2147, Grad=-0.0030233273904843433\n",
      "  Param 6: Value=1.0497, Grad=0.0009938309479082714\n",
      "  Max Abs Grad: 5.841961e-03\n",
      "------------------------------\n",
      "--- Epoch 111 / Loss: 1.600503 ---\n",
      "  Param 0: Value=3.3672, Grad=0.005814726899245091\n",
      "  Param 1: Value=0.8156, Grad=-0.00027122393274389185\n",
      "  Param 2: Value=0.4494, Grad=-0.0014972469717416275\n",
      "  Param 3: Value=-3.4259, Grad=-0.0012045820459079903\n",
      "  Param 4: Value=0.0355, Grad=-0.0007677730430491593\n",
      "  Param 5: Value=-0.2147, Grad=-0.002989080117349323\n",
      "  Param 6: Value=1.0497, Grad=0.0009812988923551802\n",
      "  Max Abs Grad: 5.814727e-03\n",
      "------------------------------\n",
      "--- Epoch 121 / Loss: 1.600502 ---\n",
      "  Param 0: Value=3.3671, Grad=0.005806513005213405\n",
      "  Param 1: Value=0.8156, Grad=-0.00026862220625887515\n",
      "  Param 2: Value=0.4495, Grad=-0.0014970027611719973\n",
      "  Param 3: Value=-3.4259, Grad=-0.0012053916684059477\n",
      "  Param 4: Value=0.0355, Grad=-0.0007674103333150747\n",
      "  Param 5: Value=-0.2147, Grad=-0.0029787385214518555\n",
      "  Param 6: Value=1.0497, Grad=0.0009775180121133337\n",
      "  Max Abs Grad: 5.806513e-03\n",
      "------------------------------\n",
      "--- Epoch 131 / Loss: 1.600502 ---\n",
      "  Param 0: Value=3.3671, Grad=0.005803518918711109\n",
      "  Param 1: Value=0.8156, Grad=-0.0002676753202013603\n",
      "  Param 2: Value=0.4495, Grad=-0.0014969144356957909\n",
      "  Param 3: Value=-3.4259, Grad=-0.0012056854816839134\n",
      "  Param 4: Value=0.0355, Grad=-0.0007672327972236315\n",
      "  Param 5: Value=-0.2147, Grad=-0.0029749675345448123\n",
      "  Param 6: Value=1.0497, Grad=0.0009761391950468574\n",
      "  Max Abs Grad: 5.803519e-03\n",
      "------------------------------\n",
      "--- Epoch 141 / Loss: 1.600502 ---\n",
      "  Param 0: Value=3.3671, Grad=0.005802502926502821\n",
      "  Param 1: Value=0.8156, Grad=-0.00026735444418319216\n",
      "  Param 2: Value=0.4495, Grad=-0.0014968852332057575\n",
      "  Param 3: Value=-3.4259, Grad=-0.001205784832835326\n",
      "  Param 4: Value=0.0355, Grad=-0.0007671655251250322\n",
      "  Param 5: Value=-0.2147, Grad=-0.0029736875302341977\n",
      "  Param 6: Value=1.0497, Grad=0.0009756710075503982\n",
      "  Max Abs Grad: 5.802503e-03\n",
      "------------------------------\n",
      "--- Epoch 151 / Loss: 1.600502 ---\n",
      "  Param 0: Value=3.3671, Grad=0.005802193688379641\n",
      "  Param 1: Value=0.8156, Grad=-0.0002672568998727509\n",
      "  Param 2: Value=0.4495, Grad=-0.0014968766649225015\n",
      "  Param 3: Value=-3.4259, Grad=-0.0012058149788507016\n",
      "  Param 4: Value=0.0355, Grad=-0.0007671439909736375\n",
      "  Param 5: Value=-0.2147, Grad=-0.0029732977822001724\n",
      "  Param 6: Value=1.0497, Grad=0.0009755283939503602\n",
      "  Max Abs Grad: 5.802194e-03\n",
      "------------------------------\n",
      "--- Epoch 161 / Loss: 1.600502 ---\n",
      "  Param 0: Value=3.3671, Grad=0.005802080901936301\n",
      "  Param 1: Value=0.8156, Grad=-0.0002672213654905477\n",
      "  Param 2: Value=0.4495, Grad=-0.001496873669680836\n",
      "  Param 3: Value=-3.4259, Grad=-0.0012058259414907766\n",
      "  Param 4: Value=0.0355, Grad=-0.000767135891969299\n",
      "  Param 5: Value=-0.2147, Grad=-0.0029731555616931997\n",
      "  Param 6: Value=1.0497, Grad=0.0009754763361179429\n",
      "  Max Abs Grad: 5.802081e-03\n",
      "------------------------------\n",
      "--- Epoch 171 / Loss: 1.600502 ---\n",
      "  Param 0: Value=3.3671, Grad=0.005802042754284341\n",
      "  Param 1: Value=0.8156, Grad=-0.0002672093624826097\n",
      "  Param 2: Value=0.4495, Grad=-0.0014968727075963283\n",
      "  Param 3: Value=-3.4259, Grad=-0.0012058296374412014\n",
      "  Param 4: Value=0.0355, Grad=-0.0007671330779479007\n",
      "  Param 5: Value=-0.2147, Grad=-0.002973107426790564\n",
      "  Param 6: Value=1.0497, Grad=0.0009754587118124159\n",
      "  Max Abs Grad: 5.802043e-03\n",
      "------------------------------\n",
      "--- Epoch 181 / Loss: 1.600502 ---\n",
      "  Param 0: Value=3.3671, Grad=0.005802031196596534\n",
      "  Param 1: Value=0.8156, Grad=-0.0002672057308103544\n",
      "  Param 2: Value=0.4495, Grad=-0.0014968724324807164\n",
      "  Param 3: Value=-3.4259, Grad=-0.0012058307533893507\n",
      "  Param 4: Value=0.0355, Grad=-0.0007671322033102265\n",
      "  Param 5: Value=-0.2147, Grad=-0.002973092831468604\n",
      "  Param 6: Value=1.0497, Grad=0.0009754533667544319\n",
      "  Max Abs Grad: 5.802031e-03\n",
      "------------------------------\n",
      "--- Epoch 191 / Loss: 1.600502 ---\n",
      "  Param 0: Value=3.3671, Grad=0.005802023521411674\n",
      "  Param 1: Value=0.8156, Grad=-0.00026720332279804836\n",
      "  Param 2: Value=0.4495, Grad=-0.0014968722620996872\n",
      "  Param 3: Value=-3.4259, Grad=-0.0012058314916947408\n",
      "  Param 4: Value=0.0355, Grad=-0.0007671316062403254\n",
      "  Param 5: Value=-0.2147, Grad=-0.002973083128925085\n",
      "  Param 6: Value=1.0497, Grad=0.0009754498131485755\n",
      "  Max Abs Grad: 5.802024e-03\n",
      "------------------------------\n",
      "FINAL STATE: Epoch 200, Loss: 1.6005022280270396\n",
      "  Raw (vecc) Parameters: [3.3671047020222225, 0.8156127304020526, 0.44950475169697685, -3.4258506297302818, 0.03545588816692566, -0.21467686574898073, 1.0496833323567616]\n",
      "  Interpretable Parameters:\n",
      "    sigma_sq  : 12.826226\n",
      "    range_lon : 0.442368\n",
      "    range_lat : 0.353326\n",
      "    beta      : 0.180337\n",
      "    advec_lat : 0.035456\n",
      "    advec_lon : -0.214677\n",
      "    nugget    : 2.856746\n",
      "Day 1 optimization finished in 1124.77s over 200 epochs.\n",
      "Day 1 final results (raw params + loss): [3.3671047020222225, 0.8156127304020526, 0.44950475169697685, -3.4258506297302818, 0.03545588816692566, -0.21467686574898073, 1.0496833323567616, 1.6005022280270396]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- Assume global variables are set: ---\n",
    "# daily_hourly_maps, daily_aggregated_tensors, nns_map\n",
    "# lat_lon_resolution, v, mm_cond_number, nheads\n",
    "# lr, patience, factor, epochs\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "day_indices = [0] \n",
    "for day_idx in day_indices:  \n",
    "\n",
    "    daily_hourly_map = daily_hourly_maps[day_idx]\n",
    "    daily_aggregated_tensor = daily_aggregated_tensors[day_idx]\n",
    "\n",
    "    # --- ðŸ’¥ Correct Parameter Initialization (SPATIO-TEMPORAL) ---\n",
    "    init_sigmasq   = 15.0\n",
    "    init_range_lat = 0.66 \n",
    "    init_range_lon = 0.7 \n",
    "    init_nugget    = 1.5\n",
    "    init_beta      = 0.1  # <-- NEW (Temporal range)\n",
    "    init_advec_lat = 0.02  # <-- NEW (Start with no advection)\n",
    "    init_advec_lon = -0.08  # <-- NEW (Start with no advection)\n",
    "    \n",
    "    # Map model parameters to the 'phi' reparameterization\n",
    "    init_phi2 = 1.0 / init_range_lon                # 1/range_lon\n",
    "    init_phi1 = init_sigmasq * init_phi2            # sigmasq / range_lon\n",
    "    init_phi3 = (init_range_lon / init_range_lat)**2  # (range_lon / range_lat)^2\n",
    "    init_phi4 = init_beta**2                        # beta^2\n",
    "    \n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # ðŸ’¥ 7-parameter spatio-temporal list\n",
    "    params_list = [\n",
    "        torch.tensor([np.log(init_phi1)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [0] log_phi1\n",
    "        torch.tensor([np.log(init_phi2)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [1] log_phi2\n",
    "        torch.tensor([np.log(init_phi3)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [2] log_phi3\n",
    "        torch.tensor([np.log(init_phi4)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [3] log_phi4\n",
    "        torch.tensor([init_advec_lat],         requires_grad=True, dtype=torch.float64, device=device_str ), # [4] advec_lat (NOT log)\n",
    "        torch.tensor([init_advec_lon],         requires_grad=True, dtype=torch.float64, device=device_str ), # [5] advec_lon (NOT log)\n",
    "        torch.tensor([np.log(init_nugget)],    requires_grad=True, dtype=torch.float64, device=device_str )  # [6] log_nugget\n",
    "    ]\n",
    "\n",
    "    # --- Define learning rates and parameter groups ---\n",
    "    # ðŸ’¥ Group all 7 parameters.\n",
    "    # For a more advanced setup, you could create a separate group\n",
    "    # for advection params [4, 5] with a different (e.g., smaller) LR.\n",
    "    lr_all = lr \n",
    "    all_indices = [0, 1, 2, 3, 4, 5, 6] \n",
    "    \n",
    "    param_groups = [\n",
    "        {'params': [params_list[idx] for idx in all_indices], 'lr': lr_all, 'name': 'all_params'}\n",
    "    ]\n",
    "\n",
    "    # --- Print Job Info ---\n",
    "    res_calc = (113 // lat_lon_resolution[0]) * (158 // lat_lon_resolution[0]) \n",
    "    print(f'\\n--- Starting Day {day_idx+1} (2024-07-{day_idx+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n')\n",
    "    for i, p in enumerate(params_list):\n",
    "        print(f\"  Param {i}: {p.item():.4f}\")\n",
    "            \n",
    "    # --- ðŸ’¥ Instantiate the Correct Class ---\n",
    "    # Assumes fit_vecchia_adams is defined in your current scope\n",
    "    model_instance = kernels_reparam_space_time.fit_vecchia_adams(\n",
    "            smooth = v,\n",
    "            input_map = daily_hourly_map,\n",
    "            aggregated_data = daily_aggregated_tensor,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- Set Optimizer (This part is unchanged) ---\n",
    "    optimizer, scheduler = model_instance.set_optimizer(\n",
    "            param_groups,     \n",
    "            lr=lr,            \n",
    "            betas=(0.9, 0.99), \n",
    "            eps=1e-5, \n",
    "            scheduler_type='plateau', # Explicitly set to plateau\n",
    "            patience=patience,        \n",
    "            factor=factor             \n",
    "        )\n",
    "\n",
    "    # --- ðŸ’¥ Call the Correct Fit Method ---\n",
    "    out, epoch_ran = model_instance.fit_model(  # <-- Changed method name\n",
    "            params_list,\n",
    "            optimizer,\n",
    "            scheduler, \n",
    "            model_instance.matern_cov_aniso_STABLE_log_reparam, # <-- Changed covariance function\n",
    "            epochs=epochs\n",
    "        )\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Day {day_idx+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day_idx+1} final results (raw params + loss): {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dedca71",
   "metadata": {},
   "source": [
    "# L bfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04483492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 1092, smooth: 0.5\n",
      "mm_cond_number: 8,\n",
      "initial parameters: \n",
      "\n",
      "  Param 0: 3.0647\n",
      "  Param 1: 0.3567\n",
      "  Param 2: 0.1177\n",
      "  Param 3: -4.6052\n",
      "  Param 4: 0.0200\n",
      "  Param 5: -0.0800\n",
      "  Param 6: 0.4055\n",
      "--- Starting L-BFGS Optimization ---\n",
      "--- Step 1/50 / Loss: 1.753064 ---\n",
      "  Param 0: Value=3.1112, Grad=1.6542075974134585e-06\n",
      "  Param 1: Value=0.5747, Grad=-7.918325239806735e-07\n",
      "  Param 2: Value=0.6239, Grad=-1.0859535723617813e-06\n",
      "  Param 3: Value=-3.1758, Grad=4.875418142862508e-07\n",
      "  Param 4: Value=0.0386, Grad=3.5165812885850205e-06\n",
      "  Param 5: Value=-0.2145, Grad=-8.485080755571978e-06\n",
      "  Param 6: Value=1.2493, Grad=2.4363685241185446e-09\n",
      "  Max Abs Grad: 8.485081e-06\n",
      "------------------------------\n",
      "\n",
      "Converged on gradient norm (max|grad| < 1e-05) at step 1\n",
      "FINAL STATE: Step 1, Loss: 1.7530641938262586\n",
      "  Raw (vecc) Parameters: [3.1112141446960853, 0.5747205647289292, 0.6238819518366033, -3.175843531960354, 0.03864259329907545, -0.21449336370243605, 1.2493402126336262]\n",
      "  Interpretable Parameters:\n",
      "    sigma_sq  : 12.635289\n",
      "    range_lon : 0.562862\n",
      "    range_lat : 0.412029\n",
      "    beta      : 0.204350\n",
      "    advec_lat : 0.038643\n",
      "    advec_lon : -0.214493\n",
      "    nugget    : 3.488041\n",
      "Day 1 optimization finished in 142.22s over 1 L-BFGS steps.\n",
      "Day 1 final results (raw params + loss): [3.1112141446960853, 0.5747205647289292, 0.6238819518366033, -3.175843531960354, 0.03864259329907545, -0.21449336370243605, 1.2493402126336262, 1.7530641938262586]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import time\n",
    "# NOTE: Assuming the necessary classes and global variables are defined elsewhere, \n",
    "# specifically the referenced 'fit_vecchia_lbfgs' class and 'kernels_reparam_space_time' module.\n",
    "\n",
    "# --- Assume global variables are set: ---\n",
    "# daily_hourly_maps, daily_aggregated_tensors, nns_map\n",
    "# lat_lon_resolution, v, mm_cond_number, nheads\n",
    "# lr, patience, factor, epochs\n",
    "\n",
    "# --- L-BFGS SPECIFIC GLOBAL PARAMETERS ---\n",
    "LBFGS_LR = 1.0\n",
    "LBFGS_MAX_STEPS = 50       # Number of outer optimization steps\n",
    "LBFGS_HISTORY_SIZE = 100   # Memory for Hessian approximation\n",
    "LBFGS_MAX_EVAL = 50        # Max evaluations (line search) per step\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "day_indices = [0] \n",
    "for day_idx in day_indices:  \n",
    "\n",
    "    # Assuming data access is correct\n",
    "    daily_hourly_map = daily_hourly_maps[day_idx]\n",
    "    daily_aggregated_tensor = daily_aggregated_tensors[day_idx]\n",
    "\n",
    "    # --- Parameter Initialization (SPATIO-TEMPORAL) ---\n",
    "    init_sigmasq   = 15.0\n",
    "    init_range_lat = 0.66 \n",
    "    init_range_lon = 0.7 \n",
    "    init_nugget    = 1.5\n",
    "    init_beta      = 0.1\n",
    "    init_advec_lat = 0.02\n",
    "    init_advec_lon = -0.08\n",
    "    \n",
    "    # Map model parameters to the 'phi' reparameterization\n",
    "    init_phi2 = 1.0 / init_range_lon                # 1/range_lon\n",
    "    init_phi1 = init_sigmasq * init_phi2            # sigmasq / range_lon\n",
    "    init_phi3 = (init_range_lon / init_range_lat)**2  # (range_lon / range_lat)^2\n",
    "    init_phi4 = init_beta**2                        # beta^2\n",
    "    \n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # 7-parameter spatio-temporal list (Log/Linear)\n",
    "    params_list = [\n",
    "        torch.tensor([np.log(init_phi1)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [0] log(phi1)\n",
    "        torch.tensor([np.log(init_phi2)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [1] log(phi2)\n",
    "        torch.tensor([np.log(init_phi3)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [2] log(phi3)\n",
    "        torch.tensor([np.log(init_phi4)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [3] log(phi4)\n",
    "        torch.tensor([init_advec_lat],         requires_grad=True, dtype=torch.float64, device=device_str ), # [4] advec_lat (linear)\n",
    "        torch.tensor([init_advec_lon],         requires_grad=True, dtype=torch.float64, device=device_str ), # [5] advec_lon (linear)\n",
    "        torch.tensor([np.log(init_nugget)],    requires_grad=True, dtype=torch.float64, device=device_str )  # [6] log(nugget)\n",
    "    ]\n",
    "\n",
    "    # --- Define parameter groups ---\n",
    "    lr_all = LBFGS_LR\n",
    "    all_indices = [0, 1, 2, 3, 4, 5, 6] \n",
    "    \n",
    "    # L-BFGS requires the parameters to be iterable in a single list or group\n",
    "    param_groups = [\n",
    "        {'params': [params_list[idx] for idx in all_indices], 'lr': lr_all, 'name': 'all_params'}\n",
    "    ]\n",
    "\n",
    "    # --- Print Job Info (using placeholder print variables) ---\n",
    "    res_calc = (113 // lat_lon_resolution[0]) * (158 // lat_lon_resolution[0]) \n",
    "    print(f'\\n--- Starting Day {day_idx+1} (2024-07-{day_idx+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n')\n",
    "    for i, p in enumerate(params_list):\n",
    "        print(f\"  Param {i}: {p.item():.4f}\")\n",
    "            \n",
    "    # --- ðŸ’¥ Instantiate the L-BFGS Class ---\n",
    "    # NOTE: Assuming fit_vecchia_lbfgs is available via kernels_reparam_space_time\n",
    "    model_instance = kernels_reparam_space_time.fit_vecchia_lbfgs(\n",
    "            smooth = v,\n",
    "            input_map = daily_hourly_map,\n",
    "            aggregated_data = daily_aggregated_tensor,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- ðŸ’¥ Set L-BFGS Optimizer ---\n",
    "    # L-BFGS specific arguments are passed here\n",
    "    optimizer = model_instance.set_optimizer(\n",
    "            param_groups,     \n",
    "            lr=LBFGS_LR,            \n",
    "            max_iter=LBFGS_MAX_EVAL,        # max_iter in LBFGS is the line search limit\n",
    "            history_size=LBFGS_HISTORY_SIZE \n",
    "        )\n",
    "\n",
    "    # --- ðŸ’¥ Call the L-BFGS Fit Method ---\n",
    "    out, steps_ran = model_instance.fit_vecc_lbfgs(\n",
    "            params_list,\n",
    "            optimizer,\n",
    "            model_instance.matern_cov_aniso_STABLE_log_reparam, \n",
    "            max_steps=LBFGS_MAX_STEPS # Outer loop steps\n",
    "        )\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Day {day_idx+1} optimization finished in {epoch_time:.2f}s over {steps_ran+1} L-BFGS steps.\")\n",
    "    print(f\"Day {day_idx+1} final results (raw params + loss): {out}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
