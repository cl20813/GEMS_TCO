{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f33ee41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating on: cpu\n"
     ]
    }
   ],
   "source": [
    "# reflected location error in ozone data simulation\n",
    "\n",
    "import torch\n",
    "import torch.fft\n",
    "import numpy as np\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "import os\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "\n",
    "# Data manipulation and analysis\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy                    # clone tensor\n",
    "import time\n",
    "from sklearn.neighbors import BallTree\n",
    "# Custom imports\n",
    "\n",
    "\n",
    "from GEMS_TCO import kernels_reparam_space_time_gpu as kernels_reparam_space_time_gpu\n",
    "\n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import alg_optimization, BaseLogger\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "import typer\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "from GEMS_TCO import configuration as config\n",
    "from GEMS_TCO.data_loader import load_data2, exact_location_filter\n",
    "from GEMS_TCO import debiased_whittle\n",
    "from torch.nn import Parameter\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# Check for Mac GPU (MPS) first, then CUDA, then CPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Optional: Force CPU if you encounter Cholesky errors later\n",
    "# DEVICE = torch.device(\"cpu\") \n",
    "DTYPE = torch.float32 if DEVICE.type == 'mps' else torch.float64\n",
    "\n",
    "print(f\"Simulating on: {DEVICE}\")\n",
    "\n",
    "# TRUE PARAMETERS\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lon = 0.195 \n",
    "init_range_lat = 0.154 \n",
    "init_advec_lat = 0.0418\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "# Map parameters\n",
    "init_phi2 = 1.0 / init_range_lon\n",
    "init_phi1 = init_sigmasq * init_phi2\n",
    "init_phi3 = (init_range_lon / init_range_lat)**2\n",
    "init_phi4 = (init_range_lon / init_range_time)**2\n",
    "\n",
    "# Create Initial Parameters\n",
    "initial_vals = [np.log(init_phi1), np.log(init_phi2), np.log(init_phi3), \n",
    "                np.log(init_phi4), init_advec_lat, init_advec_lon, np.log(init_nugget)]\n",
    "\n",
    "params_list = [\n",
    "    torch.tensor([val], requires_grad=True, dtype=DTYPE, device=DEVICE)\n",
    "    for val in initial_vals\n",
    "]\n",
    "\n",
    "# Mean Ozone\n",
    "OZONE_MEAN = 260.0\n",
    "\n",
    "# --- 2. EXACT COVARIANCE ---\n",
    "def get_model_covariance_on_grid(lags_x, lags_y, lags_t, params):\n",
    "    phi1, phi2, phi3, phi4 = torch.exp(params[0]), torch.exp(params[1]), torch.exp(params[2]), torch.exp(params[3])\n",
    "    advec_lat, advec_lon = params[4], params[5]\n",
    "    sigmasq = phi1 / phi2\n",
    "\n",
    "    u_lat_eff = lags_x - advec_lat * lags_t\n",
    "    u_lon_eff = lags_y - advec_lon * lags_t\n",
    "    \n",
    "    dist_sq = (u_lat_eff.pow(2) * phi3) + (u_lon_eff.pow(2)) + (lags_t.pow(2) * phi4)\n",
    "    distance = torch.sqrt(dist_sq + 1e-8)\n",
    "    \n",
    "    return sigmasq * torch.exp(-distance * phi2)\n",
    "\n",
    "# --- 3. FFT SIMULATION ---\n",
    "def get_wrapped_covariance(lags_x, lags_y, lags_t, params, Lx_len, Ly_len, Lt_len):\n",
    "    phi1, phi2, phi3, phi4 = torch.exp(params[0]), torch.exp(params[1]), torch.exp(params[2]), torch.exp(params[3])\n",
    "    advec_lat, advec_lon = params[4], params[5]\n",
    "    sigmasq = phi1 / phi2\n",
    "\n",
    "    # [ÌïµÏã¨] Folding Logic: Í±∞Î¶¨Í∞Ä Ï†ÑÏ≤¥ Í∏∏Ïù¥Ïùò Ï†àÎ∞òÏùÑ ÎÑòÏñ¥Í∞ÄÎ©¥ Î∞òÎåÄÌé∏ Í±∞Î¶¨Î°ú Í≥ÑÏÇ∞\n",
    "    # Ïòà: Í∏∏Ïù¥Í∞Ä 10Ïù∏Îç∞ Í±∞Î¶¨Í∞Ä 8Ïù¥Î©¥, ÏÇ¨Ïã§ÏÉÅ Î∞òÎåÄÌé∏ÏúºÎ°ú 2ÎßåÌÅº Îñ®Ïñ¥ÏßÑ Í≤ÉÏûÑ.\n",
    "    \n",
    "    # 1. Advection Ï†ÅÏö©\n",
    "    u_lat = lags_x - advec_lat * lags_t\n",
    "    u_lon = lags_y - advec_lon * lags_t\n",
    "    \n",
    "    # 2. Wrap-around (Torus) Í±∞Î¶¨ Í≥ÑÏÇ∞\n",
    "    # torch.remainderÎ•º Ïç®ÏÑú Ï£ºÍ∏∞ÏÑ±ÏùÑ ÎßåÎì¶\n",
    "    u_lat_wrapped = torch.remainder(u_lat + Lx_len/2, Lx_len) - Lx_len/2\n",
    "    u_lon_wrapped = torch.remainder(u_lon + Ly_len/2, Ly_len) - Ly_len/2\n",
    "    t_wrapped     = torch.remainder(lags_t + Lt_len/2, Lt_len) - Lt_len/2\n",
    "\n",
    "    dist_sq = (u_lat_wrapped.pow(2) * phi3) + (u_lon_wrapped.pow(2)) + (t_wrapped.pow(2) * phi4)\n",
    "    distance = torch.sqrt(dist_sq + 1e-8)\n",
    "    \n",
    "    return sigmasq * torch.exp(-distance * phi2)\n",
    "\n",
    "# --- 4. REGULAR GRID FUNCTIONS (FIXED WITH ROUNDING) ---\n",
    "\n",
    "def make_target_grid(lat_start, lat_end, lat_step, lon_start, lon_end, lon_step, device, dtype):\n",
    "    \"\"\"\n",
    "    Constructs a grid explicitly from start to end.\n",
    "    CRITICAL: Includes rounding to 4 decimal places to prevent \"Tensor size does not match\" errors.\n",
    "    \"\"\"\n",
    "    # 1. Generate Latitudes (Descending from 5.0)\n",
    "    # We use a small epsilon to ensure the 'end' is included if it's a multiple\n",
    "    lats = torch.arange(lat_start, lat_end - 0.0001, lat_step, device=device, dtype=dtype)\n",
    "    lats = torch.round(lats * 10000) / 10000  # <--- FIX: Round to 4 decimals\n",
    "    \n",
    "    # 2. Generate Longitudes (Descending from 133.0)\n",
    "    lons = torch.arange(lon_start, lon_end - 0.0001, lon_step, device=device, dtype=dtype)\n",
    "    lons = torch.round(lons * 10000) / 10000  # <--- FIX: Round to 4 decimals\n",
    "\n",
    "    print(f\"Grid Generation debug: Lat Range {lats[0]:.4f}-{lats[-1]:.4f}, Lon Range {lons[0]:.4f}-{lons[-1]:.4f}\")\n",
    "    print(f\"Unique Lats: {len(lats)}, Unique Lons: {len(lons)}\")\n",
    "\n",
    "    # 3. Meshgrid (indexing='ij' -> Lat is rows, Lon is cols)\n",
    "    grid_lat, grid_lon = torch.meshgrid(lats, lons, indexing='ij')\n",
    "\n",
    "    # 4. Flatten\n",
    "    flat_lats = grid_lat.flatten()\n",
    "    flat_lons = grid_lon.flatten()\n",
    "\n",
    "    # 5. Stack\n",
    "    center_points = torch.stack([flat_lats, flat_lons], dim=1)\n",
    "    \n",
    "    # Return grid AND dimensions (Nx, Ny) for verification\n",
    "    return center_points, len(lats), len(lons)\n",
    "\n",
    "def coarse_by_center_tensor(input_map_tensors: dict, target_grid_tensor: torch.Tensor):\n",
    "    coarse_map = {}\n",
    "    \n",
    "    # BallTree requires CPU Numpy\n",
    "    query_points_np = target_grid_tensor.cpu().numpy()\n",
    "    query_points_rad = np.radians(query_points_np)\n",
    "    \n",
    "    for key, val_tensor in input_map_tensors.items():\n",
    "        # Source locations (Perturbed)\n",
    "        source_locs_np = val_tensor[:, :2].cpu().numpy()\n",
    "        source_locs_rad = np.radians(source_locs_np)\n",
    "        \n",
    "        # NN Search\n",
    "        tree = BallTree(source_locs_rad, metric='haversine')\n",
    "        dist, ind = tree.query(query_points_rad, k=1)\n",
    "        nearest_indices = ind.flatten()\n",
    "        \n",
    "        # Map values back to tensor\n",
    "        indices_tensor = torch.tensor(nearest_indices, device=val_tensor.device, dtype=torch.long)\n",
    "        gathered_vals = val_tensor[indices_tensor, 2]\n",
    "        gathered_times = val_tensor[indices_tensor, 3]\n",
    "        \n",
    "        # Construct Regular Tensor\n",
    "        new_tensor = torch.stack([\n",
    "            target_grid_tensor[:, 0], # Regular Lat\n",
    "            target_grid_tensor[:, 1], # Regular Lon\n",
    "            gathered_vals,            # Mapped Value\n",
    "            gathered_times            # Mapped Time\n",
    "        ], dim=1)\n",
    "        \n",
    "        coarse_map[key] = new_tensor\n",
    "\n",
    "    return coarse_map\n",
    "\n",
    "# --- 3. FFT SIMULATION (FOLDING VERSION) ---\n",
    "\n",
    "# [ÏàòÏ†ïÎêú ÏãúÎÆ¨Î†àÏù¥ÏÖò Ìï®Ïàò] 2Î∞∞ Îª•ÌäÄÍ∏∞ ÏóÜÏù¥, Folding Í≥µÎ∂ÑÏÇ∞ ÏÇ¨Ïö©\n",
    "def generate_folded_gems_field(lat_coords, lon_coords, t_steps, params):\n",
    "    Nx = len(lat_coords)\n",
    "    Ny = len(lon_coords)\n",
    "    Nt = t_steps\n",
    "    \n",
    "    # [Ï∞®Ïù¥Ï†ê 1] 2Î∞∞ ÌôïÏû•(Padding) ÌïòÏßÄ ÏïäÏùå! ÏûÖÎ†• ÌÅ¨Í∏∞ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©\n",
    "    # Ïù¥ÎØ∏ 1.25Î∞∞ ÌôïÏû•Îêú Í≤©ÏûêÍ∞Ä Îì§Ïñ¥Ïò§ÎØÄÎ°ú Ïù¥Í±∏Î°ú Ï∂©Î∂ÑÌï®\n",
    "    Px, Py, Pt = Nx, Ny, Nt\n",
    "    \n",
    "    print(f\"Folded Simulation Grid: {Px} x {Py} x {Pt} (No 2x Padding)\")\n",
    "    \n",
    "    dlat = float(lat_coords[1] - lat_coords[0])\n",
    "    dlon = float(lon_coords[1] - lon_coords[0])\n",
    "    dt = 1.0 \n",
    "    \n",
    "    # Ï†ÑÏ≤¥ ÎèÑÎ©îÏù∏ Î¨ºÎ¶¨Ï†Å Í∏∏Ïù¥ (Ï£ºÍ∏∞)\n",
    "    Lx_len = abs(Px * dlat)\n",
    "    Ly_len = abs(Py * dlon)\n",
    "    Lt_len = abs(Pt * dt)\n",
    "    \n",
    "    # Í≤©Ïûê ÏÉùÏÑ±\n",
    "    lags_x = torch.arange(Px, device=DEVICE, dtype=DTYPE) * dlat # dlat Î∂ÄÌò∏ Í∑∏ÎåÄÎ°ú\n",
    "    lags_y = torch.arange(Py, device=DEVICE, dtype=DTYPE) * dlon\n",
    "    lags_t = torch.arange(Pt, device=DEVICE, dtype=DTYPE) * dt\n",
    "    \n",
    "    # Meshgrid\n",
    "    L_x, L_y, L_t = torch.meshgrid(lags_x, lags_y, lags_t, indexing='ij')\n",
    "\n",
    "    # [Ï∞®Ïù¥Ï†ê 2] ÏùºÎ∞ò Í≥µÎ∂ÑÏÇ∞ ÎåÄÏã† 'Wrapped(Folding)' Í≥µÎ∂ÑÏÇ∞ Ìò∏Ï∂ú\n",
    "    C_vals = get_wrapped_covariance(L_x, L_y, L_t, params, Lx_len, Ly_len, Lt_len)\n",
    "\n",
    "    # FFT Simulation\n",
    "    S = torch.fft.fftn(C_vals)\n",
    "    S.real = torch.clamp(S.real, min=0) # Í∑ºÏÇ¨ Ïò§Ï∞®Î°ú Ïù∏Ìïú ÏùåÏàò Ï†úÍ±∞\n",
    "\n",
    "    random_phase = torch.fft.fftn(torch.randn(Px, Py, Pt, device=DEVICE, dtype=DTYPE))\n",
    "    weighted_freq = torch.sqrt(S.real) * random_phase\n",
    "    field_sim = torch.fft.ifftn(weighted_freq).real\n",
    "    \n",
    "    # ÌÅ¨Í∏∞Í∞Ä Í∞ôÏúºÎØÄÎ°ú Slicing ÏóÜÏù¥ Í∑∏ÎåÄÎ°ú Î∞òÌôò\n",
    "    return field_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f77ba58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Grid: Lat 2.0~-3.0, Lon 121.0~131.0\n",
      "Padding Added: Lat +/- 0.625, Lon +/- 1.250\n",
      "1. Generating True Field (Folded)...\n",
      "Folded Simulation Grid: 143 x 199 x 8 (No 2x Padding)\n",
      "\n",
      "--- Enforcing Regular Grid & Cropping ---\n",
      "Grid Generation debug: Lat Range 2.0000--2.9720, Lon Range 121.0000-130.9540\n",
      "Unique Lats: 114, Unique Lons: 159\n",
      "Final 'inputmap' keys: 8\n",
      "Final 'aggregated_data' Shape: torch.Size([145008, 4])\n",
      "  -> Should match Target Grid Size: 18126\n"
     ]
    }
   ],
   "source": [
    "# --- 5. EXECUTION (5/4 Expansion Strategy & Cropping) ---\n",
    "\n",
    "# [ÏÑ§Ï†ï] Î™©Ìëú ÌÉÄÍ≤ü Î≤îÏúÑ (Target Domain - Clean Zone)\n",
    "target_lat_start, target_lat_end = 2.0, -3.0    # Span 5.0 (Î∂Å -> ÎÇ®)\n",
    "target_lon_start, target_lon_end = 121.0, 131.0 # Span 10.0 (ÏÑú -> Îèô)\n",
    "\n",
    "# [Ï†ÑÎûµ] 5/4Î∞∞ (1.25Î∞∞) ÌôïÏû• (Expansion)\n",
    "lat_span = abs(target_lat_start - target_lat_end)\n",
    "lon_span = abs(target_lon_start - target_lon_end)\n",
    "expansion_factor = 1.25 \n",
    "\n",
    "lat_padding = (lat_span * expansion_factor - lat_span) / 2  # 0.625\n",
    "lon_padding = (lon_span * expansion_factor - lon_span) / 2  # 1.25\n",
    "\n",
    "print(f\"Target Grid: Lat {target_lat_start}~{target_lat_end}, Lon {target_lon_start}~{target_lon_end}\")\n",
    "print(f\"Padding Added: Lat +/- {lat_padding:.3f}, Lon +/- {lon_padding:.3f}\")\n",
    "\n",
    "# 1. ÌôïÏû•Îêú ÏãúÎÆ¨Î†àÏù¥ÏÖò Í≤©Ïûê ÏÉùÏÑ±\n",
    "lats_sim_extended = torch.arange(\n",
    "    target_lat_start + lat_padding,       # 2.625\n",
    "    target_lat_end - lat_padding - 0.001, # -3.625\n",
    "    -0.044, device=DEVICE, dtype=DTYPE\n",
    ")\n",
    "lons_sim_extended = torch.arange(\n",
    "    target_lon_start - lon_padding,       # 119.75\n",
    "    target_lon_end + lon_padding + 0.001, # 132.25\n",
    "    0.063, device=DEVICE, dtype=DTYPE\n",
    ")\n",
    "\n",
    "t_def = 8\n",
    "LOC_ERR_STD = 0.01 \n",
    "\n",
    "# --- 5. EXECUTION (5/4 Expansion Strategy & Cropping) ---\n",
    "\n",
    "# ... (ÌÉÄÍ≤ü ÏÑ§Ï†ï Î∞è ÌôïÏû• Í≤©Ïûê ÏÉùÏÑ± ÏΩîÎìúÎäî Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ) ...\n",
    "\n",
    "print(\"1. Generating True Field (Folded)...\")\n",
    "\n",
    "# [ÏàòÏ†ï] generate_exact_gems_field -> generate_folded_gems_field Î°ú Î≥ÄÍ≤Ω\n",
    "sim_field = generate_folded_gems_field(lats_sim_extended, lons_sim_extended, t_def, params_list)\n",
    "\n",
    "# ... (Ïù¥ÌõÑ Perturbation Î∞è Cropping ÏΩîÎìúÎäî Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ) ...\n",
    "# 2. Perturbation & Formatting (Extended Data)\n",
    "raw_extended_map = {} \n",
    "nugget_std = torch.sqrt(torch.exp(params_list[6]))\n",
    "\n",
    "grid_lat, grid_lon = torch.meshgrid(lats_sim_extended, lons_sim_extended, indexing='ij')\n",
    "flat_lats = grid_lat.flatten()\n",
    "flat_lons = grid_lon.flatten()\n",
    "\n",
    "for t in range(t_def):\n",
    "    field_t = sim_field[:, :, t]\n",
    "    flat_vals = field_t.flatten()\n",
    "    \n",
    "    # Noise & Perturbation\n",
    "    obs_vals = flat_vals + (torch.randn_like(flat_vals) * nugget_std) + OZONE_MEAN\n",
    "    lat_noise = torch.randn_like(flat_lats) * LOC_ERR_STD\n",
    "    lon_noise = torch.randn_like(flat_lons) * LOC_ERR_STD\n",
    "    \n",
    "    # ÌôïÏû•Îêú Îç∞Ïù¥ÌÑ∞ ÌÖêÏÑú ÏÉùÏÑ± (Padding Ìè¨Ìï®)\n",
    "    row_tensor = torch.stack([\n",
    "        flat_lats + lat_noise,  # Lat (Perturbed)\n",
    "        flat_lons + lon_noise,  # Lon (Perturbed)\n",
    "        obs_vals,               # Val\n",
    "        torch.full_like(flat_lats, 21.0 + t) # Time\n",
    "    ], dim=1)\n",
    "    \n",
    "    key_str = f'2024_07_y24m07day01_hm{t:02d}:53'\n",
    "    raw_extended_map[key_str] = row_tensor.detach()\n",
    "\n",
    "# --- 6. CROPPING & CONNECTING TO DOWNSTREAM ---\n",
    "print(\"\\n--- Enforcing Regular Grid & Cropping ---\")\n",
    "\n",
    "# 1. ÌÉÄÍ≤ü Í≤©Ïûê ÏÉùÏÑ± (Clean Zone Only)\n",
    "# Ìå®Îî© ÏóÜÏù¥ 'ÏõêÎûò Î∂ÑÏÑùÌïòÎ†§Îçò Î≤îÏúÑ'Îßå ÏÉùÏÑ±\n",
    "step_lat, step_lon = 0.044, 0.063\n",
    "target_grid, Nx_reg, Ny_reg = make_target_grid(\n",
    "    lat_start=target_lat_start, lat_end=target_lat_end, lat_step=-step_lat, \n",
    "    lon_start=target_lon_start, lon_end=target_lon_end, lon_step=step_lon,  \n",
    "    device=DEVICE, dtype=DTYPE\n",
    ")\n",
    "\n",
    "# 2. Map & Crop (Í∞ÄÏû•ÏûêÎ¶¨ Î≤ÑÎ¶¨Í∏∞)\n",
    "# raw_extended_map(ÌôïÏû• Îç∞Ïù¥ÌÑ∞)ÏóêÏÑú target_grid(Ï§ëÏã¨Î∂Ä)Ïóê ÎßûÎäî Í≤ÉÎßå Í∞ÄÏ†∏Ïò¥\n",
    "# [Ï§ëÏöî] Î≥ÄÏàòÎ™Ö inputmap Ïú†ÏßÄ\n",
    "inputmap = coarse_by_center_tensor(raw_extended_map, target_grid)\n",
    "\n",
    "# [Ï§ëÏöî] aggregated_data Ïû¨ÏÉùÏÑ± (Clean Data Í∏∞Ï§Ä)\n",
    "# Ïù¥Ï†ÑÏóê raw_extended_mapÏúºÎ°ú ÎßåÎì† aggregated_dataÎäî Î≤ÑÎ¶¨Í≥†,\n",
    "# ÏûòÎùºÎÇ∏ inputmapÏùÑ Í∏∞Ï§ÄÏúºÎ°ú Îã§Ïãú Ìï©Ï≥êÏïº Ìï©ÎãàÎã§.\n",
    "aggregated_list = list(inputmap.values())\n",
    "aggregated_data = torch.cat(aggregated_list, dim=0)\n",
    "\n",
    "print(f\"Final 'inputmap' keys: {len(inputmap)}\")\n",
    "print(f\"Final 'aggregated_data' Shape: {aggregated_data.shape}\")\n",
    "\n",
    "# Í≤ÄÏ¶ù\n",
    "check_tensor = list(inputmap.values())[0]\n",
    "print(f\"  -> Should match Target Grid Size: {target_grid.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cacde8",
   "metadata": {},
   "source": [
    "set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b66f52d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GEMS_TCO import orderings as _orderings\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "# inputmap, aggregated_data Î≥ÄÏàòÎäî Ïô∏Î∂ÄÏóê ÏûàÎã§Í≥† Í∞ÄÏ†ï\n",
    "\n",
    "def get_spatial_ordering(\n",
    "        input_maps: dict,\n",
    "        mm_cond_number: int = 10\n",
    "    ) -> Tuple[np.ndarray, list]: # Î∞òÌôò ÌÉÄÏûÖÌûåÌä∏ Î≥ÄÍ≤Ω (list)\n",
    "        \n",
    "        key_list = list(input_maps.keys())\n",
    "        data_for_coord = input_maps[key_list[0]]\n",
    "        \n",
    "        # Tensor -> Numpy Î≥ÄÌôò\n",
    "        if isinstance(data_for_coord, torch.Tensor):\n",
    "            data_for_coord = data_for_coord.cpu().numpy()\n",
    "\n",
    "        x1 = data_for_coord[:, 0]\n",
    "        y1 = data_for_coord[:, 1]\n",
    "        \n",
    "        coords1 = np.stack((x1, y1), axis=-1)\n",
    "\n",
    "        # 1. MaxMin Ordering\n",
    "        ord_mm = _orderings.maxmin_cpp(coords1)\n",
    "        \n",
    "        # 2. Reorder coordinates\n",
    "        data_for_coord_reordered = data_for_coord[ord_mm]\n",
    "        coords1_reordered = np.stack(\n",
    "            (data_for_coord_reordered[:, 0], data_for_coord_reordered[:, 1]), \n",
    "            axis=-1\n",
    "        )\n",
    "        \n",
    "        # 3. Calculate nearest neighbors map (Dictionary Î∞òÌôòÎê®)\n",
    "        nns_map_dict = _orderings.find_nns_l2(locs=coords1_reordered, max_nn=mm_cond_number)\n",
    "        \n",
    "        # --- üî¥ [FIX 1] Dictionary -> List Î≥ÄÌôò (TypeError Î∞©ÏßÄ) ---\n",
    "        # ÌÇ§(Key) ÏàúÏÑúÎåÄÎ°ú Í∞í(Value)Îßå ÎΩëÏïÑÏÑú Î¶¨Ïä§Ìä∏Î°ú ÎßåÎì≠ÎãàÎã§.\n",
    "        nns_map_list = [nns_map_dict[i] for i in range(len(nns_map_dict))]\n",
    "        # -----------------------------------------------------\n",
    "        \n",
    "        return ord_mm, nns_map_list\n",
    "\n",
    "# --- üî¥ [FIX 2] mm_cond_number Î≥ÄÍ≤Ω (16 -> 10) ---\n",
    "# 16Í∞úÎ°ú ÌïòÎ©¥ Î©îÎ™®Î¶¨(35)Í∞Ä ÌÑ∞ÏßëÎãàÎã§. 3*10 + 5 = 35 Ïù¥ÎØÄÎ°ú 10Ïù¥ ÌïúÍ≥ÑÏûÖÎãàÎã§.\n",
    "ord_mm, nns_map = get_spatial_ordering(inputmap, mm_cond_number=15)\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Ïû¨Ï†ïÎ†¨ (Ïù¥Í±¥ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©)\n",
    "mm_input_map = {}\n",
    "for key in inputmap:\n",
    "    mm_input_map[key] = inputmap[key][ord_mm]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51634ad",
   "metadata": {},
   "source": [
    "Likelihood vs. Truth: A model with the wrong parameters (short range) might produce a higher Vecchia likelihood because it fits the high-frequency noise better, but it will be terrible at prediction (Kriging) away from data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7191bead",
   "metadata": {},
   "source": [
    "# Fit vecchia max min time 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85190f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "========================================\n",
      "--- Initializing VecchiaBatched Model ---\n",
      "========================================\n",
      "\n",
      "========================================\n",
      "--- Running L-BFGS Optimization ---\n",
      "========================================\n",
      "üöÄ Pre-computing (Corrected Vectorization)... ‚úÖ Done in 1.0000s. (Heads: 2400, Tails: 142608)\n",
      "--- Starting Batched L-BFGS Optimization (GPU) ---\n",
      "--- Step 1/3 / Loss: 1.249389 ---\n",
      "  Param 0: Value=4.2541, Grad=-1.5762379574032798e-07\n",
      "  Param 1: Value=1.7243, Grad=9.95809473811304e-08\n",
      "  Param 2: Value=0.3339, Grad=-3.099168185039211e-07\n",
      "  Param 3: Value=-2.3352, Grad=1.150912478730759e-07\n",
      "  Param 4: Value=0.0621, Grad=-2.313199227550137e-07\n",
      "  Param 5: Value=-0.1779, Grad=-5.3748181868922e-07\n",
      "  Param 6: Value=-1.4740, Grad=-1.919515572459652e-07\n",
      "  Max Abs Grad: 5.374818e-07\n",
      "------------------------------\n",
      "--- Step 2/3 / Loss: 1.247604 ---\n",
      "  Param 0: Value=4.2541, Grad=-1.5762379574032798e-07\n",
      "  Param 1: Value=1.7243, Grad=9.95809473811304e-08\n",
      "  Param 2: Value=0.3339, Grad=-3.099168185039211e-07\n",
      "  Param 3: Value=-2.3352, Grad=1.150912478730759e-07\n",
      "  Param 4: Value=0.0621, Grad=-2.313199227550137e-07\n",
      "  Param 5: Value=-0.1779, Grad=-5.3748181868922e-07\n",
      "  Param 6: Value=-1.4740, Grad=-1.919515572459652e-07\n",
      "  Max Abs Grad: 5.374818e-07\n",
      "------------------------------\n",
      "--- Step 3/3 / Loss: 1.247604 ---\n",
      "  Param 0: Value=4.2541, Grad=-1.5762379574032798e-07\n",
      "  Param 1: Value=1.7243, Grad=9.95809473811304e-08\n",
      "  Param 2: Value=0.3339, Grad=-3.099168185039211e-07\n",
      "  Param 3: Value=-2.3352, Grad=1.150912478730759e-07\n",
      "  Param 4: Value=0.0621, Grad=-2.313199227550137e-07\n",
      "  Param 5: Value=-0.1779, Grad=-5.3748181868922e-07\n",
      "  Param 6: Value=-1.4740, Grad=-1.919515572459652e-07\n",
      "  Max Abs Grad: 5.374818e-07\n",
      "------------------------------\n",
      "Final Interpretable Params: {'sigma_sq': 12.550968451994, 'range_lon': 0.17829175252165666, 'range_lat': 0.15087767760409757, 'range_time': 0.5730659105565772, 'advec_lat': 0.06208971176953433, 'advec_lon': -0.17790341869704374, 'nugget': 0.22901237722387074}\n",
      "\n",
      "Optimization finished in 85.70s.\n",
      "Results after 2 steps: [4.254131841461119, 1.7243340113710215, 0.33390368236413404, -2.335158939275716, 0.06208971176953433, -0.17790341869704374, -1.4739792278759567, 1.2476043286412608]\n",
      "Final Params: [ 4.25413184  1.72433401  0.33390368 -2.33515894  0.06208971 -0.17790342\n",
      " -1.47397923]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "v = 0.5              # Smoothness\n",
    "mm_cond_number = 8   # Neighbors\n",
    "#mm_cond_number = 16   # Neighbors\n",
    "nheads = 300           # 0 = Pure Vecchia\n",
    "lr = 1.0             # LBFGS learning rate\n",
    "LBFGS_MAX_STEPS = 3\n",
    "LBFGS_HISTORY_SIZE = 100 # 100\n",
    "LBFGS_LR = 1.0\n",
    "LBFGS_MAX_EVAL = 30    \n",
    "\n",
    "#DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- 1. SETUP PARAMETERS (List of Scalars) ---\n",
    "# Truth: [4.18, 1.94, 0.24, -3.97, 0.014, -0.20, -0.85]\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lat = 0.154 \n",
    "init_range_lon = 0.195\n",
    "init_advec_lat = 0.0218\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "# Map model parameters to the 'phi' reparameterization\n",
    "init_phi2 = 1.0 / init_range_lon                # 1/range_lon\n",
    "init_phi1 = init_sigmasq * init_phi2            # sigmasq / range_lon\n",
    "init_phi3 = (init_range_lon / init_range_lat)**2  # (range_lon / range_lat)^2\n",
    "init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "# Create Initial Parameters (Float64, Requires Grad)\n",
    "initial_vals = [np.log(init_phi1), np.log(init_phi2), np.log(init_phi3), \n",
    "                np.log(init_phi4), init_advec_lat, init_advec_lon, np.log(init_nugget)]\n",
    "\n",
    "# [4.2042, 1.6348, 0.4721, -3.2695, 0.0218, -0.1689, -1.3984]\n",
    "params_list = [\n",
    "    torch.tensor([val], requires_grad=True, dtype=torch.float64, device=DEVICE)\n",
    "    for val in initial_vals\n",
    "]\n",
    "\n",
    "# --- 2. INSTANTIATE MODEL ---\n",
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'--- Initializing VecchiaBatched Model ---')\n",
    "print(f'{\"=\"*40}')\n",
    "\n",
    "if isinstance(aggregated_data, torch.Tensor):\n",
    "    aggregated_data = aggregated_data.to(DEVICE)\n",
    "\n",
    "# Instantiate\n",
    "model_instance = kernels_reparam_space_time_gpu.fit_vecchia_lbfgs(\n",
    "    smooth=v,\n",
    "    input_map=mm_input_map,\n",
    "    aggregated_data=aggregated_data,\n",
    "    nns_map=nns_map,\n",
    "    mm_cond_number=mm_cond_number,\n",
    "    nheads=nheads\n",
    ")\n",
    "\n",
    "# --- 3. OPTIMIZATION LOOP ---\n",
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'--- Running L-BFGS Optimization ---')\n",
    "print(f'{\"=\"*40}')\n",
    "\n",
    "# Optimizer takes the LIST of scalars\n",
    "optimizer_vecc = model_instance.set_optimizer(\n",
    "            params_list,     \n",
    "            lr=LBFGS_LR,            \n",
    "            max_iter=LBFGS_MAX_EVAL,        \n",
    "            history_size=LBFGS_HISTORY_SIZE \n",
    "        )\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "out, steps_ran = model_instance.fit_vecc_lbfgs(\n",
    "        params_list,\n",
    "        optimizer_vecc,\n",
    "        # covariance_function argument is GONE\n",
    "        max_steps=LBFGS_MAX_STEPS, \n",
    "        grad_tol=1e-7\n",
    "    )\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "epoch_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nOptimization finished in {epoch_time:.2f}s.\")\n",
    "print(f\"Results after {steps_ran} steps: {out}\")\n",
    "print(f\"Final Params: {torch.cat(params_list).detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ad271",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final Interpretable Params: {'sigma_sq': 12.550968451994, \n",
    "                             'range_lon': 0.17829175252165666,\n",
    "                               'range_lat': 0.15087767760409757, \n",
    "                               'range_time': 0.5730659105565772,\n",
    "                                 'advec_lat': 0.06208971176953433,\n",
    "                                   'advec_lon': -0.17790341869704374,\n",
    "                                     'nugget': 0.22901237722387074}\n",
    "\n",
    "Optimization finished in 118.26s.\n",
    "Results after 9 steps: [4.254131841461119, 1.7243340113710215, 0.33390368236413404, -2.335158939275716, 0.06208971176953433, -0.17790341869704374, -1.4739792278759567, 1.2476043286412608]\n",
    "Final Params: [ 4.25413184  1.72433401  0.33390368 -2.33515894  0.06208971 -0.17790342\n",
    " -1.47397923]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c961fa4",
   "metadata": {},
   "source": [
    "# fit dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9b3e3",
   "metadata": {},
   "source": [
    "difference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d43435c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([142832, 4])\n",
      "142832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.8000e-02,  1.2305e+02,  0.0000e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2311e+02,  1.9113e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2317e+02,  3.1846e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2323e+02,  9.3044e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2330e+02,  7.5384e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2336e+02, -8.2040e-01,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2342e+02, -1.2852e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2349e+02, -3.2619e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2355e+02, -6.2629e-01,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2361e+02,  1.3124e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2368e+02,  4.2001e-01,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2374e+02, -4.3216e-01,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2380e+02, -5.5636e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2386e+02, -2.0360e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2393e+02,  4.5638e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2399e+02, -5.2148e-02,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2405e+02, -3.7626e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2412e+02,  3.4245e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2418e+02,  5.3648e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2424e+02,  1.9309e+00,  2.1000e+01]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [11.0474, 0.0623, 0.2445, 1.0972, 0.0101, -0.1671, 1.1825]\n",
    "day = 0 # 0 index\n",
    "lat_range= [0,5]\n",
    "lon_range= [123.0, 133.0]\n",
    "#lat_range= [1,3]\n",
    "#lon_range= [125, 129.0]\n",
    "\n",
    "daily_aggregated_tensors_dw = [aggregated_data]\n",
    "daily_hourly_maps_dw = [input_map]\n",
    "\n",
    "db = debiased_whittle.debiased_whittle_preprocess(daily_aggregated_tensors_dw, daily_hourly_maps_dw, day_idx=day, params_list=a, lat_range=lat_range, lon_range=lon_range)\n",
    "\n",
    "\n",
    "subsetted_aggregated_day = db.generate_spatially_filtered_days(0,5,123,133)\n",
    "print(subsetted_aggregated_day.shape)\n",
    "N2= subsetted_aggregated_day.shape[0]\n",
    "print(N2)\n",
    "subsetted_aggregated_day[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2497b8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Pre-computing J-vector (Hamming taper)...\n",
      "Pre-computing sample periodogram...\n",
      "Pre-computing Hamming taper autocorrelation...\n",
      "Data grid: 113x158, 8 time points. J-vector, Periodogram, Taper Autocorr on cpu.\n",
      "\n",
      "============================== Initialization Run 1/1 ==============================\n",
      "Starting with FIXED params (raw log-scale): [4.2042, 1.6348, 0.4721, -2.5562, 0.0218, -0.1689, -1.3984]\n",
      "Starting optimization run 1 on device cpu (Hamming, 7-param ST kernel, L-BFGS)...\n",
      "--- Step 1/20 ---\n",
      " Loss: 1.906929 | Max Grad: 6.274846e-04\n",
      "  Params (Raw Log): log_phi1: 4.3182, log_phi2: 1.7745, log_phi3: 0.0776, log_phi4: -3.5323, advec_lat: 0.0363, advec_lon: -0.1510, log_nugget: -1.0186\n",
      "--- Step 2/20 ---\n",
      " Loss: 1.817385 | Max Grad: 4.095367e-05\n",
      "  Params (Raw Log): log_phi1: 4.3182, log_phi2: 1.7744, log_phi3: 0.0775, log_phi4: -3.5320, advec_lat: 0.0363, advec_lon: -0.1510, log_nugget: -1.0177\n",
      "--- Step 3/20 ---\n",
      " Loss: 1.817385 | Max Grad: 4.095367e-05\n",
      "  Params (Raw Log): log_phi1: 4.3182, log_phi2: 1.7744, log_phi3: 0.0775, log_phi4: -3.5320, advec_lat: 0.0363, advec_lon: -0.1510, log_nugget: -1.0177\n",
      "--- Step 4/20 ---\n",
      " Loss: 1.817385 | Max Grad: 4.095367e-05\n",
      "  Params (Raw Log): log_phi1: 4.3182, log_phi2: 1.7744, log_phi3: 0.0775, log_phi4: -3.5320, advec_lat: 0.0363, advec_lon: -0.1510, log_nugget: -1.0177\n",
      "\n",
      "--- Converged on loss change (change < 1e-12) at step 4 ---\n",
      "\n",
      "--- Training Complete ---\n",
      "\n",
      "FINAL BEST STATE ACHIEVED (during training):\n",
      "Best Loss: 1.817\n",
      "\n",
      "\n",
      "========================= Overall Result from Run ========================= =========================\n",
      "Best Run Loss: 1.817 (after 4 steps)\n",
      "Final Parameters (Natural Scale): sigmasq: 12.7277, range_lat: 0.1631, range_lon: 0.1696, range_time: 0.9917, advec_lat: 0.0363, advec_lon: -0.1510, nugget: 0.3614\n",
      "Final Parameters (Phi Scale)    : phi1: 75.0512, phi2: 5.8967, phi3: 1.0806, phi4: 0.0292, advec_lat: 0.0363, advec_lon: -0.1510, nugget: 0.3614\n",
      "Final Parameters (Raw Log Scale): log_phi1: 4.3182, log_phi2: 1.7744, log_phi3: 0.0775, log_phi4: -3.5320, advec_lat: 0.0363, advec_lon: -0.1510, log_nugget: -1.0177\n",
      "\n",
      "Total execution time: 40.40 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dwl = debiased_whittle.debiased_whittle_likelihood()\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    DAY_TO_RUN = 3 # data is decided above\n",
    "    TAPERING_FUNC = dwl.cgn_hamming # Use Hamming taper\n",
    "    NUM_RUNS = 1\n",
    "    MAX_STEPS = 20 # L-BFGS usually converges in far fewer steps\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063 \n",
    "\n",
    "    LAT_COL, LON_COL = 0, 1\n",
    "    VAL_COL = 2 # Spatially differenced value\n",
    "    TIME_COL = 3\n",
    "\n",
    "\n",
    "    cur_df =subsetted_aggregated_day\n",
    "    \n",
    "    if cur_df.numel() == 0 or cur_df.shape[1] <= max(LAT_COL, LON_COL, VAL_COL, TIME_COL):\n",
    "        print(f\"Error: Data for Day {DAY_TO_RUN} is empty or invalid.\")\n",
    "        exit()\n",
    "\n",
    "    unique_times = torch.unique(cur_df[:, TIME_COL])\n",
    "    time_slices_list = [cur_df[cur_df[:, TIME_COL] == t_val] for t_val in unique_times]\n",
    "\n",
    "    # --- 1. Pre-compute J-vector, Taper Grid, and Taper Autocorrelation ---\n",
    "    print(\"Pre-computing J-vector (Hamming taper)...\")\n",
    "    \n",
    "    # --- üí• REVISED: Renamed 'p' to 'p_time' üí• ---\n",
    "    J_vec, n1, n2, p_time, taper_grid = dwl.generate_Jvector_tapered( \n",
    "        time_slices_list,\n",
    "        tapering_func=TAPERING_FUNC, \n",
    "        lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if J_vec is None or J_vec.numel() == 0 or n1 == 0 or n2 == 0 or p_time == 0:\n",
    "       print(f\"Error: J-vector generation failed for Day {DAY_TO_RUN}.\")\n",
    "       exit()\n",
    "       \n",
    "    print(\"Pre-computing sample periodogram...\")\n",
    "    I_sample = dwl.calculate_sample_periodogram_vectorized(J_vec)\n",
    "\n",
    "    print(\"Pre-computing Hamming taper autocorrelation...\")\n",
    "    taper_autocorr_grid = dwl.calculate_taper_autocorrelation_fft(taper_grid, n1, n2, DEVICE)\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Error: NaN/Inf in sample periodogram.\")\n",
    "        exit()\n",
    "    if torch.isnan(taper_autocorr_grid).any() or torch.isinf(taper_autocorr_grid).any():\n",
    "        print(\"Error: NaN/Inf in taper autocorrelation.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Data grid: {n1}x{n2}, {p_time} time points. J-vector, Periodogram, Taper Autocorr on {DEVICE}.\")\n",
    "    # --- END REVISION ---\n",
    "\n",
    "    # --- 2. Optimization Loop ---\n",
    "    all_final_results = []\n",
    "    all_final_losses = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        print(f\"\\n{'='*30} Initialization Run {i+1}/{NUM_RUNS} {'='*30}\")\n",
    "\n",
    "        # --- 7-PARAMETER initialization ---\n",
    "        ''' \n",
    "        init_sigmasq   = 15.0\n",
    "        init_range_lat = 0.66 \n",
    "        init_range_lon = 0.7 \n",
    "        init_nugget    = 1.5\n",
    "        init_beta      = 0.1  # Temporal range ratio\n",
    "        init_advec_lat = 0.02\n",
    "        init_advec_lon = -0.08\n",
    "        '''\n",
    "        init_sigmasq   = 13.059\n",
    "        init_range_lat = 0.154 \n",
    "        init_range_lon = 0.195\n",
    "        init_advec_lat = 0.0218\n",
    "        init_range_time = 0.7\n",
    "        init_advec_lon = -0.1689\n",
    "        init_nugget    = 0.247\n",
    "\n",
    "        init_phi2 = 1.0 / init_range_lon\n",
    "        init_phi1 = init_sigmasq * init_phi2\n",
    "        init_phi3 = (init_range_lon / init_range_lat)**2\n",
    "        # Change needed to match the spatial-temporal distance formula:\n",
    "        init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "        initial_params_values = [\n",
    "            np.log(init_phi1),    # [0] log_phi1\n",
    "            np.log(init_phi2),    # [1] log_phi2\n",
    "            np.log(init_phi3),    # [2] log_phi3\n",
    "            np.log(init_phi4),    # [3] log_phi4\n",
    "            init_advec_lat,       # [4] advec_lat (NOT log)\n",
    "            init_advec_lon,       # [5] advec_lon (NOT log)\n",
    "            np.log(init_nugget)   # [6] log_nugget\n",
    "        ]\n",
    "        \n",
    "        print(f\"Starting with FIXED params (raw log-scale): {[round(p, 4) for p in initial_params_values]}\")\n",
    "\n",
    "        params_list = [\n",
    "            Parameter(torch.tensor([val], dtype=torch.float64))\n",
    "            for val in initial_params_values\n",
    "        ]\n",
    "\n",
    "        # Helper to define the boundary globally for clarity\n",
    "        NUGGET_LOWER_BOUND = 0.05\n",
    "        LOG_NUGGET_LOWER_BOUND = np.log(NUGGET_LOWER_BOUND) # Approx -2.9957\n",
    "\n",
    "        # --- üí• REVISED: Use L-BFGS Optimizer üí• ---\n",
    "        optimizer = torch.optim.LBFGS(\n",
    "            params_list,\n",
    "            lr=1.0,           # Initial step length for line search\n",
    "            max_iter=20,      # Iterations per step\n",
    "            history_size=100,\n",
    "            line_search_fn=\"strong_wolfe\", # Often more robust\n",
    "            tolerance_grad=1e-5\n",
    "        )\n",
    "        # --- END REVISION ---\n",
    "\n",
    "        print(f\"Starting optimization run {i+1} on device {DEVICE} (Hamming, 7-param ST kernel, L-BFGS)...\")\n",
    "        \n",
    "        # --- üí• REVISED: Call L-BFGS trainer, pass p_time üí• ---\n",
    "        nat_params_str, phi_params_str, raw_params_str, loss, steps_run = dwl.run_lbfgs_tapered(\n",
    "            params_list=params_list,\n",
    "            optimizer=optimizer,\n",
    "            I_sample=I_sample,\n",
    "            n1=n1, n2=n2, p_time=p_time,\n",
    "            taper_autocorr_grid=taper_autocorr_grid, \n",
    "            max_steps=MAX_STEPS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        # --- END REVISION ---\n",
    "        \n",
    "        if loss is not None:\n",
    "            all_final_results.append((nat_params_str, phi_params_str, raw_params_str))\n",
    "            all_final_losses.append(loss)\n",
    "        else:\n",
    "            all_final_losses.append(float('inf'))\n",
    "\n",
    "    print(f\"\\n\\n{'='*25} Overall Result from Run {'='*25} {'='*25}\")\n",
    "    \n",
    "    valid_losses = [l for l in all_final_losses if l is not None and l != float('inf')]\n",
    "\n",
    "    if not valid_losses:\n",
    "        print(f\"The run failed or resulted in an invalid loss for Day {DAY_TO_RUN}.\")\n",
    "    else:\n",
    "        best_loss = min(valid_losses)\n",
    "        best_run_index = all_final_losses.index(best_loss)\n",
    "        best_results = all_final_results[best_run_index]\n",
    "        \n",
    "        print(f\"Best Run Loss: {best_loss} (after {steps_run} steps)\")\n",
    "        print(f\"Final Parameters (Natural Scale): {best_results[0]}\")\n",
    "        print(f\"Final Parameters (Phi Scale)    : {best_results[1]}\")\n",
    "        print(f\"Final Parameters (Raw Log Scale): {best_results[2]}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61060411",
   "metadata": {},
   "source": [
    "init_sigmasq   = 13.059\n",
    "init_range_lon = 0.195 \n",
    "init_range_lat = 0.154 \n",
    "init_advec_lat = 0.0418\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "1 st simulation (1 vs 3)\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 12.939260896610579, 'range_lon': 0.17242543281507933, 'range_lat': 0.1631231346200311, 'range_time': 1.1358209227858689, 'advec_lat': 0.045119029109988974, 'advec_lon': -0.17809677784942035, 'nugget': 0.3009582276680578}\n",
    "\n",
    "Final Parameters (Natural Scale): sigmasq: 13.2415, range_lat: 0.1685, range_lon: 0.1739, range_time: 0.9681, advec_lat: 0.0395, advec_lon: -0.1732, nugget: 0.3216\n",
    "\n",
    "2nd simulation (1 vs 3)\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 12.765293287952144, 'range_lon': 0.17039368470743024, 'range_lat': 0.16152132799710625, 'range_time': 1.081124889959751, 'advec_lat': 0.04635511983762563, 'advec_lon': -0.1775715292039452, 'nugget': 0.31503884896742074}\n",
    "\n",
    "\n",
    "sigmasq: 13.2415, range_lat: 0.1685, range_lon: 0.1739, range_time: 0.9681, advec_lat: 0.0395, advec_lon: -0.1732, nugget: 0.3216\n",
    "\n",
    "3nd simulation (mm 15 two times)\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 12.292570063933866, 'range_lon': 0.16170186578939172, 'range_lat': 0.1537053192245325, 'range_time': 0.9671689003322674, 'advec_lat': 0.04155171168950867, 'advec_lon': -0.16218073366456207, 'nugget': 0.29945872751676345}\n",
    "\n",
    "Final Parameters (Natural Scale): sigmasq: 12.7277, range_lat: 0.1631, range_lon: 0.1696, range_time: 0.9917, advec_lat: 0.0363, advec_lon: -0.1510, nugget: 0.3614\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
