{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58d09e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed set to: 42\n",
      "Simulating on: cpu \n",
      "1. Generating True Field...\n",
      "Exact Grid Size: 114 (Lat) x 159 (Lon) x 8 (Time) = 145008 points\n",
      "2. Formatting Output...\n",
      "\n",
      "Done.\n",
      "Aggregated Tensor Shape: torch.Size([145008, 4])\n",
      "Device: cpu\n",
      "Dtype: torch.float64\n",
      "Sample Output (Lat Desc, Lon Desc):\n",
      "tensor([[4.9720e+00, 1.3295e+02, 2.6223e+02, 2.1000e+01],\n",
      "        [4.9720e+00, 1.3289e+02, 2.6288e+02, 2.1000e+01],\n",
      "        [4.9720e+00, 1.3283e+02, 2.6242e+02, 2.1000e+01],\n",
      "        [4.9720e+00, 1.3276e+02, 2.6312e+02, 2.1000e+01],\n",
      "        [4.9720e+00, 1.3270e+02, 2.6185e+02, 2.1000e+01],\n",
      "        [4.9720e+00, 1.3264e+02, 2.6098e+02, 2.1000e+01]], dtype=torch.float64)\n",
      "\n",
      "Gradient Check: False (Should be False)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.fft\n",
    "import numpy as np\n",
    "import random  # Added for completeness\n",
    "\n",
    "# --- 0. SET RANDOM SEED (FIXED) ---\n",
    "SEED = 42\n",
    "# You must include these lines for the seed to take effect:\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"Random Seed set to: {SEED}\")\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# Check for Mac GPU (MPS) first, then CUDA, then CPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE= torch.float32 if DEVICE.type == 'mps' else torch.float64\n",
    "\n",
    "\n",
    "print(f\"Simulating on: {DEVICE} \")\n",
    "\n",
    "# TRUE PARAMETERS\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lon = 0.195 \n",
    "init_range_lat = 0.154 \n",
    "init_advec_lat = 0.0218\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "# Map parameters\n",
    "init_phi2 = 1.0 / init_range_lon\n",
    "init_phi1 = init_sigmasq * init_phi2\n",
    "init_phi3 = (init_range_lon / init_range_lat)**2\n",
    "init_phi4 = (init_range_lon / init_range_time)**2\n",
    "\n",
    "# Create Initial Parameters\n",
    "initial_vals = [np.log(init_phi1), np.log(init_phi2), np.log(init_phi3), \n",
    "                np.log(init_phi4), init_advec_lat, init_advec_lon, np.log(init_nugget)]\n",
    "\n",
    "params_list = [\n",
    "    # Changed: uses dynamic DTYPE\n",
    "    torch.tensor([val], requires_grad=True, dtype=torch.float64, device=DEVICE)\n",
    "    for val in initial_vals\n",
    "]\n",
    "\n",
    "# Mean Ozone\n",
    "OZONE_MEAN = 260.0\n",
    "\n",
    "# --- 2. EXACT COVARIANCE ---\n",
    "def get_model_covariance_on_grid(lags_x, lags_y, lags_t, params):\n",
    "    phi1, phi2, phi3, phi4 = torch.exp(params[0]), torch.exp(params[1]), torch.exp(params[2]), torch.exp(params[3])\n",
    "    advec_lat, advec_lon = params[4], params[5]\n",
    "    sigmasq = phi1 / phi2\n",
    "\n",
    "    u_lat_eff = lags_x - advec_lat * lags_t\n",
    "    u_lon_eff = lags_y - advec_lon * lags_t\n",
    "    \n",
    "    dist_sq = (u_lat_eff.pow(2) * phi3) + (u_lon_eff.pow(2)) + (lags_t.pow(2) * phi4)\n",
    "    distance = torch.sqrt(dist_sq + 1e-8)\n",
    "    \n",
    "    return sigmasq * torch.exp(-distance * phi2)\n",
    "\n",
    "# --- 3. FFT SIMULATION ---\n",
    "def generate_exact_gems_field(lat_coords, lon_coords, t_steps, params):\n",
    "    Nx = len(lat_coords)\n",
    "    Ny = len(lon_coords)\n",
    "    Nt = t_steps\n",
    "    \n",
    "    print(f\"Exact Grid Size: {Nx} (Lat) x {Ny} (Lon) x {Nt} (Time) = {Nx*Ny*Nt} points\")\n",
    "    \n",
    "    # 1. Calculate Steps\n",
    "    dlat = float(lat_coords[1] - lat_coords[0])\n",
    "    dlon = float(lon_coords[1] - lon_coords[0])\n",
    "    dt = 1.0 \n",
    "    \n",
    "    # 2. Padding (2x for non-circular simulation)\n",
    "    Px, Py, Pt = 2*Nx, 2*Ny, 2*Nt\n",
    "    \n",
    "    # 3. Lags Construction\n",
    "    # Changed: uses dynamic DTYPE\n",
    "    Lx_len = Px * dlat   \n",
    "    lags_x = torch.arange(Px, device=DEVICE, dtype=DTYPE) * dlat\n",
    "    lags_x[Px//2:] -= Lx_len \n",
    "    \n",
    "    Ly_len = Py * dlon   \n",
    "    lags_y = torch.arange(Py, device=DEVICE, dtype=DTYPE) * dlon\n",
    "    lags_y[Py//2:] -= Ly_len\n",
    "\n",
    "    Lt_len = Pt * dt     \n",
    "    lags_t = torch.arange(Pt, device=DEVICE, dtype=DTYPE) * dt\n",
    "    lags_t[Pt//2:] -= Lt_len\n",
    "\n",
    "    # Meshgrid & Covariance\n",
    "    L_x, L_y, L_t = torch.meshgrid(lags_x, lags_y, lags_t, indexing='ij')\n",
    "    C_vals = get_model_covariance_on_grid(L_x, L_y, L_t, params)\n",
    "\n",
    "    # FFT & Convolution\n",
    "    S = torch.fft.fftn(C_vals)\n",
    "    S.real = torch.clamp(S.real, min=0)\n",
    "\n",
    "    # Changed: uses dynamic DTYPE\n",
    "    random_phase = torch.fft.fftn(torch.randn(Px, Py, Pt, device=DEVICE, dtype=DTYPE))\n",
    "    weighted_freq = torch.sqrt(S.real) * random_phase\n",
    "    field_sim = torch.fft.ifftn(weighted_freq).real\n",
    "    \n",
    "    return field_sim[:Nx, :Ny, :Nt]\n",
    "\n",
    "# --- 4. EXECUTION ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Grid: Lat (0 to 5), Lon (123 to 133)\n",
    "    # Changed: uses dynamic DTYPE\n",
    "    lats_sim = torch.arange(0, 5.0 + 0.001, 0.044, device=DEVICE, dtype=DTYPE)\n",
    "    lons_sim = torch.arange(123.0, 133.0 + 0.001, 0.063, device=DEVICE, dtype=DTYPE)\n",
    "    t_def = 8\n",
    "    \n",
    "    print(\"1. Generating True Field...\")\n",
    "    sim_field = generate_exact_gems_field(lats_sim, lons_sim, t_def, params_list)\n",
    "    \n",
    "    print(\"2. Formatting Output...\")\n",
    "    \n",
    "    input_map = {}\n",
    "    aggregated_list = [] \n",
    "    \n",
    "    nugget_std = torch.sqrt(torch.exp(params_list[6]))\n",
    "    \n",
    "    # Flip to Descending Order\n",
    "    lats_flip = torch.flip(lats_sim, dims=[0])\n",
    "    lons_flip = torch.flip(lons_sim, dims=[0])\n",
    "    \n",
    "    grid_lat, grid_lon = torch.meshgrid(lats_flip, lons_flip, indexing='ij')\n",
    "    flat_lats = grid_lat.flatten()\n",
    "    flat_lons = grid_lon.flatten()\n",
    "    \n",
    "    for t in range(t_def):\n",
    "        # Flip field to match coordinates\n",
    "        field_t = sim_field[:, :, t] \n",
    "        field_t_flipped = torch.flip(field_t, dims=[0, 1]) \n",
    "        \n",
    "        flat_vals = field_t_flipped.flatten()\n",
    "        \n",
    "        # Add Noise + Mean\n",
    "        obs_vals = flat_vals + (torch.randn_like(flat_vals) * nugget_std) + OZONE_MEAN\n",
    "        \n",
    "        time_val = 21.0 + t\n",
    "        flat_times = torch.full_like(flat_lats, time_val)\n",
    "        \n",
    "        row_tensor = torch.stack([flat_lats, flat_lons, obs_vals, flat_times], dim=1)\n",
    "        \n",
    "        # Changed: REMOVED .cpu() call. Keeps data on Mac GPU (mps)\n",
    "        clean_tensor = row_tensor.detach()\n",
    "        \n",
    "        key_str = f'2024_07_y24m07day01_hm{t:02d}:53'\n",
    "        input_map[key_str] = clean_tensor\n",
    "        aggregated_list.append(clean_tensor)\n",
    "\n",
    "    aggregated_data = torch.cat(aggregated_list, dim=0)\n",
    "\n",
    "    print(f\"\\nDone.\")\n",
    "    print(f\"Aggregated Tensor Shape: {aggregated_data.shape}\")\n",
    "    print(f\"Device: {aggregated_data.device}\")\n",
    "    print(f\"Dtype: {aggregated_data.dtype}\")\n",
    "    \n",
    "    torch.set_printoptions(precision=4, sci_mode=True)\n",
    "    print(\"Sample Output (Lat Desc, Lon Desc):\")\n",
    "    print(aggregated_data[:6])\n",
    "    \n",
    "    print(f\"\\nGradient Check: {aggregated_data.requires_grad} (Should be False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cacde8",
   "metadata": {},
   "source": [
    "set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e359cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "import os\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy                    # clone tensor\n",
    "import time\n",
    "\n",
    "# Custom imports\n",
    "\n",
    "\n",
    "from GEMS_TCO import kernels_reparam_space_time_gpu as kernels_reparam_space_time_gpu\n",
    "from GEMS_TCO import kernels_reparam_space_time_cpu_010126 as kernels_reparam_space_time_gpu0101\n",
    "\n",
    "from GEMS_TCO import kernels_columns as kernels_columns\n",
    "# from GEMS_TCO import kernels_gpu_past1  as kernels_gpu_past1\n",
    "\n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import alg_optimization, BaseLogger\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "import typer\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "from GEMS_TCO import configuration as config\n",
    "from GEMS_TCO.data_loader import load_data2, exact_location_filter\n",
    "from GEMS_TCO import debiased_whittle\n",
    "from torch.nn import Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21c10d6",
   "metadata": {},
   "source": [
    "# col cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5185324a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Generating True Field...\n",
      "Exact Grid Size: 114 (Lat) x 159 (Lon) x 8 (Time) = 145008 points\n",
      "2. Formatting Output...\n",
      "\n",
      "Done.\n",
      "Aggregated Tensor Shape: torch.Size([145008, 4])\n",
      "Device: cpu\n",
      "Dtype: torch.float64\n",
      "Sample Output (Lon Desc, Lat Desc [North-East start, moving South]):\n",
      "tensor([[4.9720e+00, 1.3295e+02, 2.6225e+02, 2.1000e+01],\n",
      "        [4.9280e+00, 1.3295e+02, 2.5999e+02, 2.1000e+01],\n",
      "        [4.8840e+00, 1.3295e+02, 2.6110e+02, 2.1000e+01],\n",
      "        [4.8400e+00, 1.3295e+02, 2.6034e+02, 2.1000e+01],\n",
      "        [4.7960e+00, 1.3295e+02, 2.5986e+02, 2.1000e+01],\n",
      "        [4.7520e+00, 1.3295e+02, 2.5720e+02, 2.1000e+01],\n",
      "        [4.7080e+00, 1.3295e+02, 2.6108e+02, 2.1000e+01],\n",
      "        [4.6640e+00, 1.3295e+02, 2.6078e+02, 2.1000e+01],\n",
      "        [4.6200e+00, 1.3295e+02, 2.5832e+02, 2.1000e+01],\n",
      "        [4.5760e+00, 1.3295e+02, 2.5935e+02, 2.1000e+01]], dtype=torch.float64)\n",
      "\n",
      "Gradient Check: False (Should be False)\n"
     ]
    }
   ],
   "source": [
    "# --- 4. EXECUTION ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Grid: Lat (0 to 5), Lon (123 to 133)\n",
    "    # Changed: uses dynamic DTYPE\n",
    "    lats_sim = torch.arange(0, 5.0 + 0.001, 0.044, device=DEVICE, dtype=DTYPE)\n",
    "    lons_sim = torch.arange(123.0, 133.0 + 0.001, 0.063, device=DEVICE, dtype=DTYPE)\n",
    "    t_def = 8\n",
    "    \n",
    "    print(\"1. Generating True Field...\")\n",
    "    sim_field = generate_exact_gems_field(lats_sim, lons_sim, t_def, params_list)\n",
    "    \n",
    "    print(\"2. Formatting Output...\")\n",
    "    \n",
    "    input_map = {}\n",
    "    aggregated_list = [] \n",
    "    \n",
    "    nugget_std = torch.sqrt(torch.exp(params_list[6]))\n",
    "    \n",
    "    # Flip to Descending Order (North->South, East->West)\n",
    "    lats_flip = torch.flip(lats_sim, dims=[0])\n",
    "    lons_flip = torch.flip(lons_sim, dims=[0])\n",
    "    \n",
    "    # --- MODIFIED SECTION: REORDERING ---\n",
    "    # We want: Outer Loop = Longitude (East to West), Inner Loop = Latitude (North to South)\n",
    "    # So we create the meshgrid with Lon as the first dimension (Slow) and Lat as second (Fast)\n",
    "    grid_lon, grid_lat = torch.meshgrid(lons_flip, lats_flip, indexing='ij')\n",
    "    \n",
    "    # Flattening row-major means we iterate the last dimension (Lat) fastest\n",
    "    flat_lats = grid_lat.flatten()\n",
    "    flat_lons = grid_lon.flatten()\n",
    "    \n",
    "    for t in range(t_def):\n",
    "        # field_t is shape [Lat, Lon] (Ascending)\n",
    "        field_t = sim_field[:, :, t] \n",
    "        \n",
    "        # 1. Flip to [Lat Descending, Lon Descending]\n",
    "        field_t_flipped = torch.flip(field_t, dims=[0, 1]) \n",
    "        \n",
    "        # 2. Transpose to [Lon Descending, Lat Descending]\n",
    "        # This ensures that when we flatten, we scan down the Lats for a fixed Lon\n",
    "        field_ordered = field_t_flipped.permute(1, 0)\n",
    "        \n",
    "        flat_vals = field_ordered.flatten()\n",
    "        \n",
    "        # Add Noise + Mean\n",
    "        obs_vals = flat_vals + (torch.randn_like(flat_vals) * nugget_std) + OZONE_MEAN\n",
    "        \n",
    "        time_val = 21.0 + t\n",
    "        flat_times = torch.full_like(flat_lats, time_val)\n",
    "        \n",
    "        row_tensor = torch.stack([flat_lats, flat_lons, obs_vals, flat_times], dim=1)\n",
    "        \n",
    "        clean_tensor = row_tensor.detach()\n",
    "        \n",
    "        key_str = f'2024_07_y24m07day01_hm{t:02d}:53'\n",
    "        input_map[key_str] = clean_tensor\n",
    "        aggregated_list.append(clean_tensor)\n",
    "\n",
    "    aggregated_data = torch.cat(aggregated_list, dim=0)\n",
    "\n",
    "    print(f\"\\nDone.\")\n",
    "    print(f\"Aggregated Tensor Shape: {aggregated_data.shape}\")\n",
    "    print(f\"Device: {aggregated_data.device}\")\n",
    "    print(f\"Dtype: {aggregated_data.dtype}\")\n",
    "    \n",
    "    torch.set_printoptions(precision=4, sci_mode=True)\n",
    "    print(\"Sample Output (Lon Desc, Lat Desc [North-East start, moving South]):\")\n",
    "    print(aggregated_data[:10])\n",
    "    \n",
    "    print(f\"\\nGradient Check: {aggregated_data.requires_grad} (Should be False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4a5695c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "========================================\n",
      "--- Initializing VecchiaStructuredGrid (114x159x8) ---\n",
      "========================================\n",
      "\n",
      "========================================\n",
      "--- Running L-BFGS Optimization ---\n",
      "========================================\n",
      "Pre-computing with Dynamic Stencil Grouping (User's Logic)...\n",
      "Precompute Done. Unique Geometric Patterns: 13\n",
      "  Batch 0: 1092 points (Stencil Size: 31)\n",
      "  Batch 1: 1092 points (Stencil Size: 39)\n",
      "  Batch 2: 1092 points (Stencil Size: 47)\n",
      "  Batch 3: 1092 points (Stencil Size: 55)\n",
      "  Batch 4: 1092 points (Stencil Size: 63)\n",
      "  Batch 5: 1092 points (Stencil Size: 65)\n",
      "  Batch 6: 1092 points (Stencil Size: 67)\n",
      "  Batch 7: 1092 points (Stencil Size: 69)\n",
      "  Batch 8: 111384 points (Stencil Size: 71)\n",
      "  Batch 9: 1092 points (Stencil Size: 65)\n",
      "  Batch 10: 1092 points (Stencil Size: 59)\n",
      "  Batch 11: 1092 points (Stencil Size: 53)\n",
      "  Batch 12: 1092 points (Stencil Size: 47)\n",
      "--- Starting Optimization ---\n",
      "Step 1 | Loss: 1.244964 | Max Grad: 8.42e-04\n",
      "Step 2 | Loss: 1.242101 | Max Grad: 1.10e-06\n",
      "\n",
      "Optimization finished in 252.21s.\n",
      "Final Raw Params: [4.202167852554384, 1.303684884749464, 0.46816360570388366, -3.2794795317960395, 0.02495976342844325, -0.19532225997925434, -15.457682788107618]\n",
      "------------------------------\n",
      "FINAL PARAMETERS (Original Scale)\n",
      "------------------------------\n",
      "Sigma Squared (Variance): 18.14660\n",
      "Range Latitude          : 0.21486\n",
      "Range Longitude         : 0.27153\n",
      "Range Time              : 1.39942\n",
      "Advection Latitude      : 0.02496\n",
      "Advection Longitude     : -0.19532\n",
      "Nugget                  : 0.00000\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "v = 0.5                  # Smoothness\n",
    "mm_cond_number = 14      # Neighbors (Not strictly used in Structured, but kept for interface)\n",
    "lr = 1.0                 # LBFGS learning rate\n",
    "LBFGS_MAX_STEPS = 50     # Steps in fit loop\n",
    "LBFGS_HISTORY_SIZE = 100\n",
    "LBFGS_LR = 1.0\n",
    "LBFGS_MAX_EVAL = 20      # Max evaluations per step (line search)\n",
    "\n",
    "# Specific Grid Dimensions\n",
    "N_LAT = 114\n",
    "N_LON = 159\n",
    "N_TIME = 8\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- 1. SETUP PARAMETERS (List of Scalars) ---\n",
    "# Truth: [4.18, 1.94, 0.24, -3.97, 0.014, -0.20, -0.85]\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lat = 0.154 \n",
    "init_range_lon = 0.195\n",
    "init_advec_lat = 0.0218\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "# Map model parameters to the 'phi' reparameterization\n",
    "init_phi2 = 1.0 / init_range_lon                # 1/range_lon\n",
    "init_phi1 = init_sigmasq * init_phi2            # sigmasq / range_lon\n",
    "init_phi3 = (init_range_lon / init_range_lat)**2  # (range_lon / range_lat)^2\n",
    "init_phi4 = (init_range_lon / init_range_time)**2 # (range_lon / range_time)^2\n",
    "\n",
    "# Create Initial Parameters\n",
    "# CRITICAL: Use float64 because the internal Cholesky ops are float64\n",
    "initial_vals = [np.log(init_phi1), np.log(init_phi2), np.log(init_phi3), \n",
    "                np.log(init_phi4), init_advec_lat, init_advec_lon, np.log(init_nugget)]\n",
    "\n",
    "params_list = [\n",
    "    torch.tensor([val], requires_grad=True, dtype=torch.float64, device=DEVICE)\n",
    "    for val in initial_vals\n",
    "]\n",
    "\n",
    "# --- 2. INSTANTIATE MODEL ---\n",
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'--- Initializing VecchiaStructuredGrid (114x159x8) ---')\n",
    "print(f'{\"=\"*40}')\n",
    "\n",
    "# Ensure data is on device\n",
    "if isinstance(aggregated_data, torch.Tensor):\n",
    "    aggregated_data = aggregated_data.to(DEVICE)\n",
    "\n",
    "# Instantiate the specialized class\n",
    "model_instance = kernels_columns.VecchiaStructuredGrid(\n",
    "    smooth=v,\n",
    "    input_map=input_map,\n",
    "    aggregated_data=aggregated_data,\n",
    "    nns_map=nns_map,           # Passed but likely ignored by internal stencils\n",
    "    mm_cond_number=mm_cond_number,\n",
    "    n_lat=N_LAT,\n",
    "    n_lon=N_LON,\n",
    "    n_time=N_TIME\n",
    ")\n",
    "\n",
    "# --- 3. OPTIMIZATION LOOP ---\n",
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'--- Running L-BFGS Optimization ---')\n",
    "print(f'{\"=\"*40}')\n",
    "\n",
    "# Setup standard PyTorch LBFGS\n",
    "optimizer_vecc = optim.LBFGS(\n",
    "    params_list, \n",
    "    lr=LBFGS_LR, \n",
    "    max_iter=LBFGS_MAX_EVAL, \n",
    "    history_size=LBFGS_HISTORY_SIZE,\n",
    "    line_search_fn=\"strong_wolfe\" # Optional: helps convergence\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the fit\n",
    "final_params_list = model_instance.fit_vecc_lbfgs_structured(\n",
    "    params_list,\n",
    "    optimizer_vecc,\n",
    "    max_steps=LBFGS_MAX_STEPS\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "epoch_time = end_time - start_time\n",
    "\n",
    "# Formatting output for display\n",
    "final_vals = [p for p in final_params_list]\n",
    "\n",
    "print(f\"\\nOptimization finished in {epoch_time:.2f}s.\")\n",
    "print(f\"Final Raw Params: {final_vals}\")\n",
    "\n",
    "# Assuming final_vals is a list of floats (e.g., [p.item() for p in params_list])\n",
    "\n",
    "# 1. Extract Raw Optimized Values\n",
    "log_phi1   = final_vals[0]\n",
    "log_phi2   = final_vals[1]\n",
    "log_phi3   = final_vals[2]\n",
    "log_phi4   = final_vals[3]\n",
    "advec_lat  = final_vals[4] # No log\n",
    "advec_lon  = final_vals[5] # No log\n",
    "log_nugget = final_vals[6]\n",
    "\n",
    "# 2. Exponentiate to get Phi values\n",
    "phi1 = np.exp(log_phi1)\n",
    "phi2 = np.exp(log_phi2)\n",
    "phi3 = np.exp(log_phi3)\n",
    "phi4 = np.exp(log_phi4)\n",
    "nugget = np.exp(log_nugget)\n",
    "\n",
    "# 3. Convert back to Physical Interpretations\n",
    "# Relations:\n",
    "# phi2 = 1 / range_lon\n",
    "# phi1 = sigmasq / range_lon\n",
    "# phi3 = (range_lon / range_lat)^2\n",
    "# phi4 = (range_lon / range_time)^2\n",
    "\n",
    "range_lon = 1.0 / phi2\n",
    "sigmasq   = phi1 / phi2\n",
    "range_lat = range_lon / np.sqrt(phi3)\n",
    "range_time = range_lon / np.sqrt(phi4)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"FINAL PARAMETERS (Original Scale)\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Sigma Squared (Variance): {sigmasq:.5f}\")\n",
    "print(f\"Range Latitude          : {range_lat:.5f}\")\n",
    "print(f\"Range Longitude         : {range_lon:.5f}\")\n",
    "print(f\"Range Time              : {range_time:.5f}\")\n",
    "print(f\"Advection Latitude      : {advec_lat:.5f}\")\n",
    "print(f\"Advection Longitude     : {advec_lon:.5f}\")\n",
    "print(f\"Nugget                  : {nugget:.5f}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d17240",
   "metadata": {},
   "source": [
    "위3 오른쪽 3개씩 9개하면 시간이 1.5분으로 빠른건 맞아 \n",
    "\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lat = 0.154 \n",
    "init_range_lon = 0.195\n",
    "init_advec_lat = 0.0218\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "2분 40초 위 3점 오른쪽 스파스하게 현재 위치랑 상관없이 9곱하기 3점\n",
    "Sigma Squared (Variance): 15.47293\n",
    "Range Latitude          : 0.18870\n",
    "Range Longitude         : 0.22885\n",
    "Range Time              : 1.16144\n",
    "Advection Latitude      : 0.04304\n",
    "Advection Longitude     : -0.16480\n",
    "Nugget                  : 0.00000\n",
    "\n",
    "같은 줄 위쪽 연속 8개 점, 오른쪽 3줄에 관하여 위 아래 8줄씩 ( 맨 위 맨 아래 줄은 조정 따로)\n",
    "\n",
    "FINAL PARAMETERS (Original Scale)\n",
    "------------------------------\n",
    "Sigma Squared (Variance): 18.14660\n",
    "Range Latitude          : 0.21486\n",
    "Range Longitude         : 0.27153\n",
    "Range Time              : 1.39942\n",
    "Advection Latitude      : 0.02496\n",
    "Advection Longitude     : -0.19532\n",
    "Nugget                  : 0.0\n",
    "------------------------------\n",
    "\n",
    "12*2 = 144   144*140,000   200,0000\n",
    "66*2 = 4,356    4,356*140,000   6 억 (30배)\n",
    "\n",
    "맥민 0, 8 \n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 12.46549405440879, 'range_lon': 0.17716284188340664, 'range_lat': 0.13945338260837867, 'range_time': 1.077153731375784, 'advec_lat': 0.019911903466863015, 'advec_lon': -0.18039218823051228, 'nugget': 0.132943144211764}\n",
    "\n",
    "300,8\n",
    "Final Interpretable Params: {'sigma_sq': 13.282223939681629, 'range_lon': 0.1929438094090938, 'range_lat': 0.15027103361797597, 'range_time': 1.1917215940900596, 'advec_lat': 0.021449218228711178, 'advec_lon': -0.18165359024450423, 'nugget': 0.152859926964583}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0890ad",
   "metadata": {},
   "source": [
    "maxmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0816f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GEMS_TCO import orderings as _orderings\n",
    "\n",
    "def get_spatial_ordering(\n",
    "        \n",
    "        input_maps: torch.Tensor,\n",
    "        mm_cond_number: int = 10\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \n",
    "        key_list = list(input_maps.keys())\n",
    "        data_for_coord = input_maps[key_list[0]]\n",
    "        \n",
    "        # --- FIX START ---\n",
    "        # Check if input is Tensor, if so convert to Numpy for KDTree processing\n",
    "        if isinstance(data_for_coord, torch.Tensor):\n",
    "            data_for_coord = data_for_coord.cpu().numpy()\n",
    "        # --- FIX END ---\n",
    "\n",
    "        x1 = data_for_coord[:, 0]\n",
    "        y1 = data_for_coord[:, 1]\n",
    "        \n",
    "        # Now this works because x1, y1 are numpy arrays\n",
    "        coords1 = np.stack((x1, y1), axis=-1)\n",
    "\n",
    "        # Calculate MaxMin ordering\n",
    "        ord_mm = _orderings.maxmin_cpp(coords1)\n",
    "        \n",
    "        # Reorder coordinates to find nearest neighbors\n",
    "        data_for_coord_reordered = data_for_coord[ord_mm]\n",
    "        coords1_reordered = np.stack(\n",
    "            (data_for_coord_reordered[:, 0], data_for_coord_reordered[:, 1]), \n",
    "            axis=-1\n",
    "        )\n",
    "        \n",
    "        # Calculate nearest neighbors map\n",
    "        nns_map = _orderings.find_nns_l2(locs=coords1_reordered, max_nn=mm_cond_number)\n",
    "        \n",
    "        return ord_mm, nns_map\n",
    "\n",
    "ord_mm, nns_map = get_spatial_ordering(input_map, mm_cond_number=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa3990cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_input_map = {}\n",
    "for key in input_map:\n",
    "    mm_input_map[key] = input_map[key][ord_mm]  # Extract only Lat and Lon columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7191bead",
   "metadata": {},
   "source": [
    "# Fit vecchia max min time 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e322400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "========================================\n",
      "--- Initializing VecchiaBatched Model ---\n",
      "========================================\n",
      "\n",
      "========================================\n",
      "--- Running L-BFGS Optimization ---\n",
      "========================================\n",
      "Pre-computing for CPU (NumPy Native)... Done. CPU Tensors Ready. Size: torch.Size([142608, 27, 3])\n",
      "--- Starting Batched L-BFGS Optimization (GPU) ---\n",
      "--- Step 1/10 / Loss: 1.245602 ---\n",
      "  Param 0: Value=4.2234, Grad=6.041967873603196e-06\n",
      "  Param 1: Value=1.6463, Grad=-9.515232797853795e-07\n",
      "  Param 2: Value=0.4774, Grad=1.3540113323312688e-06\n",
      "  Param 3: Value=-3.5684, Grad=2.9460067162777434e-07\n",
      "  Param 4: Value=0.0048, Grad=1.2825714388042808e-06\n",
      "  Param 5: Value=-0.1847, Grad=-1.799312979371831e-06\n",
      "  Param 6: Value=-1.6297, Grad=3.75465157118855e-07\n",
      "  Max Abs Grad: 6.041968e-06\n",
      "------------------------------\n",
      "--- Step 2/10 / Loss: 1.245438 ---\n",
      "  Param 0: Value=4.2234, Grad=6.041967873603196e-06\n",
      "  Param 1: Value=1.6463, Grad=-9.515232797853795e-07\n",
      "  Param 2: Value=0.4774, Grad=1.3540113323312688e-06\n",
      "  Param 3: Value=-3.5684, Grad=2.9460067162777434e-07\n",
      "  Param 4: Value=0.0048, Grad=1.2825714388042808e-06\n",
      "  Param 5: Value=-0.1847, Grad=-1.799312979371831e-06\n",
      "  Param 6: Value=-1.6297, Grad=3.75465157118855e-07\n",
      "  Max Abs Grad: 6.041968e-06\n",
      "------------------------------\n",
      "--- Step 3/10 / Loss: 1.245438 ---\n",
      "  Param 0: Value=4.2234, Grad=6.041967873603196e-06\n",
      "  Param 1: Value=1.6463, Grad=-9.515232797853795e-07\n",
      "  Param 2: Value=0.4774, Grad=1.3540113323312688e-06\n",
      "  Param 3: Value=-3.5684, Grad=2.9460067162777434e-07\n",
      "  Param 4: Value=0.0048, Grad=1.2825714388042808e-06\n",
      "  Param 5: Value=-0.1847, Grad=-1.799312979371831e-06\n",
      "  Param 6: Value=-1.6297, Grad=3.75465157118855e-07\n",
      "  Max Abs Grad: 6.041968e-06\n",
      "------------------------------\n",
      "--- Step 4/10 / Loss: 1.245438 ---\n",
      "  Param 0: Value=4.2234, Grad=6.041967873603196e-06\n",
      "  Param 1: Value=1.6463, Grad=-9.515232797853795e-07\n",
      "  Param 2: Value=0.4774, Grad=1.3540113323312688e-06\n",
      "  Param 3: Value=-3.5684, Grad=2.9460067162777434e-07\n",
      "  Param 4: Value=0.0048, Grad=1.2825714388042808e-06\n",
      "  Param 5: Value=-0.1847, Grad=-1.799312979371831e-06\n",
      "  Param 6: Value=-1.6297, Grad=3.75465157118855e-07\n",
      "  Max Abs Grad: 6.041968e-06\n",
      "------------------------------\n",
      "--- Step 5/10 / Loss: 1.245438 ---\n",
      "  Param 0: Value=4.2234, Grad=6.041967873603196e-06\n",
      "  Param 1: Value=1.6463, Grad=-9.515232797853795e-07\n",
      "  Param 2: Value=0.4774, Grad=1.3540113323312688e-06\n",
      "  Param 3: Value=-3.5684, Grad=2.9460067162777434e-07\n",
      "  Param 4: Value=0.0048, Grad=1.2825714388042808e-06\n",
      "  Param 5: Value=-0.1847, Grad=-1.799312979371831e-06\n",
      "  Param 6: Value=-1.6297, Grad=3.75465157118855e-07\n",
      "  Max Abs Grad: 6.041968e-06\n",
      "------------------------------\n",
      "--- Step 6/10 / Loss: 1.245438 ---\n",
      "  Param 0: Value=4.2234, Grad=6.041967873603196e-06\n",
      "  Param 1: Value=1.6463, Grad=-9.515232797853795e-07\n",
      "  Param 2: Value=0.4774, Grad=1.3540113323312688e-06\n",
      "  Param 3: Value=-3.5684, Grad=2.9460067162777434e-07\n",
      "  Param 4: Value=0.0048, Grad=1.2825714388042808e-06\n",
      "  Param 5: Value=-0.1847, Grad=-1.799312979371831e-06\n",
      "  Param 6: Value=-1.6297, Grad=3.75465157118855e-07\n",
      "  Max Abs Grad: 6.041968e-06\n",
      "------------------------------\n",
      "--- Step 7/10 / Loss: 1.245438 ---\n",
      "  Param 0: Value=4.2234, Grad=6.041967873603196e-06\n",
      "  Param 1: Value=1.6463, Grad=-9.515232797853795e-07\n",
      "  Param 2: Value=0.4774, Grad=1.3540113323312688e-06\n",
      "  Param 3: Value=-3.5684, Grad=2.9460067162777434e-07\n",
      "  Param 4: Value=0.0048, Grad=1.2825714388042808e-06\n",
      "  Param 5: Value=-0.1847, Grad=-1.799312979371831e-06\n",
      "  Param 6: Value=-1.6297, Grad=3.75465157118855e-07\n",
      "  Max Abs Grad: 6.041968e-06\n",
      "------------------------------\n",
      "--- Step 8/10 / Loss: 1.245438 ---\n",
      "  Param 0: Value=4.2234, Grad=6.041967873603196e-06\n",
      "  Param 1: Value=1.6463, Grad=-9.515232797853795e-07\n",
      "  Param 2: Value=0.4774, Grad=1.3540113323312688e-06\n",
      "  Param 3: Value=-3.5684, Grad=2.9460067162777434e-07\n",
      "  Param 4: Value=0.0048, Grad=1.2825714388042808e-06\n",
      "  Param 5: Value=-0.1847, Grad=-1.799312979371831e-06\n",
      "  Param 6: Value=-1.6297, Grad=3.75465157118855e-07\n",
      "  Max Abs Grad: 6.041968e-06\n",
      "------------------------------\n",
      "--- Step 9/10 / Loss: 1.245438 ---\n",
      "  Param 0: Value=4.2234, Grad=6.041967873603196e-06\n",
      "  Param 1: Value=1.6463, Grad=-9.515232797853795e-07\n",
      "  Param 2: Value=0.4774, Grad=1.3540113323312688e-06\n",
      "  Param 3: Value=-3.5684, Grad=2.9460067162777434e-07\n",
      "  Param 4: Value=0.0048, Grad=1.2825714388042808e-06\n",
      "  Param 5: Value=-0.1847, Grad=-1.799312979371831e-06\n",
      "  Param 6: Value=-1.6297, Grad=3.75465157118855e-07\n",
      "  Max Abs Grad: 6.041968e-06\n",
      "------------------------------\n",
      "--- Step 10/10 / Loss: 1.245438 ---\n",
      "  Param 0: Value=4.2234, Grad=6.041967873603196e-06\n",
      "  Param 1: Value=1.6463, Grad=-9.515232797853795e-07\n",
      "  Param 2: Value=0.4774, Grad=1.3540113323312688e-06\n",
      "  Param 3: Value=-3.5684, Grad=2.9460067162777434e-07\n",
      "  Param 4: Value=0.0048, Grad=1.2825714388042808e-06\n",
      "  Param 5: Value=-0.1847, Grad=-1.799312979371831e-06\n",
      "  Param 6: Value=-1.6297, Grad=3.75465157118855e-07\n",
      "  Max Abs Grad: 6.041968e-06\n",
      "------------------------------\n",
      "Final Interpretable Params: {'sigma_sq': 13.15844996567829, 'range_lon': 0.19276173131915894, 'range_lat': 0.15182791212773075, 'range_time': 1.1478652960270026, 'advec_lat': 0.004831984184459988, 'advec_lon': -0.18473387021816454, 'nugget': 0.19599660548011516}\n",
      "\n",
      "Optimization finished in 114.18s.\n",
      "Results after 9 steps: [4.223364540712825, 1.6463004055157537, 0.477414302096888, -3.5684087171458385, 0.004831984184459988, -0.18473387021816454, -1.6296579388805994, 1.2454383285164397]\n",
      "Final Params: [ 4.22336454  1.64630041  0.4774143  -3.56840872  0.00483198 -0.18473387\n",
      " -1.62965794]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "v = 0.5              # Smoothness\n",
    "mm_cond_number = 8   # Neighbors\n",
    "nheads = 300           # 0 = Pure Vecchia\n",
    "lr = 1.0             # LBFGS learning rate\n",
    "LBFGS_MAX_STEPS = 10\n",
    "LBFGS_HISTORY_SIZE = 100\n",
    "LBFGS_LR = 1.0\n",
    "LBFGS_MAX_EVAL = 100    \n",
    "\n",
    "#DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- 1. SETUP PARAMETERS (List of Scalars) ---\n",
    "# Truth: [4.18, 1.94, 0.24, -3.97, 0.014, -0.20, -0.85]\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lat = 0.154 \n",
    "init_range_lon = 0.195\n",
    "init_advec_lat = 0.0218\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "# Map model parameters to the 'phi' reparameterization\n",
    "init_phi2 = 1.0 / init_range_lon                # 1/range_lon\n",
    "init_phi1 = init_sigmasq * init_phi2            # sigmasq / range_lon\n",
    "init_phi3 = (init_range_lon / init_range_lat)**2  # (range_lon / range_lat)^2\n",
    "init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "# Create Initial Parameters (Float64, Requires Grad)\n",
    "initial_vals = [np.log(init_phi1), np.log(init_phi2), np.log(init_phi3), \n",
    "                np.log(init_phi4), init_advec_lat, init_advec_lon, np.log(init_nugget)]\n",
    "\n",
    "params_list = [\n",
    "    torch.tensor([val], requires_grad=True, dtype=torch.float64, device=DEVICE)\n",
    "    for val in initial_vals\n",
    "]\n",
    "\n",
    "# --- 2. INSTANTIATE MODEL ---\n",
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'--- Initializing VecchiaBatched Model ---')\n",
    "print(f'{\"=\"*40}')\n",
    "\n",
    "if isinstance(aggregated_data, torch.Tensor):\n",
    "    aggregated_data = aggregated_data.to(DEVICE)\n",
    "\n",
    "# Instantiate\n",
    "model_instance = kernels_reparam_space_time_gpu0101.fit_vecchia_lbfgs(\n",
    "    smooth=v,\n",
    "    input_map=mm_input_map,\n",
    "    aggregated_data=aggregated_data,\n",
    "    nns_map=nns_map,\n",
    "    mm_cond_number=mm_cond_number,\n",
    "    nheads=nheads\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# --- 3. OPTIMIZATION LOOP ---\n",
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'--- Running L-BFGS Optimization ---')\n",
    "print(f'{\"=\"*40}')\n",
    "\n",
    "# Optimizer takes the LIST of scalars\n",
    "optimizer_vecc = model_instance.set_optimizer(\n",
    "            params_list,     \n",
    "            lr=LBFGS_LR,            \n",
    "            max_iter=LBFGS_MAX_EVAL,        \n",
    "            history_size=LBFGS_HISTORY_SIZE \n",
    "        )\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "out, steps_ran = model_instance.fit_vecc_lbfgs(\n",
    "        params_list,\n",
    "        optimizer_vecc,\n",
    "        # covariance_function argument is GONE\n",
    "        max_steps=LBFGS_MAX_STEPS, \n",
    "        grad_tol=1e-7\n",
    "    )\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "epoch_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nOptimization finished in {epoch_time:.2f}s.\")\n",
    "print(f\"Results after {steps_ran} steps: {out}\")\n",
    "print(f\"Final Params: {torch.cat(params_list).detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85190f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "========================================\n",
      "--- Initializing VecchiaBatched Model ---\n",
      "========================================\n",
      "\n",
      "========================================\n",
      "--- Running L-BFGS Optimization ---\n",
      "========================================\n",
      "Pre-computing Batched Tensors (Optimized CPU -> GPU)... Moving to GPU... Done. Heads: 2400, Tails: 142608\n",
      "--- Starting Batched L-BFGS Optimization (GPU) ---\n",
      "--- Step 1/10 / Loss: 1.246696 ---\n",
      "  Param 0: Value=4.1966, Grad=-2.436492271610968e-06\n",
      "  Param 1: Value=1.6155, Grad=-1.6093591245277665e-07\n",
      "  Param 2: Value=0.4700, Grad=-1.0740323001150915e-06\n",
      "  Param 3: Value=-3.2792, Grad=3.78124821961927e-07\n",
      "  Param 4: Value=0.0026, Grad=-2.730252524105409e-06\n",
      "  Param 5: Value=-0.1780, Grad=6.305887895457314e-06\n",
      "  Param 6: Value=-1.2869, Grad=-4.724524812927075e-08\n",
      "  Max Abs Grad: 6.305888e-06\n",
      "------------------------------\n",
      "--- Step 2/10 / Loss: 1.246632 ---\n",
      "  Param 0: Value=4.1966, Grad=-2.436492271610968e-06\n",
      "  Param 1: Value=1.6155, Grad=-1.6093591245277665e-07\n",
      "  Param 2: Value=0.4700, Grad=-1.0740323001150915e-06\n",
      "  Param 3: Value=-3.2792, Grad=3.78124821961927e-07\n",
      "  Param 4: Value=0.0026, Grad=-2.730252524105409e-06\n",
      "  Param 5: Value=-0.1780, Grad=6.305887895457314e-06\n",
      "  Param 6: Value=-1.2869, Grad=-4.724524812927075e-08\n",
      "  Max Abs Grad: 6.305888e-06\n",
      "------------------------------\n",
      "--- Step 3/10 / Loss: 1.246632 ---\n",
      "  Param 0: Value=4.1966, Grad=-2.436492271610968e-06\n",
      "  Param 1: Value=1.6155, Grad=-1.6093591245277665e-07\n",
      "  Param 2: Value=0.4700, Grad=-1.0740323001150915e-06\n",
      "  Param 3: Value=-3.2792, Grad=3.78124821961927e-07\n",
      "  Param 4: Value=0.0026, Grad=-2.730252524105409e-06\n",
      "  Param 5: Value=-0.1780, Grad=6.305887895457314e-06\n",
      "  Param 6: Value=-1.2869, Grad=-4.724524812927075e-08\n",
      "  Max Abs Grad: 6.305888e-06\n",
      "------------------------------\n",
      "--- Step 4/10 / Loss: 1.246632 ---\n",
      "  Param 0: Value=4.1966, Grad=-2.436492271610968e-06\n",
      "  Param 1: Value=1.6155, Grad=-1.6093591245277665e-07\n",
      "  Param 2: Value=0.4700, Grad=-1.0740323001150915e-06\n",
      "  Param 3: Value=-3.2792, Grad=3.78124821961927e-07\n",
      "  Param 4: Value=0.0026, Grad=-2.730252524105409e-06\n",
      "  Param 5: Value=-0.1780, Grad=6.305887895457314e-06\n",
      "  Param 6: Value=-1.2869, Grad=-4.724524812927075e-08\n",
      "  Max Abs Grad: 6.305888e-06\n",
      "------------------------------\n",
      "--- Step 5/10 / Loss: 1.246632 ---\n",
      "  Param 0: Value=4.1966, Grad=-2.436492271610968e-06\n",
      "  Param 1: Value=1.6155, Grad=-1.6093591245277665e-07\n",
      "  Param 2: Value=0.4700, Grad=-1.0740323001150915e-06\n",
      "  Param 3: Value=-3.2792, Grad=3.78124821961927e-07\n",
      "  Param 4: Value=0.0026, Grad=-2.730252524105409e-06\n",
      "  Param 5: Value=-0.1780, Grad=6.305887895457314e-06\n",
      "  Param 6: Value=-1.2869, Grad=-4.724524812927075e-08\n",
      "  Max Abs Grad: 6.305888e-06\n",
      "------------------------------\n",
      "--- Step 6/10 / Loss: 1.246632 ---\n",
      "  Param 0: Value=4.1966, Grad=-2.436492271610968e-06\n",
      "  Param 1: Value=1.6155, Grad=-1.6093591245277665e-07\n",
      "  Param 2: Value=0.4700, Grad=-1.0740323001150915e-06\n",
      "  Param 3: Value=-3.2792, Grad=3.78124821961927e-07\n",
      "  Param 4: Value=0.0026, Grad=-2.730252524105409e-06\n",
      "  Param 5: Value=-0.1780, Grad=6.305887895457314e-06\n",
      "  Param 6: Value=-1.2869, Grad=-4.724524812927075e-08\n",
      "  Max Abs Grad: 6.305888e-06\n",
      "------------------------------\n",
      "--- Step 7/10 / Loss: 1.246632 ---\n",
      "  Param 0: Value=4.1966, Grad=-2.436492271610968e-06\n",
      "  Param 1: Value=1.6155, Grad=-1.6093591245277665e-07\n",
      "  Param 2: Value=0.4700, Grad=-1.0740323001150915e-06\n",
      "  Param 3: Value=-3.2792, Grad=3.78124821961927e-07\n",
      "  Param 4: Value=0.0026, Grad=-2.730252524105409e-06\n",
      "  Param 5: Value=-0.1780, Grad=6.305887895457314e-06\n",
      "  Param 6: Value=-1.2869, Grad=-4.724524812927075e-08\n",
      "  Max Abs Grad: 6.305888e-06\n",
      "------------------------------\n",
      "--- Step 8/10 / Loss: 1.246632 ---\n",
      "  Param 0: Value=4.1966, Grad=-2.436492271610968e-06\n",
      "  Param 1: Value=1.6155, Grad=-1.6093591245277665e-07\n",
      "  Param 2: Value=0.4700, Grad=-1.0740323001150915e-06\n",
      "  Param 3: Value=-3.2792, Grad=3.78124821961927e-07\n",
      "  Param 4: Value=0.0026, Grad=-2.730252524105409e-06\n",
      "  Param 5: Value=-0.1780, Grad=6.305887895457314e-06\n",
      "  Param 6: Value=-1.2869, Grad=-4.724524812927075e-08\n",
      "  Max Abs Grad: 6.305888e-06\n",
      "------------------------------\n",
      "--- Step 9/10 / Loss: 1.246632 ---\n",
      "  Param 0: Value=4.1966, Grad=-2.436492271610968e-06\n",
      "  Param 1: Value=1.6155, Grad=-1.6093591245277665e-07\n",
      "  Param 2: Value=0.4700, Grad=-1.0740323001150915e-06\n",
      "  Param 3: Value=-3.2792, Grad=3.78124821961927e-07\n",
      "  Param 4: Value=0.0026, Grad=-2.730252524105409e-06\n",
      "  Param 5: Value=-0.1780, Grad=6.305887895457314e-06\n",
      "  Param 6: Value=-1.2869, Grad=-4.724524812927075e-08\n",
      "  Max Abs Grad: 6.305888e-06\n",
      "------------------------------\n",
      "--- Step 10/10 / Loss: 1.246632 ---\n",
      "  Param 0: Value=4.1966, Grad=-2.436492271610968e-06\n",
      "  Param 1: Value=1.6155, Grad=-1.6093591245277665e-07\n",
      "  Param 2: Value=0.4700, Grad=-1.0740323001150915e-06\n",
      "  Param 3: Value=-3.2792, Grad=3.78124821961927e-07\n",
      "  Param 4: Value=0.0026, Grad=-2.730252524105409e-06\n",
      "  Param 5: Value=-0.1780, Grad=6.305887895457314e-06\n",
      "  Param 6: Value=-1.2869, Grad=-4.724524812927075e-08\n",
      "  Max Abs Grad: 6.305888e-06\n",
      "------------------------------\n",
      "Final Interpretable Params: {'sigma_sq': 13.211827411365775, 'range_lon': 0.19880034338163063, 'range_lat': 0.15716199567911632, 'range_time': 1.0244576659952744, 'advec_lat': 0.0025999187378799317, 'advec_lon': -0.17800632685626624, 'nugget': 0.27612217737831274}\n",
      "\n",
      "Optimization finished in 96.15s.\n",
      "Results after 9 steps: [4.196566701911534, 1.6154542574893713, 0.47004785698443, -3.279235247415023, 0.0025999187378799317, -0.17800632685626624, -1.2869118395546277, 1.246631983185473]\n",
      "Final Params: [ 4.19656670e+00  1.61545426e+00  4.70047857e-01 -3.27923525e+00\n",
      "  2.59991874e-03 -1.78006327e-01 -1.28691184e+00]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "v = 0.5              # Smoothness\n",
    "mm_cond_number = 8   # Neighbors\n",
    "nheads = 300           # 0 = Pure Vecchia\n",
    "lr = 1.0             # LBFGS learning rate\n",
    "LBFGS_MAX_STEPS = 10\n",
    "LBFGS_HISTORY_SIZE = 100\n",
    "LBFGS_LR = 1.0\n",
    "LBFGS_MAX_EVAL = 100    \n",
    "\n",
    "#DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- 1. SETUP PARAMETERS (List of Scalars) ---\n",
    "# Truth: [4.18, 1.94, 0.24, -3.97, 0.014, -0.20, -0.85]\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lat = 0.154 \n",
    "init_range_lon = 0.195\n",
    "init_advec_lat = 0.0218\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "# Map model parameters to the 'phi' reparameterization\n",
    "init_phi2 = 1.0 / init_range_lon                # 1/range_lon\n",
    "init_phi1 = init_sigmasq * init_phi2            # sigmasq / range_lon\n",
    "init_phi3 = (init_range_lon / init_range_lat)**2  # (range_lon / range_lat)^2\n",
    "init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "# Create Initial Parameters (Float64, Requires Grad)\n",
    "initial_vals = [np.log(init_phi1), np.log(init_phi2), np.log(init_phi3), \n",
    "                np.log(init_phi4), init_advec_lat, init_advec_lon, np.log(init_nugget)]\n",
    "\n",
    "params_list = [\n",
    "    torch.tensor([val], requires_grad=True, dtype=torch.float64, device=DEVICE)\n",
    "    for val in initial_vals\n",
    "]\n",
    "\n",
    "# --- 2. INSTANTIATE MODEL ---\n",
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'--- Initializing VecchiaBatched Model ---')\n",
    "print(f'{\"=\"*40}')\n",
    "\n",
    "if isinstance(aggregated_data, torch.Tensor):\n",
    "    aggregated_data = aggregated_data.to(DEVICE)\n",
    "\n",
    "# Instantiate\n",
    "model_instance = kernels_reparam_space_time_gpu.fit_vecchia_lbfgs(\n",
    "    smooth=v,\n",
    "    input_map=mm_input_map,\n",
    "    aggregated_data=aggregated_data,\n",
    "    nns_map=nns_map,\n",
    "    mm_cond_number=mm_cond_number,\n",
    "    nheads=nheads\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# --- 3. OPTIMIZATION LOOP ---\n",
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'--- Running L-BFGS Optimization ---')\n",
    "print(f'{\"=\"*40}')\n",
    "\n",
    "# Optimizer takes the LIST of scalars\n",
    "optimizer_vecc = model_instance.set_optimizer(\n",
    "            params_list,     \n",
    "            lr=LBFGS_LR,            \n",
    "            max_iter=LBFGS_MAX_EVAL,        \n",
    "            history_size=LBFGS_HISTORY_SIZE \n",
    "        )\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "out, steps_ran = model_instance.fit_vecc_lbfgs(\n",
    "        params_list,\n",
    "        optimizer_vecc,\n",
    "        # covariance_function argument is GONE\n",
    "        max_steps=LBFGS_MAX_STEPS, \n",
    "        grad_tol=1e-7\n",
    "    )\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "epoch_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nOptimization finished in {epoch_time:.2f}s.\")\n",
    "print(f\"Results after {steps_ran} steps: {out}\")\n",
    "print(f\"Final Params: {torch.cat(params_list).detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09614dc",
   "metadata": {},
   "source": [
    "Final Interpretable Params: {'sigma_sq': 13.220112275539044, 'range_lon': 0.19893996947538484, 'range_lat': 0.15726615620968531, 'range_time': 1.023655610496125, 'advec_lat': 0.002306534241501554, 'advec_lon': -0.17791920567023048, 'nugget': 0.2761774460309547}\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 13.211827411365775, 'range_lon': 0.19880034338163063, 'range_lat': 0.15716199567911632, 'range_time': 1.0244576659952744, 'advec_lat': 0.0025999187378799317, 'advec_lon': -0.17800632685626624, 'nugget': 0.27612217737831274}\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 13.199425990161394, 'range_lon': 0.19851674753741236, 'range_lat': 0.15691835083244873, 'range_time': 1.0224799532349134, 'advec_lat': 0.0036040445600274084, 'advec_lon': -0.1780005673354996, 'nugget': 0.2749515243103896}\n",
    "\n",
    "\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lon = 0.195 \n",
    "init_range_lat = 0.154 \n",
    "init_advec_lat = 0.0218\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 13.15844996567829, 'range_lon': 0.19276173131915894, 'range_lat': 0.15182791212773075, 'range_time': 1.1478652960270026, 'advec_lat': 0.004831984184459988, 'advec_lon': -0.18473387021816454, 'nugget': 0.19599660548011516}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f08460",
   "metadata": {},
   "source": [
    "# vecc time 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45f5e1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "========================================\n",
      "--- Initializing VecchiaBatched Model ---\n",
      "========================================\n",
      "\n",
      "========================================\n",
      "--- Running L-BFGS Optimization ---\n",
      "========================================\n",
      "Pre-computing Batched Tensors (Padding Strategy)... Done. Heads: 0, Batched Tails: 145008\n",
      "--- Starting Batched L-BFGS Optimization (GPU) ---\n",
      "--- Step 1/10 / Loss: 1.239743 ---\n",
      "  Param 0: Value=4.2256, Grad=-6.959610383200575e-08\n",
      "  Param 1: Value=1.6973, Grad=-1.068152907998466e-06\n",
      "  Param 2: Value=0.4941, Grad=-7.162888808699521e-07\n",
      "  Param 3: Value=-3.7021, Grad=2.215068929605948e-07\n",
      "  Param 4: Value=0.0269, Grad=-1.0142547031410931e-07\n",
      "  Param 5: Value=-0.1839, Grad=-2.9175096793038202e-06\n",
      "  Param 6: Value=-1.8100, Grad=-1.6566882167386242e-07\n",
      "  Max Abs Grad: 2.917510e-06\n",
      "------------------------------\n",
      "--- Step 2/10 / Loss: 1.239444 ---\n",
      "  Param 0: Value=4.2256, Grad=-6.959610383200575e-08\n",
      "  Param 1: Value=1.6973, Grad=-1.068152907998466e-06\n",
      "  Param 2: Value=0.4941, Grad=-7.162888808699521e-07\n",
      "  Param 3: Value=-3.7021, Grad=2.215068929605948e-07\n",
      "  Param 4: Value=0.0269, Grad=-1.0142547031410931e-07\n",
      "  Param 5: Value=-0.1839, Grad=-2.9175096793038202e-06\n",
      "  Param 6: Value=-1.8100, Grad=-1.6566882167386242e-07\n",
      "  Max Abs Grad: 2.917510e-06\n",
      "------------------------------\n",
      "--- Step 3/10 / Loss: 1.239444 ---\n",
      "  Param 0: Value=4.2256, Grad=-6.959610383200575e-08\n",
      "  Param 1: Value=1.6973, Grad=-1.068152907998466e-06\n",
      "  Param 2: Value=0.4941, Grad=-7.162888808699521e-07\n",
      "  Param 3: Value=-3.7021, Grad=2.215068929605948e-07\n",
      "  Param 4: Value=0.0269, Grad=-1.0142547031410931e-07\n",
      "  Param 5: Value=-0.1839, Grad=-2.9175096793038202e-06\n",
      "  Param 6: Value=-1.8100, Grad=-1.6566882167386242e-07\n",
      "  Max Abs Grad: 2.917510e-06\n",
      "------------------------------\n",
      "--- Step 4/10 / Loss: 1.239444 ---\n",
      "  Param 0: Value=4.2256, Grad=-6.959610383200575e-08\n",
      "  Param 1: Value=1.6973, Grad=-1.068152907998466e-06\n",
      "  Param 2: Value=0.4941, Grad=-7.162888808699521e-07\n",
      "  Param 3: Value=-3.7021, Grad=2.215068929605948e-07\n",
      "  Param 4: Value=0.0269, Grad=-1.0142547031410931e-07\n",
      "  Param 5: Value=-0.1839, Grad=-2.9175096793038202e-06\n",
      "  Param 6: Value=-1.8100, Grad=-1.6566882167386242e-07\n",
      "  Max Abs Grad: 2.917510e-06\n",
      "------------------------------\n",
      "--- Step 5/10 / Loss: 1.239444 ---\n",
      "  Param 0: Value=4.2256, Grad=-6.959610383200575e-08\n",
      "  Param 1: Value=1.6973, Grad=-1.068152907998466e-06\n",
      "  Param 2: Value=0.4941, Grad=-7.162888808699521e-07\n",
      "  Param 3: Value=-3.7021, Grad=2.215068929605948e-07\n",
      "  Param 4: Value=0.0269, Grad=-1.0142547031410931e-07\n",
      "  Param 5: Value=-0.1839, Grad=-2.9175096793038202e-06\n",
      "  Param 6: Value=-1.8100, Grad=-1.6566882167386242e-07\n",
      "  Max Abs Grad: 2.917510e-06\n",
      "------------------------------\n",
      "--- Step 6/10 / Loss: 1.239444 ---\n",
      "  Param 0: Value=4.2256, Grad=-6.959610383200575e-08\n",
      "  Param 1: Value=1.6973, Grad=-1.068152907998466e-06\n",
      "  Param 2: Value=0.4941, Grad=-7.162888808699521e-07\n",
      "  Param 3: Value=-3.7021, Grad=2.215068929605948e-07\n",
      "  Param 4: Value=0.0269, Grad=-1.0142547031410931e-07\n",
      "  Param 5: Value=-0.1839, Grad=-2.9175096793038202e-06\n",
      "  Param 6: Value=-1.8100, Grad=-1.6566882167386242e-07\n",
      "  Max Abs Grad: 2.917510e-06\n",
      "------------------------------\n",
      "--- Step 7/10 / Loss: 1.239444 ---\n",
      "  Param 0: Value=4.2256, Grad=-6.959610383200575e-08\n",
      "  Param 1: Value=1.6973, Grad=-1.068152907998466e-06\n",
      "  Param 2: Value=0.4941, Grad=-7.162888808699521e-07\n",
      "  Param 3: Value=-3.7021, Grad=2.215068929605948e-07\n",
      "  Param 4: Value=0.0269, Grad=-1.0142547031410931e-07\n",
      "  Param 5: Value=-0.1839, Grad=-2.9175096793038202e-06\n",
      "  Param 6: Value=-1.8100, Grad=-1.6566882167386242e-07\n",
      "  Max Abs Grad: 2.917510e-06\n",
      "------------------------------\n",
      "--- Step 8/10 / Loss: 1.239444 ---\n",
      "  Param 0: Value=4.2256, Grad=-6.959610383200575e-08\n",
      "  Param 1: Value=1.6973, Grad=-1.068152907998466e-06\n",
      "  Param 2: Value=0.4941, Grad=-7.162888808699521e-07\n",
      "  Param 3: Value=-3.7021, Grad=2.215068929605948e-07\n",
      "  Param 4: Value=0.0269, Grad=-1.0142547031410931e-07\n",
      "  Param 5: Value=-0.1839, Grad=-2.9175096793038202e-06\n",
      "  Param 6: Value=-1.8100, Grad=-1.6566882167386242e-07\n",
      "  Max Abs Grad: 2.917510e-06\n",
      "------------------------------\n",
      "--- Step 9/10 / Loss: 1.239444 ---\n",
      "  Param 0: Value=4.2256, Grad=-6.959610383200575e-08\n",
      "  Param 1: Value=1.6973, Grad=-1.068152907998466e-06\n",
      "  Param 2: Value=0.4941, Grad=-7.162888808699521e-07\n",
      "  Param 3: Value=-3.7021, Grad=2.215068929605948e-07\n",
      "  Param 4: Value=0.0269, Grad=-1.0142547031410931e-07\n",
      "  Param 5: Value=-0.1839, Grad=-2.9175096793038202e-06\n",
      "  Param 6: Value=-1.8100, Grad=-1.6566882167386242e-07\n",
      "  Max Abs Grad: 2.917510e-06\n",
      "------------------------------\n",
      "--- Step 10/10 / Loss: 1.239444 ---\n",
      "  Param 0: Value=4.2256, Grad=-6.959610383200575e-08\n",
      "  Param 1: Value=1.6973, Grad=-1.068152907998466e-06\n",
      "  Param 2: Value=0.4941, Grad=-7.162888808699521e-07\n",
      "  Param 3: Value=-3.7021, Grad=2.215068929605948e-07\n",
      "  Param 4: Value=0.0269, Grad=-1.0142547031410931e-07\n",
      "  Param 5: Value=-0.1839, Grad=-2.9175096793038202e-06\n",
      "  Param 6: Value=-1.8100, Grad=-1.6566882167386242e-07\n",
      "  Max Abs Grad: 2.917510e-06\n",
      "------------------------------\n",
      "Final Interpretable Params: {'sigma_sq': 12.531565434378132, 'range_lon': 0.18317637134919773, 'range_lat': 0.14307715839115615, 'range_time': 1.1661863706474473, 'advec_lat': 0.026879346058263948, 'advec_lon': -0.1839300180406578, 'nugget': 0.16365870466414187}\n",
      "\n",
      "Optimization finished in 297.60s.\n",
      "Results after 9 steps: [4.225556508422678, 1.6973058124119418, 0.49413082531293334, -3.7020894503451767, 0.026879346058263948, -0.1839300180406578, -1.8099720887123303, 1.2394439182190515]\n",
      "Final Params: [ 4.22555651  1.69730581  0.49413083 -3.70208945  0.02687935 -0.18393002\n",
      " -1.80997209]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "v = 0.5              # Smoothness\n",
    "mm_cond_number = 12   # Neighbors\n",
    "nheads = 0           # 0 = Pure Vecchia\n",
    "lr = 1.0             # LBFGS learning rate\n",
    "LBFGS_MAX_STEPS = 10\n",
    "LBFGS_HISTORY_SIZE = 100\n",
    "LBFGS_LR = 1.0\n",
    "LBFGS_MAX_EVAL = 100    \n",
    "\n",
    "#DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- 1. SETUP PARAMETERS (List of Scalars) ---\n",
    "# Truth: [4.18, 1.94, 0.24, -3.97, 0.014, -0.20, -0.85]\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lat = 0.154 \n",
    "init_range_lon = 0.195\n",
    "init_advec_lat = 0.0218\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "# Map model parameters to the 'phi' reparameterization\n",
    "init_phi2 = 1.0 / init_range_lon                # 1/range_lon\n",
    "init_phi1 = init_sigmasq * init_phi2            # sigmasq / range_lon\n",
    "init_phi3 = (init_range_lon / init_range_lat)**2  # (range_lon / range_lat)^2\n",
    "init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "# Create Initial Parameters (Float64, Requires Grad)\n",
    "initial_vals = [np.log(init_phi1), np.log(init_phi2), np.log(init_phi3), \n",
    "                np.log(init_phi4), init_advec_lat, init_advec_lon, np.log(init_nugget)]\n",
    "\n",
    "params_list = [\n",
    "    torch.tensor([val], requires_grad=True, dtype=torch.float64, device=DEVICE)\n",
    "    for val in initial_vals\n",
    "]\n",
    "\n",
    "# --- 2. INSTANTIATE MODEL ---\n",
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'--- Initializing VecchiaBatched Model ---')\n",
    "print(f'{\"=\"*40}')\n",
    "\n",
    "if isinstance(aggregated_data, torch.Tensor):\n",
    "    aggregated_data = aggregated_data.to(DEVICE)\n",
    "\n",
    "# Instantiate\n",
    "model_instance = kernels_reparam_space_time_gpu_past1.fit_vecchia_lbfgs(\n",
    "    smooth=v,\n",
    "    input_map=mm_input_map,\n",
    "    aggregated_data=aggregated_data,\n",
    "    nns_map=nns_map,\n",
    "    mm_cond_number=mm_cond_number,\n",
    "    nheads=nheads\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# --- 3. OPTIMIZATION LOOP ---\n",
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'--- Running L-BFGS Optimization ---')\n",
    "print(f'{\"=\"*40}')\n",
    "\n",
    "# Optimizer takes the LIST of scalars\n",
    "optimizer_vecc = model_instance.set_optimizer(\n",
    "            params_list,     \n",
    "            lr=LBFGS_LR,            \n",
    "            max_iter=LBFGS_MAX_EVAL,        \n",
    "            history_size=LBFGS_HISTORY_SIZE \n",
    "        )\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "out, steps_ran = model_instance.fit_vecc_lbfgs(\n",
    "        params_list,\n",
    "        optimizer_vecc,\n",
    "        # covariance_function argument is GONE\n",
    "        max_steps=LBFGS_MAX_STEPS, \n",
    "        grad_tol=1e-7\n",
    "    )\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "epoch_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nOptimization finished in {epoch_time:.2f}s.\")\n",
    "print(f\"Results after {steps_ran} steps: {out}\")\n",
    "print(f\"Final Params: {torch.cat(params_list).detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9285cc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3.6*170/8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b732eaf6",
   "metadata": {},
   "source": [
    "# fit vecc column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc6c33e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "========================================\n",
      "--- Initializing VecchiaBatched Model ---\n",
      "========================================\n",
      "\n",
      "========================================\n",
      "--- Running L-BFGS Optimization ---\n",
      "========================================\n",
      "Pre-computing Batched Tensors (Optimized Geometric Strategy)... Done. Heads: 2736, Batched Tails: 142272, Neighbors per point: 14\n",
      "--- Starting Batched L-BFGS Optimization (GPU) ---\n",
      "--- Step 1/10 / Loss: 1.252622 ---\n",
      "  Param 0: Value=4.3421, Grad=7.911366992630064e-07\n",
      "  Param 1: Value=2.2323, Grad=-1.3886892702430487e-06\n",
      "  Param 2: Value=0.3790, Grad=5.84215740673244e-07\n",
      "  Param 3: Value=-54.0534, Grad=-2.3439421640243724e-33\n",
      "  Param 4: Value=1.6350, Grad=-2.921111796894138e-09\n",
      "  Param 5: Value=0.4555, Grad=8.524303485302198e-10\n",
      "  Param 6: Value=-14.6408, Grad=7.244628230651529e-10\n",
      "  Max Abs Grad: 1.388689e-06\n",
      "------------------------------\n",
      "--- Step 2/10 / Loss: 1.239938 ---\n",
      "  Param 0: Value=4.3421, Grad=7.911366992630064e-07\n",
      "  Param 1: Value=2.2323, Grad=-1.3886892702430487e-06\n",
      "  Param 2: Value=0.3790, Grad=5.84215740673244e-07\n",
      "  Param 3: Value=-54.0534, Grad=-2.3439421640243724e-33\n",
      "  Param 4: Value=1.6350, Grad=-2.921111796894138e-09\n",
      "  Param 5: Value=0.4555, Grad=8.524303485302198e-10\n",
      "  Param 6: Value=-14.6408, Grad=7.244628230651529e-10\n",
      "  Max Abs Grad: 1.388689e-06\n",
      "------------------------------\n",
      "--- Step 3/10 / Loss: 1.239938 ---\n",
      "  Param 0: Value=4.3421, Grad=7.911366992630064e-07\n",
      "  Param 1: Value=2.2323, Grad=-1.3886892702430487e-06\n",
      "  Param 2: Value=0.3790, Grad=5.84215740673244e-07\n",
      "  Param 3: Value=-54.0534, Grad=-2.3439421640243724e-33\n",
      "  Param 4: Value=1.6350, Grad=-2.921111796894138e-09\n",
      "  Param 5: Value=0.4555, Grad=8.524303485302198e-10\n",
      "  Param 6: Value=-14.6408, Grad=7.244628230651529e-10\n",
      "  Max Abs Grad: 1.388689e-06\n",
      "------------------------------\n",
      "--- Step 4/10 / Loss: 1.239938 ---\n",
      "  Param 0: Value=4.3421, Grad=7.911366992630064e-07\n",
      "  Param 1: Value=2.2323, Grad=-1.3886892702430487e-06\n",
      "  Param 2: Value=0.3790, Grad=5.84215740673244e-07\n",
      "  Param 3: Value=-54.0534, Grad=-2.3439421640243724e-33\n",
      "  Param 4: Value=1.6350, Grad=-2.921111796894138e-09\n",
      "  Param 5: Value=0.4555, Grad=8.524303485302198e-10\n",
      "  Param 6: Value=-14.6408, Grad=7.244628230651529e-10\n",
      "  Max Abs Grad: 1.388689e-06\n",
      "------------------------------\n",
      "--- Step 5/10 / Loss: 1.239938 ---\n",
      "  Param 0: Value=4.3421, Grad=7.911366992630064e-07\n",
      "  Param 1: Value=2.2323, Grad=-1.3886892702430487e-06\n",
      "  Param 2: Value=0.3790, Grad=5.84215740673244e-07\n",
      "  Param 3: Value=-54.0534, Grad=-2.3439421640243724e-33\n",
      "  Param 4: Value=1.6350, Grad=-2.921111796894138e-09\n",
      "  Param 5: Value=0.4555, Grad=8.524303485302198e-10\n",
      "  Param 6: Value=-14.6408, Grad=7.244628230651529e-10\n",
      "  Max Abs Grad: 1.388689e-06\n",
      "------------------------------\n",
      "--- Step 6/10 / Loss: 1.239938 ---\n",
      "  Param 0: Value=4.3421, Grad=7.911366992630064e-07\n",
      "  Param 1: Value=2.2323, Grad=-1.3886892702430487e-06\n",
      "  Param 2: Value=0.3790, Grad=5.84215740673244e-07\n",
      "  Param 3: Value=-54.0534, Grad=-2.3439421640243724e-33\n",
      "  Param 4: Value=1.6350, Grad=-2.921111796894138e-09\n",
      "  Param 5: Value=0.4555, Grad=8.524303485302198e-10\n",
      "  Param 6: Value=-14.6408, Grad=7.244628230651529e-10\n",
      "  Max Abs Grad: 1.388689e-06\n",
      "------------------------------\n",
      "--- Step 7/10 / Loss: 1.239938 ---\n",
      "  Param 0: Value=4.3421, Grad=7.911366992630064e-07\n",
      "  Param 1: Value=2.2323, Grad=-1.3886892702430487e-06\n",
      "  Param 2: Value=0.3790, Grad=5.84215740673244e-07\n",
      "  Param 3: Value=-54.0534, Grad=-2.3439421640243724e-33\n",
      "  Param 4: Value=1.6350, Grad=-2.921111796894138e-09\n",
      "  Param 5: Value=0.4555, Grad=8.524303485302198e-10\n",
      "  Param 6: Value=-14.6408, Grad=7.244628230651529e-10\n",
      "  Max Abs Grad: 1.388689e-06\n",
      "------------------------------\n",
      "--- Step 8/10 / Loss: 1.239938 ---\n",
      "  Param 0: Value=4.3421, Grad=7.911366992630064e-07\n",
      "  Param 1: Value=2.2323, Grad=-1.3886892702430487e-06\n",
      "  Param 2: Value=0.3790, Grad=5.84215740673244e-07\n",
      "  Param 3: Value=-54.0534, Grad=-2.3439421640243724e-33\n",
      "  Param 4: Value=1.6350, Grad=-2.921111796894138e-09\n",
      "  Param 5: Value=0.4555, Grad=8.524303485302198e-10\n",
      "  Param 6: Value=-14.6408, Grad=7.244628230651529e-10\n",
      "  Max Abs Grad: 1.388689e-06\n",
      "------------------------------\n",
      "--- Step 9/10 / Loss: 1.239938 ---\n",
      "  Param 0: Value=4.3421, Grad=7.911366992630064e-07\n",
      "  Param 1: Value=2.2323, Grad=-1.3886892702430487e-06\n",
      "  Param 2: Value=0.3790, Grad=5.84215740673244e-07\n",
      "  Param 3: Value=-54.0534, Grad=-2.3439421640243724e-33\n",
      "  Param 4: Value=1.6350, Grad=-2.921111796894138e-09\n",
      "  Param 5: Value=0.4555, Grad=8.524303485302198e-10\n",
      "  Param 6: Value=-14.6408, Grad=7.244628230651529e-10\n",
      "  Max Abs Grad: 1.388689e-06\n",
      "------------------------------\n",
      "--- Step 10/10 / Loss: 1.239938 ---\n",
      "  Param 0: Value=4.3421, Grad=7.911366992630064e-07\n",
      "  Param 1: Value=2.2323, Grad=-1.3886892702430487e-06\n",
      "  Param 2: Value=0.3790, Grad=5.84215740673244e-07\n",
      "  Param 3: Value=-54.0534, Grad=-2.3439421640243724e-33\n",
      "  Param 4: Value=1.6350, Grad=-2.921111796894138e-09\n",
      "  Param 5: Value=0.4555, Grad=8.524303485302198e-10\n",
      "  Param 6: Value=-14.6408, Grad=7.244628230651529e-10\n",
      "  Max Abs Grad: 1.388689e-06\n",
      "------------------------------\n",
      "Final Interpretable Params: {'sigma_sq': 8.246856095764418, 'range_lon': 0.10728446370599494, 'range_lat': 0.0887650790925078, 'range_time': 58624250667.79141, 'advec_lat': 1.6350435018539429, 'advec_lon': 0.4554915726184845, 'nugget': 4.380916642428308e-07}\n",
      "\n",
      "Optimization finished in 724.01s.\n",
      "Results after 9 steps: [4.342103481292725, 2.232271432876587, 0.3789810538291931, -54.05337142944336, 1.6350435018539429, 0.4554915726184845, -14.640837669372559, 1.2399376370351087]\n",
      "Final Params: [  4.3421035    2.2322714    0.37898105 -54.05337      1.6350435\n",
      "   0.45549157 -14.640838  ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "v = 0.5              # Smoothness\n",
    "mm_cond_number = 14    # Neighbors\n",
    "nheads = 113*3           # 0 = Pure Vecchia\n",
    "lr = 1.0             # LBFGS learning rate\n",
    "LBFGS_MAX_STEPS = 10\n",
    "LBFGS_HISTORY_SIZE = 100\n",
    "LBFGS_LR = 1.0\n",
    "LBFGS_MAX_EVAL = 100    \n",
    "\n",
    "#DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- 1. SETUP PARAMETERS (List of Scalars) ---\n",
    "# Truth: [4.18, 1.94, 0.24, -3.97, 0.014, -0.20, -0.85]\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lat = 0.154 \n",
    "init_range_lon = 0.195\n",
    "init_advec_lat = 0.0218\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "# Map model parameters to the 'phi' reparameterization\n",
    "init_phi2 = 1.0 / init_range_lon                # 1/range_lon\n",
    "init_phi1 = init_sigmasq * init_phi2            # sigmasq / range_lon\n",
    "init_phi3 = (init_range_lon / init_range_lat)**2  # (range_lon / range_lat)^2\n",
    "init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "# Create Initial Parameters (Float64, Requires Grad)\n",
    "initial_vals = [np.log(init_phi1), np.log(init_phi2), np.log(init_phi3), \n",
    "                np.log(init_phi4), init_advec_lat, init_advec_lon, np.log(init_nugget)]\n",
    "\n",
    "params_list = [\n",
    "    torch.tensor([val], requires_grad=True, dtype=torch.float32, device=DEVICE)\n",
    "    for val in initial_vals\n",
    "]\n",
    "\n",
    "# --- 2. INSTANTIATE MODEL ---\n",
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'--- Initializing VecchiaBatched Model ---')\n",
    "print(f'{\"=\"*40}')\n",
    "\n",
    "if isinstance(aggregated_data, torch.Tensor):\n",
    "    aggregated_data = aggregated_data.to(DEVICE)\n",
    "\n",
    "# Instantiate\n",
    "model_instance = kernels_gpu_st_simulation_column.fit_vecchia_lbfgs(\n",
    "    smooth=v,\n",
    "    input_map=input_map,\n",
    "    aggregated_data=aggregated_data,\n",
    "    nns_map=nns_map,\n",
    "    mm_cond_number=mm_cond_number,\n",
    "    nheads=nheads\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# --- 3. OPTIMIZATION LOOP ---\n",
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'--- Running L-BFGS Optimization ---')\n",
    "print(f'{\"=\"*40}')\n",
    "\n",
    "# Optimizer takes the LIST of scalars\n",
    "optimizer_vecc = model_instance.set_optimizer(\n",
    "            params_list,     \n",
    "            lr=LBFGS_LR,            \n",
    "            max_iter=LBFGS_MAX_EVAL,        \n",
    "            history_size=LBFGS_HISTORY_SIZE \n",
    "        )\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "out, steps_ran = model_instance.fit_vecc_lbfgs(\n",
    "        params_list,\n",
    "        optimizer_vecc,\n",
    "        # covariance_function argument is GONE\n",
    "        max_steps=LBFGS_MAX_STEPS, \n",
    "        grad_tol=1e-7\n",
    "    )\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "epoch_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nOptimization finished in {epoch_time:.2f}s.\")\n",
    "print(f\"Results after {steps_ran} steps: {out}\")\n",
    "print(f\"Final Params: {torch.cat(params_list).detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c961fa4",
   "metadata": {},
   "source": [
    "# fit dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9b3e3",
   "metadata": {},
   "source": [
    "difference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d43435c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([142832, 4])\n",
      "142832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.2300e+02,  3.8945e-01,  2.1000e+01],\n",
       "        [ 0.0000e+00,  1.2306e+02,  7.9436e+00,  2.1000e+01],\n",
       "        [ 0.0000e+00,  1.2313e+02, -9.8576e+00,  2.1000e+01],\n",
       "        [ 0.0000e+00,  1.2319e+02,  6.2259e+00,  2.1000e+01],\n",
       "        [ 0.0000e+00,  1.2325e+02, -1.3111e+00,  2.1000e+01],\n",
       "        [ 0.0000e+00,  1.2331e+02,  8.5856e+00,  2.1000e+01],\n",
       "        [ 0.0000e+00,  1.2338e+02,  5.7867e+00,  2.1000e+01],\n",
       "        [ 0.0000e+00,  1.2344e+02, -5.0939e+00,  2.1000e+01],\n",
       "        [ 0.0000e+00,  1.2350e+02,  7.6338e+00,  2.1000e+01],\n",
       "        [ 0.0000e+00,  1.2357e+02, -2.4320e+00,  2.1000e+01],\n",
       "        [ 0.0000e+00,  1.2363e+02, -7.3716e-01,  2.1000e+01],\n",
       "        [ 0.0000e+00,  1.2369e+02, -4.5453e+00,  2.1000e+01],\n",
       "        [ 0.0000e+00,  1.2376e+02,  4.4043e+00,  2.1000e+01],\n",
       "        [ 0.0000e+00,  1.2382e+02,  8.4446e-03,  2.1000e+01],\n",
       "        [ 0.0000e+00,  1.2388e+02,  3.5328e+00,  2.1000e+01],\n",
       "        [ 0.0000e+00,  1.2394e+02, -2.7751e+00,  2.1000e+01],\n",
       "        [ 0.0000e+00,  1.2401e+02,  8.3439e+00,  2.1000e+01],\n",
       "        [ 0.0000e+00,  1.2407e+02,  5.3083e+00,  2.1000e+01],\n",
       "        [ 0.0000e+00,  1.2413e+02, -9.5722e-01,  2.1000e+01],\n",
       "        [ 0.0000e+00,  1.2420e+02, -2.7983e+00,  2.1000e+01]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [11.0474, 0.0623, 0.2445, 1.0972, 0.0101, -0.1671, 1.1825]\n",
    "day = 0 # 0 index\n",
    "lat_range= [0,5]\n",
    "lon_range= [123.0, 133.0]\n",
    "#lat_range= [1,3]\n",
    "#lon_range= [125, 129.0]\n",
    "\n",
    "daily_aggregated_tensors_dw = [aggregated_data]\n",
    "daily_hourly_maps_dw = [input_map]\n",
    "\n",
    "db = debiased_whittle.debiased_whittle_preprocess(daily_aggregated_tensors_dw, daily_hourly_maps_dw, day_idx=day, params_list=a, lat_range=lat_range, lon_range=lon_range)\n",
    "\n",
    "\n",
    "subsetted_aggregated_day = db.generate_spatially_filtered_days(0,5,123,133)\n",
    "print(subsetted_aggregated_day.shape)\n",
    "N2= subsetted_aggregated_day.shape[0]\n",
    "print(N2)\n",
    "subsetted_aggregated_day[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2497b8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Pre-computing J-vector (Hamming taper)...\n",
      "Pre-computing sample periodogram...\n",
      "Pre-computing Hamming taper autocorrelation...\n",
      "Data grid: 113x158, 8 time points. J-vector, Periodogram, Taper Autocorr on cpu.\n",
      "\n",
      "============================== Initialization Run 1/1 ==============================\n",
      "Starting with FIXED params (raw log-scale): [4.2042, 1.6348, 0.4721, -2.5562, 0.0218, -0.1689, -1.3984]\n",
      "Starting optimization run 1 on device cpu (Hamming, 7-param ST kernel, L-BFGS)...\n",
      "--- Step 1/20 ---\n",
      " Loss: 1.516678 | Max Grad: 2.666972e-05\n",
      "  Params (Raw Log): log_phi1: 4.1939, log_phi2: 1.6314, log_phi3: 0.4976, log_phi4: -3.2157, advec_lat: 0.0274, advec_lon: -0.1753, log_nugget: -1.4295\n",
      "--- Step 2/20 ---\n",
      " Loss: 1.505549 | Max Grad: 2.666972e-05\n",
      "  Params (Raw Log): log_phi1: 4.1939, log_phi2: 1.6314, log_phi3: 0.4976, log_phi4: -3.2157, advec_lat: 0.0274, advec_lon: -0.1753, log_nugget: -1.4295\n",
      "--- Step 3/20 ---\n",
      " Loss: 1.505549 | Max Grad: 2.666972e-05\n",
      "  Params (Raw Log): log_phi1: 4.1939, log_phi2: 1.6314, log_phi3: 0.4976, log_phi4: -3.2157, advec_lat: 0.0274, advec_lon: -0.1753, log_nugget: -1.4295\n",
      "--- Step 4/20 ---\n",
      " Loss: 1.505549 | Max Grad: 2.666972e-05\n",
      "  Params (Raw Log): log_phi1: 4.1939, log_phi2: 1.6314, log_phi3: 0.4976, log_phi4: -3.2157, advec_lat: 0.0274, advec_lon: -0.1753, log_nugget: -1.4295\n",
      "\n",
      "--- Converged on loss change (change < 1e-12) at step 4 ---\n",
      "\n",
      "--- Training Complete ---\n",
      "\n",
      "FINAL BEST STATE ACHIEVED (during training):\n",
      "Best Loss: 1.506\n",
      "\n",
      "\n",
      "========================= Overall Result from Run ========================= =========================\n",
      "Best Run Loss: 1.506 (after 4 steps)\n",
      "Final Parameters (Natural Scale): sigmasq: 12.9674, range_lat: 0.1526, range_lon: 0.1956, range_time: 0.9767, advec_lat: 0.0274, advec_lon: -0.1753, nugget: 0.2394\n",
      "Final Parameters (Phi Scale)    : phi1: 66.2785, phi2: 5.1112, phi3: 1.6448, phi4: 0.0401, advec_lat: 0.0274, advec_lon: -0.1753, nugget: 0.2394\n",
      "Final Parameters (Raw Log Scale): log_phi1: 4.1939, log_phi2: 1.6314, log_phi3: 0.4976, log_phi4: -3.2157, advec_lat: 0.0274, advec_lon: -0.1753, log_nugget: -1.4295\n",
      "\n",
      "Total execution time: 39.10 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dwl = debiased_whittle.debiased_whittle_likelihood()\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    DAY_TO_RUN = 3 # data is decided above\n",
    "    TAPERING_FUNC = dwl.cgn_hamming # Use Hamming taper\n",
    "    NUM_RUNS = 1\n",
    "    MAX_STEPS = 20 # L-BFGS usually converges in far fewer steps\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063 \n",
    "\n",
    "    LAT_COL, LON_COL = 0, 1\n",
    "    VAL_COL = 2 # Spatially differenced value\n",
    "    TIME_COL = 3\n",
    "\n",
    "\n",
    "    cur_df =subsetted_aggregated_day\n",
    "    \n",
    "    if cur_df.numel() == 0 or cur_df.shape[1] <= max(LAT_COL, LON_COL, VAL_COL, TIME_COL):\n",
    "        print(f\"Error: Data for Day {DAY_TO_RUN} is empty or invalid.\")\n",
    "        exit()\n",
    "\n",
    "    unique_times = torch.unique(cur_df[:, TIME_COL])\n",
    "    time_slices_list = [cur_df[cur_df[:, TIME_COL] == t_val] for t_val in unique_times]\n",
    "\n",
    "    # --- 1. Pre-compute J-vector, Taper Grid, and Taper Autocorrelation ---\n",
    "    print(\"Pre-computing J-vector (Hamming taper)...\")\n",
    "    \n",
    "    # --- 💥 REVISED: Renamed 'p' to 'p_time' 💥 ---\n",
    "    J_vec, n1, n2, p_time, taper_grid = dwl.generate_Jvector_tapered( \n",
    "        time_slices_list,\n",
    "        tapering_func=TAPERING_FUNC, \n",
    "        lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if J_vec is None or J_vec.numel() == 0 or n1 == 0 or n2 == 0 or p_time == 0:\n",
    "       print(f\"Error: J-vector generation failed for Day {DAY_TO_RUN}.\")\n",
    "       exit()\n",
    "       \n",
    "    print(\"Pre-computing sample periodogram...\")\n",
    "    I_sample = dwl.calculate_sample_periodogram_vectorized(J_vec)\n",
    "\n",
    "    print(\"Pre-computing Hamming taper autocorrelation...\")\n",
    "    taper_autocorr_grid = dwl.calculate_taper_autocorrelation_fft(taper_grid, n1, n2, DEVICE)\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Error: NaN/Inf in sample periodogram.\")\n",
    "        exit()\n",
    "    if torch.isnan(taper_autocorr_grid).any() or torch.isinf(taper_autocorr_grid).any():\n",
    "        print(\"Error: NaN/Inf in taper autocorrelation.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Data grid: {n1}x{n2}, {p_time} time points. J-vector, Periodogram, Taper Autocorr on {DEVICE}.\")\n",
    "    # --- END REVISION ---\n",
    "\n",
    "    # --- 2. Optimization Loop ---\n",
    "    all_final_results = []\n",
    "    all_final_losses = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        print(f\"\\n{'='*30} Initialization Run {i+1}/{NUM_RUNS} {'='*30}\")\n",
    "\n",
    "        # --- 7-PARAMETER initialization ---\n",
    "        ''' \n",
    "        init_sigmasq   = 15.0\n",
    "        init_range_lat = 0.66 \n",
    "        init_range_lon = 0.7 \n",
    "        init_nugget    = 1.5\n",
    "        init_beta      = 0.1  # Temporal range ratio\n",
    "        init_advec_lat = 0.02\n",
    "        init_advec_lon = -0.08\n",
    "        '''\n",
    "        init_sigmasq   = 13.059\n",
    "        init_range_lat = 0.154 \n",
    "        init_range_lon = 0.195\n",
    "        init_advec_lat = 0.0218\n",
    "        init_range_time = 0.7\n",
    "        init_advec_lon = -0.1689\n",
    "        init_nugget    = 0.247\n",
    "\n",
    "        init_phi2 = 1.0 / init_range_lon\n",
    "        init_phi1 = init_sigmasq * init_phi2\n",
    "        init_phi3 = (init_range_lon / init_range_lat)**2\n",
    "        # Change needed to match the spatial-temporal distance formula:\n",
    "        init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "        initial_params_values = [\n",
    "            np.log(init_phi1),    # [0] log_phi1\n",
    "            np.log(init_phi2),    # [1] log_phi2\n",
    "            np.log(init_phi3),    # [2] log_phi3\n",
    "            np.log(init_phi4),    # [3] log_phi4\n",
    "            init_advec_lat,       # [4] advec_lat (NOT log)\n",
    "            init_advec_lon,       # [5] advec_lon (NOT log)\n",
    "            np.log(init_nugget)   # [6] log_nugget\n",
    "        ]\n",
    "        \n",
    "        print(f\"Starting with FIXED params (raw log-scale): {[round(p, 4) for p in initial_params_values]}\")\n",
    "\n",
    "        params_list = [\n",
    "            Parameter(torch.tensor([val], dtype=torch.float64))\n",
    "            for val in initial_params_values\n",
    "        ]\n",
    "\n",
    "        # Helper to define the boundary globally for clarity\n",
    "        NUGGET_LOWER_BOUND = 0.05\n",
    "        LOG_NUGGET_LOWER_BOUND = np.log(NUGGET_LOWER_BOUND) # Approx -2.9957\n",
    "\n",
    "        # --- 💥 REVISED: Use L-BFGS Optimizer 💥 ---\n",
    "        optimizer = torch.optim.LBFGS(\n",
    "            params_list,\n",
    "            lr=1.0,           # Initial step length for line search\n",
    "            max_iter=20,      # Iterations per step\n",
    "            history_size=100,\n",
    "            line_search_fn=\"strong_wolfe\", # Often more robust\n",
    "            tolerance_grad=1e-5\n",
    "        )\n",
    "        # --- END REVISION ---\n",
    "\n",
    "        print(f\"Starting optimization run {i+1} on device {DEVICE} (Hamming, 7-param ST kernel, L-BFGS)...\")\n",
    "        \n",
    "        # --- 💥 REVISED: Call L-BFGS trainer, pass p_time 💥 ---\n",
    "        nat_params_str, phi_params_str, raw_params_str, loss, steps_run = dwl.run_lbfgs_tapered(\n",
    "            params_list=params_list,\n",
    "            optimizer=optimizer,\n",
    "            I_sample=I_sample,\n",
    "            n1=n1, n2=n2, p_time=p_time,\n",
    "            taper_autocorr_grid=taper_autocorr_grid, \n",
    "            max_steps=MAX_STEPS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        # --- END REVISION ---\n",
    "        \n",
    "        if loss is not None:\n",
    "            all_final_results.append((nat_params_str, phi_params_str, raw_params_str))\n",
    "            all_final_losses.append(loss)\n",
    "        else:\n",
    "            all_final_losses.append(float('inf'))\n",
    "\n",
    "    print(f\"\\n\\n{'='*25} Overall Result from Run {'='*25} {'='*25}\")\n",
    "    \n",
    "    valid_losses = [l for l in all_final_losses if l is not None and l != float('inf')]\n",
    "\n",
    "    if not valid_losses:\n",
    "        print(f\"The run failed or resulted in an invalid loss for Day {DAY_TO_RUN}.\")\n",
    "    else:\n",
    "        best_loss = min(valid_losses)\n",
    "        best_run_index = all_final_losses.index(best_loss)\n",
    "        best_results = all_final_results[best_run_index]\n",
    "        \n",
    "        print(f\"Best Run Loss: {best_loss} (after {steps_run} steps)\")\n",
    "        print(f\"Final Parameters (Natural Scale): {best_results[0]}\")\n",
    "        print(f\"Final Parameters (Phi Scale)    : {best_results[1]}\")\n",
    "        print(f\"Final Parameters (Raw Log Scale): {best_results[2]}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gems_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
