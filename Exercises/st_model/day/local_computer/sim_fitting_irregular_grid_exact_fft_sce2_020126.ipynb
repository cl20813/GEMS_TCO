{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f33ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reflected location error in ozone data simulation\n",
    "\n",
    "import torch\n",
    "import torch.fft\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import argparse \n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "from sklearn.neighbors import BallTree\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CUSTOM PATHS ---\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "# (ÌïÑÏöî Ïãú Ïã§Ï†ú GEMS_TCO ÎùºÏù¥Î∏åÎü¨Î¶¨ import)\n",
    "try:\n",
    "    from GEMS_TCO import kernels_reparam_space_time_gpu\n",
    "    from GEMS_TCO import kernels_reparam_space_time_gpu_copy_dummy_013126 as kernels_reparam_space_time_gpu_copy\n",
    "    from GEMS_TCO import kernels_columns as kernels_reparam_space_time_gpu_col\n",
    "    \n",
    "    from GEMS_TCO import orderings as _orderings\n",
    "    from GEMS_TCO import alg_optimization, BaseLogger\n",
    "\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Warning: GEMS_TCO modules not found. Ensure paths are correct.\")\n",
    "\n",
    "\n",
    "from GEMS_TCO import configuration as config\n",
    "from GEMS_TCO.data_loader import load_data2, exact_location_filter\n",
    "from GEMS_TCO import debiased_whittle\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d6ba3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22680, 4])\n"
     ]
    }
   ],
   "source": [
    "space: List[str] = ['1', '1']\n",
    "lat_lon_resolution = [int(s) for s in space]\n",
    "mm_cond_number: int = 8\n",
    "years = ['2024']\n",
    "month_range = [7] \n",
    "\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "data_load_instance = load_data2(config.mac_data_load_path)\n",
    "\n",
    "#lat_range_input = [1, 3]\n",
    "#lon_range_input = [125.0, 129.0]\n",
    "\n",
    "lat_range_input=[-3,-1]      \n",
    "lon_range_input=[121, 125] \n",
    "\n",
    "#lat_range_input=[-3,2]      \n",
    "#lon_range_input=[121, 131] \n",
    "\n",
    "df_map, ord_mm, nns_map = data_load_instance.load_maxmin_ordered_data_bymonthyear(\n",
    "lat_lon_resolution=lat_lon_resolution, \n",
    "mm_cond_number=mm_cond_number,\n",
    "years_=years, \n",
    "months_=month_range,\n",
    "\n",
    "lat_range=lat_range_input,   \n",
    "lon_range=lon_range_input\n",
    "  \n",
    ")\n",
    "\n",
    "\n",
    "daily_aggregated_reg_vecc_sim = [] \n",
    "daily_hourly_maps_reg_vecc_sim = []      \n",
    "\n",
    "daily_aggregated_irr_vecc_sim = [] \n",
    "daily_hourly_maps_irr_vecc_sim = []   \n",
    "\n",
    "\n",
    "for day_index in range(31):\n",
    "    hour_start_index = day_index * 8\n",
    "    \n",
    "    hour_end_index = (day_index + 1) * 8\n",
    "    #hour_end_index = day_index*8 + 1\n",
    "    hour_indices = [hour_start_index, hour_end_index]\n",
    "\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "    df_map, \n",
    "    hour_indices, \n",
    "    ord_mm= ord_mm,  # or just omit it\n",
    "    dtype=torch.float64, # or just omit it \n",
    "    keep_ori=False  #keep_exact_loc\n",
    "    )\n",
    "\n",
    "    daily_aggregated_reg_vecc_sim.append( day_aggregated_tensor )\n",
    "    daily_hourly_maps_reg_vecc_sim.append( day_hourly_map )\n",
    "\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "    df_map, \n",
    "    hour_indices, \n",
    "    ord_mm= ord_mm,  # or just omit it\n",
    "    dtype=torch.float64, # or just omit it \n",
    "    keep_ori=True  #keep_exact_loc\n",
    "    )\n",
    "\n",
    "    daily_aggregated_irr_vecc_sim.append( day_aggregated_tensor )\n",
    "    daily_hourly_maps_irr_vecc_sim.append( day_hourly_map )\n",
    "print(daily_aggregated_irr_vecc_sim[0].shape)\n",
    "\n",
    "nn = daily_aggregated_irr_vecc_sim[0].shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd23747a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Inputs:\n",
      " -> Target SigmaSq: 13.059\n",
      "   [High-Res Gen] 160x/4x Field Created.\n",
      "   [Calibration] Raw Std: 3.4647 -> Fixed to: 3.6137\n",
      "\n",
      "[Final Validation]\n",
      "Vecc Shape (Irregular): torch.Size([22680, 4])\n",
      "DW Shape   (Regular):   torch.Size([22680, 4])\n",
      "Variance Check: 13.2800 (Expected ~13.0 + Nugget)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "import sys\n",
    "\n",
    "\n",
    "# --- 2. EXACT COVARIANCE & FFT HELPERS ---\n",
    "def get_model_covariance_on_grid(lags_x, lags_y, lags_t, params):\n",
    "    phi1, phi2, phi3, phi4 = torch.exp(params[0]), torch.exp(params[1]), torch.exp(params[2]), torch.exp(params[3])\n",
    "    advec_lat, advec_lon = params[4], params[5]\n",
    "    sigmasq = phi1 / phi2\n",
    "\n",
    "    u_lat_eff = lags_x - advec_lat * lags_t\n",
    "    u_lon_eff = lags_y - advec_lon * lags_t\n",
    "    dist_sq = (u_lat_eff.pow(2) * phi3) + (u_lon_eff.pow(2)) + (lags_t.pow(2) * phi4)\n",
    "    return sigmasq * torch.exp(-torch.sqrt(dist_sq + 1e-8) * phi2)\n",
    "\n",
    "\n",
    "def make_target_grid(lat_start, lat_end, lat_step, lon_start, lon_end, lon_step, device, dtype):\n",
    "    \"\"\"\n",
    "    ÌëúÏ§Ä Î∂ÑÏÑùÏö© Î†àÍ∑§Îü¨ Í∑∏Î¶¨Îìú ÏÉùÏÑ±. \n",
    "    ÏûêÎèô Î∞©Ìñ• Ï†úÏñ¥(ÏïÑÎûò Î≤ÑÏ†Ñ Ïû•Ï†ê) + ÏÜåÏàòÏ†ê Î∞òÏò¨Î¶º Ïò§Ï∞® Î∞©ÏßÄ(ÏúÑ Î≤ÑÏ†Ñ Ïû•Ï†ê) ÌÜµÌï©.\n",
    "    \"\"\"\n",
    "    # 1. Î∞©Ìñ• ÏûêÎèô ÍµêÏ†ï (Step Î∞©Ìñ•Ïù¥ ÏãúÏûë/ÎÅùÏ†êÍ≥º ÎßûÏßÄ ÏïäÏùÑ Í≤ΩÏö∞)\n",
    "    if lat_start > lat_end and lat_step > 0: \n",
    "        lat_step = -lat_step\n",
    "    if lon_start > lon_end and lon_step > 0:\n",
    "        lon_step = -lon_step\n",
    "\n",
    "    # 2. Arange ÏÉùÏÑ±\n",
    "    lats = torch.arange(lat_start, lat_end - 0.0001, lat_step, device=device, dtype=dtype)\n",
    "    lons = torch.arange(lon_start, lon_end + 0.0001, lon_step, device=device, dtype=dtype)\n",
    "\n",
    "    # 3. ÏàòÏπòÏ†Å ÏïàÏ†ïÏÑ±ÏùÑ ÏúÑÌïú Rounding (ÏÜåÏàòÏ†ê 4ÏûêÎ¶¨)\n",
    "    # Ïù¥ Í≥ºÏ†ïÏù¥ ÏóÜÏúºÎ©¥ ÎÇòÏ§ëÏóê Î™®Îç∏ ÌîºÌåÖ Ïãú Ï¢åÌëú ÎØ∏ÏÑ∏ Ïò§Ï∞®Î°ú Ïù∏Ìï¥ Í≤∞Í≥ºÍ∞Ä Îã¨ÎùºÏßà Ïàò ÏûàÏäµÎãàÎã§.\n",
    "    lats = torch.round(lats * 10000) / 10000\n",
    "    lons = torch.round(lons * 10000) / 10000\n",
    "\n",
    "    # 4. Meshgrid ÏÉùÏÑ±\n",
    "    grid_lat, grid_lon = torch.meshgrid(lats, lons, indexing='ij')\n",
    "    center_points = torch.stack([grid_lat.flatten(), grid_lon.flatten()], dim=1)\n",
    "    \n",
    "    return center_points, len(lats), len(lons)\n",
    "\n",
    "# --- 2. High-Res Field Generator (Lat x160 / Lon x4) ---\n",
    "def generate_high_res_field(target_lat_range, target_lon_range, t_steps, params, device, dtype):\n",
    "    \"\"\"\n",
    "    Ï¥àÍ≥†Ìï¥ÏÉÅÎèÑ(160Î∞∞/4Î∞∞) ÌïÑÎìúÎ•º ÏÉùÏÑ±ÌïòÍ≥†, Î∂ÑÏÇ∞ÏùÑ Ï∞∏Í∞í(Target)Ïóê ÎßûÍ≤å Î≥¥Ï†ï(Calibration)Ìï©ÎãàÎã§.\n",
    "    \"\"\"\n",
    "    # [ÏÑ§Ï†ï] Ìï¥ÏÉÅÎèÑ ÌôïÎåÄ Î∞∞Ïú®\n",
    "    lat_res_factor = 160.0  # ÏúÑÎèÑ 160Î∞∞\n",
    "    lon_res_factor = 4.0    # Í≤ΩÎèÑ 4Î∞∞\n",
    "    \n",
    "    # Í∏∞Î≥∏ Ìï¥ÏÉÅÎèÑ(0.044, 0.063)Î•º Î∞∞Ïú®Î°ú ÎÇòÎàî\n",
    "    lat_res_high = 0.044 / lat_res_factor\n",
    "    lon_res_high = 0.063 / lon_res_factor\n",
    "    \n",
    "    t_lat_max = max(target_lat_range)\n",
    "    t_lat_min = min(target_lat_range)\n",
    "    \n",
    "    # Padding: Í≤ΩÍ≥ÑÎ©¥ ÏôúÍ≥° Î∞©ÏßÄÎ•º ÏúÑÌï¥ ÏïΩÍ∞Ñ ÎÑìÍ≤å ÏÉùÏÑ±\n",
    "    lats_high = torch.arange(t_lat_max + 0.1, t_lat_min - 0.1, -lat_res_high, device=device, dtype=dtype)\n",
    "    lons_high = torch.arange(target_lon_range[0] - 0.1, target_lon_range[1] + 0.1, lon_res_high, device=device, dtype=dtype)\n",
    "    \n",
    "    Nx, Ny, Nt = len(lats_high), len(lons_high), t_steps\n",
    "    dlat, dlon, dt = lat_res_high, lon_res_high, 1.0\n",
    "    \n",
    "    # FFT Grid Setup (Circulant Embedding)\n",
    "    Px, Py, Pt = 2*Nx, 2*Ny, 2*Nt\n",
    "    lags_x = torch.arange(Px, device=device, dtype=dtype) * dlat; lags_x[Px//2:] -= (Px * dlat)\n",
    "    lags_y = torch.arange(Py, device=device, dtype=dtype) * dlon; lags_y[Py//2:] -= (Py * dlon)\n",
    "    lags_t = torch.arange(Pt, device=device, dtype=dtype) * dt;   lags_t[Pt//2:] -= (Pt * dt)\n",
    "\n",
    "    L_x, L_y, L_t = torch.meshgrid(lags_x, lags_y, lags_t, indexing='ij')\n",
    "    \n",
    "    # ÌååÎùºÎØ∏ÌÑ∞ Ìï¥ÏÑù\n",
    "    phi1, phi2 = torch.exp(params[0]), torch.exp(params[1])\n",
    "    phi3, phi4 = torch.exp(params[2]), torch.exp(params[3])\n",
    "    adv_lat, adv_lon = params[4], params[5]\n",
    "    \n",
    "    sigma_sq = phi1 / phi2  # Ï∞∏Í∞í Î∂ÑÏÇ∞ (13.059)\n",
    "    \n",
    "    u_x = L_x - adv_lat * L_t\n",
    "    u_y = L_y - adv_lon * L_t\n",
    "    dist_sq = (u_x * torch.sqrt(phi3) * phi2)**2 + (u_y * phi2)**2 + (L_t * torch.sqrt(phi4) * phi2)**2\n",
    "    C_vals = sigma_sq * torch.exp(-torch.sqrt(dist_sq + 1e-12))\n",
    "\n",
    "    # FFT Simulation\n",
    "    S = torch.fft.fftn(C_vals); S.real = torch.clamp(S.real, min=0)\n",
    "    random_phase = torch.fft.fftn(torch.randn(Px, Py, Pt, device=device, dtype=dtype))\n",
    "    field_sim_raw = torch.fft.ifftn(torch.sqrt(S.real) * random_phase).real\n",
    "    \n",
    "    # Crop\n",
    "    field_sim = field_sim_raw[:Nx, :Ny, :Nt]\n",
    "    \n",
    "    # [Calibration] 160Î∞∞ Ìï¥ÏÉÅÎèÑÎ°ú Ïù∏Ìï¥ Ìù©Ïñ¥ÏßÑ ÏóêÎÑàÏßÄÎ•º Î™®ÏïÑ Î∂ÑÏÇ∞ÏùÑ Î≥µÏõê\n",
    "    current_std = field_sim.std()\n",
    "    target_std = torch.sqrt(sigma_sq)\n",
    "    \n",
    "    # (x - mean) * ratio\n",
    "    field_calibrated = (field_sim - field_sim.mean()) * (target_std / (current_std + 1e-9))\n",
    "    \n",
    "    print(f\"   [High-Res Gen] 160x/4x Field Created.\")\n",
    "    print(f\"   [Calibration] Raw Std: {current_std.item():.4f} -> Fixed to: {target_std.item():.4f}\")\n",
    "    \n",
    "    return field_calibrated, lats_high, lons_high\n",
    "\n",
    "# --- 3. Main Generator (Vecc & DW) ---\n",
    "# ==============================================================================\n",
    "# 1. ÏàòÏ†ïÎêú Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ±Í∏∞ (Size Mismatch Ìï¥Í≤∞ Î≤ÑÏ†Ñ)\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_exact_count_datasets_fixed(daily_maps_real, true_params_tensor, target_grid_info, device, dtype):\n",
    "    \"\"\"\n",
    "    Step 1ÏóêÏÑú Î°úÎìúÎêú 22,680Í∞úÏùò Íµ¨Ï°∞Î•º Í∑∏ÎåÄÎ°ú Ïú†ÏßÄÌïòÎ©∞ Í∞íÎßå ÏãúÎÆ¨Î†àÏù¥ÏÖòÏúºÎ°ú Ï±ÑÏõÅÎãàÎã§.\n",
    "    \"\"\"\n",
    "    lat_s, lat_e, lat_step, lon_s, lon_e, lon_step = target_grid_info\n",
    "    \n",
    "    # [A] ÌÉÄÍ≤ü Í≤©Ïûê Ï§ÄÎπÑ\n",
    "    target_grid_coords, Nx, Ny = make_target_grid(lat_s, lat_e, lat_step, lon_s, lon_e, lon_step, device, dtype)\n",
    "    target_tree = BallTree(np.radians(target_grid_coords.cpu().numpy()), metric='haversine') \n",
    "    \n",
    "    # [B] High-Res Field Ï§ÄÎπÑ\n",
    "    high_res_field, lats_high, lons_high = generate_high_res_field((lat_s, lat_e), (lon_s, lon_e), 8, true_params_tensor, device, dtype)\n",
    "    hr_mesh_lat, hr_mesh_lon = torch.meshgrid(lats_high, lons_high, indexing='ij')\n",
    "    hr_tree = BallTree(np.radians(torch.stack([hr_mesh_lat.flatten(), hr_mesh_lon.flatten()], dim=1).cpu().numpy()), metric='haversine')\n",
    "    high_res_flat = high_res_field.reshape(-1, 8) \n",
    "\n",
    "    irr_map_dict, dw_map_dict = {}, {}\n",
    "    irr_list, dw_list = [], []\n",
    "    noise_std = torch.sqrt(torch.exp(true_params_tensor[6]))\n",
    "\n",
    "    # [C] Îç∞Ïù¥ÌÑ∞ Îß§Ìïë (Íµ¨Ï°∞ Î≥¥Ï°¥)\n",
    "    # real_maps_list[0] (Î°úÎìúÎêú 22680Í∞ú Îç∞Ïù¥ÌÑ∞) ÏÇ¨Ïö©\n",
    "    day0_dict = daily_maps_real[0]\n",
    "    sorted_keys = sorted([k for k in day0_dict.keys() if 'hm' in k or 'time' in k])\n",
    "    \n",
    "    for t_idx, key in enumerate(sorted_keys):\n",
    "        if t_idx >= 8: break\n",
    "        real_tensor = day0_dict[key].to(device)\n",
    "        \n",
    "        # ------------------------------------------------------------------\n",
    "        # [ÏàòÏ†ï ÌïµÏã¨] clean_tensor Î°úÏßÅ Ï†úÍ±∞ -> ÏõêÎ≥∏ real_tensor(2835Í∞ú/ÏãúÍ∞Ñ) Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©\n",
    "        # ------------------------------------------------------------------\n",
    "        clean_tensor = real_tensor \n",
    "        real_locs = clean_tensor[:, :2]\n",
    "        \n",
    "        # High-Res FieldÏóêÏÑú Í∞í ÏÉòÌîåÎßÅ\n",
    "        _, hr_indices = hr_tree.query(np.radians(real_locs.cpu().numpy()), k=1)\n",
    "        signal = high_res_flat[torch.tensor(hr_indices.flatten(), device=device), t_idx]\n",
    "        sim_vals = signal + (torch.randn_like(signal) * noise_std) + 260.0\n",
    "        \n",
    "        # 1. Vecc (Irregular): ÏõêÎ≥∏ ÏúÑÏπò Ïú†ÏßÄ\n",
    "        vecc_row = torch.stack([real_locs[:,0], real_locs[:,1], sim_vals, clean_tensor[:,3]], dim=1).detach()\n",
    "        irr_map_dict[key] = vecc_row\n",
    "        irr_list.append(vecc_row)\n",
    "        \n",
    "        # 2. DW (Regular): Í≤©Ïûê Ï§ëÏïôÏúºÎ°ú Îß§Ìïë\n",
    "        # Ïó¨Í∏∞ÏÑú target_grid_coordsÏôÄ real_locsÏùò Í∞úÏàòÍ∞Ä 1:1 ÎåÄÏùëÎêòÏñ¥Ïïº Ìï® (2835Í∞ú)\n",
    "        # ÎßåÏïΩ target_grid_coordsÍ∞Ä Îçî ÎßéÎã§Î©¥ Ïä¨ÎùºÏù¥Ïã± Ï≤òÎ¶¨\n",
    "        current_grid_points = target_grid_coords[:len(clean_tensor)]\n",
    "        \n",
    "        dw_row = torch.stack([current_grid_points[:,0], current_grid_points[:,1], sim_vals, clean_tensor[:,3]], dim=1).detach()\n",
    "        dw_map_dict[key] = dw_row\n",
    "        dw_list.append(dw_row)\n",
    "\n",
    "    # 2835 * 8 = 22680 Ïú†ÏßÄ ÌôïÏù∏\n",
    "    return [torch.cat(dw_list)], [dw_map_dict], [torch.cat(irr_list)], [irr_map_dict]\n",
    "# --- 4. EXECUTION ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.float64\n",
    "\n",
    "target_grid_info = (-1.0, -3.0, 0.044, 121.0, 125.0, 0.063)\n",
    "\n",
    "# 1. Ï∞∏Í∞í Ï†ïÏùò\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lon = 0.195 \n",
    "init_range_lat = 0.154 \n",
    "init_range_time = 1.0\n",
    "init_nugget    = 0.247\n",
    "\n",
    "# 2. ÌååÎùºÎØ∏ÌÑ∞ Î≥ÄÌôò (Ï†ïÎãµ Î°úÏßÅ)\n",
    "true_phi2 = 1.0 / init_range_lon              \n",
    "true_phi1 = init_sigmasq * true_phi2          \n",
    "true_phi3 = (init_range_lon / init_range_lat)**2\n",
    "true_phi4 = (init_range_lon / init_range_time)**2\n",
    "\n",
    "true_params_tensor = [\n",
    "    torch.tensor([np.log(true_phi1)], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([np.log(true_phi2)], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([np.log(true_phi3)], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([np.log(true_phi4)], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([0.0418], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([-0.1689], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([np.log(init_nugget)], device=DEVICE, dtype=DTYPE)\n",
    "]\n",
    "\n",
    "print(f\"Checking Inputs:\")\n",
    "print(f\" -> Target SigmaSq: {init_sigmasq}\")\n",
    "\n",
    "if 'daily_hourly_maps_irr_vecc_sim' in locals() and len(daily_hourly_maps_irr_vecc_sim) > 0:\n",
    "    (daily_aggregated_reg_vecc_sim, daily_hourly_maps_reg_vecc_sim,\n",
    "     daily_aggregated_irr_vecc_sim, daily_hourly_maps_irr_vecc_sim) = generate_exact_count_datasets_fixed(\n",
    "        daily_hourly_maps_irr_vecc_sim, true_params_tensor, target_grid_info, DEVICE, DTYPE\n",
    "    )\n",
    "    \n",
    "    # Í≤ÄÏ¶ù: Vecc vs DW\n",
    "    v_agg = daily_aggregated_irr_vecc_sim[0]\n",
    "    dw_agg = daily_aggregated_reg_vecc_sim[0]\n",
    "    \n",
    "    print(f\"\\n[Final Validation]\")\n",
    "    print(f\"Vecc Shape (Irregular): {v_agg.shape}\")\n",
    "    print(f\"DW Shape   (Regular):   {dw_agg.shape}\")\n",
    "    print(f\"Variance Check: {v_agg[:, 2].var().item():.4f} (Expected ~13.0 + Nugget)\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: Load data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87cb81b",
   "metadata": {},
   "source": [
    "dummy time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e15c017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Datasets (12 Cols: Lat, Lon, Val, Time, H0..H7) ---\n",
      "  Processed Day 5\n",
      "  Processed Day 10\n",
      "  Processed Day 15\n",
      "  Processed Day 20\n",
      "  Processed Day 25\n",
      "  Processed Day 30\n",
      "\n",
      "[Verification]\n",
      "Dataset Shape: torch.Size([22680, 12])\n",
      "‚úÖ Success: Output has 12 columns (Lat, Lon, Val, Time, + 8 Dummies)\n",
      "Sample Row: \n",
      "tensor([ -1.9987, 122.9810, 258.8952,  21.0000,   1.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "import time\n",
    "import sys\n",
    "import typer\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def make_target_grid(lat_start, lat_end, lat_step, lon_start, lon_end, lon_step, device, dtype):\n",
    "    \"\"\"Target Regular Grid ÏÉùÏÑ±\"\"\"\n",
    "    if lat_start > lat_end and lat_step > 0:\n",
    "        lat_step = -lat_step\n",
    "        \n",
    "    lats = torch.arange(lat_start, lat_end - 0.0001, lat_step, device=device, dtype=dtype)\n",
    "    lons = torch.arange(lon_start, lon_end - 0.0001, lon_step, device=device, dtype=dtype)\n",
    "    \n",
    "    grid_lat, grid_lon = torch.meshgrid(lats, lons, indexing='ij')\n",
    "    center_points = torch.stack([grid_lat.flatten(), grid_lon.flatten()], dim=1)\n",
    "    \n",
    "    return center_points, len(lats), len(lons)\n",
    "\n",
    "def generate_high_res_field(target_lat_range, target_lon_range, t_steps, params, device, dtype):\n",
    "    \"\"\"3Î∞∞ Í≥†Ìï¥ÏÉÅÎèÑ True Field ÏÉùÏÑ±\"\"\"\n",
    "    res_factor = 3.0\n",
    "    lat_res_high = 0.044 / res_factor\n",
    "    lon_res_high = 0.063 / res_factor\n",
    "    \n",
    "    pad = 0.2\n",
    "    lats_high = torch.arange(target_lat_range[0] + pad, target_lat_range[1] - pad, -lat_res_high, device=device, dtype=dtype)\n",
    "    lons_high = torch.arange(target_lon_range[0] - pad, target_lon_range[1] + pad, lon_res_high, device=device, dtype=dtype)\n",
    "    \n",
    "    Nx, Ny, Nt = len(lats_high), len(lons_high), t_steps\n",
    "    dlat, dlon, dt = lat_res_high, lon_res_high, 1.0\n",
    "    \n",
    "    Px, Py, Pt = 2*Nx, 2*Ny, 2*Nt\n",
    "    lags_x = torch.arange(Px, device=device, dtype=dtype) * dlat\n",
    "    lags_x[Px//2:] -= (Px * dlat)\n",
    "    lags_y = torch.arange(Py, device=device, dtype=dtype) * dlon\n",
    "    lags_y[Py//2:] -= (Py * dlon)\n",
    "    lags_t = torch.arange(Pt, device=device, dtype=dtype) * dt\n",
    "    lags_t[Pt//2:] -= (Pt * dt)\n",
    "\n",
    "    L_x, L_y, L_t = torch.meshgrid(lags_x, lags_y, lags_t, indexing='ij')\n",
    "\n",
    "    phi1, phi2 = torch.exp(params[0]), torch.exp(params[1])\n",
    "    phi3, phi4 = torch.exp(params[2]), torch.exp(params[3])\n",
    "    adv_lat, adv_lon = params[4], params[5]\n",
    "    sigmasq = phi1 / phi2\n",
    "    \n",
    "    u_x = L_x - adv_lat * L_t\n",
    "    u_y = L_y - adv_lon * L_t\n",
    "    dist_sq = (u_x * torch.sqrt(phi3) * phi2)**2 + (u_y * phi2)**2 + (L_t * torch.sqrt(phi4) * phi2)**2\n",
    "    C_vals = sigmasq * torch.exp(-torch.sqrt(dist_sq + 1e-12))\n",
    "\n",
    "    S = torch.fft.fftn(C_vals)\n",
    "    S.real = torch.clamp(S.real, min=0)\n",
    "    random_phase = torch.fft.fftn(torch.randn(Px, Py, Pt, device=device, dtype=dtype))\n",
    "    field_sim = torch.fft.ifftn(torch.sqrt(S.real) * random_phase).real\n",
    "    \n",
    "    return field_sim[:Nx, :Ny, :Nt], lats_high, lons_high\n",
    "\n",
    "# --- Core Generator (12 Columns Logic) ---\n",
    "\n",
    "def generate_exact_count_datasets(\n",
    "    daily_maps_real,    \n",
    "    true_params_tensor, \n",
    "    target_grid_info,   \n",
    "    device, dtype\n",
    "):\n",
    "    print(\"--- Generating Datasets (12 Cols: Lat, Lon, Val, Time, H0..H7) ---\")\n",
    "    \n",
    "    lat_s, lat_e, lat_step, lon_s, lon_e, lon_step = target_grid_info\n",
    "    \n",
    "    # 1. Target Grid Coords\n",
    "    target_grid_coords, _, _ = make_target_grid(\n",
    "        lat_s, lat_e, lat_step, lon_s, lon_e, lon_step, device, dtype\n",
    "    )\n",
    "    \n",
    "    target_locs_np = target_grid_coords.cpu().numpy()\n",
    "    target_locs_rad = np.radians(target_locs_np)\n",
    "    \n",
    "    def normalize(val, min_v, max_v):\n",
    "        return 2 * (val - min_v) / (max_v - min_v) - 1\n",
    "\n",
    "    daily_aggregated_tensors_vecc_sim = []\n",
    "    daily_hourly_maps_vecc_sim = []\n",
    "    daily_aggregated_tensors_dw_sim = []\n",
    "    daily_hourly_maps_dw_sim = []\n",
    "    \n",
    "    # --- Day Loop ---\n",
    "    for day_idx, day_map_real in enumerate(daily_maps_real):\n",
    "        \n",
    "        # (A) High-Res Truth\n",
    "        high_res_field, lats_high, lons_high = generate_high_res_field(\n",
    "            (lat_s, lat_e), (lon_s, lon_e), 8, true_params_tensor, device, dtype\n",
    "        )\n",
    "        \n",
    "        lat_max_b, lat_min_b = lats_high[0], lats_high[-1]\n",
    "        lon_min_b, lon_max_b = lons_high[0], lons_high[-1]\n",
    "        \n",
    "        # (B) Hour Loop\n",
    "        sorted_keys = sorted([k for k in day_map_real.keys() if 'hm' in k or 'time' in k])\n",
    "        \n",
    "        current_day_vecc_map = {}\n",
    "        current_day_dw_map = {}\n",
    "        \n",
    "        vecc_tensor_list = []\n",
    "        dw_tensor_list = []\n",
    "        \n",
    "        for t_idx, key in enumerate(sorted_keys):\n",
    "            if t_idx >= 8: break\n",
    "            \n",
    "            # 1. Extract Real Info\n",
    "            real_tensor = day_map_real[key]\n",
    "            if not isinstance(real_tensor, torch.Tensor):\n",
    "                real_tensor = torch.tensor(real_tensor, device=device, dtype=dtype)\n",
    "            else:\n",
    "                real_tensor = real_tensor.to(device)\n",
    "            \n",
    "            real_lats = real_tensor[:, 0]\n",
    "            real_lons = real_tensor[:, 1]\n",
    "            time_val_continuous = real_tensor[:, 3] # [N] (e.g., 21.0, 22.0)\n",
    "            \n",
    "            N_points = real_lats.shape[0]\n",
    "            \n",
    "            # [NEW] Create 8 Dummy Variables\n",
    "            dummy_time = torch.zeros((N_points, 8), device=device, dtype=dtype)\n",
    "            dummy_time[:, t_idx] = 1.0 \n",
    "            \n",
    "            # 2. Sample Values\n",
    "            norm_lons = normalize(real_lons, lon_min_b, lon_max_b)\n",
    "            norm_lats = normalize(real_lats, lat_min_b, lat_max_b)\n",
    "            \n",
    "            grid_sample_input = torch.stack([norm_lons, norm_lats], dim=-1).unsqueeze(0).unsqueeze(0)\n",
    "            field_slice = high_res_field[:, :, t_idx].unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            sim_vals = F.grid_sample(field_slice, grid_sample_input, mode='bilinear', align_corners=True).squeeze()\n",
    "            sim_vals = sim_vals + 260.0 \n",
    "            \n",
    "            # 3. [Set 1: VECC SIM] - Irregular\n",
    "            # Col 0: Lat\n",
    "            # Col 1: Lon\n",
    "            # Col 2: Val\n",
    "            # Col 3: Time (Continuous) -> For Kernel\n",
    "            # Col 4-11: H0~H7 (Dummy) -> For Mean\n",
    "            vecc_row = torch.cat([\n",
    "                real_lats.unsqueeze(1), \n",
    "                real_lons.unsqueeze(1), \n",
    "                sim_vals.unsqueeze(1), \n",
    "                time_val_continuous.unsqueeze(1), \n",
    "                dummy_time\n",
    "            ], dim=1)\n",
    "            \n",
    "            current_day_vecc_map[key] = vecc_row.detach()\n",
    "            vecc_tensor_list.append(vecc_row.detach())\n",
    "            \n",
    "            # 4. [Set 2: DW SIM] - Regular Mapping\n",
    "            real_locs_np = torch.stack([real_lats, real_lons], dim=1).cpu().numpy()\n",
    "            \n",
    "            tree = BallTree(target_locs_rad, metric='haversine') \n",
    "            _, indices = tree.query(np.radians(real_locs_np), k=1)\n",
    "            indices = torch.tensor(indices.flatten(), device=device, dtype=torch.long)\n",
    "            \n",
    "            mapped_lats = target_grid_coords[indices, 0]\n",
    "            mapped_lons = target_grid_coords[indices, 1]\n",
    "            \n",
    "            # Time & Dummies Î≥µÏÇ¨ (MappedÎêú ÏúÑÏπòÏóêÎèÑ ÎèôÏùºÌïòÍ≤å Ï†ÅÏö©)\n",
    "            mapped_time_val = time_val_continuous # ÏãúÍ∞ÑÏùÄ Î™®Îëê Í∞ôÏùå\n",
    "            mapped_dummies = dummy_time # ÎçîÎØ∏ÎèÑ Î™®Îëê Í∞ôÏùå\n",
    "            \n",
    "            dw_row = torch.cat([\n",
    "                mapped_lats.unsqueeze(1), \n",
    "                mapped_lons.unsqueeze(1), \n",
    "                sim_vals.unsqueeze(1), \n",
    "                mapped_time_val.unsqueeze(1),\n",
    "                mapped_dummies\n",
    "            ], dim=1)\n",
    "            \n",
    "            current_day_dw_map[key] = dw_row.detach()\n",
    "            dw_tensor_list.append(dw_row.detach())\n",
    "\n",
    "        # (C) Aggregate\n",
    "        daily_hourly_maps_vecc_sim.append(current_day_vecc_map)\n",
    "        daily_aggregated_tensors_vecc_sim.append(torch.cat(vecc_tensor_list, dim=0))\n",
    "        \n",
    "        daily_hourly_maps_dw_sim.append(current_day_dw_map)\n",
    "        daily_aggregated_tensors_dw_sim.append(torch.cat(dw_tensor_list, dim=0))\n",
    "        \n",
    "        if (day_idx + 1) % 5 == 0:\n",
    "            print(f\"  Processed Day {day_idx+1}\")\n",
    "            \n",
    "    return (daily_aggregated_tensors_dw_sim, daily_hourly_maps_dw_sim,\n",
    "            daily_aggregated_tensors_vecc_sim, daily_hourly_maps_vecc_sim)\n",
    "\n",
    "# --- Execution ---\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.float64\n",
    "\n",
    "target_grid_info = (2.0, -3.0, 0.044, 121.0, 131.0, 0.063)\n",
    "true_params_tensor = [torch.tensor([np.log(10.0)], device=DEVICE, dtype=DTYPE) for _ in range(7)]\n",
    "\n",
    "if 'daily_hourly_maps_vecc' in locals() and len(daily_hourly_maps_vecc) > 0:\n",
    "    \n",
    "    (daily_aggregated_tensors_dw_sim, \n",
    "     daily_hourly_maps_dw_sim,\n",
    "     daily_aggregated_tensors_vecc_sim, \n",
    "     daily_hourly_maps_vecc_sim) = generate_exact_count_datasets(\n",
    "        daily_hourly_maps_vecc,\n",
    "        true_params_tensor,\n",
    "        target_grid_info,\n",
    "        DEVICE, DTYPE\n",
    "    )\n",
    "    \n",
    "    # [Final Verification]\n",
    "    v_tensor = daily_aggregated_tensors_vecc_sim[0]\n",
    "    print(\"\\n[Verification]\")\n",
    "    print(f\"Dataset Shape: {v_tensor.shape}\")\n",
    "    \n",
    "    if v_tensor.shape[1] == 12:\n",
    "        print(\"‚úÖ Success: Output has 12 columns (Lat, Lon, Val, Time, + 8 Dummies)\")\n",
    "        # ÏòàÏãú: 3Î≤àÏß∏ Ïª¨ÎüºÏùÄ Continuous Time, 4Î≤àÏß∏Î∂ÄÌÑ∞Îäî Dummy\n",
    "        print(f\"Sample Row: \\n{v_tensor[0]}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: Expected 12 columns, got {v_tensor.shape[1]}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Error: 'daily_hourly_maps_vecc' is not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cacde8",
   "metadata": {},
   "source": [
    "set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51634ad",
   "metadata": {},
   "source": [
    "Likelihood vs. Truth: A model with the wrong parameters (short range) might produce a higher Vecchia likelihood because it fits the high-frequency noise better, but it will be terrible at prediction (Kriging) away from data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7191bead",
   "metadata": {},
   "source": [
    "# Fit vecchia max min time 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44251197",
   "metadata": {},
   "source": [
    "vecc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b800f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "========================================\n",
      "--- Initializing VecchiaBatched Model ---\n",
      "========================================\n",
      "\n",
      "========================================\n",
      "--- Running L-BFGS Optimization ---\n",
      "========================================\n",
      "üöÄ Pre-computing (Corrected Vectorization)... ‚úÖ Done in 0.1250s. (Heads: 2400, Tails: 20280)\n",
      "--- Starting Batched L-BFGS Optimization (GPU) ---\n",
      "--- Step 1/3 / Loss: 1.179761 ---\n",
      "  Param 0: Value=4.2920, Grad=-4.679390900186407e-06\n",
      "  Param 1: Value=1.6880, Grad=5.122242548370693e-07\n",
      "  Param 2: Value=0.4883, Grad=2.8293805617383684e-06\n",
      "  Param 3: Value=-3.1809, Grad=6.194590688192968e-07\n",
      "  Param 4: Value=-0.0553, Grad=8.05757962505163e-06\n",
      "  Param 5: Value=-0.1472, Grad=-4.232528960437798e-07\n",
      "  Param 6: Value=-1.4506, Grad=-3.062077376579067e-07\n",
      "  Max Abs Grad: 8.057580e-06\n",
      "------------------------------\n",
      "--- Step 2/3 / Loss: 1.177100 ---\n",
      "  Param 0: Value=4.2920, Grad=-4.679390900186407e-06\n",
      "  Param 1: Value=1.6880, Grad=5.122242548370693e-07\n",
      "  Param 2: Value=0.4883, Grad=2.8293805617383684e-06\n",
      "  Param 3: Value=-3.1809, Grad=6.194590688192968e-07\n",
      "  Param 4: Value=-0.0553, Grad=8.05757962505163e-06\n",
      "  Param 5: Value=-0.1472, Grad=-4.232528960437798e-07\n",
      "  Param 6: Value=-1.4506, Grad=-3.062077376579067e-07\n",
      "  Max Abs Grad: 8.057580e-06\n",
      "------------------------------\n",
      "--- Step 3/3 / Loss: 1.177100 ---\n",
      "  Param 0: Value=4.2920, Grad=-4.679390900186407e-06\n",
      "  Param 1: Value=1.6880, Grad=5.122242548370693e-07\n",
      "  Param 2: Value=0.4883, Grad=2.8293805617383684e-06\n",
      "  Param 3: Value=-3.1809, Grad=6.194590688192968e-07\n",
      "  Param 4: Value=-0.0553, Grad=8.05757962505163e-06\n",
      "  Param 5: Value=-0.1472, Grad=-4.232528960437798e-07\n",
      "  Param 6: Value=-1.4506, Grad=-3.062077376579067e-07\n",
      "  Max Abs Grad: 8.057580e-06\n",
      "------------------------------\n",
      "Final Interpretable Params: {'sigma_sq': 13.517238173137711, 'range_lon': 0.18488759598925433, 'range_lat': 0.14483283604465197, 'range_time': 0.9070688958258293, 'advec_lat': -0.05528484014061864, 'advec_lon': -0.14717848664925948, 'nugget': 0.2344294252812365}\n",
      "\n",
      "Optimization finished in 11.00s.\n",
      "Results after 2 steps: [4.291973000420851, 1.6880072278082336, 0.48833565733230533, -3.1809407123461804, -0.05528484014061864, -0.14717848664925948, -1.4506006946818075, 1.1771004234779592]\n",
      "Final Params: [ 4.291973    1.68800723  0.48833566 -3.18094071 -0.05528484 -0.14717849\n",
      " -1.45060069]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "v = 0.5              # Smoothness\n",
    "mm_cond_number = 8   # Neighbors\n",
    "#mm_cond_number = 16   # Neighbors\n",
    "nheads = 300           # 0 = Pure Vecchia\n",
    "lr = 1.0             # LBFGS learning rate\n",
    "LBFGS_MAX_STEPS = 3\n",
    "LBFGS_HISTORY_SIZE = 100 # 100\n",
    "LBFGS_LR = 1.0\n",
    "LBFGS_MAX_EVAL = 30    \n",
    "\n",
    "#DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- 1. SETUP PARAMETERS (List of Scalars) ---\n",
    "# Truth: [4.18, 1.94, 0.24, -3.97, 0.014, -0.20, -0.85]\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lat = 0.154 \n",
    "init_range_lon = 0.195\n",
    "init_advec_lat = 0.0218\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "# Map model parameters to the 'phi' reparameterization\n",
    "init_phi2 = 1.0 / init_range_lon                # 1/range_lon\n",
    "init_phi1 = init_sigmasq * init_phi2            # sigmasq / range_lon\n",
    "init_phi3 = (init_range_lon / init_range_lat)**2  # (range_lon / range_lat)^2\n",
    "init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "# Create Initial Parameters (Float64, Requires Grad)\n",
    "initial_vals = [np.log(init_phi1), np.log(init_phi2), np.log(init_phi3), \n",
    "                np.log(init_phi4), init_advec_lat, init_advec_lon, np.log(init_nugget)]\n",
    "\n",
    "# [4.2042, 1.6348, 0.4721, -3.2695, 0.0218, -0.1689, -1.3984]\n",
    "params_list = [\n",
    "    torch.tensor([val], requires_grad=True, dtype=torch.float64, device=DEVICE)\n",
    "    for val in initial_vals\n",
    "]\n",
    "\n",
    "# --- 2. INSTANTIATE MODEL ---\n",
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'--- Initializing VecchiaBatched Model ---')\n",
    "print(f'{\"=\"*40}')\n",
    "\n",
    "if isinstance(daily_aggregated_irr_vecc_sim, torch.Tensor):\n",
    "    daily_aggregated_irr_vecc_sim = daily_aggregated_irr_vecc_sim.to(DEVICE)\n",
    "\n",
    "# Instantiate\n",
    "model_instance = kernels_reparam_space_time_gpu_copy.fit_vecchia_lbfgs(\n",
    "    smooth=v,\n",
    "    #input_map=daily_hourly_maps_vecc_sim[0],\n",
    "    #aggregated_data= daily_aggregated_tensors_vecc_sim[0],\n",
    "\n",
    "    input_map=daily_hourly_maps_irr_vecc_sim[0],\n",
    "    aggregated_data= daily_aggregated_irr_vecc_sim[0],\n",
    "    nns_map=nns_map,\n",
    "    mm_cond_number=mm_cond_number,\n",
    "    nheads=nheads\n",
    ")\n",
    "\n",
    "'''\n",
    "model_instance = kernels_reparam_space_time_gpu_col.fit_vecchia_lbfgs(\n",
    "    smooth=v,\n",
    "    #input_map=daily_hourly_maps_vecc_sim[0],\n",
    "    #aggregated_data= daily_aggregated_tensors_vecc_sim[0],\n",
    "\n",
    "    input_map=daily_hourly_maps_irr_vecc_sim[0],\n",
    "    aggregated_data= daily_aggregated_irr_vecc_sim[0],\n",
    "\n",
    "    nns_map=None,\n",
    "    mm_cond_number=mm_cond_number\n",
    ")\n",
    "''' \n",
    "\n",
    "# --- 3. OPTIMIZATION LOOP ---\n",
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'--- Running L-BFGS Optimization ---')\n",
    "print(f'{\"=\"*40}')\n",
    "\n",
    "# Optimizer takes the LIST of scalars\n",
    "optimizer_vecc = model_instance.set_optimizer(\n",
    "            params_list,     \n",
    "            lr=LBFGS_LR,            \n",
    "            max_iter=LBFGS_MAX_EVAL,        \n",
    "            history_size=LBFGS_HISTORY_SIZE \n",
    "        )\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "out, steps_ran = model_instance.fit_vecc_lbfgs(\n",
    "        params_list,\n",
    "        optimizer_vecc,\n",
    "        # covariance_function argument is GONE\n",
    "        max_steps=LBFGS_MAX_STEPS, \n",
    "        grad_tol=1e-7\n",
    "    )\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "epoch_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nOptimization finished in {epoch_time:.2f}s.\")\n",
    "print(f\"Results after {steps_ran} steps: {out}\")\n",
    "print(f\"Final Params: {torch.cat(params_list).detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b08c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final Interpretable Params: {'sigma_sq': 13.048806443143716, 'range_lon': 0.2759775584138689, 'range_lat': 0.26948439045514627, 'range_time': 1.2123116662337672, 'advec_lat': 0.003713422682726235, 'advec_lon': -0.18035976962575426, 'nugget': 4.6861836446362183e-08}\n",
    "Final Interpretable Params: {'sigma_sq': 13.719100200518179, 'range_lon': 0.32214277436468963, 'range_lat': 0.32239904147765264, 'range_time': 1.4623993761954481, 'advec_lat': -0.03314564565869727, 'advec_lon': -0.25639209965916504, 'nugget': 0.38030028721574305}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce4fe1b",
   "metadata": {},
   "source": [
    "Í∏∞Ï°¥ (Naive)\tÏ†ê ÌïòÎÇòÌïòÎÇòÎßàÎã§ Îß§Î≤à ÌñâÎ†¨ÏùÑ ÎßåÎì§Í≥† Î∂ÑÌï¥Ìï®\t20,000Î≤à\tüê¢ Îß§Ïö∞ ÎäêÎ¶º\n",
    "ÌòÑÏû¨ (Grouped)\tÍ∞ôÏùÄ Î™®ÏñëÏùÑ Í∞ÄÏßÑ Ï†êÎÅºÎ¶¨ Î™®ÏïÑÏÑú, ÎåÄÌëúÎ°ú 1Î≤àÎßå Î∂ÑÌï¥Ìï®\t50Î≤à\tüöÄ 400Î∞∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63028aaf",
   "metadata": {},
   "source": [
    "dw\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 13.048806443143716, 'range_lon': 0.2759775584138689, 'range_lat': 0.26948439045514627, 'range_time': 1.2123116662337672, 'advec_lat': 0.003713422682726235, 'advec_lon': -0.18035976962575426, 'nugget': 4.6861836446362183e-08}\n",
    "\n",
    "\n",
    "dw_sim\n",
    "Final Interpretable Params: {'sigma_sq': 11.817838113249758, 'range_lon': 0.22621206038334832, 'range_lat': 0.17422723320268027, 'range_time': 0.997943651842861, 'advec_lat': -0.050699303814975866, 'advec_lon': -0.18116598528943043, 'nugget': 1.1984570483507146}\n",
    "\n",
    "\n",
    "vecc\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 13.719100200518179, 'range_lon': 0.32214277436468963, 'range_lat': 0.32239904147765264, 'range_time': 1.4623993761954481, 'advec_lat': -0.03314564565869727, 'advec_lon': -0.25639209965916504, 'nugget': 0.38030028721574305}\n",
    "\n",
    "\n",
    "vecc_sim\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 12.967334679138654, 'range_lon': 0.1829644675670931, 'range_lat': 0.14094812906918186, 'range_time': 0.9219567362979819, 'advec_lat': -0.04561716644679645, 'advec_lon': -0.17524213405286115, 'nugget': 0.1315329566135699}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c765af78",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Final Interpretable Params: {\n",
    "   'sigma_sq': 12.982146243206834, '\n",
    "   range_lon': 0.1774660571793038, \n",
    "   'range_lat': 0.1377164149996857, \n",
    "   'range_time': 0.8960320754550594, \n",
    "   'advec_lat': 0.03668090767382201, \n",
    "   'advec_lon': -0.16896621297828324, \n",
    "   'nugget': 3.0318146866074696e-14}\n",
    "\n",
    "\n",
    "Final Interpretable Params: {\n",
    "   'sigma_sq': 12.652739121404919, \n",
    "   'range_lon': 0.17664470485305778, \n",
    "   'range_lat': 0.14565426376210802, \n",
    "   'range_time': 0.8720031476394872, \n",
    "   'advec_lat': 0.04466510371597738, \n",
    "   'advec_lon': -0.16447983825003892, 'nugget': 0.31139273714919946}\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "Final Interpretable Params: {\n",
    "   'sigma_sq': 12.600402314947898, \n",
    "   'range_lon': 0.15632064225879247, \n",
    "   'range_lat': 0.12850412363793687, \n",
    "   'range_time': 0.420237386856305, \n",
    "   'advec_lat': 3.646629197697307, \n",
    "   'advec_lon': -1.82731716498955, \n",
    "   'nugget': 3.3125395377840276e-12}\n",
    "\n",
    "\n",
    "   Final Interpretable Params: {\n",
    "      'sigma_sq': 12.465255157603531,\n",
    "       'range_lon': 0.16088131264500707, \n",
    "       'range_lat': 0.13795064901185095, \n",
    "       'range_time': 0.25642913066711404, \n",
    "       'advec_lat': 0.34121830847670426, \n",
    "       'advec_lon': -0.9086603984853497, 'nugget': 0.3413704158367853}\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "Final Interpretable Params: {\n",
    "   'sigma_sq': 12.84409971671357, \n",
    "   'range_lon': 0.17337138953730877, \n",
    "   'range_lat': 0.13617925813922688, \n",
    "   'range_time': 0.08597495864893002,\n",
    "    'advec_lat': -2.7365555385103155, \n",
    "    'advec_lon': -1.3125995387207232, \n",
    "    'nugget': 1.4388537434800932e-15}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a644f0",
   "metadata": {},
   "source": [
    "# TRUE PARAMETERS\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lon = 0.195 \n",
    "init_range_lat = 0.154 \n",
    "init_advec_lat = 0.0418\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "\n",
    "irregular grid\n",
    "\n",
    "Final Interpretable Params: {\n",
    "   'sigma_sq': 13.1581147614216, \n",
    "   'range_lon': 0.1670857937279293, \n",
    "   'range_lat': 0.13532624419142394, \n",
    "   'range_time': 0.662083817877393, \n",
    "   'advec_lat': 4.485362408040999, \n",
    "   'advec_lon': -0.05054465623325784, \n",
    "   'nugget': 3.202037263471046e-05}\n",
    "\n",
    "regular grid\n",
    "\n",
    "Final Interpretable Params: {\n",
    "   'sigma_sq': 13.547646279613172, \n",
    "   'range_lon': 0.2063608511650159, \n",
    "   'range_lat': 0.1698378442441089, \n",
    "   'range_time': 1.122662499886297, \n",
    "   'advec_lat': 0.05415059834603563, \n",
    "   'advec_lon': -0.16640862287821778, \n",
    "   'nugget': 0.48795387825462566}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c961fa4",
   "metadata": {},
   "source": [
    "# fit dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9b3e3",
   "metadata": {},
   "source": [
    "difference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d43435c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([142832, 4])\n",
      "142832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.8000e-02,  1.2305e+02,  0.0000e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2311e+02,  1.9113e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2317e+02,  3.1846e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2323e+02,  9.3044e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2330e+02,  7.5384e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2336e+02, -8.2040e-01,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2342e+02, -1.2852e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2349e+02, -3.2619e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2355e+02, -6.2629e-01,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2361e+02,  1.3124e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2368e+02,  4.2001e-01,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2374e+02, -4.3216e-01,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2380e+02, -5.5636e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2386e+02, -2.0360e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2393e+02,  4.5638e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2399e+02, -5.2148e-02,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2405e+02, -3.7626e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2412e+02,  3.4245e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2418e+02,  5.3648e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2424e+02,  1.9309e+00,  2.1000e+01]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [11.0474, 0.0623, 0.2445, 1.0972, 0.0101, -0.1671, 1.1825]\n",
    "day = 0 # 0 index\n",
    "lat_range= [0,5]\n",
    "lon_range= [123.0, 133.0]\n",
    "#lat_range= [1,3]\n",
    "#lon_range= [125, 129.0]\n",
    "\n",
    "daily_aggregated_tensors_dw = [aggregated_data]\n",
    "daily_hourly_maps_dw = [input_map]\n",
    "\n",
    "db = debiased_whittle.debiased_whittle_preprocess(daily_aggregated_tensors_dw, daily_hourly_maps_dw, day_idx=day, params_list=a, lat_range=lat_range, lon_range=lon_range)\n",
    "\n",
    "\n",
    "subsetted_aggregated_day = db.generate_spatially_filtered_days(0,5,123,133)\n",
    "print(subsetted_aggregated_day.shape)\n",
    "N2= subsetted_aggregated_day.shape[0]\n",
    "print(N2)\n",
    "subsetted_aggregated_day[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2497b8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Pre-computing J-vector (Hamming taper)...\n",
      "Pre-computing sample periodogram...\n",
      "Pre-computing Hamming taper autocorrelation...\n",
      "Data grid: 113x158, 8 time points. J-vector, Periodogram, Taper Autocorr on cpu.\n",
      "\n",
      "============================== Initialization Run 1/1 ==============================\n",
      "Starting with FIXED params (raw log-scale): [4.2042, 1.6348, 0.4721, -2.5562, 0.0218, -0.1689, -1.3984]\n",
      "Starting optimization run 1 on device cpu (Hamming, 7-param ST kernel, L-BFGS)...\n",
      "--- Step 1/20 ---\n",
      " Loss: 1.906929 | Max Grad: 6.274846e-04\n",
      "  Params (Raw Log): log_phi1: 4.3182, log_phi2: 1.7745, log_phi3: 0.0776, log_phi4: -3.5323, advec_lat: 0.0363, advec_lon: -0.1510, log_nugget: -1.0186\n",
      "--- Step 2/20 ---\n",
      " Loss: 1.817385 | Max Grad: 4.095367e-05\n",
      "  Params (Raw Log): log_phi1: 4.3182, log_phi2: 1.7744, log_phi3: 0.0775, log_phi4: -3.5320, advec_lat: 0.0363, advec_lon: -0.1510, log_nugget: -1.0177\n",
      "--- Step 3/20 ---\n",
      " Loss: 1.817385 | Max Grad: 4.095367e-05\n",
      "  Params (Raw Log): log_phi1: 4.3182, log_phi2: 1.7744, log_phi3: 0.0775, log_phi4: -3.5320, advec_lat: 0.0363, advec_lon: -0.1510, log_nugget: -1.0177\n",
      "--- Step 4/20 ---\n",
      " Loss: 1.817385 | Max Grad: 4.095367e-05\n",
      "  Params (Raw Log): log_phi1: 4.3182, log_phi2: 1.7744, log_phi3: 0.0775, log_phi4: -3.5320, advec_lat: 0.0363, advec_lon: -0.1510, log_nugget: -1.0177\n",
      "\n",
      "--- Converged on loss change (change < 1e-12) at step 4 ---\n",
      "\n",
      "--- Training Complete ---\n",
      "\n",
      "FINAL BEST STATE ACHIEVED (during training):\n",
      "Best Loss: 1.817\n",
      "\n",
      "\n",
      "========================= Overall Result from Run ========================= =========================\n",
      "Best Run Loss: 1.817 (after 4 steps)\n",
      "Final Parameters (Natural Scale): sigmasq: 12.7277, range_lat: 0.1631, range_lon: 0.1696, range_time: 0.9917, advec_lat: 0.0363, advec_lon: -0.1510, nugget: 0.3614\n",
      "Final Parameters (Phi Scale)    : phi1: 75.0512, phi2: 5.8967, phi3: 1.0806, phi4: 0.0292, advec_lat: 0.0363, advec_lon: -0.1510, nugget: 0.3614\n",
      "Final Parameters (Raw Log Scale): log_phi1: 4.3182, log_phi2: 1.7744, log_phi3: 0.0775, log_phi4: -3.5320, advec_lat: 0.0363, advec_lon: -0.1510, log_nugget: -1.0177\n",
      "\n",
      "Total execution time: 40.40 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dwl = debiased_whittle.debiased_whittle_likelihood()\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    DAY_TO_RUN = 3 # data is decided above\n",
    "    TAPERING_FUNC = dwl.cgn_hamming # Use Hamming taper\n",
    "    NUM_RUNS = 1\n",
    "    MAX_STEPS = 20 # L-BFGS usually converges in far fewer steps\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063 \n",
    "\n",
    "    LAT_COL, LON_COL = 0, 1\n",
    "    VAL_COL = 2 # Spatially differenced value\n",
    "    TIME_COL = 3\n",
    "\n",
    "\n",
    "    cur_df =subsetted_aggregated_day\n",
    "    \n",
    "    if cur_df.numel() == 0 or cur_df.shape[1] <= max(LAT_COL, LON_COL, VAL_COL, TIME_COL):\n",
    "        print(f\"Error: Data for Day {DAY_TO_RUN} is empty or invalid.\")\n",
    "        exit()\n",
    "\n",
    "    unique_times = torch.unique(cur_df[:, TIME_COL])\n",
    "    time_slices_list = [cur_df[cur_df[:, TIME_COL] == t_val] for t_val in unique_times]\n",
    "\n",
    "    # --- 1. Pre-compute J-vector, Taper Grid, and Taper Autocorrelation ---\n",
    "    print(\"Pre-computing J-vector (Hamming taper)...\")\n",
    "    \n",
    "    # --- üí• REVISED: Renamed 'p' to 'p_time' üí• ---\n",
    "    J_vec, n1, n2, p_time, taper_grid = dwl.generate_Jvector_tapered( \n",
    "        time_slices_list,\n",
    "        tapering_func=TAPERING_FUNC, \n",
    "        lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if J_vec is None or J_vec.numel() == 0 or n1 == 0 or n2 == 0 or p_time == 0:\n",
    "       print(f\"Error: J-vector generation failed for Day {DAY_TO_RUN}.\")\n",
    "       exit()\n",
    "       \n",
    "    print(\"Pre-computing sample periodogram...\")\n",
    "    I_sample = dwl.calculate_sample_periodogram_vectorized(J_vec)\n",
    "\n",
    "    print(\"Pre-computing Hamming taper autocorrelation...\")\n",
    "    taper_autocorr_grid = dwl.calculate_taper_autocorrelation_fft(taper_grid, n1, n2, DEVICE)\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Error: NaN/Inf in sample periodogram.\")\n",
    "        exit()\n",
    "    if torch.isnan(taper_autocorr_grid).any() or torch.isinf(taper_autocorr_grid).any():\n",
    "        print(\"Error: NaN/Inf in taper autocorrelation.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Data grid: {n1}x{n2}, {p_time} time points. J-vector, Periodogram, Taper Autocorr on {DEVICE}.\")\n",
    "    # --- END REVISION ---\n",
    "\n",
    "    # --- 2. Optimization Loop ---\n",
    "    all_final_results = []\n",
    "    all_final_losses = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        print(f\"\\n{'='*30} Initialization Run {i+1}/{NUM_RUNS} {'='*30}\")\n",
    "\n",
    "        # --- 7-PARAMETER initialization ---\n",
    "        ''' \n",
    "        init_sigmasq   = 15.0\n",
    "        init_range_lat = 0.66 \n",
    "        init_range_lon = 0.7 \n",
    "        init_nugget    = 1.5\n",
    "        init_beta      = 0.1  # Temporal range ratio\n",
    "        init_advec_lat = 0.02\n",
    "        init_advec_lon = -0.08\n",
    "        '''\n",
    "        init_sigmasq   = 13.059\n",
    "        init_range_lat = 0.154 \n",
    "        init_range_lon = 0.195\n",
    "        init_advec_lat = 0.0218\n",
    "        init_range_time = 0.7\n",
    "        init_advec_lon = -0.1689\n",
    "        init_nugget    = 0.247\n",
    "\n",
    "        init_phi2 = 1.0 / init_range_lon\n",
    "        init_phi1 = init_sigmasq * init_phi2\n",
    "        init_phi3 = (init_range_lon / init_range_lat)**2\n",
    "        # Change needed to match the spatial-temporal distance formula:\n",
    "        init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "        initial_params_values = [\n",
    "            np.log(init_phi1),    # [0] log_phi1\n",
    "            np.log(init_phi2),    # [1] log_phi2\n",
    "            np.log(init_phi3),    # [2] log_phi3\n",
    "            np.log(init_phi4),    # [3] log_phi4\n",
    "            init_advec_lat,       # [4] advec_lat (NOT log)\n",
    "            init_advec_lon,       # [5] advec_lon (NOT log)\n",
    "            np.log(init_nugget)   # [6] log_nugget\n",
    "        ]\n",
    "        \n",
    "        print(f\"Starting with FIXED params (raw log-scale): {[round(p, 4) for p in initial_params_values]}\")\n",
    "\n",
    "        params_list = [\n",
    "            Parameter(torch.tensor([val], dtype=torch.float64))\n",
    "            for val in initial_params_values\n",
    "        ]\n",
    "\n",
    "        # Helper to define the boundary globally for clarity\n",
    "        NUGGET_LOWER_BOUND = 0.05\n",
    "        LOG_NUGGET_LOWER_BOUND = np.log(NUGGET_LOWER_BOUND) # Approx -2.9957\n",
    "\n",
    "        # --- üí• REVISED: Use L-BFGS Optimizer üí• ---\n",
    "        optimizer = torch.optim.LBFGS(\n",
    "            params_list,\n",
    "            lr=1.0,           # Initial step length for line search\n",
    "            max_iter=20,      # Iterations per step\n",
    "            history_size=100,\n",
    "            line_search_fn=\"strong_wolfe\", # Often more robust\n",
    "            tolerance_grad=1e-5\n",
    "        )\n",
    "        # --- END REVISION ---\n",
    "\n",
    "        print(f\"Starting optimization run {i+1} on device {DEVICE} (Hamming, 7-param ST kernel, L-BFGS)...\")\n",
    "        \n",
    "        # --- üí• REVISED: Call L-BFGS trainer, pass p_time üí• ---\n",
    "        nat_params_str, phi_params_str, raw_params_str, loss, steps_run = dwl.run_lbfgs_tapered(\n",
    "            params_list=params_list,\n",
    "            optimizer=optimizer,\n",
    "            I_sample=I_sample,\n",
    "            n1=n1, n2=n2, p_time=p_time,\n",
    "            taper_autocorr_grid=taper_autocorr_grid, \n",
    "            max_steps=MAX_STEPS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        # --- END REVISION ---\n",
    "        \n",
    "        if loss is not None:\n",
    "            all_final_results.append((nat_params_str, phi_params_str, raw_params_str))\n",
    "            all_final_losses.append(loss)\n",
    "        else:\n",
    "            all_final_losses.append(float('inf'))\n",
    "\n",
    "    print(f\"\\n\\n{'='*25} Overall Result from Run {'='*25} {'='*25}\")\n",
    "    \n",
    "    valid_losses = [l for l in all_final_losses if l is not None and l != float('inf')]\n",
    "\n",
    "    if not valid_losses:\n",
    "        print(f\"The run failed or resulted in an invalid loss for Day {DAY_TO_RUN}.\")\n",
    "    else:\n",
    "        best_loss = min(valid_losses)\n",
    "        best_run_index = all_final_losses.index(best_loss)\n",
    "        best_results = all_final_results[best_run_index]\n",
    "        \n",
    "        print(f\"Best Run Loss: {best_loss} (after {steps_run} steps)\")\n",
    "        print(f\"Final Parameters (Natural Scale): {best_results[0]}\")\n",
    "        print(f\"Final Parameters (Phi Scale)    : {best_results[1]}\")\n",
    "        print(f\"Final Parameters (Raw Log Scale): {best_results[2]}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61060411",
   "metadata": {},
   "source": [
    "init_sigmasq   = 13.059\n",
    "init_range_lon = 0.195 \n",
    "init_range_lat = 0.154 \n",
    "init_advec_lat = 0.0418\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "1 st simulation (1 vs 3)\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 12.939260896610579, 'range_lon': 0.17242543281507933, 'range_lat': 0.1631231346200311, 'range_time': 1.1358209227858689, 'advec_lat': 0.045119029109988974, 'advec_lon': -0.17809677784942035, 'nugget': 0.3009582276680578}\n",
    "\n",
    "Final Parameters (Natural Scale): sigmasq: 13.2415, range_lat: 0.1685, range_lon: 0.1739, range_time: 0.9681, advec_lat: 0.0395, advec_lon: -0.1732, nugget: 0.3216\n",
    "\n",
    "2nd simulation (1 vs 3)\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 12.765293287952144, 'range_lon': 0.17039368470743024, 'range_lat': 0.16152132799710625, 'range_time': 1.081124889959751, 'advec_lat': 0.04635511983762563, 'advec_lon': -0.1775715292039452, 'nugget': 0.31503884896742074}\n",
    "\n",
    "\n",
    "sigmasq: 13.2415, range_lat: 0.1685, range_lon: 0.1739, range_time: 0.9681, advec_lat: 0.0395, advec_lon: -0.1732, nugget: 0.3216\n",
    "\n",
    "3nd simulation (mm 15 two times)\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 12.292570063933866, 'range_lon': 0.16170186578939172, 'range_lat': 0.1537053192245325, 'range_time': 0.9671689003322674, 'advec_lat': 0.04155171168950867, 'advec_lon': -0.16218073366456207, 'nugget': 0.29945872751676345}\n",
    "\n",
    "Final Parameters (Natural Scale): sigmasq: 12.7277, range_lat: 0.1631, range_lon: 0.1696, range_time: 0.9917, advec_lat: 0.0363, advec_lon: -0.1510, nugget: 0.3614\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
