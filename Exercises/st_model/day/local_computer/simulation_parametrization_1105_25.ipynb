{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d975f044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy                    # clone tensor\n",
    "import time\n",
    "\n",
    "# Custom imports\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels\n",
    "from GEMS_TCO import data_preprocess \n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "import typer\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO.data_loader import load_data2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116189cf",
   "metadata": {},
   "source": [
    "# square parameteriazation + stablize parametrization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccca2ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Generation ---\n",
      "--- Data Generation Complete ---\n",
      "\n",
      "--- A. Starting MLE Optimization (PyTorch L-BFGS) - STABLE ---\n",
      "LBFGS Step 5/50, NLL: 1791.75, Params: [ÏƒÂ²: 19.998, a: 0.875, Î·Â²: 2.841], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: -0.0001, log(Î·Â²)_raw: -0.0002]\n",
      "LBFGS Step 10/50, NLL: 1791.75, Params: [ÏƒÂ²: 19.998, a: 0.875, Î·Â²: 2.841], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: -0.0001, log(Î·Â²)_raw: -0.0002]\n",
      "LBFGS Step 15/50, NLL: 1791.75, Params: [ÏƒÂ²: 19.998, a: 0.875, Î·Â²: 2.841], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: -0.0001, log(Î·Â²)_raw: -0.0002]\n",
      "LBFGS Step 20/50, NLL: 1791.75, Params: [ÏƒÂ²: 19.998, a: 0.875, Î·Â²: 2.841], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: -0.0001, log(Î·Â²)_raw: -0.0002]\n",
      "LBFGS Step 25/50, NLL: 1791.75, Params: [ÏƒÂ²: 19.998, a: 0.875, Î·Â²: 2.841], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: -0.0001, log(Î·Â²)_raw: -0.0002]\n",
      "LBFGS Step 30/50, NLL: 1791.75, Params: [ÏƒÂ²: 19.998, a: 0.875, Î·Â²: 2.841], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: -0.0001, log(Î·Â²)_raw: -0.0002]\n",
      "LBFGS Step 35/50, NLL: 1791.75, Params: [ÏƒÂ²: 19.998, a: 0.875, Î·Â²: 2.841], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: -0.0001, log(Î·Â²)_raw: -0.0002]\n",
      "LBFGS Step 40/50, NLL: 1791.75, Params: [ÏƒÂ²: 19.998, a: 0.875, Î·Â²: 2.841], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: -0.0001, log(Î·Â²)_raw: -0.0002]\n",
      "LBFGS Step 45/50, NLL: 1791.75, Params: [ÏƒÂ²: 19.998, a: 0.875, Î·Â²: 2.841], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: -0.0001, log(Î·Â²)_raw: -0.0002]\n",
      "LBFGS Step 50/50, NLL: 1791.75, Params: [ÏƒÂ²: 19.998, a: 0.875, Î·Â²: 2.841], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: -0.0001, log(Î·Â²)_raw: -0.0002]\n",
      "\n",
      "--- B. Starting MLE Optimization (PyTorch Adam) - STABLE ---\n",
      "Adam Epoch 50/500, NLL: 1804.07, Params: [ÏƒÂ²: 21.051, a: 0.621, Î·Â²: 0.481], Grads: [Î¦1_raw: -19.0918, Î¦2_raw: -0.0797, log(Î·Â²)_raw: -9.2203]\n",
      "Adam Epoch 100/500, NLL: 1799.15, Params: [ÏƒÂ²: 20.444, a: 0.564, Î·Â²: 0.691], Grads: [Î¦1_raw: -2.8200, Î¦2_raw: -0.0979, log(Î·Â²)_raw: -5.3552]\n",
      "Adam Epoch 150/500, NLL: 1797.98, Params: [ÏƒÂ²: 20.183, a: 0.555, Î·Â²: 0.894], Grads: [Î¦1_raw: 2.7355, Î¦2_raw: 0.0226, log(Î·Â²)_raw: -3.3799]\n",
      "Adam Epoch 200/500, NLL: 1797.07, Params: [ÏƒÂ²: 20.056, a: 0.567, Î·Â²: 1.094], Grads: [Î¦1_raw: 4.2991, Î¦2_raw: 0.0460, log(Î·Â²)_raw: -2.6585]\n",
      "Adam Epoch 250/500, NLL: 1796.04, Params: [ÏƒÂ²: 19.962, a: 0.589, Î·Â²: 1.317], Grads: [Î¦1_raw: 4.7555, Î¦2_raw: 0.0526, log(Î·Â²)_raw: -2.2564]\n",
      "Adam Epoch 300/500, NLL: 1794.97, Params: [ÏƒÂ²: 19.885, a: 0.618, Î·Â²: 1.562], Grads: [Î¦1_raw: 4.8039, Î¦2_raw: 0.0547, log(Î·Â²)_raw: -1.8096]\n",
      "Adam Epoch 350/500, NLL: 1793.98, Params: [ÏƒÂ²: 19.836, a: 0.653, Î·Â²: 1.810], Grads: [Î¦1_raw: 4.4721, Î¦2_raw: 0.0521, log(Î·Â²)_raw: -1.3884]\n",
      "Adam Epoch 400/500, NLL: 1793.16, Params: [ÏƒÂ²: 19.817, a: 0.692, Î·Â²: 2.042], Grads: [Î¦1_raw: 3.8693, Î¦2_raw: 0.0457, log(Î·Â²)_raw: -1.0323]\n",
      "Adam Epoch 450/500, NLL: 1792.57, Params: [ÏƒÂ²: 19.825, a: 0.731, Î·Â²: 2.248], Grads: [Î¦1_raw: 3.1327, Î¦2_raw: 0.0378, log(Î·Â²)_raw: -0.7444]\n",
      "Adam Epoch 500/500, NLL: 1792.18, Params: [ÏƒÂ²: 19.851, a: 0.767, Î·Â²: 2.419], Grads: [Î¦1_raw: 2.3825, Î¦2_raw: 0.0291, log(Î·Â²)_raw: -0.5198]\n",
      "\n",
      "======================================================================\n",
      "TARGET PARAMETERS: Variance (ÏƒÂ²)=25.0, Range (a)=1, Nugget (Î·Â²)=3.0\n",
      "INITIAL NUGGET GUESS: 0.3\n",
      "======================================================================\n",
      "âœ¨ PyTorch L-BFGS Results (Stable Reparameterization):\n",
      "  * Fitted Variance (ÏƒÂ²): 19.998 (Target: 25.0)\n",
      "  * Fitted Range (a): 0.875 (Target: 1)\n",
      "  * Fitted Nugget (Î·Â²): 2.841 (Target: 3.0)\n",
      "  * Final -LL Value: 1791.75\n",
      "  * Optimization Steps: 50 steps\n",
      "\n",
      "ðŸš€ PyTorch Adam Results (Stable Reparameterization):\n",
      "  * Fitted Variance (ÏƒÂ²): 19.851 (Target: 25.0)\n",
      "  * Fitted Range (a): 0.767 (Target: 1)\n",
      "  * Fitted Nugget (Î·Â²): 2.419 (Target: 3.0)\n",
      "  * Final -LL Value: 1792.18\n",
      "  * Optimization Steps: 500 epochs\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from scipy.spatial.distance import cdist\n",
    "from torch.nn import Parameter\n",
    "\n",
    "# --- 0. Global Parameters and Utility Functions ---\n",
    "N_SPATIAL_POINTS = 1120\n",
    "N_DAYS = 31\n",
    "N_HOURS_PER_DAY = 8\n",
    "N_FEATURES = 4\n",
    "GRID_X = 40  \n",
    "GRID_Y = 28  \n",
    "LAT_MIN, LAT_MAX = 0, 5\n",
    "LON_MIN, LON_MAX = 113, 123\n",
    "BASE_DATE = '2024_07_y24m07day'\n",
    "\n",
    "# --- 0. Global Parameters and Utility Functions ---\n",
    "N_SPATIAL_POINTS = 1120\n",
    "N_FEATURES = 4\n",
    "# ... (rest of constants remain the same) ...\n",
    "\n",
    "# Exponential Kernel Parameters (Targets)\n",
    "SIGMA2_TRUE = 25.0 # TARGET Variance\n",
    "RANGE_A_TRUE = 1    # TARGET Range\n",
    "NUGGET_TRUE = 3.0   # TARGET Nugget for data generation\n",
    "\n",
    "# Optimization Setup\n",
    "ADAM_ITERATIONS = 500\n",
    "ADAM_LEARNING_RATE = 0.01\n",
    "LBFGS_MAX_STEPS = 50 \n",
    "LBFGS_MAX_EVAL = 50 \n",
    "\n",
    "OZONE_MEAN = 240.0\n",
    "\n",
    "# --- COVARIANCE FUNCTIONS ---\n",
    "\n",
    "def exponential_covariance_numpy(distances, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (NumPy for Generation).\"\"\"\n",
    "    # Cov(h) = sigma^2 * exp(-h/a)\n",
    "    cov = sigma2 * np.exp(-distances / a)\n",
    "    if distances.shape[0] == distances.shape[1]:\n",
    "        cov[np.diag_indices_from(distances)] += (nugget + 1e-6)\n",
    "    return cov\n",
    "\n",
    "def exponential_covariance_torch(distances_torch, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (PyTorch for Optimization).\"\"\"\n",
    "    cov = sigma2 * torch.exp(-distances_torch / a)\n",
    "    \n",
    "    # Add nugget effect + jitter to the diagonal\n",
    "    if distances_torch.shape[0] == distances_torch.shape[1]:\n",
    "        jitter = 1e-6 \n",
    "        diag_mask = torch.eye(cov.shape[0], device=cov.device)\n",
    "        # Nugget is now a torch tensor parameter\n",
    "        cov = cov + diag_mask * (nugget + jitter)\n",
    "    return cov\n",
    "\n",
    "# --- SHARED NLL Function using STABLE REPARAMETERIZATION ---\n",
    "def neg_log_likelihood_torch_stable(raw_params, distances_torch, z_centered_torch):\n",
    "    \"\"\"\n",
    "    Calculates -LL for PyTorch, optimizing Stable Reparameterization + log(nugget).\n",
    "    raw_params[0] = raw_phi1_sqrt (for sigma2/a)\n",
    "    raw_params[1] = raw_phi2_sqrt (for 1/a)\n",
    "    raw_params[2] = log_nugget (for log(eta^2))\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Apply Reparameterization and Positivity Constraints\n",
    "    phi1 = raw_params[0].pow(2).squeeze() # Phi1 = sigma2 / a\n",
    "    phi2 = raw_params[1].pow(2).squeeze() # Phi2 = 1 / a\n",
    "    \n",
    "    # Nugget is log-transformed for optimization\n",
    "    nugget = torch.exp(raw_params[2]).squeeze() \n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # 2. Derive Original Parameters\n",
    "    range_a = 1.0 / (phi2 + epsilon)          # Range: a = 1 / Phi2\n",
    "    sigma2 = phi1 / (phi2 + epsilon)          # Variance: sigma2 = Phi1 / Phi2\n",
    "    \n",
    "    C = exponential_covariance_torch(distances_torch, sigma2, range_a, nugget)\n",
    "    \n",
    "    try:\n",
    "        # Cholesky decomposition\n",
    "        L = torch.linalg.cholesky(C)\n",
    "        log_det = 2.0 * torch.sum(torch.log(torch.diag(L)))\n",
    "        alpha = torch.linalg.solve(C, z_centered_torch.unsqueeze(1))\n",
    "        quad_term = z_centered_torch.unsqueeze(0) @ alpha\n",
    "        neg_LL = 0.5 * log_det + 0.5 * quad_term.squeeze()\n",
    "        return neg_LL\n",
    "    except RuntimeError:\n",
    "        return torch.tensor(1e15, device=C.device)\n",
    "\n",
    "# --- Data Generation Function (Unchanged) ---\n",
    "def generate_ozone_data_map(coords, sigma2, a, nugget, mean, time_index):\n",
    "    n_points = coords.shape[0]\n",
    "    distances = cdist(coords, coords, metric='euclidean')\n",
    "    Cov = exponential_covariance_numpy(distances, sigma2, a, nugget) \n",
    "    Cov = (Cov + Cov.T) / 2\n",
    "    \n",
    "    try:\n",
    "        L = np.linalg.cholesky(Cov)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.zeros((n_points, N_FEATURES))\n",
    "\n",
    "    W = np.random.normal(0, 1, size=(n_points, 1))\n",
    "    Z_correlated = L @ W\n",
    "    ozone_values = mean + Z_correlated\n",
    "    \n",
    "    data_np = np.zeros((n_points, N_FEATURES))\n",
    "    data_np[:, 0:1] = ozone_values             \n",
    "    data_np[:, 1] = coords[:, 1] * 10 + 2      \n",
    "    data_np[:, 2] = coords[:, 0] * 40 + 250    \n",
    "    data_np[:, 3] = time_index                 \n",
    "    return data_np\n",
    "\n",
    "\n",
    "# --- 1. Data Generation Execution ---\n",
    "df_day_aggregated_list = []\n",
    "print(\"--- Starting Data Generation ---\")\n",
    "lat_coords = np.linspace(LAT_MIN, LAT_MAX, GRID_Y)\n",
    "lon_coords = np.linspace(LON_MIN, LON_MAX, GRID_X)\n",
    "coords_latlon = np.array([[lat, lon] for lat in lat_coords for lon in lon_coords])\n",
    "\n",
    "# Generate only one hour of data for fitting the spatial model\n",
    "data_np = generate_ozone_data_map(\n",
    "    coords_latlon, SIGMA2_TRUE, RANGE_A_TRUE, NUGGET_TRUE, OZONE_MEAN, 21.0\n",
    ")\n",
    "df_day_aggregated_list.append(torch.tensor(data_np, dtype=torch.float))\n",
    "\n",
    "print(\"--- Data Generation Complete ---\")\n",
    "\n",
    "# --- 2. Data Preparation ---\n",
    "data_to_fit = df_day_aggregated_list[0][:N_SPATIAL_POINTS, :] \n",
    "z_data = data_to_fit[:, 0].numpy()\n",
    "coordinates = coords_latlon[:, [1, 0]] \n",
    "distances_np = cdist(coordinates, coordinates, metric='euclidean')\n",
    "z_centered_np = z_data - np.mean(z_data)\n",
    "\n",
    "# Convert to Torch Tensors\n",
    "distances_torch = torch.tensor(distances_np, dtype=torch.float)\n",
    "z_centered_torch = torch.tensor(z_centered_np, dtype=torch.float)\n",
    "\n",
    "# --- Initial Parameter Setup (Shared) ---\n",
    "PHI1_TARGET = SIGMA2_TRUE / RANGE_A_TRUE\n",
    "PHI2_TARGET = 1.0 / RANGE_A_TRUE\n",
    "\n",
    "raw_phi1_sqrt_start = np.sqrt(PHI1_TARGET + 4.0) \n",
    "raw_phi2_sqrt_start = np.sqrt(PHI2_TARGET + 0.133) \n",
    "\n",
    "# ðŸ’¥ NEW: Initialize nugget parameter (log scale) to 0.3\n",
    "NUGGET_INIT = 0.3\n",
    "LOG_NUGGET_START = np.log(NUGGET_INIT) \n",
    "\n",
    "initial_params_stable = [raw_phi1_sqrt_start, raw_phi2_sqrt_start, LOG_NUGGET_START]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# A. Optimization with L-BFGS (PyTorch) - STABLE\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Reset parameters for LBFGS\n",
    "# ðŸ’¥ NEW: raw_params_lbfgs is a tensor of 3 parameters\n",
    "raw_params_lbfgs = torch.tensor(\n",
    "    initial_params_stable, \n",
    "    dtype=torch.float, \n",
    "    requires_grad=True\n",
    ")\n",
    "\n",
    "optimizer_lbfgs = optim.LBFGS(\n",
    "    [raw_params_lbfgs], \n",
    "    lr=1.0, \n",
    "    max_iter=LBFGS_MAX_STEPS,\n",
    "    max_eval=LBFGS_MAX_EVAL \n",
    ")\n",
    "\n",
    "final_loss_lbfgs = torch.tensor(0.0)\n",
    "print(\"\\n--- A. Starting MLE Optimization (PyTorch L-BFGS) - STABLE ---\")\n",
    "\n",
    "# L-BFGS requires a \"closure\" function\n",
    "def closure_lbfgs():\n",
    "    optimizer_lbfgs.zero_grad()\n",
    "    # ðŸ’¥ NEW: Call NLL without the fixed_nugget argument\n",
    "    loss = neg_log_likelihood_torch_stable(raw_params_lbfgs, distances_torch, z_centered_torch)\n",
    "    if not torch.isinf(loss) and not torch.isnan(loss):\n",
    "        loss.backward()\n",
    "    return loss\n",
    "\n",
    "# L-BFGS Optimization Loop\n",
    "for step in range(LBFGS_MAX_STEPS):\n",
    "    loss = optimizer_lbfgs.step(closure_lbfgs)\n",
    "    final_loss_lbfgs = loss\n",
    "    \n",
    "    if (step + 1) % 5 == 0: \n",
    "        phi1 = raw_params_lbfgs[0].pow(2).item()\n",
    "        phi2 = raw_params_lbfgs[1].pow(2).item()\n",
    "        current_nugget = torch.exp(raw_params_lbfgs[2]).item() # ðŸ’¥ NEW: Get current nugget\n",
    "        current_sigma2 = phi1 / (phi2 + 1e-6)\n",
    "        current_a = 1.0 / (phi2 + 1e-6)\n",
    "        \n",
    "        grad_phi1 = raw_params_lbfgs.grad[0].item() if raw_params_lbfgs.grad is not None else 0.0\n",
    "        grad_phi2 = raw_params_lbfgs.grad[1].item() if raw_params_lbfgs.grad is not None else 0.0\n",
    "        grad_nugget = raw_params_lbfgs.grad[2].item() if raw_params_lbfgs.grad is not None else 0.0\n",
    "        \n",
    "        print(f\"LBFGS Step {step + 1}/{LBFGS_MAX_STEPS}, NLL: {loss.item():.2f}, Params: [ÏƒÂ²: {current_sigma2:.3f}, a: {current_a:.3f}, Î·Â²: {current_nugget:.3f}], Grads: [Î¦1_raw: {grad_phi1:.4f}, Î¦2_raw: {grad_phi2:.4f}, log(Î·Â²)_raw: {grad_nugget:.4f}]\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# B. Optimization with Adam (PyTorch) - STABLE\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Reset parameters for Adam (Use the same start point)\n",
    "# ðŸ’¥ NEW: raw_params_adam is a tensor of 3 parameters\n",
    "raw_params_adam = torch.tensor(\n",
    "    initial_params_stable, \n",
    "    dtype=torch.float, \n",
    "    requires_grad=True\n",
    ")\n",
    "\n",
    "optimizer_adam = optim.Adam(\n",
    "    [raw_params_adam], \n",
    "    lr=ADAM_LEARNING_RATE\n",
    ")\n",
    "\n",
    "final_loss_adam = torch.tensor(0.0)\n",
    "print(f\"\\n--- B. Starting MLE Optimization (PyTorch Adam) - STABLE ---\")\n",
    "\n",
    "# Adam Optimization Loop\n",
    "for epoch in range(ADAM_ITERATIONS):\n",
    "    optimizer_adam.zero_grad()\n",
    "    \n",
    "    # ðŸ’¥ NEW: Call NLL without the fixed_nugget argument\n",
    "    loss = neg_log_likelihood_torch_stable(raw_params_adam, distances_torch, z_centered_torch)\n",
    "    \n",
    "    if torch.isinf(loss) or torch.isnan(loss):\n",
    "        loss = torch.tensor(1e15, device=loss.device)\n",
    "        break\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer_adam.step()\n",
    "    final_loss_adam = loss\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0: \n",
    "        phi1 = raw_params_adam[0].pow(2).item()\n",
    "        phi2 = raw_params_adam[1].pow(2).item()\n",
    "        current_nugget = torch.exp(raw_params_adam[2]).item() # ðŸ’¥ NEW: Get current nugget\n",
    "        current_sigma2 = phi1 / (phi2 + 1e-6)\n",
    "        current_a = 1.0 / (phi2 + 1e-6)\n",
    "        \n",
    "        grad_phi1 = raw_params_adam.grad[0].item() if raw_params_adam.grad is not None else 0.0\n",
    "        grad_phi2 = raw_params_adam.grad[1].item() if raw_params_adam.grad is not None else 0.0\n",
    "        grad_nugget = raw_params_adam.grad[2].item() if raw_params_adam.grad is not None else 0.0\n",
    "        \n",
    "        print(f\"Adam Epoch {epoch + 1}/{ADAM_ITERATIONS}, NLL: {loss.item():.2f}, Params: [ÏƒÂ²: {current_sigma2:.3f}, a: {current_a:.3f}, Î·Â²: {current_nugget:.3f}], Grads: [Î¦1_raw: {grad_phi1:.4f}, Î¦2_raw: {grad_phi2:.4f}, log(Î·Â²)_raw: {grad_nugget:.4f}]\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. Display Results\n",
    "# ----------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"TARGET PARAMETERS: Variance (ÏƒÂ²)={SIGMA2_TRUE}, Range (a)={RANGE_A_TRUE}, Nugget (Î·Â²)={NUGGET_TRUE}\")\n",
    "print(f\"INITIAL NUGGET GUESS: {NUGGET_INIT}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --- L-BFGS Results ---\n",
    "phi1_lbfgs = raw_params_lbfgs[0].pow(2).detach().numpy().item()\n",
    "phi2_lbfgs = raw_params_lbfgs[1].pow(2).detach().numpy().item()\n",
    "log_nugget_lbfgs = raw_params_lbfgs[2].detach().numpy().item() # ðŸ’¥ NEW\n",
    "fitted_nugget_lbfgs = np.exp(log_nugget_lbfgs)                 # ðŸ’¥ NEW\n",
    "\n",
    "fitted_sigma2_lbfgs = phi1_lbfgs / (phi2_lbfgs + 1e-6)\n",
    "fitted_range_a_lbfgs = 1.0 / (phi2_lbfgs + 1e-6)\n",
    "\n",
    "print(\"âœ¨ PyTorch L-BFGS Results (Stable Reparameterization):\")\n",
    "print(f\"  * Fitted Variance (ÏƒÂ²): {fitted_sigma2_lbfgs:.3f} (Target: {SIGMA2_TRUE})\")\n",
    "print(f\"  * Fitted Range (a): {fitted_range_a_lbfgs:.3f} (Target: {RANGE_A_TRUE})\")\n",
    "print(f\"  * Fitted Nugget (Î·Â²): {fitted_nugget_lbfgs:.3f} (Target: {NUGGET_TRUE})\") # ðŸ’¥ NEW\n",
    "print(f\"  * Final -LL Value: {final_loss_lbfgs.item():.2f}\")\n",
    "print(f\"  * Optimization Steps: {LBFGS_MAX_STEPS} steps\")\n",
    "\n",
    "# --- Adam Results ---\n",
    "phi1_adam = raw_params_adam[0].pow(2).detach().numpy().item()\n",
    "phi2_adam = raw_params_adam[1].pow(2).detach().numpy().item()\n",
    "log_nugget_adam = raw_params_adam[2].detach().numpy().item() # ðŸ’¥ NEW\n",
    "fitted_nugget_adam = np.exp(log_nugget_adam)                 # ðŸ’¥ NEW\n",
    "\n",
    "fitted_sigma2_adam = phi1_adam / (phi2_adam + 1e-6)\n",
    "fitted_range_a_adam = 1.0 / (phi2_adam + 1e-6)\n",
    "\n",
    "print(\"\\nðŸš€ PyTorch Adam Results (Stable Reparameterization):\")\n",
    "print(f\"  * Fitted Variance (ÏƒÂ²): {fitted_sigma2_adam:.3f} (Target: {SIGMA2_TRUE})\")\n",
    "print(f\"  * Fitted Range (a): {fitted_range_a_adam:.3f} (Target: {RANGE_A_TRUE})\")\n",
    "print(f\"  * Fitted Nugget (Î·Â²): {fitted_nugget_adam:.3f} (Target: {NUGGET_TRUE})\") # ðŸ’¥ NEW\n",
    "print(f\"  * Final -LL Value: {final_loss_adam.item():.2f}\")\n",
    "print(f\"  * Optimization Steps: {ADAM_ITERATIONS} epochs\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9885290",
   "metadata": {},
   "source": [
    "# test 11/10/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41734385",
   "metadata": {},
   "source": [
    "# L BFGS vs Adams  1120 reparametrization for anisotrpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "916aee43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting 1 Simulation Runs ---\n",
      "--- Grid Size: 80x56 (N = 4480) ---\n",
      "--- Running Simulation 1/1 ---\n",
      "--- Simulation 1 complete. Time elapsed: 248.29s ---\n",
      "\n",
      "--- All Simulations Complete ---\n",
      "\n",
      "===========================================================================\n",
      "TARGET PARAMETERS: Variance (ÏƒÂ²)=11.0, Range (a)=1.9, Anisotropy (Î¸â‚ƒ-ratio)=0.5, Nugget (Î·Â²)=0.3\n",
      "AGGREGATE RESULTS OVER 1 RUNS\n",
      "Grid Size: 80x56 (N = 4480)\n",
      "Total time: 248.37 seconds\n",
      "===========================================================================\n",
      "âœ¨ PyTorch L-BFGS Average Results (1/1 Valid Runs):\n",
      "  * Fitted Variance (ÏƒÂ²): 10.539 (Std: 0.000) (Target: 11.0)\n",
      "  * Fitted Range (a): 1.702 (Std: 0.000) (Target: 1.9)\n",
      "  * Fitted Anisotropy (Î¸â‚ƒ-ratio): 0.491 (Std: 0.000) (Target: 0.5)\n",
      "  * Fitted Nugget (Î·Â²): 0.244 (Std: 0.000) (Target: 0.3)\n",
      "  * Final -LL Value: 2770.48 (Std: 0.00)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "âœ¨ PyTorch Adam Average Results (1/1 Valid Runs):\n",
      "  * Fitted Variance (ÏƒÂ²): 11.066 (Std: 0.000) (Target: 11.0)\n",
      "  * Fitted Range (a): 1.980 (Std: 0.000) (Target: 1.9)\n",
      "  * Fitted Anisotropy (Î¸â‚ƒ-ratio): 0.472 (Std: 0.000) (Target: 0.5)\n",
      "  * Fitted Nugget (Î·Â²): 0.299 (Std: 0.000) (Target: 0.3)\n",
      "  * Final -LL Value: 2772.87 (Std: 0.00)\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from scipy.spatial.distance import cdist\n",
    "from torch.nn import Parameter\n",
    "import time\n",
    "\n",
    "# --- 0. Global Parameters and Utility Functions ---\n",
    "# ðŸ’¥ MODIFIED: Increased grid density\n",
    "N_SPATIAL_POINTS = 4480  # 80 * 56\n",
    "GRID_X = 80              # ðŸ’¥ MODIFIED\n",
    "GRID_Y = 56              # ðŸ’¥ MODIFIED\n",
    "\n",
    "N_DAYS = 31\n",
    "N_HOURS_PER_DAY = 8\n",
    "N_FEATURES = 4\n",
    "LAT_MIN, LAT_MAX = 0, 5\n",
    "LON_MIN, LON_MAX = 113, 123\n",
    "BASE_DATE = '2024_07_y24m07day' \n",
    "\n",
    "# Exponential Kernel Parameters (Targets)\n",
    "SIGMA2_TRUE = 11.0      # TARGET Variance\n",
    "RANGE_A_TRUE = 1.9      # TARGET Range\n",
    "ANISOTROPY_RATIO_TRUE = 0.5 # TARGET Anisotropy\n",
    "PHI3_TARGET_SQ = ANISOTROPY_RATIO_TRUE**2 \n",
    "NUGGET_TRUE = 0.3       # TARGET Nugget for data generation\n",
    "\n",
    "# --- Simulation Controls ---\n",
    "NUM_SIMULATIONS = 1      \n",
    "PRINT_EPOCHS = False      # Set to False to silence epoch logging\n",
    "\n",
    "# Optimization Setup\n",
    "ADAM_ITERATIONS = 500\n",
    "ADAM_LEARNING_RATE = 0.01\n",
    "\n",
    "# L-BFGS Setup\n",
    "LBFGS_MAX_STEPS = 50 \n",
    "LBFGS_MAX_EVAL = 50 \n",
    "\n",
    "OZONE_MEAN = 240.0\n",
    "\n",
    "# --- COVARIANCE FUNCTIONS ---\n",
    "\n",
    "def exponential_covariance_numpy(distances, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (NumPy for Generation).\"\"\"\n",
    "    cov = sigma2 * np.exp(-distances / a)\n",
    "    if distances.shape[0] == distances.shape[1]:\n",
    "        cov[np.diag_indices_from(distances)] += (nugget + 1e-6)\n",
    "    return cov\n",
    "\n",
    "def exponential_covariance_torch(distances_torch, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (PyTorch for Optimization).\"\"\"\n",
    "    cov = sigma2 * torch.exp(-distances_torch / a)\n",
    "    \n",
    "    if distances_torch.shape[0] == distances_torch.shape[1]:\n",
    "        jitter = 1e-6 \n",
    "        diag_mask = torch.eye(cov.shape[0], device=cov.device)\n",
    "        cov = cov + diag_mask * (nugget + jitter)\n",
    "    return cov\n",
    "\n",
    "# --- NLL Function (4 parameters) ---\n",
    "def neg_log_likelihood_torch_stable(raw_params, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch):\n",
    "    \"\"\"\n",
    "    Calculates -LL for PyTorch (optimizing Stable Reparameterization + log(nugget)).\n",
    "    \"\"\"\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # 1. Apply Reparameterization\n",
    "    phi1 = raw_params[0].pow(2).squeeze() + epsilon # theta_1 = sigma2 / a\n",
    "    phi2 = raw_params[1].pow(2).squeeze() + epsilon # theta_2 = 1 / a\n",
    "    phi3 = raw_params[2].pow(2).squeeze() + epsilon # phi_3 = theta_3^2\n",
    "    nugget = torch.exp(raw_params[3]).squeeze()     # Nugget = exp(log_nugget)\n",
    "    \n",
    "    # 2. Derive Original Parameters\n",
    "    range_a = 1.0 / phi2          \n",
    "    sigma2 = phi1 / phi2          \n",
    "    \n",
    "    # 3. Compute Anisotropic Distance\n",
    "    aniso_dist_sq = (d_lon_sq_torch / phi3) + d_lat_sq_torch\n",
    "    aniso_dist = torch.sqrt(aniso_dist_sq + epsilon)\n",
    "    \n",
    "    # 4. Calculate Covariance Matrix C\n",
    "    C = exponential_covariance_torch(aniso_dist, sigma2, range_a, nugget)\n",
    "    \n",
    "    try:\n",
    "        L = torch.linalg.cholesky(C)\n",
    "        log_det = 2.0 * torch.sum(torch.log(torch.diag(L)))\n",
    "        alpha = torch.linalg.solve(C, z_centered_torch.unsqueeze(1))\n",
    "        quad_term = z_centered_torch.unsqueeze(0) @ alpha\n",
    "        neg_LL = 0.5 * log_det + 0.5 * quad_term.squeeze()\n",
    "        \n",
    "        if torch.isnan(neg_LL) or torch.isinf(neg_LL):\n",
    "            return torch.tensor(1e15, device=C.device, dtype=torch.float) + raw_params.sum() * 0.0\n",
    "\n",
    "        return neg_LL\n",
    "    except RuntimeError: # Catch Cholesky failures\n",
    "        return torch.tensor(1e15, device=C.device, dtype=torch.float) + raw_params.sum() * 0.0\n",
    "\n",
    "\n",
    "# --- Data Generation Function ---\n",
    "def generate_ozone_data_map(coords, sigma2, a, nugget, mean, time_index, anisotropy_ratio):\n",
    "    n_points = coords.shape[0]\n",
    "    coords_transformed = coords.copy()\n",
    "    coords_transformed[:, 1] = coords_transformed[:, 1] / anisotropy_ratio\n",
    "    \n",
    "    distances = cdist(coords_transformed, coords_transformed, metric='euclidean')\n",
    "    \n",
    "    Cov = exponential_covariance_numpy(distances, sigma2, a, nugget) \n",
    "    Cov = (Cov + Cov.T) / 2\n",
    "    \n",
    "    try:\n",
    "        L = np.linalg.cholesky(Cov)\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(f\"Warning: Cholesky failed in data generation with N={n_points}. Cov matrix may be singular.\")\n",
    "        return np.zeros((n_points, N_FEATURES))\n",
    "\n",
    "    W = np.random.normal(0, 1, size=(n_points, 1))\n",
    "    Z_correlated = L @ W\n",
    "    ozone_values = mean + Z_correlated\n",
    "    \n",
    "    data_np = np.zeros((n_points, N_FEATURES))\n",
    "    data_np[:, 0:1] = ozone_values             \n",
    "    data_np[:, 1] = coords[:, 1] * 10 + 2      # Original lon\n",
    "    data_np[:, 2] = coords[:, 0] * 40 + 250    # Original lat\n",
    "    data_np[:, 3] = time_index                 \n",
    "    return data_np\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# --- MAIN SIMULATION LOOP ---\n",
    "# ==========================================================\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # --- 1. Setup Simulation ---\n",
    "    start_time_total = time.time()\n",
    "    print(f\"--- Starting {NUM_SIMULATIONS} Simulation Runs ---\")\n",
    "    print(f\"--- Grid Size: {GRID_X}x{GRID_Y} (N = {N_SPATIAL_POINTS}) ---\")\n",
    "    \n",
    "    # Pre-calculate coordinate geometry (this doesn't change)\n",
    "    lat_coords = np.linspace(LAT_MIN, LAT_MAX, GRID_Y)\n",
    "    lon_coords = np.linspace(LON_MIN, LON_MAX, GRID_X)\n",
    "    coords_latlon = np.array([[lat, lon] for lat in lat_coords for lon in lon_coords]) # [lat, lon]\n",
    "    \n",
    "    coordinates = coords_latlon[:, [1, 0]] # Switch to [lon, lat]\n",
    "    lons = coordinates[:, 0:1] \n",
    "    lats = coordinates[:, 1:2] \n",
    "    d_lon_np = cdist(lons, lons, metric='euclidean')\n",
    "    d_lat_np = cdist(lats, lats, metric='euclidean')\n",
    "    d_lon_sq_np = np.square(d_lon_np)\n",
    "    d_lat_sq_np = np.square(d_lat_np)\n",
    "    \n",
    "    d_lon_sq_torch = torch.tensor(d_lon_sq_np, dtype=torch.float)\n",
    "    d_lat_sq_torch = torch.tensor(d_lat_sq_np, dtype=torch.float)\n",
    "    \n",
    "    # Calculate initial parameter guesses (these are reset every loop)\n",
    "    PHI1_TARGET = SIGMA2_TRUE / RANGE_A_TRUE\n",
    "    PHI2_TARGET = 1.0 / RANGE_A_TRUE\n",
    "    PHI3_TARGET = PHI3_TARGET_SQ\n",
    "    \n",
    "    raw_phi1_sqrt_start = np.sqrt(PHI1_TARGET - 3.0) \n",
    "    raw_phi2_sqrt_start = np.sqrt(PHI2_TARGET - 0.1) \n",
    "    raw_phi3_sqrt_start = np.sqrt(3.0) \n",
    "    NUGGET_INIT_GUESS = 0.3\n",
    "    LOG_NUGGET_START = np.log(NUGGET_INIT_GUESS)\n",
    "\n",
    "    initial_params_stable = [\n",
    "        raw_phi1_sqrt_start, \n",
    "        raw_phi2_sqrt_start,\n",
    "        raw_phi3_sqrt_start,\n",
    "        LOG_NUGGET_START\n",
    "    ]\n",
    "    \n",
    "    # --- 2. Initialize Result Storage ---\n",
    "    results_lbfgs = []\n",
    "    results_adam = []\n",
    "\n",
    "    # --- 3. Run Simulation Loop ---\n",
    "    for i in range(NUM_SIMULATIONS):\n",
    "        run_start_time = time.time()\n",
    "        print(f\"--- Running Simulation {i+1}/{NUM_SIMULATIONS} ---\")\n",
    "\n",
    "        # --- A. Generate NEW Data ---\n",
    "        data_np = generate_ozone_data_map(\n",
    "            coords_latlon, SIGMA2_TRUE, RANGE_A_TRUE, NUGGET_TRUE, OZONE_MEAN, 21.0,\n",
    "            ANISOTROPY_RATIO_TRUE\n",
    "        )\n",
    "        data_to_fit = torch.tensor(data_np, dtype=torch.float)[:N_SPATIAL_POINTS, :]\n",
    "        z_data = data_to_fit[:, 0].numpy()\n",
    "        z_centered_np = z_data - np.mean(z_data)\n",
    "        z_centered_torch = torch.tensor(z_centered_np, dtype=torch.float)\n",
    "\n",
    "        # --- B. Run L-BFGS ---\n",
    "        raw_params_lbfgs = torch.tensor(\n",
    "            initial_params_stable, \n",
    "            dtype=torch.float, \n",
    "            requires_grad=True\n",
    "        )\n",
    "        optimizer_lbfgs = optim.LBFGS(\n",
    "            [raw_params_lbfgs], \n",
    "            lr=1.0, \n",
    "            max_iter=LBFGS_MAX_STEPS,\n",
    "            max_eval=LBFGS_MAX_EVAL \n",
    "        )\n",
    "        final_loss_lbfgs = torch.tensor(0.0)\n",
    "\n",
    "        def closure_lbfgs():\n",
    "            optimizer_lbfgs.zero_grad()\n",
    "            loss = neg_log_likelihood_torch_stable(\n",
    "                raw_params_lbfgs, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch\n",
    "            )\n",
    "            if not torch.isinf(loss) and not torch.isnan(loss):\n",
    "                loss.backward()\n",
    "            return loss\n",
    "\n",
    "        for step in range(LBFGS_MAX_STEPS):\n",
    "            loss = optimizer_lbfgs.step(closure_lbfgs)\n",
    "            final_loss_lbfgs = loss\n",
    "            \n",
    "            if PRINT_EPOCHS and ((step + 1) % 5 == 0): \n",
    "                # (Logging is silenced by default)\n",
    "                pass \n",
    "\n",
    "        # --- C. Run Adam ---\n",
    "        raw_params_adam = torch.tensor(\n",
    "            initial_params_stable, \n",
    "            dtype=torch.float, \n",
    "            requires_grad=True\n",
    "        )\n",
    "        optimizer_adam = optim.Adam([raw_params_adam], lr=ADAM_LEARNING_RATE)\n",
    "        final_loss_adam = torch.tensor(0.0)\n",
    "\n",
    "        for epoch in range(ADAM_ITERATIONS):\n",
    "            optimizer_adam.zero_grad()\n",
    "            loss = neg_log_likelihood_torch_stable(\n",
    "                raw_params_adam, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch\n",
    "            )\n",
    "            if torch.isinf(loss) or torch.isnan(loss):\n",
    "                continue \n",
    "            loss.backward()\n",
    "            optimizer_adam.step()\n",
    "            final_loss_adam = loss\n",
    "            \n",
    "            if PRINT_EPOCHS and ((epoch + 1) % 50 == 0): \n",
    "                # (Logging is silenced by default)\n",
    "                pass\n",
    "        \n",
    "        # --- D. Store Results for this run ---\n",
    "        \n",
    "        # L-BFGS Results\n",
    "        with torch.no_grad():\n",
    "            phi1_lbfgs = raw_params_lbfgs[0].pow(2).item()\n",
    "            phi2_lbfgs = raw_params_lbfgs[1].pow(2).item()\n",
    "            phi3_lbfgs = raw_params_lbfgs[2].pow(2).item()\n",
    "            fitted_nugget_lbfgs = torch.exp(raw_params_lbfgs[3]).item()\n",
    "        \n",
    "        results_lbfgs.append({\n",
    "            'sigma2': phi1_lbfgs / (phi2_lbfgs + 1e-6),\n",
    "            'range_a': 1.0 / (phi2_lbfgs + 1e-6),\n",
    "            'ratio': np.sqrt(phi3_lbfgs),\n",
    "            'nugget': fitted_nugget_lbfgs,\n",
    "            'nll': final_loss_lbfgs.item()\n",
    "        })\n",
    "        \n",
    "        # Adam Results\n",
    "        with torch.no_grad():\n",
    "            phi1_adam = raw_params_adam[0].pow(2).item()\n",
    "            phi2_adam = raw_params_adam[1].pow(2).item()\n",
    "            phi3_adam = raw_params_adam[2].pow(2).item()\n",
    "            fitted_nugget_adam = torch.exp(raw_params_adam[3]).item()\n",
    "        \n",
    "        results_adam.append({\n",
    "            'sigma2': phi1_adam / (phi2_adam + 1e-6),\n",
    "            'range_a': 1.0 / (phi2_adam + 1e-6),\n",
    "            'ratio': np.sqrt(phi3_adam),\n",
    "            'nugget': fitted_nugget_adam,\n",
    "            'nll': final_loss_adam.item()\n",
    "        })\n",
    "        \n",
    "        run_end_time = time.time()\n",
    "        print(f\"--- Simulation {i+1} complete. Time elapsed: {run_end_time - run_start_time:.2f}s ---\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- All Simulations Complete ---\")\n",
    "    end_time_total = time.time()\n",
    "\n",
    "    # ==========================================================\n",
    "    # --- 4. Display Aggregate Results ---\n",
    "    # ==========================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*75)\n",
    "    print(f\"TARGET PARAMETERS: Variance (ÏƒÂ²)={SIGMA2_TRUE}, Range (a)={RANGE_A_TRUE}, Anisotropy (Î¸â‚ƒ-ratio)={ANISOTROPY_RATIO_TRUE}, Nugget (Î·Â²)={NUGGET_TRUE}\")\n",
    "    print(f\"AGGREGATE RESULTS OVER {NUM_SIMULATIONS} RUNS\")\n",
    "    print(f\"Grid Size: {GRID_X}x{GRID_Y} (N = {N_SPATIAL_POINTS})\")\n",
    "    print(f\"Total time: {end_time_total - start_time_total:.2f} seconds\")\n",
    "    print(\"=\"*75)\n",
    "\n",
    "    # Helper function to calculate and print stats\n",
    "    def print_stats(results_list, optimizer_name):\n",
    "        # Convert list of dicts to a dict of numpy arrays\n",
    "        # Filter out NaN/Inf values that could arise from failed runs\n",
    "        params = {}\n",
    "        for key in results_list[0].keys():\n",
    "            valid_values = [res[key] for res in results_list if np.isfinite(res[key])]\n",
    "            if not valid_values:\n",
    "                valid_values = [np.nan] # Handle case where all runs failed for a param\n",
    "            params[key] = np.array(valid_values)\n",
    "        \n",
    "        num_valid = len(params['nll'])\n",
    "        num_total = len(results_list)\n",
    "\n",
    "        print(f\"âœ¨ {optimizer_name} Average Results ({num_valid}/{num_total} Valid Runs):\")\n",
    "        \n",
    "        # Helper for printing mean/std\n",
    "        def ms(key):\n",
    "            return f\"{np.mean(params[key]):.3f} (Std: {np.std(params[key]):.3f})\"\n",
    "\n",
    "        print(f\"  * Fitted Variance (ÏƒÂ²): {ms('sigma2')} (Target: {SIGMA2_TRUE})\")\n",
    "        print(f\"  * Fitted Range (a): {ms('range_a')} (Target: {RANGE_A_TRUE})\")\n",
    "        print(f\"  * Fitted Anisotropy (Î¸â‚ƒ-ratio): {ms('ratio')} (Target: {ANISOTROPY_RATIO_TRUE})\")\n",
    "        print(f\"  * Fitted Nugget (Î·Â²): {ms('nugget')} (Target: {NUGGET_TRUE})\")\n",
    "        print(f\"  * Final -LL Value: {np.mean(params['nll']):.2f} (Std: {np.std(params['nll']):.2f})\")\n",
    "\n",
    "    try:\n",
    "        if results_lbfgs:\n",
    "            print_stats(results_lbfgs, \"PyTorch L-BFGS\")\n",
    "        else:\n",
    "            print(\"No valid L-BFGS results to display.\")\n",
    "            \n",
    "        print(\"\\n\" + \"-\"*75 + \"\\n\") # Add a separator\n",
    "        \n",
    "        if results_adam:\n",
    "            print_stats(results_adam, \"PyTorch Adam\")\n",
    "        else:\n",
    "            print(\"No valid Adam results to display.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during final reporting: {e}\")\n",
    "        print(\"\\nRaw L-BFGS Results:\", results_lbfgs)\n",
    "        print(\"\\nRaw Adam Results:\", results_adam)\n",
    "\n",
    "    print(\"=\"*75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a4b0a",
   "metadata": {},
   "source": [
    "use log transformation instead of optimizing raw prams .pow()     torch.exp(raw_log_param) this guarantees positivity without epsilon hack and optimization more lobust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28774c9b",
   "metadata": {},
   "source": [
    "# log + reparam + non zero nugget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed9f4c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting 1 Simulation Runs ---\n",
      "--- Grid Size: 80x56 (N = 4480) ---\n",
      "--- Running Simulation 1/1 ---\n",
      "--- Simulation 1 complete. Time elapsed: 252.11s ---\n",
      "\n",
      "--- All Simulations Complete ---\n",
      "\n",
      "===========================================================================\n",
      "TARGET PARAMETERS: Variance (ÏƒÂ²)=11.0, Range (a)=1.9, Anisotropy (Î¸â‚ƒ-ratio)=0.5, Nugget (Î·Â²)=0.3\n",
      "AGGREGATE RESULTS OVER 1 RUNS\n",
      "Grid Size: 80x56 (N = 4480)\n",
      "Total time: 252.26 seconds\n",
      "===========================================================================\n",
      "âœ¨ PyTorch L-BFGS Average Results (1/1 Valid Runs):\n",
      "  * Fitted Variance (ÏƒÂ²): 7.925 (Std: 0.000) (Target: 11.0)\n",
      "  * Fitted Range (a): 1.326 (Std: 0.000) (Target: 1.9)\n",
      "  * Fitted Anisotropy (Î¸â‚ƒ-ratio): 0.476 (Std: 0.000) (Target: 0.5)\n",
      "  * Fitted Nugget (Î·Â²): 0.328 (Std: 0.000) (Target: 0.3)\n",
      "  * Final -LL Value: 2919.92 (Std: 0.00)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "âœ¨ PyTorch Adam Average Results (1/1 Valid Runs):\n",
      "  * Fitted Variance (ÏƒÂ²): 8.071 (Std: 0.000) (Target: 11.0)\n",
      "  * Fitted Range (a): 1.241 (Std: 0.000) (Target: 1.9)\n",
      "  * Fitted Anisotropy (Î¸â‚ƒ-ratio): 0.594 (Std: 0.000) (Target: 0.5)\n",
      "  * Fitted Nugget (Î·Â²): 0.341 (Std: 0.000) (Target: 0.3)\n",
      "  * Final -LL Value: 2936.05 (Std: 0.00)\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from scipy.spatial.distance import cdist\n",
    "from torch.nn import Parameter\n",
    "import time\n",
    "\n",
    "# --- 0. Global Parameters and Utility Functions ---\n",
    "# ðŸ’¥ MODIFIED: Increased grid density\n",
    "N_SPATIAL_POINTS = 4480  # 80 * 56\n",
    "GRID_X = 80              # ðŸ’¥ MODIFIED\n",
    "GRID_Y = 56              # ðŸ’¥ MODIFIED\n",
    "\n",
    "N_DAYS = 31\n",
    "N_HOURS_PER_DAY = 8\n",
    "N_FEATURES = 4\n",
    "LAT_MIN, LAT_MAX = 0, 5\n",
    "LON_MIN, LON_MAX = 113, 123\n",
    "BASE_DATE = '2024_07_y24m07day' \n",
    "\n",
    "# Exponential Kernel Parameters (Targets)\n",
    "SIGMA2_TRUE = 11.0      # TARGET Variance\n",
    "RANGE_A_TRUE = 1.9      # TARGET Range\n",
    "ANISOTROPY_RATIO_TRUE = 0.5 # TARGET Anisotropy\n",
    "PHI3_TARGET_SQ = ANISOTROPY_RATIO_TRUE**2 \n",
    "NUGGET_TRUE = 0.3       # TARGET Nugget for data generation\n",
    "\n",
    "# --- Simulation Controls ---\n",
    "NUM_SIMULATIONS = 1      \n",
    "PRINT_EPOCHS = False      # Set to False to silence epoch logging\n",
    "\n",
    "# Optimization Setup\n",
    "ADAM_ITERATIONS = 500\n",
    "ADAM_LEARNING_RATE = 0.01\n",
    "\n",
    "# L-BFGS Setup\n",
    "LBFGS_MAX_STEPS = 50 \n",
    "LBFGS_MAX_EVAL = 50 \n",
    "\n",
    "OZONE_MEAN = 240.0\n",
    "\n",
    "# --- COVARIANCE FUNCTIONS ---\n",
    "\n",
    "def exponential_covariance_numpy(distances, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (NumPy for Generation).\"\"\"\n",
    "    cov = sigma2 * np.exp(-distances / a)\n",
    "    if distances.shape[0] == distances.shape[1]:\n",
    "        cov[np.diag_indices_from(distances)] += (nugget + 1e-6)\n",
    "    return cov\n",
    "\n",
    "def exponential_covariance_torch(distances_torch, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (PyTorch for Optimization).\"\"\"\n",
    "    cov = sigma2 * torch.exp(-distances_torch / a)\n",
    "    \n",
    "    if distances_torch.shape[0] == distances_torch.shape[1]:\n",
    "        jitter = 1e-6 \n",
    "        diag_mask = torch.eye(cov.shape[0], device=cov.device)\n",
    "        cov = cov + diag_mask * (nugget + jitter)\n",
    "    return cov\n",
    "\n",
    "# --- ðŸ’¥ MODIFIED NLL Function (Log-Frame) ðŸ’¥ ---\n",
    "def neg_log_likelihood_torch_stable(raw_params, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch):\n",
    "    \"\"\"\n",
    "    Calculates -LL for PyTorch (optimizing Log-Reparameterization).\n",
    "    \"\"\"\n",
    "    \n",
    "    epsilon = 1e-6 # Retain epsilon for sqrt and division, but not for exp\n",
    "    \n",
    "    # 1. Apply Log-Reparameterization\n",
    "    phi1   = torch.exp(raw_params[0]).squeeze() # theta_1 = sigma2 / a\n",
    "    phi2   = torch.exp(raw_params[1]).squeeze() # theta_2 = 1 / a\n",
    "    phi3   = torch.exp(raw_params[2]).squeeze() # phi_3 = theta_3^2\n",
    "    nugget = torch.exp(raw_params[3]).squeeze() # Nugget = exp(log_nugget)\n",
    "    \n",
    "    # 2. Derive Original Parameters\n",
    "    range_a = 1.0 / (phi2 + epsilon) # Add epsilon to prevent 1/0\n",
    "    sigma2 = phi1 / (phi2 + epsilon)\n",
    "    \n",
    "    # 3. Compute Anisotropic Distance\n",
    "    aniso_dist_sq = (d_lon_sq_torch / (phi3 + epsilon)) + d_lat_sq_torch # Add epsilon\n",
    "    aniso_dist = torch.sqrt(aniso_dist_sq + epsilon)\n",
    "    \n",
    "    # 4. Calculate Covariance Matrix C\n",
    "    C = exponential_covariance_torch(aniso_dist, sigma2, range_a, nugget)\n",
    "    \n",
    "    try:\n",
    "        L = torch.linalg.cholesky(C)\n",
    "        log_det = 2.0 * torch.sum(torch.log(torch.diag(L)))\n",
    "        alpha = torch.linalg.solve(C, z_centered_torch.unsqueeze(1))\n",
    "        quad_term = z_centered_torch.unsqueeze(0) @ alpha\n",
    "        neg_LL = 0.5 * log_det + 0.5 * quad_term.squeeze()\n",
    "        \n",
    "        if torch.isnan(neg_LL) or torch.isinf(neg_LL):\n",
    "            return torch.tensor(1e15, device=C.device, dtype=torch.float) + raw_params.sum() * 0.0\n",
    "\n",
    "        return neg_LL\n",
    "    except RuntimeError: # Catch Cholesky failures\n",
    "        return torch.tensor(1e15, device=C.device, dtype=torch.float) + raw_params.sum() * 0.0\n",
    "\n",
    "\n",
    "# --- Data Generation Function ---\n",
    "def generate_ozone_data_map(coords, sigma2, a, nugget, mean, time_index, anisotropy_ratio):\n",
    "    n_points = coords.shape[0]\n",
    "    coords_transformed = coords.copy()\n",
    "    coords_transformed[:, 1] = coords_transformed[:, 1] / anisotropy_ratio\n",
    "    \n",
    "    distances = cdist(coords_transformed, coords_transformed, metric='euclidean')\n",
    "    \n",
    "    Cov = exponential_covariance_numpy(distances, sigma2, a, nugget) \n",
    "    Cov = (Cov + Cov.T) / 2\n",
    "    \n",
    "    try:\n",
    "        L = np.linalg.cholesky(Cov)\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(f\"Warning: Cholesky failed in data generation with N={n_points}. Cov matrix may be singular.\")\n",
    "        return np.zeros((n_points, N_FEATURES))\n",
    "\n",
    "    W = np.random.normal(0, 1, size=(n_points, 1))\n",
    "    Z_correlated = L @ W\n",
    "    ozone_values = mean + Z_correlated\n",
    "    \n",
    "    data_np = np.zeros((n_points, N_FEATURES))\n",
    "    data_np[:, 0:1] = ozone_values             \n",
    "    data_np[:, 1] = coords[:, 1] * 10 + 2      # Original lon\n",
    "    data_np[:, 2] = coords[:, 0] * 40 + 250    # Original lat\n",
    "    data_np[:, 3] = time_index                 \n",
    "    return data_np\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# --- MAIN SIMULATION LOOP ---\n",
    "# ==========================================================\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # --- 1. Setup Simulation ---\n",
    "    start_time_total = time.time()\n",
    "    print(f\"--- Starting {NUM_SIMULATIONS} Simulation Runs ---\")\n",
    "    print(f\"--- Grid Size: {GRID_X}x{GRID_Y} (N = {N_SPATIAL_POINTS}) ---\")\n",
    "    \n",
    "    # Pre-calculate coordinate geometry (this doesn't change)\n",
    "    lat_coords = np.linspace(LAT_MIN, LAT_MAX, GRID_Y)\n",
    "    lon_coords = np.linspace(LON_MIN, LON_MAX, GRID_X)\n",
    "    coords_latlon = np.array([[lat, lon] for lat in lat_coords for lon in lon_coords]) # [lat, lon]\n",
    "    \n",
    "    coordinates = coords_latlon[:, [1, 0]] # Switch to [lon, lat]\n",
    "    lons = coordinates[:, 0:1] \n",
    "    lats = coordinates[:, 1:2] \n",
    "    d_lon_np = cdist(lons, lons, metric='euclidean')\n",
    "    d_lat_np = cdist(lats, lats, metric='euclidean')\n",
    "    d_lon_sq_np = np.square(d_lon_np)\n",
    "    d_lat_sq_np = np.square(d_lat_np)\n",
    "    \n",
    "    d_lon_sq_torch = torch.tensor(d_lon_sq_np, dtype=torch.float)\n",
    "    d_lat_sq_torch = torch.tensor(d_lat_sq_np, dtype=torch.float)\n",
    "    \n",
    "    # Calculate initial parameter guesses (these are reset every loop)\n",
    "    PHI1_TARGET = SIGMA2_TRUE / RANGE_A_TRUE\n",
    "    PHI2_TARGET = 1.0 / RANGE_A_TRUE\n",
    "    PHI3_TARGET = PHI3_TARGET_SQ\n",
    "    \n",
    "    # --- ðŸ’¥ MODIFIED: Initial Guesses are now in Log-Frame ðŸ’¥ ---\n",
    "    # Use log of the (off-target) values\n",
    "    raw_log_phi1_start = np.log(max(PHI1_TARGET - 3.0, 1e-6))\n",
    "    raw_log_phi2_start = np.log(max(PHI2_TARGET - 0.1, 1e-6))\n",
    "    raw_log_phi3_start = np.log(max(3.0, 1e-6)) # Start at 3.0\n",
    "    NUGGET_INIT_GUESS = 0.3\n",
    "    LOG_NUGGET_START = np.log(NUGGET_INIT_GUESS)\n",
    "\n",
    "    initial_params_stable = [\n",
    "        raw_log_phi1_start, \n",
    "        raw_log_phi2_start,\n",
    "        raw_log_phi3_start,\n",
    "        LOG_NUGGET_START\n",
    "    ]\n",
    "    \n",
    "    # --- 2. Initialize Result Storage ---\n",
    "    results_lbfgs = []\n",
    "    results_adam = []\n",
    "\n",
    "    # --- 3. Run Simulation Loop ---\n",
    "    for i in range(NUM_SIMULATIONS):\n",
    "        run_start_time = time.time()\n",
    "        print(f\"--- Running Simulation {i+1}/{NUM_SIMULATIONS} ---\")\n",
    "\n",
    "        # --- A. Generate NEW Data ---\n",
    "        data_np = generate_ozone_data_map(\n",
    "            coords_latlon, SIGMA2_TRUE, RANGE_A_TRUE, NUGGET_TRUE, OZONE_MEAN, 21.0,\n",
    "            ANISOTROPY_RATIO_TRUE\n",
    "        )\n",
    "        data_to_fit = torch.tensor(data_np, dtype=torch.float)[:N_SPATIAL_POINTS, :]\n",
    "        z_data = data_to_fit[:, 0].numpy()\n",
    "        z_centered_np = z_data - np.mean(z_data)\n",
    "        z_centered_torch = torch.tensor(z_centered_np, dtype=torch.float)\n",
    "\n",
    "        # --- B. Run L-BFGS ---\n",
    "        raw_params_lbfgs = torch.tensor(\n",
    "            initial_params_stable, \n",
    "            dtype=torch.float, \n",
    "            requires_grad=True\n",
    "        )\n",
    "        optimizer_lbfgs = optim.LBFGS(\n",
    "            [raw_params_lbfgs], \n",
    "            lr=1.0, \n",
    "            max_iter=LBFGS_MAX_STEPS,\n",
    "            max_eval=LBFGS_MAX_EVAL \n",
    "        )\n",
    "        final_loss_lbfgs = torch.tensor(0.0)\n",
    "\n",
    "        def closure_lbfgs():\n",
    "            optimizer_lbfgs.zero_grad()\n",
    "            loss = neg_log_likelihood_torch_stable(\n",
    "                raw_params_lbfgs, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch\n",
    "            )\n",
    "            if not torch.isinf(loss) and not torch.isnan(loss):\n",
    "                loss.backward()\n",
    "            return loss\n",
    "\n",
    "        for step in range(LBFGS_MAX_STEPS):\n",
    "            loss = optimizer_lbfgs.step(closure_lbfgs)\n",
    "            final_loss_lbfgs = loss\n",
    "            \n",
    "            if PRINT_EPOCHS and ((step + 1) % 5 == 0): \n",
    "                # (Logging is silenced by default)\n",
    "                pass \n",
    "\n",
    "        # --- C. Run Adam ---\n",
    "        raw_params_adam = torch.tensor(\n",
    "            initial_params_stable, \n",
    "            dtype=torch.float, \n",
    "            requires_grad=True\n",
    "        )\n",
    "        optimizer_adam = optim.Adam([raw_params_adam], lr=ADAM_LEARNING_RATE)\n",
    "        final_loss_adam = torch.tensor(0.0)\n",
    "\n",
    "        for epoch in range(ADAM_ITERATIONS):\n",
    "            optimizer_adam.zero_grad()\n",
    "            loss = neg_log_likelihood_torch_stable(\n",
    "                raw_params_adam, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch\n",
    "            )\n",
    "            if torch.isinf(loss) or torch.isnan(loss):\n",
    "                continue \n",
    "            loss.backward()\n",
    "            optimizer_adam.step()\n",
    "            final_loss_adam = loss\n",
    "            \n",
    "            if PRINT_EPOCHS and ((epoch + 1) % 50 == 0): \n",
    "                # (Logging is silenced by default)\n",
    "                pass\n",
    "        \n",
    "        # --- D. Store Results for this run ---\n",
    "        \n",
    "        # --- ðŸ’¥ MODIFIED: L-BFGS Results (Log-Frame) ðŸ’¥ ---\n",
    "        with torch.no_grad():\n",
    "            phi1_lbfgs = torch.exp(raw_params_lbfgs[0]).item()\n",
    "            phi2_lbfgs = torch.exp(raw_params_lbfgs[1]).item()\n",
    "            phi3_lbfgs = torch.exp(raw_params_lbfgs[2]).item()\n",
    "            fitted_nugget_lbfgs = torch.exp(raw_params_lbfgs[3]).item()\n",
    "        \n",
    "        results_lbfgs.append({\n",
    "            'sigma2': phi1_lbfgs / (phi2_lbfgs + 1e-6),\n",
    "            'range_a': 1.0 / (phi2_lbfgs + 1e-6),\n",
    "            'ratio': np.sqrt(phi3_lbfgs),\n",
    "            'nugget': fitted_nugget_lbfgs,\n",
    "            'nll': final_loss_lbfgs.item()\n",
    "        })\n",
    "        \n",
    "        # --- ðŸ’¥ MODIFIED: Adam Results (Log-Frame) ðŸ’¥ ---\n",
    "        with torch.no_grad():\n",
    "            phi1_adam = torch.exp(raw_params_adam[0]).item()\n",
    "            phi2_adam = torch.exp(raw_params_adam[1]).item()\n",
    "            phi3_adam = torch.exp(raw_params_adam[2]).item()\n",
    "            fitted_nugget_adam = torch.exp(raw_params_adam[3]).item()\n",
    "        \n",
    "        results_adam.append({\n",
    "            'sigma2': phi1_adam / (phi2_adam + 1e-6),\n",
    "            'range_a': 1.0 / (phi2_adam + 1e-6),\n",
    "            'ratio': np.sqrt(phi3_adam),\n",
    "            'nugget': fitted_nugget_adam,\n",
    "            'nll': final_loss_adam.item()\n",
    "        })\n",
    "        \n",
    "        run_end_time = time.time()\n",
    "        print(f\"--- Simulation {i+1} complete. Time elapsed: {run_end_time - run_start_time:.2f}s ---\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- All Simulations Complete ---\")\n",
    "    end_time_total = time.time()\n",
    "\n",
    "    # ==========================================================\n",
    "    # --- 4. Display Aggregate Results ---\n",
    "    # ==========================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*75)\n",
    "    print(f\"TARGET PARAMETERS: Variance (ÏƒÂ²)={SIGMA2_TRUE}, Range (a)={RANGE_A_TRUE}, Anisotropy (Î¸â‚ƒ-ratio)={ANISOTROPY_RATIO_TRUE}, Nugget (Î·Â²)={NUGGET_TRUE}\")\n",
    "    print(f\"AGGREGATE RESULTS OVER {NUM_SIMULATIONS} RUNS\")\n",
    "    print(f\"Grid Size: {GRID_X}x{GRID_Y} (N = {N_SPATIAL_POINTS})\")\n",
    "    print(f\"Total time: {end_time_total - start_time_total:.2f} seconds\")\n",
    "    print(\"=\"*75)\n",
    "\n",
    "    # Helper function to calculate and print stats\n",
    "    def print_stats(results_list, optimizer_name):\n",
    "        # Convert list of dicts to a dict of numpy arrays\n",
    "        # Filter out NaN/Inf values that could arise from failed runs\n",
    "        params = {}\n",
    "        for key in results_list[0].keys():\n",
    "            valid_values = [res[key] for res in results_list if np.isfinite(res[key])]\n",
    "            if not valid_values:\n",
    "                valid_values = [np.nan] # Handle case where all runs failed for a param\n",
    "            params[key] = np.array(valid_values)\n",
    "        \n",
    "        num_valid = len(params['nll'])\n",
    "        num_total = len(results_list)\n",
    "\n",
    "        print(f\"âœ¨ {optimizer_name} Average Results ({num_valid}/{num_total} Valid Runs):\")\n",
    "        \n",
    "        # Helper for printing mean/std\n",
    "        def ms(key):\n",
    "            return f\"{np.mean(params[key]):.3f} (Std: {np.std(params[key]):.3f})\"\n",
    "\n",
    "        print(f\"  * Fitted Variance (ÏƒÂ²): {ms('sigma2')} (Target: {SIGMA2_TRUE})\")\n",
    "        print(f\"  * Fitted Range (a): {ms('range_a')} (Target: {RANGE_A_TRUE})\")\n",
    "        print(f\"  * Fitted Anisotropy (Î¸â‚ƒ-ratio): {ms('ratio')} (Target: {ANISOTROPY_RATIO_TRUE})\")\n",
    "        print(f\"  * Fitted Nugget (Î·Â²): {ms('nugget')} (Target: {NUGGET_TRUE})\")\n",
    "        print(f\"  * Final -LL Value: {np.mean(params['nll']):.2f} (Std: {np.std(params['nll']):.2f})\")\n",
    "\n",
    "    try:\n",
    "        if results_lbfgs:\n",
    "            print_stats(results_lbfgs, \"PyTorch L-BFGS\")\n",
    "        else:\n",
    "            print(\"No valid L-BFGS results to display.\")\n",
    "            \n",
    "        print(\"\\n\" + \"-\"*75 + \"\\n\") # Add a separator\n",
    "        \n",
    "        if results_adam:\n",
    "            print_stats(results_adam, \"PyTorch Adam\")\n",
    "        else:\n",
    "            print(\"No valid Adam results to display.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during final reporting: {e}\")\n",
    "        print(\"\\nRaw L-BFGS Results:\", results_lbfgs)\n",
    "        print(\"\\nRaw Adam Results:\", results_adam)\n",
    "\n",
    "    print(\"=\"*75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
