{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f33ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reflected location error in ozone data simulation\n",
    "\n",
    "import torch\n",
    "import torch.fft\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import argparse \n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "from sklearn.neighbors import BallTree\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CUSTOM PATHS ---\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "# (ÌïÑÏöî Ïãú Ïã§Ï†ú GEMS_TCO ÎùºÏù¥Î∏åÎü¨Î¶¨ import)\n",
    "try:\n",
    "    from GEMS_TCO import kernels_for_simulation_no_trend_020626 as kernels_simulation\n",
    "    \n",
    "    from GEMS_TCO import kernels_columns as kernels_reparam_space_time_gpu_col\n",
    "    \n",
    "    from GEMS_TCO import orderings as _orderings\n",
    "    from GEMS_TCO import alg_optimization, BaseLogger\n",
    "\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Warning: GEMS_TCO modules not found. Ensure paths are correct.\")\n",
    "\n",
    "\n",
    "from GEMS_TCO import configuration as config\n",
    "from GEMS_TCO.data_loader import load_data2, exact_location_filter\n",
    "from GEMS_TCO import debiased_whittle\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d6ba3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Global Monthly Mean for 2024-7: 257.0667 ---\n",
      "Global Monthly Mean: 257.0666873781436\n",
      "torch.Size([22680, 11])\n"
     ]
    }
   ],
   "source": [
    "space: List[str] = ['1', '1']\n",
    "lat_lon_resolution = [int(s) for s in space]\n",
    "mm_cond_number: int = 8\n",
    "years = ['2024']\n",
    "month_range = [7] \n",
    "\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "data_load_instance = load_data2(config.mac_data_load_path)\n",
    "\n",
    "#lat_range_input = [1, 3]\n",
    "#lon_range_input = [125.0, 129.0]\n",
    "\n",
    "lat_range_input=[-3,-1]      \n",
    "lon_range_input=[121, 125] \n",
    "\n",
    "#lat_range_input=[-3,2]      \n",
    "#lon_range_input=[121, 131] \n",
    "\n",
    "# Í∏∞Ï°¥: df_map, ord_mm, nns_map, day_offsets = ...\n",
    "# ÏàòÏ†ï ÌõÑ: Î≥ÄÏàòÎ™ÖÏùÑ monthly_meanÏúºÎ°ú Î≥ÄÍ≤Ω\n",
    "\n",
    "df_map, ord_mm, nns_map, monthly_mean = data_load_instance.load_maxmin_ordered_data_bymonthyear(\n",
    "    lat_lon_resolution=lat_lon_resolution, \n",
    "    mm_cond_number=mm_cond_number,\n",
    "    years_=years, \n",
    "    months_=month_range,\n",
    "    lat_range=lat_range_input,   \n",
    "    lon_range=lon_range_input\n",
    ")\n",
    "\n",
    "print(f\"Global Monthly Mean: {monthly_mean}\") # ÌôïÏù∏Ïö© Ï∂úÎ†•\n",
    "\n",
    "\n",
    "daily_aggregated_reg_vecc = [] \n",
    "daily_hourly_maps_reg_vecc = []      \n",
    "\n",
    "daily_aggregated_irr_vecc = [] \n",
    "daily_hourly_maps_irr_vecc = []   \n",
    "\n",
    "\n",
    "for day_index in range(31):\n",
    "    hour_start_index = day_index * 8\n",
    "    \n",
    "    hour_end_index = (day_index + 1) * 8\n",
    "    #hour_end_index = day_index*8 + 1\n",
    "    hour_indices = [hour_start_index, hour_end_index]\n",
    "\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "    df_map, \n",
    "    monthly_mean,  # <--- Ïù¥ Î∂ÄÎ∂ÑÏù¥ Ï∂îÍ∞ÄÎêòÏñ¥Ïïº Ìï©ÎãàÎã§\n",
    "    hour_indices, \n",
    "    ord_mm=ord_mm,\n",
    "    dtype=torch.float64, \n",
    "    keep_ori=False\n",
    "    )\n",
    "\n",
    "    daily_aggregated_reg_vecc.append( day_aggregated_tensor )\n",
    "    daily_hourly_maps_reg_vecc.append( day_hourly_map )\n",
    "\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "    df_map, \n",
    "    monthly_mean,  # <--- Ïù¥ Î∂ÄÎ∂ÑÏù¥ Ï∂îÍ∞ÄÎêòÏñ¥Ïïº Ìï©ÎãàÎã§\n",
    "    hour_indices, \n",
    "    ord_mm=ord_mm,\n",
    "    dtype=torch.float64, \n",
    "    keep_ori= True\n",
    "    )\n",
    "\n",
    "    daily_aggregated_irr_vecc.append( day_aggregated_tensor )\n",
    "    daily_hourly_maps_irr_vecc.append( day_hourly_map )\n",
    "print(daily_aggregated_irr_vecc[0].shape)\n",
    "\n",
    "nn = daily_aggregated_irr_vecc[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f4c0dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating on: cpu\n",
      "Checking Inputs:\n",
      " -> Target SigmaSq: 13.059\n",
      "\n",
      "üöÄ Generating 3 Datasets (NO TREND, 4 COLS)...\n",
      "\n",
      "[Validation]\n",
      "1. Vecc Irr (Swath): torch.Size([22680, 4]) (Cols: 4)\n",
      "2. Vecc Reg (Snapped): torch.Size([22680, 4]) (Cols: 4)\n",
      "3. DW Full (Complete): torch.Size([23552, 4]) (Cols: 4)\n",
      "   -> DW Val Mean: 259.99\n",
      "Generating NNS Map for Full Grid (Size: 23552)...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "import sys\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.float64\n",
    "print(f\"Simulating on: {DEVICE}\")\n",
    "\n",
    "# --- 2. GRID & FFT HELPERS ---\n",
    "def make_target_grid(lat_start, lat_end, lat_step, lon_start, lon_end, lon_step, device, dtype):\n",
    "    if lat_start > lat_end and lat_step > 0: lat_step = -lat_step\n",
    "    if lon_start > lon_end and lon_step > 0: lon_step = -lon_step\n",
    "    lats = torch.arange(lat_start, lat_end - 0.0001, lat_step, device=device, dtype=dtype)\n",
    "    lons = torch.arange(lon_start, lon_end + 0.0001, lon_step, device=device, dtype=dtype)\n",
    "    lats = torch.round(lats * 10000) / 10000\n",
    "    lons = torch.round(lons * 10000) / 10000\n",
    "    grid_lat, grid_lon = torch.meshgrid(lats, lons, indexing='ij')\n",
    "    center_points = torch.stack([grid_lat.flatten(), grid_lon.flatten()], dim=1)\n",
    "    return center_points, len(lats), len(lons)\n",
    "\n",
    "def generate_high_res_field(target_lat_range, target_lon_range, t_steps, params, device, dtype):\n",
    "    lat_res_factor, lon_res_factor = 200.0, 4.0\n",
    "    lat_res_high = 0.044 / lat_res_factor\n",
    "    lon_res_high = 0.063 / lon_res_factor\n",
    "    \n",
    "    t_lat_max = max(target_lat_range)\n",
    "    t_lat_min = min(target_lat_range)\n",
    "    \n",
    "    lats_high = torch.arange(t_lat_max + 0.1, t_lat_min - 0.1, -lat_res_high, device=device, dtype=dtype)\n",
    "    lons_high = torch.arange(target_lon_range[0] - 0.1, target_lon_range[1] + 0.1, lon_res_high, device=device, dtype=dtype)\n",
    "    \n",
    "    Nx, Ny, Nt = len(lats_high), len(lons_high), t_steps\n",
    "    dlat, dlon, dt = lat_res_high, lon_res_high, 1.0\n",
    "    \n",
    "    Px, Py, Pt = 2*Nx, 2*Ny, 2*Nt\n",
    "    lags_x = torch.arange(Px, device=device, dtype=dtype) * dlat; lags_x[Px//2:] -= (Px * dlat)\n",
    "    lags_y = torch.arange(Py, device=device, dtype=dtype) * dlon; lags_y[Py//2:] -= (Py * dlon)\n",
    "    lags_t = torch.arange(Pt, device=device, dtype=dtype) * dt;   lags_t[Pt//2:] -= (Pt * dt)\n",
    "\n",
    "    L_x, L_y, L_t = torch.meshgrid(lags_x, lags_y, lags_t, indexing='ij')\n",
    "    phi1, phi2, phi3, phi4 = torch.exp(params[0]), torch.exp(params[1]), torch.exp(params[2]), torch.exp(params[3])\n",
    "    adv_lat, adv_lon = params[4], params[5]\n",
    "    sigma_sq = phi1 / phi2 \n",
    "    \n",
    "    u_x = L_x - adv_lat * L_t\n",
    "    u_y = L_y - adv_lon * L_t\n",
    "    dist_sq = (u_x * torch.sqrt(phi3) * phi2)**2 + (u_y * phi2)**2 + (L_t * torch.sqrt(phi4) * phi2)**2\n",
    "    C_vals = sigma_sq * torch.exp(-torch.sqrt(dist_sq + 1e-12))\n",
    "\n",
    "    S = torch.fft.fftn(C_vals); S.real = torch.clamp(S.real, min=0)\n",
    "    random_phase = torch.fft.fftn(torch.randn(Px, Py, Pt, device=device, dtype=dtype))\n",
    "    field_sim_raw = torch.fft.ifftn(torch.sqrt(S.real) * random_phase).real\n",
    "    \n",
    "    field_sim = field_sim_raw[:Nx, :Ny, :Nt]\n",
    "    target_std = torch.sqrt(sigma_sq)\n",
    "    field_calibrated = (field_sim - field_sim.mean()) * (target_std / (field_sim.std() + 1e-9))\n",
    "    \n",
    "    return field_calibrated, lats_high, lons_high\n",
    "\n",
    "# --- 3. [NO TREND] 3-WAY DATASET GENERATOR ---\n",
    "def generate_three_datasets_no_trend(daily_maps_real, true_params_tensor, target_grid_info, device, dtype):\n",
    "    \n",
    "    lat_s, lat_e, lat_step, lon_s, lon_e, lon_step = target_grid_info\n",
    "    \n",
    "    # 1. Target Grid (Full)\n",
    "    target_grid_coords, Nx, Ny = make_target_grid(lat_s, lat_e, lat_step, lon_s, lon_e, lon_step, device, dtype)\n",
    "    full_grid_locs_np = target_grid_coords.cpu().numpy()\n",
    "    target_tree = BallTree(np.radians(full_grid_locs_np), metric='haversine') \n",
    "    \n",
    "    # 2. High-Res GP Field\n",
    "    high_res_field, lats_high, lons_high = generate_high_res_field((lat_s, lat_e), (lon_s, lon_e), 8, true_params_tensor, device, dtype)\n",
    "    \n",
    "    hr_mesh_lat, hr_mesh_lon = torch.meshgrid(lats_high, lons_high, indexing='ij')\n",
    "    hr_tree = BallTree(np.radians(torch.stack([hr_mesh_lat.flatten(), hr_mesh_lon.flatten()], dim=1).cpu().numpy()), metric='haversine')\n",
    "    high_res_flat = high_res_field.reshape(-1, 8) \n",
    "\n",
    "    # Ï†ÄÏû•ÏÜå (3Ï¢ÖÎ•ò)\n",
    "    list_irr, list_reg, list_dw = [], [], []\n",
    "    map_irr, map_reg, map_dw = {}, {}, {}\n",
    "\n",
    "    noise_std = torch.sqrt(torch.exp(true_params_tensor[6]))\n",
    "    GLOBAL_MEAN = 260.0 # No Trend, Just Mean\n",
    "\n",
    "    day0_dict = daily_maps_real[0]\n",
    "    sorted_keys = sorted([k for k in day0_dict.keys() if 'hm' in k or 'time' in k])\n",
    "    \n",
    "    # -------------------------------------------------------------\n",
    "    # Step A: Generate [DW Full] & Cache Values for Reg\n",
    "    # -------------------------------------------------------------\n",
    "    _, hr_indices_full = hr_tree.query(np.radians(full_grid_locs_np), k=1)\n",
    "    hr_indices_full = torch.tensor(hr_indices_full.flatten(), device=device)\n",
    "    \n",
    "    full_sim_values_cache = [] \n",
    "\n",
    "    for t_idx in range(8):\n",
    "        key = f\"time_{t_idx}\"\n",
    "        \n",
    "        # GP + Noise + Mean (No Trend)\n",
    "        gp_signal = high_res_flat[hr_indices_full, t_idx]\n",
    "        sim_vals = gp_signal + (torch.randn_like(gp_signal) * noise_std) + GLOBAL_MEAN\n",
    "        \n",
    "        full_sim_values_cache.append(sim_vals)\n",
    "        \n",
    "        # Features for DW (4 Cols: Lat, Lon, Val, Time)\n",
    "        N_full = len(target_grid_coords)\n",
    "        time_col = torch.full((N_full, 1), float(t_idx), device=device, dtype=dtype)\n",
    "        \n",
    "        dw_row = torch.cat([target_grid_coords, sim_vals.unsqueeze(-1), time_col], dim=1)\n",
    "        list_dw.append(dw_row)\n",
    "        map_dw[key] = dw_row\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # Step B: Generate [Vecc Irr] & [Vecc Reg] (Path-based)\n",
    "    # -------------------------------------------------------------\n",
    "    for t_idx, key in enumerate(sorted_keys):\n",
    "        if t_idx >= 8: break\n",
    "        \n",
    "        real_tensor = day0_dict[key].to(device)\n",
    "        real_locs = real_tensor[:, :2]\n",
    "        N_points = len(real_locs)\n",
    "        real_locs_np = real_locs.cpu().numpy()\n",
    "        \n",
    "        # 1. Vecc Irr (Original Coords)\n",
    "        _, hr_indices_irr = hr_tree.query(np.radians(real_locs_np), k=1)\n",
    "        gp_signal_irr = high_res_flat[torch.tensor(hr_indices_irr.flatten(), device=device), t_idx]\n",
    "        \n",
    "        sim_vals_irr = gp_signal_irr + (torch.randn_like(gp_signal_irr) * noise_std) + GLOBAL_MEAN\n",
    "        \n",
    "        # 4 Cols\n",
    "        irr_row = torch.cat([\n",
    "            real_locs, \n",
    "            sim_vals_irr.unsqueeze(-1), \n",
    "            real_tensor[:, 3].unsqueeze(-1)\n",
    "        ], dim=1)\n",
    "        list_irr.append(irr_row)\n",
    "        map_irr[key] = irr_row\n",
    "        \n",
    "        # 2. Vecc Reg (Snapped Coords)\n",
    "        _, grid_indices = target_tree.query(np.radians(real_locs_np), k=1)\n",
    "        grid_indices = grid_indices.flatten()\n",
    "        \n",
    "        sim_vals_reg = full_sim_values_cache[t_idx][grid_indices]\n",
    "        mapped_locs = target_grid_coords[torch.tensor(grid_indices, device=device)]\n",
    "        \n",
    "        # 4 Cols\n",
    "        reg_row = torch.cat([\n",
    "            mapped_locs, \n",
    "            sim_vals_reg.unsqueeze(-1), \n",
    "            real_tensor[:, 3].unsqueeze(-1)\n",
    "        ], dim=1)\n",
    "        list_reg.append(reg_row)\n",
    "        map_reg[key] = reg_row\n",
    "\n",
    "    # Returns 3 Tuples\n",
    "    return (\n",
    "        (torch.cat(list_irr), map_irr),  # 1. Irr\n",
    "        (torch.cat(list_reg), map_reg),  # 2. Reg\n",
    "        (torch.cat(list_dw), map_dw)     # 3. DW\n",
    "    )\n",
    "\n",
    "# --- 4. EXECUTION ---\n",
    "# ... (ÏÑ§Ï†ï Î∂ÄÎ∂Ñ ÎèôÏùº) ...\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.float64\n",
    "target_grid_info = (-1.0, -3.0, 0.044, 121.0, 125.0, 0.063)\n",
    "\n",
    "# True Params\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lon = 0.195 \n",
    "init_range_lat = 0.154 \n",
    "init_range_time = 1.0\n",
    "init_nugget    = 0.247\n",
    "init_advec_lat = 0.0418\n",
    "init_advec_lon = -0.1689\n",
    "\n",
    "true_phi2 = 1.0 / init_range_lon              \n",
    "true_phi1 = init_sigmasq * true_phi2          \n",
    "true_phi3 = (init_range_lon / init_range_lat)**2\n",
    "true_phi4 = (init_range_lon / init_range_time)**2\n",
    "\n",
    "true_params_tensor = [\n",
    "    torch.tensor([np.log(true_phi1)], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([np.log(true_phi2)], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([np.log(true_phi3)], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([np.log(true_phi4)], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([init_advec_lat],    device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([init_advec_lon],    device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([np.log(init_nugget)], device=DEVICE, dtype=DTYPE)\n",
    "]\n",
    "\n",
    "print(\"Checking Inputs:\")\n",
    "print(f\" -> Target SigmaSq: {init_sigmasq}\")\n",
    "\n",
    "if 'daily_hourly_maps_irr_vecc' in locals() and len(daily_hourly_maps_irr_vecc) > 0:\n",
    "    print(\"\\nüöÄ Generating 3 Datasets (NO TREND, 4 COLS)...\")\n",
    "    \n",
    "    # [ÏàòÏ†ï] No Trend Î≤ÑÏ†Ñ Ìò∏Ï∂ú\n",
    "    (vecc_irr_data, vecc_reg_data, dw_full_data) = generate_three_datasets_no_trend(\n",
    "        daily_hourly_maps_irr_vecc, true_params_tensor, target_grid_info, DEVICE, DTYPE\n",
    "    )\n",
    "    \n",
    "    # Unpack\n",
    "    agg_irr, map_irr = vecc_irr_data\n",
    "    agg_reg, map_reg = vecc_reg_data\n",
    "    agg_dw,  map_dw  = dw_full_data\n",
    "    \n",
    "    # Global Variables for Models\n",
    "    daily_aggregated_irr_vecc_sim = [agg_irr]\n",
    "    daily_hourly_maps_irr_vecc_sim = [map_irr]\n",
    "    \n",
    "    daily_aggregated_reg_vecc_sim = [agg_reg] \n",
    "    daily_hourly_maps_reg_vecc_sim = [map_reg]\n",
    "    \n",
    "    daily_aggregated_dw_sim = [agg_dw] \n",
    "    daily_hourly_maps_dw_sim = [map_dw]\n",
    "    \n",
    "    print(f\"\\n[Validation]\")\n",
    "    print(f\"1. Vecc Irr (Swath): {agg_irr.shape} (Cols: 4)\")\n",
    "    print(f\"2. Vecc Reg (Snapped): {agg_reg.shape} (Cols: 4)\")\n",
    "    print(f\"3. DW Full (Complete): {agg_dw.shape} (Cols: 4)\")\n",
    "    \n",
    "    # Check Mean (Should be close to 260)\n",
    "    print(f\"   -> DW Val Mean: {agg_dw[:, 2].mean().item():.2f}\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: Load data first.\")\n",
    "\n",
    "# 1. DW Îç∞Ïù¥ÌÑ∞ Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "dw_tensor = daily_aggregated_dw_sim[0]\n",
    "coords_dw = dw_tensor[:, :2].cpu().numpy()\n",
    "\n",
    "print(f\"Generating NNS Map for Full Grid (Size: {len(coords_dw)})...\")\n",
    "\n",
    "# 2. MaxMin Ordering ÏàòÌñâ (Îç∞Ïù¥ÌÑ∞ Ïû¨Ï†ïÎ†¨)\n",
    "# VecchiaÎäî ÏàúÏÑúÍ∞Ä Ï§ëÏöîÌïòÎØÄÎ°ú, Î¨¥ÏûëÏúÑ Í≤©Ïûê ÏàúÏÑúÎ≥¥Îã§Îäî MaxMinÏù¥ ÏÑ±Îä•Ïù¥ Ï¢ãÏäµÎãàÎã§.\n",
    "ord_mm_dw = _orderings.maxmin_cpp(coords_dw)\n",
    "dw_tensor_ordered = dw_tensor[ord_mm_dw]  # ‚òÖ Ï§ëÏöî: Îç∞Ïù¥ÌÑ∞ ÏàúÏÑú Î≥ÄÍ≤Ω\n",
    "\n",
    "# 3. DWÏö© NNS Map ÏÉùÏÑ±\n",
    "coords_dw_ordered = dw_tensor_ordered[:, :2].cpu().numpy()\n",
    "nns_map_dw = _orderings.find_nns_l2(locs=coords_dw_ordered, max_nn=mm_cond_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7191bead",
   "metadata": {},
   "source": [
    "# Fit vecchia max min time 2 \n",
    "\n",
    "reg sim  ÏóêÎü¨ÎÇòÎäî Ïù¥Ïú†Îäî Îç∞Ïù¥ÌÑ∞ Ï§ëÎ≥µ snap ÎïåÎ¨∏\n",
    "\n",
    "Ï§ëÏöî! reg sim ÏùÄ Îç∞Ïù¥ÌÑ∞ Ï§ëÏã¨, Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ ÏúÑÏπòÏóêÏÑú Í∞ÄÏû• Í∞ÄÍπåÏö¥ Í∑∏Î¶¨ÎìúÎ°ú Ïù¥Îèô Ïù¥ÎûòÏÑú Ï§ëÎ≥µ Í∞ÄÎä•, ÏΩúÎ†àÏä§ÌÇ§ Íπ®ÏßÄÍ∏∞ÎèÑ \n",
    "\n",
    "dw Îäî Í∑∏Î¶¨Îìú Ï§ëÏã¨, ÌÉÄÍ≤ü Î†àÍ∑§Îü¨ Í∑∏Î¶¨ÎìúÏóêÏÑú Í∞ÄÏû• Í∞ÄÍπåÏö¥ Ï†ê Í∞ÄÏ†∏Ïò§Í∏∞ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b800f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "========================================\n",
      "--- Initializing VecchiaBatched Model ---\n",
      "========================================\n",
      "\n",
      "========================================\n",
      "--- Running L-BFGS Optimization ---\n",
      "========================================\n",
      "üöÄ Pre-computing (Only Intercept Mean Function)... ‚úÖ Done. (Heads: 2400, Tails: 21152)\n",
      "--- Starting Batched L-BFGS Optimization (GPU) ---\n",
      "--- Step 1/2 / Loss: 1.678867 ---\n",
      "  Param 0: Value=4.2043, Grad=3.0341090556336436e-05\n",
      "  Param 1: Value=1.6623, Grad=3.4594255702359014e-05\n",
      "  Param 2: Value=0.5959, Grad=8.476448973110525e-06\n",
      "  Param 3: Value=-3.2417, Grad=9.492158729145221e-06\n",
      "  Param 4: Value=-0.0200, Grad=-0.0001114840039180315\n",
      "  Param 5: Value=-0.1556, Grad=-1.617069151226317e-05\n",
      "  Param 6: Value=-1.9701, Grad=2.0905532961242376e-06\n",
      "  Max Abs Grad: 1.114840e-04\n",
      "------------------------------\n",
      "--- Step 2/2 / Loss: 1.678172 ---\n",
      "  Param 0: Value=4.2045, Grad=4.627622727175701e-06\n",
      "  Param 1: Value=1.6624, Grad=-2.517831589601274e-06\n",
      "  Param 2: Value=0.5962, Grad=6.41679355382614e-07\n",
      "  Param 3: Value=-3.2415, Grad=3.3341064661364904e-07\n",
      "  Param 4: Value=-0.0200, Grad=3.41412467660669e-06\n",
      "  Param 5: Value=-0.1555, Grad=-3.1915232239896783e-06\n",
      "  Param 6: Value=-1.9761, Grad=2.1641490257974684e-07\n",
      "  Max Abs Grad: 4.627623e-06\n",
      "------------------------------\n",
      "Final Interpretable Params: {'sigma_sq': 12.705874293904525, 'range_lon': 0.18968413868131598, 'range_lat': 0.14078743094011065, 'range_time': 0.9592232359480845, 'advec_lat': -0.019990541939965224, 'advec_lon': -0.1555495936509849, 'nugget': 0.13861351957110216}\n",
      "\n",
      "Optimization finished in 19.37s.\n",
      "Results after 1 steps: [4.2044594475332335, 1.6623950181815186, 0.5962181799052815, -3.241527133837573, -0.019990541939965224, -0.1555495936509849, -1.9760656531803757, 1.678171770504762]\n",
      "Final Params: [ 4.20445945  1.66239502  0.59621818 -3.24152713 -0.01999054 -0.15554959\n",
      " -1.97606565]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "v = 0.5              # Smoothness\n",
    "mm_cond_number = 8   # Neighbors\n",
    "#mm_cond_number = 16   # Neighbors\n",
    "nheads = 300           # 0 = Pure Vecchia\n",
    "lr = 1.0             # LBFGS learning rate\n",
    "LBFGS_MAX_STEPS = 2\n",
    "LBFGS_HISTORY_SIZE = 100 # 100\n",
    "LBFGS_LR = 1.0\n",
    "LBFGS_MAX_EVAL = 30    \n",
    "\n",
    "#DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- 1. SETUP PARAMETERS (List of Scalars) ---\n",
    "# Truth: [4.18, 1.94, 0.24, -3.97, 0.014, -0.20, -0.85]\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lat = 0.154 \n",
    "init_range_lon = 0.195\n",
    "init_advec_lat = 0.0218\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "# Map model parameters to the 'phi' reparameterization\n",
    "init_phi2 = 1.0 / init_range_lon                # 1/range_lon\n",
    "init_phi1 = init_sigmasq * init_phi2            # sigmasq / range_lon\n",
    "init_phi3 = (init_range_lon / init_range_lat)**2  # (range_lon / range_lat)^2\n",
    "init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "# Create Initial Parameters (Float64, Requires Grad)\n",
    "initial_vals = [np.log(init_phi1), np.log(init_phi2), np.log(init_phi3), \n",
    "                np.log(init_phi4), init_advec_lat, init_advec_lon, np.log(init_nugget)]\n",
    "\n",
    "# [4.2042, 1.6348, 0.4721, -3.2695, 0.0218, -0.1689, -1.3984]\n",
    "params_list = [\n",
    "    torch.tensor([val], requires_grad=True, dtype=torch.float64, device=DEVICE)\n",
    "    for val in initial_vals\n",
    "]\n",
    "\n",
    "# --- 2. INSTANTIATE MODEL ---\n",
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'--- Initializing VecchiaBatched Model ---')\n",
    "print(f'{\"=\"*40}')\n",
    "\n",
    "#if isinstance(daily_aggregated_irr_vecc_sim, torch.Tensor):\n",
    "#    daily_aggregated_irr_vecc_sim = daily_aggregated_irr_vecc_sim.to(DEVICE)\n",
    "\n",
    "# Instantiate\n",
    "model_instance = kernels_simulation.fit_vecchia_lbfgs(\n",
    "    smooth=v,\n",
    "\n",
    "    #input_map=daily_hourly_maps_reg_vecc_sim[0],\n",
    "    #aggregated_data= daily_aggregated_reg_vecc_sim[0],\n",
    "\n",
    "    #input_map=daily_hourly_maps_irr_vecc_sim[0],\n",
    "    #aggregated_data= daily_aggregated_irr_vecc_sim[0],\n",
    "\n",
    "    input_map=daily_hourly_maps_dw_sim[0],\n",
    "    aggregated_data= daily_aggregated_dw_sim[0],\n",
    "    nns_map=nns_map_dw,     # or nns_map for the swath data\n",
    "    mm_cond_number=mm_cond_number,\n",
    "    nheads=nheads\n",
    ")\n",
    "\n",
    "'''\n",
    "model_instance = kernels_reparam_space_time_gpu_col.fit_vecchia_lbfgs(\n",
    "    smooth=v,\n",
    "    #input_map=daily_hourly_maps_vecc_sim[0],\n",
    "    #aggregated_data= daily_aggregated_tensors_vecc_sim[0],\n",
    "\n",
    "    input_map=daily_hourly_maps_irr_vecc_sim[0],\n",
    "    aggregated_data= daily_aggregated_irr_vecc_sim[0],\n",
    "\n",
    "    nns_map=None,\n",
    "    mm_cond_number=mm_cond_number\n",
    ")\n",
    "''' \n",
    "\n",
    "# --- 3. OPTIMIZATION LOOP ---\n",
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'--- Running L-BFGS Optimization ---')\n",
    "print(f'{\"=\"*40}')\n",
    "\n",
    "# Optimizer takes the LIST of scalars\n",
    "optimizer_vecc = model_instance.set_optimizer(\n",
    "            params_list,     \n",
    "            lr=LBFGS_LR,            \n",
    "            max_iter=LBFGS_MAX_EVAL,        \n",
    "            history_size=LBFGS_HISTORY_SIZE \n",
    "        )\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "out, steps_ran = model_instance.fit_vecc_lbfgs(\n",
    "        params_list,\n",
    "        optimizer_vecc,\n",
    "        # covariance_function argument is GONE\n",
    "        max_steps=LBFGS_MAX_STEPS, \n",
    "        grad_tol=1e-7\n",
    "    )\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "epoch_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nOptimization finished in {epoch_time:.2f}s.\")\n",
    "print(f\"Results after {steps_ran} steps: {out}\")\n",
    "print(f\"Final Params: {torch.cat(params_list).detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d6a00",
   "metadata": {},
   "source": [
    "Final Interpretable Params: {'sigma_sq': 13.030470099266251, 'range_lon': 0.24297970896485985, 'range_lat': 0.18790827521584774, 'range_time': 1.1771604403664429, 'advec_lat': -0.03600038768476173, 'advec_lon': -0.15523640213037534, 'nugget': 0.8435065358207487}\n",
    "\n",
    "Optimization finished in 11.00s.\n",
    "Results after 1 steps: [3.9820678100408426, 1.4147773413303144, 0.514047983341542, -3.15576494657865, -0.03600038768476173, -0.15523640213037534, -0.17018762857237063, 1.5022917978071149]\n",
    "Final Params: [ 3.98206781  1.41477734  0.51404798 -3.15576495 -0.03600039 -0.1552364\n",
    " -0.17018763]\n",
    "\n",
    " Optimization finished in 18.53s.\n",
    "Results after 1 steps: [4.215134583546057, 1.5787214311696494, 0.5211054328936817, -3.3364985438405252, -0.04339404893171443, -0.15788050125393552, -13.706356238632745, 1.4951317132254003]\n",
    "Final Params: [  4.21513458   1.57872143   0.52110543  -3.33649854  -0.04339405\n",
    "  -0.1578805  -13.70635624]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b08c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "irr\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 13.190530313067597, 'range_lon': 0.19707111772283428, 'range_lat': 0.1527527611913852, 'range_time': 0.9929672518527729, 'advec_lat': -0.02957260581397776, 'advec_lon': -0.16218858835449518, 'nugget': 0.23616033677518194}\n",
    "\n",
    "Optimization finished in 12.21s.\n",
    "Results after 2 steps: [4.203689783333332, 1.6241906117210854, 0.5094879856667579, -3.234266034480146, -0.02957260581397776, -0.16218858835449518, -1.4432443115243747, 1.1447534519147522]\n",
    "Final Params: [ 4.20368978  1.62419061  0.50948799 -3.23426603 -0.02957261 -0.16218859\n",
    " -1.44324431]\n",
    "\n",
    "reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c961fa4",
   "metadata": {},
   "source": [
    "# fit dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9b3e3",
   "metadata": {},
   "source": [
    "difference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b9f77de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2728, 4])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsetted_aggregated_day.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0b29571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2835, 4])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_hourly_maps_reg_vecc_sim[day]['2024_07_y24m07day01_hm00:53'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d43435c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22680, 4])\n",
      "22680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ -2.9800, 121.0000,  -4.3757,   0.0000],\n",
       "        [ -2.9800, 121.0630,   5.8032,   0.0000],\n",
       "        [ -2.9800, 121.1260,   1.1170,   0.0000],\n",
       "        [ -2.9800, 121.1890,  -2.1063,   0.0000],\n",
       "        [ -2.9800, 121.2520,   0.4625,   0.0000],\n",
       "        [ -2.9800, 121.3150,   1.9560,   0.0000],\n",
       "        [ -2.9800, 121.3780,   1.2727,   0.0000],\n",
       "        [ -2.9800, 121.4410,  -2.4676,   0.0000],\n",
       "        [ -2.9800, 121.5040,   0.2430,   0.0000],\n",
       "        [ -2.9800, 121.5670,   4.0254,   0.0000],\n",
       "        [ -2.9800, 121.6300,   5.2243,   0.0000],\n",
       "        [ -2.9800, 121.6930,   8.9678,   0.0000],\n",
       "        [ -2.9800, 121.7560, -10.6579,   0.0000],\n",
       "        [ -2.9800, 121.8190,   2.2547,   0.0000],\n",
       "        [ -2.9800, 121.8820,  -0.5641,   0.0000],\n",
       "        [ -2.9800, 121.9450,   0.3427,   0.0000],\n",
       "        [ -2.9800, 122.0080,  -6.9036,   0.0000],\n",
       "        [ -2.9800, 122.0710,  11.7442,   0.0000],\n",
       "        [ -2.9800, 122.1340,  -1.8260,   0.0000],\n",
       "        [ -2.9800, 122.1970,   1.7467,   0.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [11.0474, 0.0623, 0.2445, 1.0972, 0.0101, -0.1671, 1.1825]\n",
    "day = 0 # 0 index\n",
    "lat_range= [-3, -1]\n",
    "lon_range= [121, 125]\n",
    "#lat_range= [1,3]\n",
    "#lon_range= [125, 129.0]\n",
    "\n",
    "daily_aggregated_tensors_dw = [daily_aggregated_dw_sim[day]]\n",
    "daily_hourly_maps_dw = [daily_hourly_maps_dw_sim[day]]\n",
    "\n",
    "db = debiased_whittle.debiased_whittle_preprocess(daily_aggregated_tensors_dw, daily_hourly_maps_dw, day_idx=day, params_list=a, lat_range=lat_range, lon_range=lon_range)\n",
    "\n",
    "\n",
    "subsetted_aggregated_day = db.generate_spatially_filtered_days(-3,-1,121,125)\n",
    "print(subsetted_aggregated_day.shape)\n",
    "N2= subsetted_aggregated_day.shape[0]\n",
    "print(N2)\n",
    "subsetted_aggregated_day[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2497b8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Pre-computing J-vector (Hamming taper)...\n",
      "Pre-computing sample periodogram...\n",
      "Pre-computing Hamming taper autocorrelation...\n",
      "Data grid: 45x63, 8 time points. J-vector, Periodogram, Taper Autocorr on cpu.\n",
      "\n",
      "============================== Initialization Run 1/1 ==============================\n",
      "Starting with FIXED params (raw log-scale): [4.2042, 1.6348, 0.4721, -2.5562, 0.0218, -0.1689, -1.3984]\n",
      "Starting optimization run 1 on device cpu (Hamming, 7-param ST kernel, L-BFGS)...\n",
      "--- Step 1/20 ---\n",
      " Loss: 1.829723 | Max Grad: 4.600702e-05\n",
      "  Params (Raw Log): log_phi1: 4.1783, log_phi2: 1.5572, log_phi3: 0.3815, log_phi4: -3.4342, advec_lat: 0.0125, advec_lon: -0.1670, log_nugget: -0.7674\n",
      "--- Step 2/20 ---\n",
      " Loss: 1.800890 | Max Grad: 4.600702e-05\n",
      "  Params (Raw Log): log_phi1: 4.1783, log_phi2: 1.5572, log_phi3: 0.3815, log_phi4: -3.4342, advec_lat: 0.0125, advec_lon: -0.1670, log_nugget: -0.7674\n",
      "--- Step 3/20 ---\n",
      " Loss: 1.800890 | Max Grad: 4.600702e-05\n",
      "  Params (Raw Log): log_phi1: 4.1783, log_phi2: 1.5572, log_phi3: 0.3815, log_phi4: -3.4342, advec_lat: 0.0125, advec_lon: -0.1670, log_nugget: -0.7674\n",
      "--- Step 4/20 ---\n",
      " Loss: 1.800890 | Max Grad: 4.600702e-05\n",
      "  Params (Raw Log): log_phi1: 4.1783, log_phi2: 1.5572, log_phi3: 0.3815, log_phi4: -3.4342, advec_lat: 0.0125, advec_lon: -0.1670, log_nugget: -0.7674\n",
      "\n",
      "--- Converged on loss change (change < 1e-12) at step 4 ---\n",
      "\n",
      "--- Training Complete ---\n",
      "\n",
      "FINAL BEST STATE ACHIEVED (during training):\n",
      "Best Loss: 1.801\n",
      "\n",
      "\n",
      "========================= Overall Result from Run ========================= =========================\n",
      "Best Run Loss: 1.801 (after 4 steps)\n",
      "Final Parameters (Natural Scale): sigmasq: 13.7510, range_lat: 0.1741, range_lon: 0.2107, range_time: 1.1734, advec_lat: 0.0125, advec_lon: -0.1670, nugget: 0.4642\n",
      "Final Parameters (Phi Scale)    : phi1: 65.2573, phi2: 4.7456, phi3: 1.4645, phi4: 0.0323, advec_lat: 0.0125, advec_lon: -0.1670, nugget: 0.4642\n",
      "Final Parameters (Raw Log Scale): log_phi1: 4.1783, log_phi2: 1.5572, log_phi3: 0.3815, log_phi4: -3.4342, advec_lat: 0.0125, advec_lon: -0.1670, log_nugget: -0.7674\n",
      "\n",
      "Total execution time: 2.71 seconds\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Parameter\n",
    "dwl = debiased_whittle.debiased_whittle_likelihood()\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    DAY_TO_RUN = 3 # data is decided above\n",
    "    TAPERING_FUNC = dwl.cgn_hamming # Use Hamming taper\n",
    "    NUM_RUNS = 1\n",
    "    MAX_STEPS = 20 # L-BFGS usually converges in far fewer steps\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063 \n",
    "\n",
    "    LAT_COL, LON_COL = 0, 1\n",
    "    VAL_COL = 2 # Spatially differenced value\n",
    "    TIME_COL = 3\n",
    "\n",
    "\n",
    "    cur_df =subsetted_aggregated_day\n",
    "    \n",
    "    if cur_df.numel() == 0 or cur_df.shape[1] <= max(LAT_COL, LON_COL, VAL_COL, TIME_COL):\n",
    "        print(f\"Error: Data for Day {DAY_TO_RUN} is empty or invalid.\")\n",
    "        exit()\n",
    "\n",
    "    unique_times = torch.unique(cur_df[:, TIME_COL])\n",
    "    time_slices_list = [cur_df[cur_df[:, TIME_COL] == t_val] for t_val in unique_times]\n",
    "\n",
    "    # --- 1. Pre-compute J-vector, Taper Grid, and Taper Autocorrelation ---\n",
    "    print(\"Pre-computing J-vector (Hamming taper)...\")\n",
    "    \n",
    "    # --- üí• REVISED: Renamed 'p' to 'p_time' üí• ---\n",
    "    J_vec, n1, n2, p_time, taper_grid = dwl.generate_Jvector_tapered( \n",
    "        time_slices_list,\n",
    "        tapering_func=TAPERING_FUNC, \n",
    "        lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if J_vec is None or J_vec.numel() == 0 or n1 == 0 or n2 == 0 or p_time == 0:\n",
    "       print(f\"Error: J-vector generation failed for Day {DAY_TO_RUN}.\")\n",
    "       exit()\n",
    "       \n",
    "    print(\"Pre-computing sample periodogram...\")\n",
    "    I_sample = dwl.calculate_sample_periodogram_vectorized(J_vec)\n",
    "\n",
    "    print(\"Pre-computing Hamming taper autocorrelation...\")\n",
    "    taper_autocorr_grid = dwl.calculate_taper_autocorrelation_fft(taper_grid, n1, n2, DEVICE)\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Error: NaN/Inf in sample periodogram.\")\n",
    "        exit()\n",
    "    if torch.isnan(taper_autocorr_grid).any() or torch.isinf(taper_autocorr_grid).any():\n",
    "        print(\"Error: NaN/Inf in taper autocorrelation.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Data grid: {n1}x{n2}, {p_time} time points. J-vector, Periodogram, Taper Autocorr on {DEVICE}.\")\n",
    "    # --- END REVISION ---\n",
    "\n",
    "    # --- 2. Optimization Loop ---\n",
    "    all_final_results = []\n",
    "    all_final_losses = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        print(f\"\\n{'='*30} Initialization Run {i+1}/{NUM_RUNS} {'='*30}\")\n",
    "\n",
    "        # --- 7-PARAMETER initialization ---\n",
    "        ''' \n",
    "        init_sigmasq   = 15.0\n",
    "        init_range_lat = 0.66 \n",
    "        init_range_lon = 0.7 \n",
    "        init_nugget    = 1.5\n",
    "        init_beta      = 0.1  # Temporal range ratio\n",
    "        init_advec_lat = 0.02\n",
    "        init_advec_lon = -0.08\n",
    "        '''\n",
    "        init_sigmasq   = 13.059\n",
    "        init_range_lat = 0.154 \n",
    "        init_range_lon = 0.195\n",
    "        init_advec_lat = 0.0218\n",
    "        init_range_time = 0.7\n",
    "        init_advec_lon = -0.1689\n",
    "        init_nugget    = 0.247\n",
    "\n",
    "        init_phi2 = 1.0 / init_range_lon\n",
    "        init_phi1 = init_sigmasq * init_phi2\n",
    "        init_phi3 = (init_range_lon / init_range_lat)**2\n",
    "        # Change needed to match the spatial-temporal distance formula:\n",
    "        init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "        initial_params_values = [\n",
    "            np.log(init_phi1),    # [0] log_phi1\n",
    "            np.log(init_phi2),    # [1] log_phi2\n",
    "            np.log(init_phi3),    # [2] log_phi3\n",
    "            np.log(init_phi4),    # [3] log_phi4\n",
    "            init_advec_lat,       # [4] advec_lat (NOT log)\n",
    "            init_advec_lon,       # [5] advec_lon (NOT log)\n",
    "            np.log(init_nugget)   # [6] log_nugget\n",
    "        ]\n",
    "        \n",
    "        print(f\"Starting with FIXED params (raw log-scale): {[round(p, 4) for p in initial_params_values]}\")\n",
    "\n",
    "        params_list = [\n",
    "            Parameter(torch.tensor([val], dtype=torch.float64))\n",
    "            for val in initial_params_values\n",
    "        ]\n",
    "\n",
    "        # Helper to define the boundary globally for clarity\n",
    "        NUGGET_LOWER_BOUND = 0.05\n",
    "        LOG_NUGGET_LOWER_BOUND = np.log(NUGGET_LOWER_BOUND) # Approx -2.9957\n",
    "\n",
    "        # --- üí• REVISED: Use L-BFGS Optimizer üí• ---\n",
    "        optimizer = torch.optim.LBFGS(\n",
    "            params_list,\n",
    "            lr=1.0,           # Initial step length for line search\n",
    "            max_iter=20,      # Iterations per step\n",
    "            history_size=100,\n",
    "            line_search_fn=\"strong_wolfe\", # Often more robust\n",
    "            tolerance_grad=1e-5\n",
    "        )\n",
    "        # --- END REVISION ---\n",
    "\n",
    "        print(f\"Starting optimization run {i+1} on device {DEVICE} (Hamming, 7-param ST kernel, L-BFGS)...\")\n",
    "        \n",
    "        # --- üí• REVISED: Call L-BFGS trainer, pass p_time üí• ---\n",
    "        nat_params_str, phi_params_str, raw_params_str, loss, steps_run = dwl.run_lbfgs_tapered(\n",
    "            params_list=params_list,\n",
    "            optimizer=optimizer,\n",
    "            I_sample=I_sample,\n",
    "            n1=n1, n2=n2, p_time=p_time,\n",
    "            taper_autocorr_grid=taper_autocorr_grid, \n",
    "            max_steps=MAX_STEPS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        # --- END REVISION ---\n",
    "        \n",
    "        if loss is not None:\n",
    "            all_final_results.append((nat_params_str, phi_params_str, raw_params_str))\n",
    "            all_final_losses.append(loss)\n",
    "        else:\n",
    "            all_final_losses.append(float('inf'))\n",
    "\n",
    "    print(f\"\\n\\n{'='*25} Overall Result from Run {'='*25} {'='*25}\")\n",
    "    \n",
    "    valid_losses = [l for l in all_final_losses if l is not None and l != float('inf')]\n",
    "\n",
    "    if not valid_losses:\n",
    "        print(f\"The run failed or resulted in an invalid loss for Day {DAY_TO_RUN}.\")\n",
    "    else:\n",
    "        best_loss = min(valid_losses)\n",
    "        best_run_index = all_final_losses.index(best_loss)\n",
    "        best_results = all_final_results[best_run_index]\n",
    "        \n",
    "        print(f\"Best Run Loss: {best_loss} (after {steps_run} steps)\")\n",
    "        print(f\"Final Parameters (Natural Scale): {best_results[0]}\")\n",
    "        print(f\"Final Parameters (Phi Scale)    : {best_results[1]}\")\n",
    "        print(f\"Final Parameters (Raw Log Scale): {best_results[2]}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
