{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d8d314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#from GEMS_TCO import kernels\n",
    "from GEMS_TCO import data_preprocess \n",
    "from GEMS_TCO import  kernels_reparametrization_space as kernels_repar_space\n",
    "from GEMS_TCO import kernels_reparam_space_time_gpu as kernels_reparam_space_time\n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data2\n",
    "from GEMS_TCO import alg_optimization,  BaseLogger\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO.data_loader import load_data2\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7bd05c",
   "metadata": {},
   "source": [
    "Load monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf44623d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Global Monthly Mean for 2024-7: 256.4701 ---\n"
     ]
    }
   ],
   "source": [
    "space: List[str] = ['1', '1']\n",
    "lat_lon_resolution = [int(s) for s in space]\n",
    "mm_cond_number: int = 20\n",
    "years = ['2024']\n",
    "month_range = [7] \n",
    "\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "data_load_instance = load_data2(config.mac_data_load_path)\n",
    "\n",
    "\n",
    "df_map, ord_mm, nns_map, offset = data_load_instance.load_maxmin_ordered_data_bymonthyear(\n",
    "lat_lon_resolution=lat_lon_resolution, \n",
    "mm_cond_number=mm_cond_number,\n",
    "years_=years, \n",
    "months_=month_range,\n",
    "lat_range=[-3, -1],      \n",
    "lon_range=[121.0, 131.0] \n",
    ")\n",
    "\n",
    "#days: List[str] = ['0', '31']\n",
    "#days_s_e = [int(d) for d in days]\n",
    "#days_list = list(range(days_s_e[0], days_s_e[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d2e14",
   "metadata": {},
   "source": [
    "Load daily data applying max-min ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2425932",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (7155,) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#hour_end_index = day_index*8 + 1\u001b[39;00m\n\u001b[32m      8\u001b[39m hour_indices = [hour_start_index, hour_end_index]\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m day_hourly_map, day_aggregated_tensor = \u001b[43mdata_load_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_working_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m\u001b[49m\u001b[43mdf_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m\u001b[49m\u001b[43mhour_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m\u001b[49m\u001b[43mord_mm\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mord_mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# or just omit it\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# or just omit it \u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m\u001b[49m\u001b[43mkeep_ori\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m daily_aggregated_tensors.append( day_aggregated_tensor )\n\u001b[32m     19\u001b[39m daily_hourly_maps.append( day_hourly_map )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/data_loader.py:323\u001b[39m, in \u001b[36mload_data2.load_working_data\u001b[39m\u001b[34m(self, coarse_dicts, monthly_mean, idx_for_datamap, ord_mm, dtype, keep_ori)\u001b[39m\n\u001b[32m    321\u001b[39m     ozone_vals = tmp.iloc[:, \u001b[32m2\u001b[39m].values - monthly_mean\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     ozone_vals = \u001b[43mtmp\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonthly_mean\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# Ordering ì ìš©\u001b[39;00m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ord_mm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (7155,) (2,) "
     ]
    }
   ],
   "source": [
    "daily_aggregated_tensors = [] \n",
    "daily_hourly_maps = []      \n",
    "\n",
    "for day_index in range(31):\n",
    "    hour_start_index = day_index * 8\n",
    "    hour_end_index = (day_index + 1) * 8\n",
    "    #hour_end_index = day_index*8 + 1\n",
    "    hour_indices = [hour_start_index, hour_end_index]\n",
    "    \n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "    df_map, \n",
    "    hour_indices, \n",
    "    ord_mm= ord_mm,  # or just omit it\n",
    "    dtype=torch.float, # or just omit it \n",
    "    keep_ori= False\n",
    "    )\n",
    "\n",
    "    daily_aggregated_tensors.append( day_aggregated_tensor )\n",
    "    daily_hourly_maps.append( day_hourly_map )\n",
    "\n",
    "print(daily_aggregated_tensors[0].shape)\n",
    "print(daily_aggregated_tensors[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53097afa",
   "metadata": {},
   "source": [
    "# fit l-bfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3e08131",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0.5 # smooth\n",
    "mm_cond_number = 8\n",
    "nheads = 300\n",
    "#nheads = 1230\n",
    "#lr = 0.01\n",
    "#step = 80\n",
    "#gamma_par = 0.5\n",
    "\n",
    "# --- Placeholder Global Variables ---\n",
    "# ðŸ’¥ REVISED: Added lr, patience, factor. Removed step, gamma_par\n",
    "lr=0.1\n",
    "patience = 5       # Scheduler: Epochs to wait for improvement\n",
    "factor = 0.5         # Scheduler: Factor to reduce LR by (e.g., 0.5 = 50% cut)\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a953d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 18126.0, smooth: 0.5\n",
      "mm_cond_number: 8,\n",
      "initial parameters: \n",
      "\n",
      "  Param 0: 4.2042\n",
      "  Param 1: 1.6348\n",
      "  Param 2: 0.4721\n",
      "  Param 3: -2.5562\n",
      "  Param 4: 0.0218\n",
      "  Param 5: -0.1689\n",
      "  Param 6: -1.3984\n",
      "Pre-computing Batched Tensors (Padding Strategy)... Done. Heads: 2400, Batched Tails: 142608\n",
      "--- Starting Batched L-BFGS Optimization (GPU) ---\n",
      "--- Step 1/7 / Loss: 1.243622 ---\n",
      "  Param 0: Value=4.2867, Grad=-5.3208817852082833e-08\n",
      "  Param 1: Value=1.7396, Grad=-5.134866304769628e-08\n",
      "  Param 2: Value=0.4891, Grad=-8.514844787937351e-07\n",
      "  Param 3: Value=-3.7771, Grad=-1.9466146557579975e-08\n",
      "  Param 4: Value=0.0205, Grad=-3.2878709927942028e-06\n",
      "  Param 5: Value=-0.1641, Grad=3.015833070129032e-08\n",
      "  Param 6: Value=-12.1476, Grad=7.248147498080532e-10\n",
      "  Max Abs Grad: 3.287871e-06\n",
      "------------------------------\n",
      "--- Step 2/7 / Loss: 1.241928 ---\n",
      "  Param 0: Value=4.2867, Grad=-5.3208817852082833e-08\n",
      "  Param 1: Value=1.7396, Grad=-5.134866304769628e-08\n",
      "  Param 2: Value=0.4891, Grad=-8.514844787937351e-07\n",
      "  Param 3: Value=-3.7771, Grad=-1.9466146557579975e-08\n",
      "  Param 4: Value=0.0205, Grad=-3.2878709927942028e-06\n",
      "  Param 5: Value=-0.1641, Grad=3.015833070129032e-08\n",
      "  Param 6: Value=-12.1476, Grad=7.248147498080532e-10\n",
      "  Max Abs Grad: 3.287871e-06\n",
      "------------------------------\n",
      "--- Step 3/7 / Loss: 1.241928 ---\n",
      "  Param 0: Value=4.2867, Grad=-5.3208817852082833e-08\n",
      "  Param 1: Value=1.7396, Grad=-5.134866304769628e-08\n",
      "  Param 2: Value=0.4891, Grad=-8.514844787937351e-07\n",
      "  Param 3: Value=-3.7771, Grad=-1.9466146557579975e-08\n",
      "  Param 4: Value=0.0205, Grad=-3.2878709927942028e-06\n",
      "  Param 5: Value=-0.1641, Grad=3.015833070129032e-08\n",
      "  Param 6: Value=-12.1476, Grad=7.248147498080532e-10\n",
      "  Max Abs Grad: 3.287871e-06\n",
      "------------------------------\n",
      "--- Step 4/7 / Loss: 1.241928 ---\n",
      "  Param 0: Value=4.2867, Grad=-5.3208817852082833e-08\n",
      "  Param 1: Value=1.7396, Grad=-5.134866304769628e-08\n",
      "  Param 2: Value=0.4891, Grad=-8.514844787937351e-07\n",
      "  Param 3: Value=-3.7771, Grad=-1.9466146557579975e-08\n",
      "  Param 4: Value=0.0205, Grad=-3.2878709927942028e-06\n",
      "  Param 5: Value=-0.1641, Grad=3.015833070129032e-08\n",
      "  Param 6: Value=-12.1476, Grad=7.248147498080532e-10\n",
      "  Max Abs Grad: 3.287871e-06\n",
      "------------------------------\n",
      "--- Step 5/7 / Loss: 1.241928 ---\n",
      "  Param 0: Value=4.2867, Grad=-5.3208817852082833e-08\n",
      "  Param 1: Value=1.7396, Grad=-5.134866304769628e-08\n",
      "  Param 2: Value=0.4891, Grad=-8.514844787937351e-07\n",
      "  Param 3: Value=-3.7771, Grad=-1.9466146557579975e-08\n",
      "  Param 4: Value=0.0205, Grad=-3.2878709927942028e-06\n",
      "  Param 5: Value=-0.1641, Grad=3.015833070129032e-08\n",
      "  Param 6: Value=-12.1476, Grad=7.248147498080532e-10\n",
      "  Max Abs Grad: 3.287871e-06\n",
      "------------------------------\n",
      "--- Step 6/7 / Loss: 1.241928 ---\n",
      "  Param 0: Value=4.2867, Grad=-5.3208817852082833e-08\n",
      "  Param 1: Value=1.7396, Grad=-5.134866304769628e-08\n",
      "  Param 2: Value=0.4891, Grad=-8.514844787937351e-07\n",
      "  Param 3: Value=-3.7771, Grad=-1.9466146557579975e-08\n",
      "  Param 4: Value=0.0205, Grad=-3.2878709927942028e-06\n",
      "  Param 5: Value=-0.1641, Grad=3.015833070129032e-08\n",
      "  Param 6: Value=-12.1476, Grad=7.248147498080532e-10\n",
      "  Max Abs Grad: 3.287871e-06\n",
      "------------------------------\n",
      "--- Step 7/7 / Loss: 1.241928 ---\n",
      "  Param 0: Value=4.2867, Grad=-5.3208817852082833e-08\n",
      "  Param 1: Value=1.7396, Grad=-5.134866304769628e-08\n",
      "  Param 2: Value=0.4891, Grad=-8.514844787937351e-07\n",
      "  Param 3: Value=-3.7771, Grad=-1.9466146557579975e-08\n",
      "  Param 4: Value=0.0205, Grad=-3.2878709927942028e-06\n",
      "  Param 5: Value=-0.1641, Grad=3.015833070129032e-08\n",
      "  Param 6: Value=-12.1476, Grad=7.248147498080532e-10\n",
      "  Max Abs Grad: 3.287871e-06\n",
      "------------------------------\n",
      "Final Interpretable Params: {'sigma_sq': 12.768834873678939, 'range_lon': 0.17558309390925525, 'range_lat': 0.13749454946135378, 'range_time': 1.1605683410525962, 'advec_lat': 0.02047763062541548, 'advec_lon': -0.16411373849975075, 'nugget': 5.3012487929598985e-06}\n",
      "Day 1 optimization finished in 248.17s over 7 L-BFGS steps.\n",
      "Day 1 final results (raw params + loss): [4.286650305105019, 1.7396428785591596, 0.48905624876622333, -3.777105425749588, 0.02047763062541548, -0.16411373849975075, -12.147568143847352, 1.241928143720475]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import time\n",
    "# NOTE: Assuming the necessary classes and global variables are defined elsewhere, \n",
    "# specifically the referenced 'fit_vecchia_lbfgs' class and 'kernels_reparam_space_time' module.\n",
    "\n",
    "# --- Assume global variables are set: ---\n",
    "# daily_hourly_maps, daily_aggregated_tensors, nns_map\n",
    "# lat_lon_resolution, v, mm_cond_number, nheads\n",
    "# lr, patience, factor, epochs\n",
    "\n",
    "# --- L-BFGS SPECIFIC GLOBAL PARAMETERS ---\n",
    "LBFGS_LR = 1.0\n",
    "LBFGS_MAX_STEPS = 7       # Number of outer optimization steps\n",
    "LBFGS_HISTORY_SIZE = 100   # Memory for Hessian approximation\n",
    "LBFGS_MAX_EVAL = 50        # Max evaluations (line search) per step\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "day_indices = [0] # 0 index \n",
    "for day_idx in day_indices:  \n",
    "\n",
    "    # Assuming data access is correct\n",
    "    daily_hourly_map = daily_hourly_maps[day_idx]\n",
    "    daily_aggregated_tensor = daily_aggregated_tensors[day_idx]\n",
    "\n",
    "    # --- Parameter Initialization (SPATIO-TEMPORAL) ---\n",
    "    '''  \n",
    "    init_sigmasq   = 15.0\n",
    "    init_range_lat = 0.66 \n",
    "    init_range_lon = 0.7 \n",
    "    init_nugget    = 1.5\n",
    "    init_range_time = 0.1\n",
    "    init_advec_lat = 0.02\n",
    "    init_advec_lon = -0.08\n",
    "    '''\n",
    "    init_sigmasq   = 13.059\n",
    "    init_range_lat = 0.154 \n",
    "    init_range_lon = 0.195 \n",
    "    init_nugget    = 0.247\n",
    "    init_range_time = 0.7\n",
    "    init_advec_lat = 0.0218\n",
    "    init_advec_lon = -0.1689\n",
    "\n",
    "    \n",
    "    # Map model parameters to the 'phi' reparameterization\n",
    "    init_phi2 = 1.0 / init_range_lon                # 1/range_lon\n",
    "    init_phi1 = init_sigmasq * init_phi2            # sigmasq / range_lon\n",
    "    init_phi3 = (init_range_lon / init_range_lat)**2  # (range_lon / range_lat)^2\n",
    "    init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # 7-parameter spatio-temporal list (Log/Linear)\n",
    "    params_list = [\n",
    "        torch.tensor([np.log(init_phi1)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [0] log(phi1)\n",
    "        torch.tensor([np.log(init_phi2)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [1] log(phi2)\n",
    "        torch.tensor([np.log(init_phi3)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [2] log(phi3)\n",
    "        torch.tensor([np.log(init_phi4)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [3] log(phi4)\n",
    "        torch.tensor([init_advec_lat],         requires_grad=True, dtype=torch.float64, device=device_str ), # [4] advec_lat (linear)\n",
    "        torch.tensor([init_advec_lon],         requires_grad=True, dtype=torch.float64, device=device_str ), # [5] advec_lon (linear)\n",
    "        torch.tensor([np.log(init_nugget)],    requires_grad=True, dtype=torch.float64, device=device_str )  # [6] log(nugget)\n",
    "    ]\n",
    "\n",
    "    # --- Define parameter groups ---\n",
    "    lr_all = LBFGS_LR\n",
    "    all_indices = [0, 1, 2, 3, 4, 5, 6] \n",
    "    \n",
    "    # L-BFGS requires the parameters to be iterable in a single list or group\n",
    "    param_groups = [\n",
    "        {'params': [params_list[idx] for idx in all_indices], 'lr': lr_all, 'name': 'all_params'}\n",
    "    ]\n",
    "\n",
    "    # --- Print Job Info (using placeholder print variables) ---\n",
    "\n",
    "    print(f'\\n--- Starting Day {day_idx+1} (2024-07-{day_idx+1}) ---')\n",
    "    print(f'Data size per day: { daily_aggregated_tensor.shape[0]/8}, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n')\n",
    "    for i, p in enumerate(params_list):\n",
    "        print(f\"  Param {i}: {p.item():.4f}\")\n",
    "            \n",
    "    # --- ðŸ’¥ Instantiate the L-BFGS Class ---\n",
    "    # NOTE: Assuming fit_vecchia_lbfgs is available via kernels_reparam_space_time\n",
    "    model_instance = kernels_reparam_space_time.fit_vecchia_lbfgs(\n",
    "            smooth = v,\n",
    "            input_map = daily_hourly_map,\n",
    "            aggregated_data = daily_aggregated_tensor,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- ðŸ’¥ Set L-BFGS Optimizer ---\n",
    "    # L-BFGS specific arguments are passed here\n",
    "    optimizer_vecc = model_instance.set_optimizer(\n",
    "                params_list,     \n",
    "                lr=LBFGS_LR,            \n",
    "                max_iter=LBFGS_MAX_EVAL,        \n",
    "                history_size=LBFGS_HISTORY_SIZE \n",
    "            )\n",
    "\n",
    "    out, steps_ran = model_instance.fit_vecc_lbfgs(\n",
    "            params_list,\n",
    "            optimizer_vecc,\n",
    "            # covariance_function argument is GONE\n",
    "            max_steps=LBFGS_MAX_STEPS, \n",
    "            grad_tol=1e-7\n",
    "        )\n",
    "    \n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Day {day_idx+1} optimization finished in {epoch_time:.2f}s over {steps_ran+1} L-BFGS steps.\")\n",
    "    print(f\"Day {day_idx+1} final results (raw params + loss): {out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d3ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Day 2 (2024-07-2) ---\n",
      "Data size per day: 17854, smooth: 0.5\n",
      "mm_cond_number: 10,\n",
      "initial parameters: \n",
      "\n",
      "  Param 0: 4.2042\n",
      "  Param 1: 1.6348\n",
      "  Param 2: 0.4721\n",
      "  Param 3: -2.5562\n",
      "  Param 4: 0.0218\n",
      "  Param 5: -0.1689\n",
      "  Param 6: -1.3984\n",
      "--- Starting L-BFGS Optimization ---\n",
      "--- Step 1/10 / Loss: 1.700210 ---\n",
      "  Param 0: Value=3.7896, Grad=-1.1735791810950357e-06\n",
      "  Param 1: Value=1.3146, Grad=-5.177567866476432e-07\n",
      "  Param 2: Value=0.6290, Grad=7.6302044784969e-07\n",
      "  Param 3: Value=-2.7640, Grad=-5.4658081165455354e-08\n",
      "  Param 4: Value=0.0023, Grad=-8.187702470612921e-07\n",
      "  Param 5: Value=-0.1696, Grad=3.6728984261623678e-06\n",
      "  Param 6: Value=-1.4066, Grad=-1.0927744412572334e-05\n",
      "  Max Abs Grad: 1.092774e-05\n",
      "------------------------------\n",
      "--- Step 2/10 / Loss: 1.686152 ---\n",
      "  Param 0: Value=3.7896, Grad=-1.1735791810950357e-06\n",
      "  Param 1: Value=1.3146, Grad=-5.177567866476432e-07\n",
      "  Param 2: Value=0.6290, Grad=7.6302044784969e-07\n",
      "  Param 3: Value=-2.7640, Grad=-5.4658081165455354e-08\n",
      "  Param 4: Value=0.0023, Grad=-8.187702470612921e-07\n",
      "  Param 5: Value=-0.1696, Grad=3.6728984261623678e-06\n",
      "  Param 6: Value=-1.4066, Grad=-1.0927744412572334e-05\n",
      "  Max Abs Grad: 1.092774e-05\n",
      "------------------------------\n",
      "--- Step 3/10 / Loss: 1.686152 ---\n",
      "  Param 0: Value=3.7896, Grad=-1.1735791810950357e-06\n",
      "  Param 1: Value=1.3146, Grad=-5.177567866476432e-07\n",
      "  Param 2: Value=0.6290, Grad=7.6302044784969e-07\n",
      "  Param 3: Value=-2.7640, Grad=-5.4658081165455354e-08\n",
      "  Param 4: Value=0.0023, Grad=-8.187702470612921e-07\n",
      "  Param 5: Value=-0.1696, Grad=3.6728984261623678e-06\n",
      "  Param 6: Value=-1.4066, Grad=-1.0927744412572334e-05\n",
      "  Max Abs Grad: 1.092774e-05\n",
      "------------------------------\n",
      "--- Step 4/10 / Loss: 1.686152 ---\n",
      "  Param 0: Value=3.7896, Grad=-1.1735791810950357e-06\n",
      "  Param 1: Value=1.3146, Grad=-5.177567866476432e-07\n",
      "  Param 2: Value=0.6290, Grad=7.6302044784969e-07\n",
      "  Param 3: Value=-2.7640, Grad=-5.4658081165455354e-08\n",
      "  Param 4: Value=0.0023, Grad=-8.187702470612921e-07\n",
      "  Param 5: Value=-0.1696, Grad=3.6728984261623678e-06\n",
      "  Param 6: Value=-1.4066, Grad=-1.0927744412572334e-05\n",
      "  Max Abs Grad: 1.092774e-05\n",
      "------------------------------\n",
      "--- Step 5/10 / Loss: 1.686152 ---\n",
      "  Param 0: Value=3.7896, Grad=-1.1735791810950357e-06\n",
      "  Param 1: Value=1.3146, Grad=-5.177567866476432e-07\n",
      "  Param 2: Value=0.6290, Grad=7.6302044784969e-07\n",
      "  Param 3: Value=-2.7640, Grad=-5.4658081165455354e-08\n",
      "  Param 4: Value=0.0023, Grad=-8.187702470612921e-07\n",
      "  Param 5: Value=-0.1696, Grad=3.6728984261623678e-06\n",
      "  Param 6: Value=-1.4066, Grad=-1.0927744412572334e-05\n",
      "  Max Abs Grad: 1.092774e-05\n",
      "------------------------------\n",
      "--- Step 6/10 / Loss: 1.686152 ---\n",
      "  Param 0: Value=3.7896, Grad=-1.1735791810950357e-06\n",
      "  Param 1: Value=1.3146, Grad=-5.177567866476432e-07\n",
      "  Param 2: Value=0.6290, Grad=7.6302044784969e-07\n",
      "  Param 3: Value=-2.7640, Grad=-5.4658081165455354e-08\n",
      "  Param 4: Value=0.0023, Grad=-8.187702470612921e-07\n",
      "  Param 5: Value=-0.1696, Grad=3.6728984261623678e-06\n",
      "  Param 6: Value=-1.4066, Grad=-1.0927744412572334e-05\n",
      "  Max Abs Grad: 1.092774e-05\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 94\u001b[39m\n\u001b[32m     85\u001b[39m optimizer = model_instance.set_optimizer(\n\u001b[32m     86\u001b[39m         param_groups,     \n\u001b[32m     87\u001b[39m         lr=LBFGS_LR,            \n\u001b[32m     88\u001b[39m         max_iter=LBFGS_MAX_EVAL,        \u001b[38;5;66;03m# max_iter in LBFGS is the line search limit\u001b[39;00m\n\u001b[32m     89\u001b[39m         history_size=LBFGS_HISTORY_SIZE \n\u001b[32m     90\u001b[39m     )\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# --- ðŸ’¥ Call the L-BFGS Fit Method ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m out, steps_ran = \u001b[43mmodel_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_vecc_lbfgs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatern_cov_aniso_STABLE_log_reparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLBFGS_MAX_STEPS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Outer loop steps\u001b[39;49;00m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m end_time = time.time()\n\u001b[32m    102\u001b[39m epoch_time = end_time - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/kernels_reparam_space_time.py:704\u001b[39m, in \u001b[36mfit_vecchia_lbfgs.fit_vecc_lbfgs\u001b[39m\u001b[34m(self, params_list, optimizer, covariance_function, max_steps)\u001b[39m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[32m--> \u001b[39m\u001b[32m704\u001b[39m     loss = \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    706\u001b[39m     max_abs_grad = \u001b[32m0.0\u001b[39m\n\u001b[32m    707\u001b[39m     grad_values = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    482\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    483\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    484\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    485\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/optim/lbfgs.py:330\u001b[39m, in \u001b[36mLBFGS.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    327\u001b[39m state.setdefault(\u001b[33m\"\u001b[39m\u001b[33mn_iter\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m)\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# evaluate initial f(x) and df/dx\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m orig_loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m loss = \u001b[38;5;28mfloat\u001b[39m(orig_loss)\n\u001b[32m    332\u001b[39m current_evals = \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/kernels_reparam_space_time.py:696\u001b[39m, in \u001b[36mfit_vecchia_lbfgs.fit_vecc_lbfgs.<locals>.closure\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    692\u001b[39m params = torch.cat(params_list)\n\u001b[32m    694\u001b[39m \u001b[38;5;66;03m# --- CRITICAL ---\u001b[39;00m\n\u001b[32m    695\u001b[39m \u001b[38;5;66;03m# cov_map MUST be rebuilt inside the closure.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m696\u001b[39m cov_map = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcov_structure_saver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcovariance_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    697\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.compute_vecc_nll(params, covariance_function, cov_map)\n\u001b[32m    699\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/kernels_reparam_space_time.py:325\u001b[39m, in \u001b[36mVecchiaLikelihood.cov_structure_saver\u001b[39m\u001b[34m(self, params, covariance_function)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    324\u001b[39m     jitter = torch.eye(cov_matrix.shape[\u001b[32m0\u001b[39m], device=\u001b[38;5;28mself\u001b[39m.device, dtype=torch.float64) * \u001b[32m1e-6\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     L_full = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcov_matrix\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mjitter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m torch.linalg.LinAlgError:\n\u001b[32m    327\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWarning: Cholesky (full) failed for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time_idx,\u001b[38;5;250m \u001b[39mindex)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- L-BFGS SPECIFIC GLOBAL PARAMETERS ---\n",
    "LBFGS_LR = 1.0\n",
    "LBFGS_MAX_STEPS = 10       # Number of outer optimization steps\n",
    "LBFGS_HISTORY_SIZE = 100   # Memory for Hessian approximation\n",
    "LBFGS_MAX_EVAL = 50        # Max evaluations (line search) per step\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "day_indices = [1] # 0 index \n",
    "for day_idx in day_indices:  \n",
    "\n",
    "    # Assuming data access is correct\n",
    "    daily_hourly_map = daily_hourly_maps[day_idx]\n",
    "    daily_aggregated_tensor = daily_aggregated_tensors[day_idx]\n",
    "\n",
    "    # --- Parameter Initialization (SPATIO-TEMPORAL) ---\n",
    "    '''  \n",
    "    init_sigmasq   = 15.0\n",
    "    init_range_lat = 0.66 \n",
    "    init_range_lon = 0.7 \n",
    "    init_nugget    = 1.5\n",
    "    init_range_time = 0.1\n",
    "    init_advec_lat = 0.02\n",
    "    init_advec_lon = -0.08\n",
    "    '''\n",
    "    init_sigmasq   = 13.059\n",
    "    init_range_lat = 0.154 \n",
    "    init_range_lon = 0.195 \n",
    "    init_nugget    = 0.247\n",
    "    init_range_time = 0.7\n",
    "    init_advec_lat = 0.0218\n",
    "    init_advec_lon = -0.1689\n",
    "\n",
    "    \n",
    "    # Map model parameters to the 'phi' reparameterization\n",
    "    init_phi2 = 1.0 / init_range_lon                # 1/range_lon\n",
    "    init_phi1 = init_sigmasq * init_phi2            # sigmasq / range_lon\n",
    "    init_phi3 = (init_range_lon / init_range_lat)**2  # (range_lon / range_lat)^2\n",
    "    init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # 7-parameter spatio-temporal list (Log/Linear)\n",
    "    params_list = [\n",
    "        torch.tensor([np.log(init_phi1)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [0] log(phi1)\n",
    "        torch.tensor([np.log(init_phi2)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [1] log(phi2)\n",
    "        torch.tensor([np.log(init_phi3)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [2] log(phi3)\n",
    "        torch.tensor([np.log(init_phi4)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [3] log(phi4)\n",
    "        torch.tensor([init_advec_lat],         requires_grad=True, dtype=torch.float64, device=device_str ), # [4] advec_lat (linear)\n",
    "        torch.tensor([init_advec_lon],         requires_grad=True, dtype=torch.float64, device=device_str ), # [5] advec_lon (linear)\n",
    "        torch.tensor([np.log(init_nugget)],    requires_grad=True, dtype=torch.float64, device=device_str )  # [6] log(nugget)\n",
    "    ]\n",
    "\n",
    "    # --- Define parameter groups ---\n",
    "    lr_all = LBFGS_LR\n",
    "    all_indices = [0, 1, 2, 3, 4, 5, 6] \n",
    "    \n",
    "    # L-BFGS requires the parameters to be iterable in a single list or group\n",
    "    param_groups = [\n",
    "        {'params': [params_list[idx] for idx in all_indices], 'lr': lr_all, 'name': 'all_params'}\n",
    "    ]\n",
    "\n",
    "    # --- Print Job Info (using placeholder print variables) ---\n",
    "    print(f'\\n--- Starting Day {day_idx+1} (2024-07-{day_idx+1}) ---')\n",
    "    print(f'Data size per day: { daily_aggregated_tensor.shape[0]/8 }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n')\n",
    "    for i, p in enumerate(params_list):\n",
    "        print(f\"  Param {i}: {p.item():.4f}\")\n",
    "            \n",
    "    # --- ðŸ’¥ Instantiate the L-BFGS Class ---\n",
    "    # NOTE: Assuming fit_vecchia_lbfgs is available via kernels_reparam_space_time\n",
    "    model_instance = kernels_reparam_space_time.fit_vecchia_lbfgs(\n",
    "            smooth = v,\n",
    "            input_map = daily_hourly_map,\n",
    "            aggregated_data = daily_aggregated_tensor,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- ðŸ’¥ Set L-BFGS Optimizer ---\n",
    "    # L-BFGS specific arguments are passed here\n",
    "    optimizer = model_instance.set_optimizer(\n",
    "            param_groups,     \n",
    "            lr=LBFGS_LR,            \n",
    "            max_iter=LBFGS_MAX_EVAL,        # max_iter in LBFGS is the line search limit\n",
    "            history_size=LBFGS_HISTORY_SIZE \n",
    "        )\n",
    "\n",
    "\n",
    "    # --- ðŸ’¥ Call the L-BFGS Fit Method ---\n",
    "    out, steps_ran = model_instance.fit_vecc_lbfgs(\n",
    "            params_list,\n",
    "            optimizer,\n",
    "            model_instance.matern_cov_aniso_STABLE_log_reparam, \n",
    "            max_steps=LBFGS_MAX_STEPS # Outer loop steps\n",
    "        )\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Day {day_idx+1} optimization finished in {epoch_time:.2f}s over {steps_ran+1} L-BFGS steps.\")\n",
    "    print(f\"Day {day_idx+1} final results (raw params + loss): {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c017422",
   "metadata": {},
   "source": [
    "--- Starting Day 1 (2024-07-1) ---\n",
    "Data size per day: 17854, smooth: 0.5\n",
    "mm_cond_number: 8,\n",
    "initial parameters: \n",
    "\n",
    "  Param 0: 4.2042\n",
    "  Param 1: 1.6348\n",
    "  Param 2: 0.4721\n",
    "  Param 3: -2.5562\n",
    "  Param 4: 0.0218\n",
    "  Param 5: -0.1689\n",
    "  Param 6: -1.3984\n",
    "--- Starting L-BFGS Optimization ---\n",
    "--- Step 1/10 / Loss: 1.099366 ---\n",
    "  Param 0: Value=9.1906, Grad=0.0\n",
    "  Param 1: Value=21.8702, Grad=0.0\n",
    "  Param 2: Value=-9.1816, Grad=0.0\n",
    "  Param 3: Value=-5.1759, Grad=0.0\n",
    "  Param 4: Value=0.8510, Grad=0.0\n",
    "  Param 5: Value=3.9055, Grad=0.0\n",
    "  Param 6: Value=-56.1134, Grad=-1.5296128129348282e-08\n",
    "  Max Abs Grad: 1.529613e-08\n",
    "------------------------------\n",
    "\n",
    "Converged on gradient norm (max|grad| < 1e-05) at step 1\n",
    "FINAL STATE: Step 1, Loss: 1.0993658256399734\n",
    "  Raw (vecc) Parameters: [9.190602269069139, 21.87017839865902, -9.181575702042394, -5.1759395277982945, 0.8510437730610125, 3.9054716223330805, -56.11340102891027]\n",
    "  Interpretable Parameters:\n",
    "    sigma_sq  : 0.000003\n",
    "    range_lon : 0.000000\n",
    "    range_lat : 0.000000\n",
    "    range_time: 0.000000\n",
    "    advec_lat : 0.851044\n",
    "    advec_lon : 3.905472\n",
    "    nugget    : 0.000000\n",
    "Day 1 optimization finished in 2311.03s over 1 L-BFGS steps.\n",
    "Day 1 final results (raw params + loss): [9.190602269069139, 21.87017839865902, -9.181575702042394, -5.1759395277982945, 0.8510437730610125, 3.9054716223330805, -56.11340102891027, 1.0993658256399734]\n",
    "\n",
    "\n",
    "for day2, it won't converge even after 180 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a3fa5",
   "metadata": {},
   "source": [
    "Hyper parameters for adams optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf11885",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0.5 # smooth\n",
    "mm_cond_number = 8\n",
    "nheads = 300\n",
    "#nheads = 1230\n",
    "#lr = 0.01\n",
    "#step = 80\n",
    "#gamma_par = 0.5\n",
    "\n",
    "# --- Placeholder Global Variables ---\n",
    "# ðŸ’¥ REVISED: Added lr, patience, factor. Removed step, gamma_par\n",
    "lr=0.1\n",
    "patience = 5       # Scheduler: Epochs to wait for improvement\n",
    "factor = 0.5         # Scheduler: Factor to reduce LR by (e.g., 0.5 = 50% cut)\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5946f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- End Placeholders ---\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "day_indices = [0] \n",
    "for day_idx in day_indices:  \n",
    "\n",
    "    daily_hourly_map = daily_hourly_maps[day_idx]\n",
    "    daily_aggregated_tensor = daily_aggregated_tensors[day_idx]\n",
    "\n",
    "    # --- Correct Parameter Initialization (SPATIAL-ONLY) ---\n",
    "    init_sigmasq   = 15.0\n",
    "    init_range_lat = 0.66 \n",
    "    init_range_lon = 0.7 \n",
    "    init_nugget    = 1.5\n",
    "    \n",
    "    # Map model parameters to the 'phi' reparameterization\n",
    "    init_phi2 = 1.0 / init_range_lon                \n",
    "    init_phi1 = init_sigmasq * init_phi2            \n",
    "    init_phi3 = (init_range_lon / init_range_lat)**2  \n",
    "    \n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # 4-parameter spatial-only list\n",
    "    params_list = [\n",
    "        torch.tensor([np.log(init_phi1)],      requires_grad=True, dtype=torch.float64, device=device_str ),\n",
    "        torch.tensor([np.log(init_phi2)],      requires_grad=True, dtype=torch.float64, device=device_str ),\n",
    "        torch.tensor([np.log(init_phi3)],      requires_grad=True, dtype=torch.float64, device=device_str ),\n",
    "        torch.tensor([np.log(init_nugget)],    requires_grad=True, dtype=torch.float64, device=device_str )\n",
    "    ]\n",
    "\n",
    "    # --- Define learning rates and parameter groups ---\n",
    "    # ðŸ’¥ REVISED: Set lr_fast from the global lr\n",
    "    lr_fast = lr \n",
    "    fast_indices = [0, 1, 2, 3] # All parameters\n",
    "    \n",
    "    param_groups = [\n",
    "        {'params': [params_list[idx] for idx in fast_indices], 'lr': lr_fast, 'name': 'fast_group'}\n",
    "    ]\n",
    "\n",
    "    # --- Print Job Info ---\n",
    "    # Assuming lat_lon_resolution, v, mm_cond_number are defined globally\n",
    "    res_calc = (113 // lat_lon_resolution[0]) * (158 // lat_lon_resolution[0]) \n",
    "    print(f'\\n--- Starting Day {day_idx+1} (2024-07-{day_idx+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params_list}')\n",
    "            \n",
    "    # --- Instantiate the Correct Class ---\n",
    "    model_instance = kernels_repar_space.fit_vecchia_adams_fullbatch(\n",
    "            smooth = v,\n",
    "            input_map = daily_hourly_map,\n",
    "            aggregated_data = daily_aggregated_tensor,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- ðŸ’¥ REVISED: Call the Optimizer Method with Plateau settings ---\n",
    "    optimizer, scheduler = model_instance.set_optimizer(\n",
    "            param_groups,     \n",
    "            lr=lr,            \n",
    "            betas=(0.9, 0.99), \n",
    "            eps=1e-5, \n",
    "            scheduler_type='plateau', # Explicitly set to plateau\n",
    "            patience=patience,        # Pass patience\n",
    "            factor=factor             # Pass factor\n",
    "        )\n",
    "\n",
    "    # --- Call the Correct Fit Method ---\n",
    "    out, epoch_ran = model_instance.fit_vecc_scheduler_fullbatch(\n",
    "            params_list,\n",
    "            optimizer,\n",
    "            scheduler, \n",
    "            model_instance.matern_cov_SPATIAL_log_reparam, \n",
    "            epochs=epochs\n",
    "        )\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Day {day_idx+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day_idx+1} final results: {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79e7fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Negative Log-Likelihood: (22936.561132578532, 22882.89402724899)\n",
      "Full Negative Log-Likelihood: (22940.44941805631, 22893.58097275657)\n",
      "Full Negative Log-Likelihood: (22959.63309778168, 22897.341313474775)\n"
     ]
    }
   ],
   "source": [
    "def cal(a):\n",
    "    day_indices = [0] \n",
    "    for day_idx in day_indices:  \n",
    "\n",
    "        daily_hourly_map = daily_hourly_maps[day_idx]\n",
    "        daily_aggregated_tensor = daily_aggregated_tensors[day_idx]\n",
    "\n",
    "        # Convert initial parameters to a list of 1-element tensors\n",
    "        params_list = [\n",
    "            torch.tensor([val], dtype=torch.float64, requires_grad=True, device=device_str) for val in a\n",
    "        ]\n",
    "        \n",
    "        # ðŸ’¡ CRITICAL: Concatenate the list into a single tensor\n",
    "        # The full_likelihood function expects a single tensor, not a list\n",
    "        params_tensor = torch.cat(params_list)\n",
    "\n",
    "        # Assuming 'kernels_repar_space' has your 'fit_vecchia_adams' class\n",
    "        model_instance = kernels_repar_space.fit_vecchia_adams_fullbatch(\n",
    "                smooth = v,\n",
    "                input_map = daily_hourly_map,\n",
    "                aggregated_data = daily_aggregated_tensor,\n",
    "                nns_map = nns_map,\n",
    "                mm_cond_number = mm_cond_number,\n",
    "                nheads = nheads\n",
    "            )\n",
    "        \n",
    "        # ðŸ’¡ Pass the single 'params_tensor' and the correct 4-parameter spatial covariance function\n",
    "        bb = model_instance.full_likelihood_avg(\n",
    "            params = params_tensor, \n",
    "            input_data = daily_aggregated_tensor, \n",
    "            y = daily_aggregated_tensor[:,2], \n",
    "            covariance_function = model_instance.matern_cov_SPATIAL_log_reparam\n",
    "        )\n",
    "     \n",
    "        cov_map = model_instance.cov_structure_saver(params_tensor, model_instance.matern_cov_SPATIAL_log_reparam)\n",
    "        vecchia_nll = model_instance.vecchia_space_fullbatch( # Change this to your chosen Vecchia implementation\n",
    "            params = params_tensor, \n",
    "            covariance_function = model_instance.matern_cov_SPATIAL_log_reparam, \n",
    "            cov_map = cov_map # Assuming cov_map is precomputed or computed internally\n",
    "        )\n",
    " \n",
    "    return bb, vecchia_nll\n",
    "\n",
    "\n",
    "\n",
    "a = [3.5764, 0.7940, 0.3087, 0.6157]\n",
    "bb, nll_value = cal(a)\n",
    "print(f\"Full Negative Log-Likelihood: {bb.item()*18162, nll_value.item()*18162}\")\n",
    "\n",
    "a = [3.6764, 0.7940, 0.3087, 0.6157]\n",
    "bb, nll_value = cal(a)\n",
    "print(f\"Full Negative Log-Likelihood: {bb.item()*18162, nll_value.item()*18162}\")\n",
    "\n",
    "\n",
    "a = [3.4764, 0.7940, 0.3087, 0.6157]\n",
    "bb, nll_value = cal(a)\n",
    "print(f\"Full Negative Log-Likelihood: {bb.item()*18162, nll_value.item()*18162}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gems_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
