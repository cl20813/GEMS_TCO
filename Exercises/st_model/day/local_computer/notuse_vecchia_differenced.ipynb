{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd6379e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy                    # clone tensor\n",
    "import time\n",
    "\n",
    "# Custom imports\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels\n",
    "from GEMS_TCO import data_preprocess \n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "import typer\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO.data_loader import load_data2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "315ff592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting data to lat: [0.0, 5.0], lon: [123.0, 133.0]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "# Assuming 'config' and 'load_data' class are defined and imported elsewhere\n",
    "\n",
    "# --- Parameters derived from your framework ---\n",
    "v: float = 0.5\n",
    "space: List[str] = ['4', '4']\n",
    "days: List[str] = ['0', '31']\n",
    "mm_cond_number: int = 20\n",
    "# --- End of framework parameters ---\n",
    "\n",
    "lat_lon_resolution = [int(s) for s in space]\n",
    "days_s_e = [int(d) for d in days]\n",
    "days_list = list(range(days_s_e[0], days_s_e[1]))\n",
    "\n",
    "# These values were not in the framework, so they remain as set in your snippet\n",
    "years = ['2024']\n",
    "month_range = [7] \n",
    "\n",
    "# Assuming 'config' is available in your environment\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "\n",
    "## load ozone data from amarel\n",
    "data_load_instance = load_data2(config.mac_data_load_path)\n",
    "\n",
    "# Call the function using the variables from the framework\n",
    "df_map, ord_mm, nns_map = data_load_instance.load_maxmin_ordered_data_bymonthyear(\n",
    "lat_lon_resolution=lat_lon_resolution, \n",
    "mm_cond_number=mm_cond_number,\n",
    "years_=years, \n",
    "months_=month_range,\n",
    "lat_range=[0.0, 5.0],      # <-- Add this\n",
    "lon_range=[123.0, 133.0]   # <-- Add this\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06507f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8960, 6])\n",
      "tensor([[  4.8880, 132.9840, 264.4437,  45.0000,   4.8891, 132.9843],\n",
      "        [  4.8880, 132.7320, 271.4257,  45.0000,   4.8896, 132.7330],\n",
      "        [  4.8880, 132.4800, 266.4241,  45.0000,   4.8903, 132.4819],\n",
      "        [  4.8880, 132.2280, 257.9527,  45.0000,   4.8904, 132.2309],\n",
      "        [  4.8880, 131.9760, 259.0169,  45.0000,   4.8904, 131.9805],\n",
      "        [  4.8880, 131.7240, 258.4678,  45.0000,   4.8906, 131.7299],\n",
      "        [  4.8880, 131.4720, 272.3340,  45.0000,   4.8911, 131.4797],\n",
      "        [  4.8880, 131.2200, 264.8994,  45.0000,   4.8916, 131.2293],\n",
      "        [  4.8880, 130.9680, 269.7025,  45.0000,   4.8925, 130.9793],\n",
      "        [  4.8880, 130.7160, 261.1764,  45.0000,   4.8925, 130.7288],\n",
      "        [  4.8880, 130.4640, 267.1273,  45.0000,   4.8931, 130.4789],\n",
      "        [  4.8880, 130.2120, 266.5924,  45.0000,   4.8936, 130.2286],\n",
      "        [  4.8880, 129.9600, 256.9923,  45.0000,   4.8940, 129.9786],\n",
      "        [  4.8880, 129.7080, 262.3289,  45.0000,   4.8944, 129.7282],\n",
      "        [  4.8880, 129.4560, 260.6782,  45.0000,   4.8948, 129.4787],\n",
      "        [  4.8880, 129.2040, 257.3878,  45.0000,   4.8952, 129.2280],\n",
      "        [  4.8880, 128.9520, 258.5924,  45.0000,   4.8958, 128.9786],\n",
      "        [  4.8880, 128.7000, 258.1751,  45.0000,   4.8967, 128.7285],\n",
      "        [  4.8880, 128.4480, 256.4374,  45.0000,   4.8974, 128.4787],\n",
      "        [  4.8880, 128.1960, 256.8344,  45.0000,   4.8981, 128.1664],\n",
      "        [  4.8880, 127.9440, 260.3387,  45.0000,   4.8989, 127.9162],\n",
      "        [  4.8880, 127.6920, 259.3679,  45.0000,   4.8776, 127.6670],\n",
      "        [  4.8880, 127.4400, 257.4616,  45.0000,   4.8783, 127.4167],\n",
      "        [  4.8880, 127.1880, 262.2031,  45.0000,   4.8786, 127.1675],\n",
      "        [  4.8880, 126.9360, 258.5748,  45.0000,   4.8793, 126.9176],\n",
      "        [  4.8880, 126.6840, 261.6556,  45.0000,   4.8803, 126.6675],\n",
      "        [  4.8880, 126.4320, 258.6891,  45.0000,   4.8806, 126.4178],\n",
      "        [  4.8880, 126.1800, 260.8269,  45.0000,   4.8812, 126.1672],\n",
      "        [  4.8880, 125.9280, 259.0091,  45.0000,   4.8818, 125.9167],\n",
      "        [  4.8880, 125.6760, 261.2653,  45.0000,   4.8829, 125.6669],\n",
      "        [  4.8880, 125.4240, 258.8737,  45.0000,   4.8836, 125.4164],\n",
      "        [  4.8880, 125.1720, 257.5042,  45.0000,   4.8840, 125.1657],\n",
      "        [  4.8880, 124.9200, 260.3677,  45.0000,   4.8846, 124.9157],\n",
      "        [  4.8880, 124.6680, 260.2331,  45.0000,   4.8854, 124.6648],\n",
      "        [  4.8880, 124.4160, 262.2422,  45.0000,   4.8860, 124.4143],\n",
      "        [  4.8880, 124.1640, 261.3450,  45.0000,   4.8868, 124.1635],\n",
      "        [  4.8880, 123.9120, 261.2970,  45.0000,   4.8876, 123.9122],\n",
      "        [  4.8880, 123.6600, 261.8816,  45.0000,   4.8882, 123.6615],\n",
      "        [  4.8880, 123.4080, 261.9475,  45.0000,   4.8890, 123.4102],\n",
      "        [  4.8880, 123.1560, 264.4173,  45.0000,   4.8899, 123.1594],\n",
      "        [  4.7120, 132.9840, 266.8790,  45.0000,   4.7146, 132.9812],\n",
      "        [  4.7120, 132.7320, 265.3449,  45.0000,   4.7151, 132.7301],\n",
      "        [  4.7120, 132.4800, 261.1311,  45.0000,   4.7158, 132.4790],\n",
      "        [  4.7120, 132.2280, 262.5706,  45.0000,   4.7158, 132.2281],\n",
      "        [  4.7120, 131.9760, 261.7889,  45.0000,   4.7159, 131.9778],\n",
      "        [  4.7120, 131.7240, 260.4907,  45.0000,   4.7161, 131.7273],\n",
      "        [  4.7120, 131.4720, 259.8616,  45.0000,   4.7166, 131.4771],\n",
      "        [  4.7120, 131.2200, 265.5181,  45.0000,   4.7172, 131.2268],\n",
      "        [  4.7120, 130.9680, 270.7816,  45.0000,   4.7180, 130.9769],\n",
      "        [  4.7120, 130.7160, 261.7210,  45.0000,   4.7181, 130.7264],\n",
      "        [  4.7120, 130.4640, 264.0474,  45.0000,   4.7187, 130.4766],\n",
      "        [  4.7120, 130.2120, 261.8296,  45.0000,   4.7192, 130.2264],\n",
      "        [  4.7120, 129.9600, 260.6177,  45.0000,   4.7195, 129.9765],\n",
      "        [  4.7120, 129.7080, 259.5598,  45.0000,   4.7199, 129.7262],\n",
      "        [  4.7120, 129.4560, 258.0938,  45.0000,   4.7204, 129.4768],\n",
      "        [  4.7120, 129.2040, 259.5642,  45.0000,   4.7208, 129.2261],\n",
      "        [  4.7120, 128.9520, 268.4056,  45.0000,   4.7214, 128.9768],\n",
      "        [  4.7120, 128.7000, 260.0605,  45.0000,   4.7223, 128.7268],\n",
      "        [  4.7120, 128.4480, 258.3759,  45.0000,   4.7012, 128.4769],\n",
      "        [  4.7120, 128.1960, 258.8751,  45.0000,   4.7018, 128.2268],\n",
      "        [  4.7120, 127.9440, 257.9050,  45.0000,   4.7027, 127.9146],\n",
      "        [  4.7120, 127.6920, 257.3889,  45.0000,   4.7032, 127.6657],\n",
      "        [  4.7120, 127.4400, 257.5884,  45.0000,   4.7039, 127.4155],\n",
      "        [  4.7120, 127.1880, 256.8197,  45.0000,   4.7041, 127.1663],\n",
      "        [  4.7120, 126.9360, 257.2672,  45.0000,   4.7048, 126.9165],\n",
      "        [  4.7120, 126.6840, 259.9464,  45.0000,   4.7058, 126.6665],\n",
      "        [  4.7120, 126.4320, 259.5698,  45.0000,   4.7061, 126.4168],\n",
      "        [  4.7120, 126.1800, 260.4592,  45.0000,   4.7067, 126.1664],\n",
      "        [  4.7120, 125.9280, 258.6119,  45.0000,   4.7074, 125.9160],\n",
      "        [  4.7120, 125.6760, 256.9527,  45.0000,   4.7084, 125.6663],\n",
      "        [  4.7120, 125.4240, 260.0318,  45.0000,   4.7091, 125.4158],\n",
      "        [  4.7120, 125.1720, 265.3234,  45.0000,   4.7095, 125.1652],\n",
      "        [  4.7120, 124.9200, 266.0690,  45.0000,   4.7101, 124.9153],\n",
      "        [  4.7120, 124.6680, 264.3739,  45.0000,   4.7109, 124.6644],\n",
      "        [  4.7120, 124.4160, 259.7857,  45.0000,   4.7115, 124.4140],\n",
      "        [  4.7120, 124.1640, 262.1266,  45.0000,   4.7122, 124.1633],\n",
      "        [  4.7120, 123.9120, 259.5642,  45.0000,   4.7131, 123.9121],\n",
      "        [  4.7120, 123.6600, 259.7252,  45.0000,   4.7136, 123.6615],\n",
      "        [  4.7120, 123.4080, 261.0889,  45.0000,   4.7145, 123.4103],\n",
      "        [  4.7120, 123.1560, 263.1887,  45.0000,   4.7153, 123.1596],\n",
      "        [  4.5360, 132.9840, 261.8141,  45.0000,   4.5401, 132.9782],\n",
      "        [  4.5360, 132.7320, 264.4874,  45.0000,   4.5406, 132.7271],\n",
      "        [  4.5360, 132.4800, 258.9961,  45.0000,   4.5195, 132.4758],\n",
      "        [  4.5360, 132.2280, 260.0811,  45.0000,   4.5414, 132.2253],\n",
      "        [  4.5360, 131.9760, 259.6999,  45.0000,   4.5414, 131.9751],\n",
      "        [  4.5360, 131.7240, 259.6341,  45.0000,   4.5417, 131.7246],\n",
      "        [  4.5360, 131.4720, 257.5427,  45.0000,   4.5421, 131.4745],\n",
      "        [  4.5360, 131.2200, 263.1730,  45.0000,   4.5427, 131.2243],\n",
      "        [  4.5360, 130.9680, 266.1416,  45.0000,   4.5436, 130.9745],\n",
      "        [  4.5360, 130.7160, 263.0376,  45.0000,   4.5437, 130.7241],\n",
      "        [  4.5360, 130.4640, 264.1132,  45.0000,   4.5443, 130.4744],\n",
      "        [  4.5360, 130.2120, 260.2981,  45.0000,   4.5448, 130.2243],\n",
      "        [  4.5360, 129.9600, 260.7120,  45.0000,   4.5451, 129.9744],\n",
      "        [  4.5360, 129.7080, 262.1092,  45.0000,   4.5456, 129.7242],\n",
      "        [  4.5360, 129.4560, 259.4515,  45.0000,   4.5460, 129.4748],\n",
      "        [  4.5360, 129.2040, 259.8494,  45.0000,   4.5464, 129.2243],\n",
      "        [  4.5360, 128.9520, 257.1795,  45.0000,   4.5252, 128.9749],\n",
      "        [  4.5360, 128.7000, 256.0251,  45.0000,   4.5262, 128.7249],\n",
      "        [  4.5360, 128.4480, 260.7026,  45.0000,   4.5268, 128.4753],\n",
      "        [  4.5360, 128.1960, 259.3369,  45.0000,   4.5275, 128.2253]])\n"
     ]
    }
   ],
   "source": [
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "for i in range(31):\n",
    "    idx_for_datamap = [i*8, (i+1)*8]\n",
    "\n",
    "    cur_map, cur_df = analysis_map_no_mm, agg_data_no_mm = data_load_instance.load_working_data_keep_ori(\n",
    "    df_map, \n",
    "    idx_for_datamap, \n",
    "    ord_mm=None,  # or just omit it\n",
    "    dtype=torch.float # or just omit it\n",
    ")\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "print(df_day_aggregated_list[0].shape)\n",
    "print(df_day_aggregated_list[1][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15989cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data filtering...\n",
      "Data filtering complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Assume GEMS_TCO is a custom class/module you have available\n",
    "# from your_project import GEMS_TCO\n",
    "\n",
    "# =========================================================================\n",
    "# 1. Helper Functions\n",
    "# =========================================================================\n",
    "\n",
    "def subset_tensor(df_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Subsets a tensor to a specific lat/lon range.\"\"\"\n",
    "    #lat_mask = (df_tensor[:, 0] >= -5) & (df_tensor[:, 0] <= 6.3)\n",
    "    #lon_mask = (df_tensor[:, 1] >= 118) & (df_tensor[:, 1] <= 134.2)\n",
    "    lat_mask = (df_tensor[:, 0] >= 0) & (df_tensor[:, 0] <= 5)\n",
    "    lon_mask = (df_tensor[:, 1] >= 123) & (df_tensor[:, 1] <= 133)\n",
    "\n",
    "    df_sub = df_tensor[lat_mask & lon_mask].clone()\n",
    "    return df_sub\n",
    "\n",
    "def apply_first_difference_2d_tensor(df_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies a 2D first-order difference filter using convolution.\n",
    "    This approximates Z(s) = [X(s+d_lat) - X(s)] + [X(s+d_lon) - X(s)].\n",
    "    \"\"\"\n",
    "    if df_tensor.size(0) == 0:\n",
    "        return torch.empty(0, 4)\n",
    "\n",
    "    # 1. Get grid dimensions and validate\n",
    "    unique_lats = torch.unique(df_tensor[:, 0])\n",
    "    unique_lons = torch.unique(df_tensor[:, 1])\n",
    "    lat_count, lon_count = unique_lats.size(0), unique_lons.size(0)\n",
    "\n",
    "    if df_tensor.size(0) != lat_count * lon_count:\n",
    "        raise ValueError(\"Tensor size does not match grid dimensions. Must be a complete grid.\")\n",
    "    if lat_count < 2 or lon_count < 2:\n",
    "        return torch.empty(0, 4)\n",
    "\n",
    "    # 2. Reshape data and define the correct kernel\n",
    "    ozone_data = df_tensor[:, 2].reshape(1, 1, lat_count, lon_count)\n",
    "    \n",
    "    # ✅ CORRECT KERNEL: This kernel results in the standard first-order difference:\n",
    "    # Z(i,j) = X(i+1,j) + X(i,j+1) - 2*X(i,j)\n",
    "    # Note: F.conv2d in PyTorch actually performs cross-correlation. To get a true\n",
    "    # convolution result, the kernel would need to be flipped. However, for a \n",
    "    # forward difference operator, defining the kernel for cross-correlation is more direct.\n",
    "    # The kernel below is designed for cross-correlation to achieve the desired differencing.\n",
    "    diff_kernel = torch.tensor([[[[-2., 1.],\n",
    "                                  [ 1., 0.]]]], dtype=torch.float32)\n",
    "\n",
    "    # 3. Apply convolution (which acts as cross-correlation)\n",
    "    filtered_grid = F.conv2d(ozone_data, diff_kernel, padding='valid').squeeze()\n",
    "\n",
    "    # 4. Determine coordinates for the new, smaller grid\n",
    "    # The new grid corresponds to the anchor points of the kernel\n",
    "    new_lats = unique_lats[:-1]\n",
    "    new_lons = unique_lons[:-1]\n",
    "\n",
    "    # 5. Reconstruct the output tensor\n",
    "    new_lat_grid, new_lon_grid = torch.meshgrid(new_lats, new_lons, indexing='ij')\n",
    "    filtered_values = filtered_grid.flatten()\n",
    "    time_value = df_tensor[0, 3].repeat(filtered_values.size(0))\n",
    "\n",
    "    new_tensor = torch.stack([\n",
    "        new_lat_grid.flatten(),\n",
    "        new_lon_grid.flatten(),\n",
    "        filtered_values,\n",
    "        time_value\n",
    "    ], dim=1)\n",
    "    \n",
    "    return new_tensor\n",
    "\n",
    "# =========================================================================\n",
    "# 2. Data Loading (Unchanged)\n",
    "# =========================================================================\n",
    "# ⚠️ NOTE: You must define these variables\n",
    "# mac_data_path = \"...\"\n",
    "# year = 2022\n",
    "# month_str = \"01\"\n",
    "# class GEMS_TCO: # Placeholder\n",
    "#     def load_data(self, path): return self\n",
    "#     def load_working_data_byday_wo_mm(self, data, indices):\n",
    "#         return {'key': torch.randn(100, 4)}, torch.randn(100, 4)\n",
    "\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "for i in range(31):\n",
    "    idx_for_datamap = [i*8, (i+1)*8]\n",
    "\n",
    "    cur_map, cur_df = analysis_map_no_mm, agg_data_no_mm = data_load_instance.load_working_data(\n",
    "    df_map, \n",
    "    idx_for_datamap, \n",
    "    ord_mm=None,  # or just omit it\n",
    "    dtype=torch.float # or just omit it\n",
    ")\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 3. Main Processing Loop (REVISED)\n",
    "# =========================================================================\n",
    "spatially_filtered_day_maps = [] # This will be a list of dicts\n",
    "spatially_filtered_day_aggregates = [] # This will be a list of tensors\n",
    "\n",
    "print(\"Starting data filtering...\")\n",
    "for day_idx, day_map in enumerate(df_day_map_list):\n",
    "\n",
    "    filtered_map_for_this_day = {}\n",
    "    tensors_to_aggregate_for_this_day = []\n",
    "    \n",
    "    # Sort keys to ensure proper time ordering (e.g., '0', '1', '2'...)\n",
    "    # Adjust this sort if your keys are not integer strings\n",
    "    try:\n",
    "        sorted_keys = sorted(day_map.keys(), key=lambda k: int(k))\n",
    "    except ValueError:\n",
    "        sorted_keys = sorted(day_map.keys()) # Fallback for non-integer keys\n",
    "\n",
    "    for key in sorted_keys:\n",
    "        tensor = day_map[key]\n",
    "        subsetted = subset_tensor(tensor)\n",
    "        if subsetted.size(0) > 0:\n",
    "            try:\n",
    "                diff_applied = apply_first_difference_2d_tensor(subsetted)\n",
    "                if diff_applied.size(0) > 0:\n",
    "                    # Add the filtered tensor to the map for this day\n",
    "                    filtered_map_for_this_day[key] = diff_applied\n",
    "                    # Add it to the list for the aggregated tensor\n",
    "                    tensors_to_aggregate_for_this_day.append(diff_applied)\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping data chunk {key} on day {day_idx+1} due to error: {e}\")\n",
    "\n",
    "    # Add the new filtered map (dict) to the list\n",
    "    spatially_filtered_day_maps.append(filtered_map_for_this_day)\n",
    "    \n",
    "    # Add the aggregated tensor for this day to the other list\n",
    "    if tensors_to_aggregate_for_this_day:\n",
    "        aggregated_day_tensor = torch.cat(tensors_to_aggregate_for_this_day, dim=0)\n",
    "        spatially_filtered_day_aggregates.append(aggregated_day_tensor)\n",
    "    else:\n",
    "        # Handle case where a day has no valid data after filtering\n",
    "        print(f\"Warning: Day {day_idx+1} has no data after filtering.\")\n",
    "        spatially_filtered_day_aggregates.append(torch.empty(0, 4, dtype=torch.float)) \n",
    "\n",
    "print(\"Data filtering complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4728c7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance1 = kernels.vecchia_experiment(0.5, df_day_map_list[0], df_day_aggregated_list[0], nns_map, mm_cond_number, nheads=10)\n",
    "\n",
    "a = [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]\n",
    "#a = [30.2594, 0.665, 1.8981, 0.0, 0.1317, -0.0, 1.9785]\n",
    "#a = [45.1402, 0.6299, 0.7308, -0.0003, -0.0151, 0.0, 7.8922]\n",
    "#a = [21.7335, 1.2817, 1.5946, 0.042, -0.1241, 0.218, 4.8654]\n",
    "#a = [20.453542336448137, 1.4506118600616982, 2.43096923637867, -0.03476556019978718, -0.1559262606484541, 0.1254833595232136, 3.938183829354925]\n",
    "params = torch.tensor(a, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "cov_map = instance1.cov_structure_saver(params, instance1.matern_cov_anisotropy_v05)  \n",
    "instance1.vecchia_oct22( params, instance1.matern_cov_anisotropy_v05, cov_map )\n",
    "\n",
    "v = 0.5 # smooth\n",
    "mm_cond_number = 20\n",
    "nheads = 300\n",
    "lr = 0.02\n",
    "step = 100\n",
    "gamma_par = 0.3\n",
    "epochs = 900\n",
    "\n",
    "DELTA_LAT = 0.044  # <-- REPLACE WITH YOUR ACTUAL VALUE (e.g., unique_lats[1] - unique_lats[0])\n",
    "DELTA_LON = 0.063  # <-- REPLACE WITH YOUR ACTUAL VALUE (e.g., unique_lons[1] - unique_lons[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee451135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 1250.0, smooth: 0.5\n",
      "mm_cond_number: 20,\n",
      "initial parameters: \n",
      " [28.75   0.98   1.06   0.036 -0.155  0.179  1.89 ]\n",
      "Epoch 1, Gradients: [-196.87185871 3030.8760038  2457.8183985  -352.71639333 -819.59381364\n",
      " -252.23272776    0.        ]\n",
      " Loss: 31227.155567381862, Parameters: [28.75   0.98   1.06   0.036 -0.155  0.179  1.89 ]\n",
      "Epoch 51, Gradients: [   13.78772642 -2119.456945   -1263.4687549    -70.14938054\n",
      "   -74.18145216    15.86189106     0.        ]\n",
      " Loss: 20196.289473327266, Parameters: [ 2.98522138e+01  7.78091877e-02  1.35500314e-01 -5.54604269e-03\n",
      " -3.75252782e-02  6.34196943e-01  1.89000000e+00]\n",
      "Epoch 101, Gradients: [   1.96599319 -257.1479175  -139.71705761   14.13488168   -7.91772452\n",
      "   -0.89160506    0.        ]\n",
      " Loss: 19813.565429684313, Parameters: [ 3.01839854e+01  1.28558234e-01  2.07867132e-01 -9.13521642e-04\n",
      " -6.69047038e-02  4.18648487e-01  1.89000000e+00]\n",
      "Epoch 151, Gradients: [  0.14235057 -47.31547926 -29.98173637   4.82214779   8.27075405\n",
      "  -0.35722988   0.        ]\n",
      " Loss: 19800.05332078002, Parameters: [ 3.02902584e+01  1.40348405e-01  2.14166167e-01 -8.68853591e-03\n",
      " -5.93204950e-02  4.29962962e-01  1.89000000e+00]\n",
      "Epoch 201, Gradients: [-0.21838215 -8.83914804 -7.21367587 -3.97996786 -4.67337079  0.30075153\n",
      "  0.        ]\n",
      " Loss: 19797.039175064965, Parameters: [ 3.05477197e+01  1.43260512e-01  2.18593964e-01 -8.58013453e-03\n",
      " -6.09902990e-02  4.27086688e-01  1.89000000e+00]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     66\u001b[39m final_covariance_function = partial(\n\u001b[32m     67\u001b[39m         model_instance.build_cov_matrix_spatial_difference_anisotropy, \n\u001b[32m     68\u001b[39m         delta1=DELTA_LAT, \n\u001b[32m     69\u001b[39m         delta2=DELTA_LON\n\u001b[32m     70\u001b[39m     )\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# Calling the optimized 'run_vecc_may9'\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m out, epoch_ran = \u001b[43mmodel_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_vecc_oct22\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfinal_covariance_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# --- End Correction ---\u001b[39;00m\n\u001b[32m     83\u001b[39m end_time = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/kernels.py:1568\u001b[39m, in \u001b[36mmodel_fitting.run_vecc_oct22\u001b[39m\u001b[34m(self, params, optimizer, scheduler, covariance_function, epochs)\u001b[39m\n\u001b[32m   1561\u001b[39m tol = \u001b[32m1e-4\u001b[39m  \u001b[38;5;66;03m# Convergence tolerance\u001b[39;00m\n\u001b[32m   1563\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):  \n\u001b[32m   1564\u001b[39m     \n\u001b[32m   1565\u001b[39m     \u001b[38;5;66;03m# --- FIX: Re-compute cov_map INSIDE the loop ---\u001b[39;00m\n\u001b[32m   1566\u001b[39m     \u001b[38;5;66;03m# The cov_map depends on 'params', so it must be re-computed\u001b[39;00m\n\u001b[32m   1567\u001b[39m     \u001b[38;5;66;03m# every time 'params' is updated.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1568\u001b[39m     cov_map = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcov_structure_saver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcovariance_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1570\u001b[39m     optimizer.zero_grad()  \n\u001b[32m   1571\u001b[39m     loss = \u001b[38;5;28mself\u001b[39m.compute_vecc_nll_oct22(params, covariance_function, cov_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/kernels.py:1137\u001b[39m, in \u001b[36mvecchia_experiment.cov_structure_saver\u001b[39m\u001b[34m(self, params, covariance_function)\u001b[39m\n\u001b[32m   1134\u001b[39m locs = aggregated_arr[:, :\u001b[32m2\u001b[39m]\n\u001b[32m   1135\u001b[39m aggregated_y = aggregated_arr[:, \u001b[32m2\u001b[39m] \u001b[38;5;66;03m# Get the Y-vector\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1137\u001b[39m cov_matrix = \u001b[43mcovariance_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregated_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregated_arr\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[38;5;66;03m# --- OPTIMIZATION 1: Cholesky for tmp1 ---\u001b[39;00m\n\u001b[32m   1140\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1141\u001b[39m     \u001b[38;5;66;03m# Add jitter FOR Cholesky, but not to original cov_matrix\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/kernels.py:1316\u001b[39m, in \u001b[36mvecchia_experiment.build_cov_matrix_spatial_difference_anisotropy\u001b[39m\u001b[34m(self, params, y, x, delta1, delta2)\u001b[39m\n\u001b[32m   1313\u001b[39m         \u001b[38;5;66;03m# --- Call the BASE X kernel (your old function) ---\u001b[39;00m\n\u001b[32m   1314\u001b[39m         term_cov = \u001b[38;5;28mself\u001b[39m.matern_cov_anisotropy_v05(params, x_shifted, y_shifted)\n\u001b[32m-> \u001b[39m\u001b[32m1316\u001b[39m         final_cov_matrix += w_ab * w_cd * term_cov\n\u001b[32m   1318\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m final_cov_matrix\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# This list is now just for iterating\n",
    "#days_list = range(len(df_day_map_list)) \n",
    "\n",
    "from functools import partial\n",
    "\n",
    "days_list = [0]\n",
    "for day in days_list:  \n",
    "    \n",
    "    # ==========================================================\n",
    "    # --- ‼️ CRITICAL FIX ‼️ ---\n",
    "    # Load the NEW filtered data for this day\n",
    "    analysis_data_map = spatially_filtered_day_maps[day]\n",
    "    aggregated_data = spatially_filtered_day_aggregates[day]\n",
    "\n",
    "    # If this day had no data, skip it\n",
    "    if aggregated_data.size(0) == 0:\n",
    "        print(f\"Skipping Day {day+1}, no data after filtering.\")\n",
    "        continue\n",
    "\n",
    "    #a = [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]\n",
    "    #a = [28.75, 0.98, 1.06, 0, 0, 0, 1.890]\n",
    "    a = [28.75, 0.98, 1.06, 0.036, -0.155, 0.179, 1.890]\n",
    "    params = torch.tensor(a, dtype=torch.float64, requires_grad=True)\n",
    "    \n",
    "    # Calculate resolution for printing\n",
    "    res_calc = (200 / lat_lon_resolution[0]) * (100 / lat_lon_resolution[0])\n",
    "    print(f'\\n--- Starting Day {day+1} (2024-07-{day+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params.detach().numpy()}')\n",
    "            \n",
    "    # --- Data loading is now done *before* the loop ---\n",
    "\n",
    "    # We need to define the device (though we aren't passing it anymore)\n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model_instance = kernels.model_fitting(\n",
    "            smooth = v,\n",
    "            input_map = analysis_data_map,\n",
    "            aggregated_data = aggregated_data,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "            # device = device_str  <--- REMOVED: This was causing the TypeError\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Adjusted optimizer call based on expected return values (step size changed to step)\n",
    "    optimizer, scheduler = model_instance.optimizer_fun(\n",
    "        params, \n",
    "        lr=lr, \n",
    "        betas=(0.9, 0.8), \n",
    "        eps=1e-8, \n",
    "        step_size=step, # Using the 'step' variable here\n",
    "        gamma=gamma_par  # Using gamma_par\n",
    "    ) \n",
    "\n",
    "    # --- CRITICAL CORRECTION ---\n",
    "    # 1. We no longer need to create a separate 'instance_map'.\n",
    "    #    'model_instance' is already the correct instance.\n",
    "    # 2. We do NOT pre-calculate 'cov_map'. The optimized training loop\n",
    "    #    'run_vecc_may9' does this internally on each epoch\n",
    "    #    to ensure the gradients are correct.\n",
    "    # 3. We call 'run_vecc_may9' (the optimized loop) instead of 'run_vecc_grp9'.\n",
    "    #    This version does not take 'cov_map' as an argument.\n",
    "    \n",
    "    final_covariance_function = partial(\n",
    "            model_instance.build_cov_matrix_spatial_difference_anisotropy, \n",
    "            delta1=DELTA_LAT, \n",
    "            delta2=DELTA_LON\n",
    "        )\n",
    "\n",
    "\n",
    "    # Calling the optimized 'run_vecc_may9'\n",
    "    out, epoch_ran = model_instance.run_vecc_oct22(\n",
    "        params, \n",
    "        optimizer,\n",
    "        scheduler, \n",
    "        final_covariance_function, \n",
    "        epochs=epochs\n",
    "    )\n",
    "    # --- End Correction ---\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"Day {day+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day+1} final results: {out}\")\n",
    "\n",
    "print(\"\\n--- All Days Processed ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
