{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95f58fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating on: cpu\n",
      "1. Generating True Field via FFT...\n",
      "Exact Grid Size: 114 (Lat) x 159 (Lon) x 8 (Time) = 145008 points\n",
      "2. Formatting to Exact Regular Grid Tensors...\n",
      "\n",
      "Done.\n",
      "Aggregated Tensor Shape: torch.Size([145008, 4])\n",
      "Dictionary Size: 8 keys.\n",
      "Sample Top Rows from '2024_07_y24m07day01_hm00:53':\n",
      "tensor([[  4.9720, 132.9540, 258.5280,  21.0000],\n",
      "        [  4.9720, 132.8910, 258.6161,  21.0000],\n",
      "        [  4.9720, 132.8280, 257.9525,  21.0000],\n",
      "        [  4.9720, 132.7650, 261.4519,  21.0000],\n",
      "        [  4.9720, 132.7020, 255.2433,  21.0000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.fft\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Simulating on: {DEVICE}\")\n",
    "\n",
    "# TRUE PARAMETERS (From your result)\n",
    "# [log_phi1, log_phi2, log_phi3, log_phi4, advec_lat, advec_lon, log_nugget]\n",
    "TRUE_LOG_PARAMS = torch.tensor([\n",
    "    4.1875,   # log_phi1 \n",
    "    1.9465,   # log_phi2 \n",
    "    0.2492,   # log_phi3 \n",
    "    -3.9739,  # log_phi4 \n",
    "    0.0146,   # advec_lat\n",
    "    -0.2040,  # advec_lon\n",
    "    -0.8567   # log_nugget\n",
    "], device=DEVICE, dtype=torch.float64)\n",
    "\n",
    "# Mean Ozone\n",
    "OZONE_MEAN = 260.0\n",
    "\n",
    "# --- 2. EXACT COVARIANCE (Exponential / Matern v=0.5) ---\n",
    "def get_model_covariance_on_grid(lags_x, lags_y, lags_t, params):\n",
    "    phi1, phi2, phi3, phi4 = torch.exp(params[0]), torch.exp(params[1]), torch.exp(params[2]), torch.exp(params[3])\n",
    "    advec_lat, advec_lon = params[4], params[5]\n",
    "    sigmasq = phi1 / phi2\n",
    "\n",
    "    # Grid is (Lat, Lon, Time)\n",
    "    u_lat_eff = lags_x - advec_lat * lags_t\n",
    "    u_lon_eff = lags_y - advec_lon * lags_t\n",
    "    \n",
    "    dist_sq = (u_lat_eff.pow(2) * phi3) + (u_lon_eff.pow(2)) + (lags_t.pow(2) * phi4)\n",
    "    distance = torch.sqrt(dist_sq + 1e-8)\n",
    "    \n",
    "    return sigmasq * torch.exp(-distance * phi2)\n",
    "\n",
    "# --- 3. FFT SIMULATION ---\n",
    "def generate_exact_gems_field(lat_coords, lon_coords, t_steps, params):\n",
    "    Nx = len(lat_coords)\n",
    "    Ny = len(lon_coords)\n",
    "    Nt = t_steps\n",
    "    \n",
    "    print(f\"Exact Grid Size: {Nx} (Lat) x {Ny} (Lon) x {Nt} (Time) = {Nx*Ny*Nt} points\")\n",
    "    \n",
    "    # Grid Steps\n",
    "    dlat = float(lat_coords[1] - lat_coords[0])\n",
    "    dlon = float(lon_coords[1] - lon_coords[0])\n",
    "    \n",
    "    # Padding\n",
    "    Px, Py, Pt = 2*Nx, 2*Ny, 2*Nt\n",
    "    \n",
    "    # Lags Construction\n",
    "    Lx_len = (lat_coords[-1] - lat_coords[0]) * (Px/Nx)\n",
    "    lags_x = torch.arange(Px, device=DEVICE, dtype=torch.float64) * dlat\n",
    "    lags_x[Px//2:] -= Lx_len \n",
    "    \n",
    "    Ly_len = (lon_coords[-1] - lon_coords[0]) * (Py/Ny)\n",
    "    lags_y = torch.arange(Py, device=DEVICE, dtype=torch.float64) * dlon\n",
    "    lags_y[Py//2:] -= Ly_len\n",
    "\n",
    "    dt = 1.0 # 1 Hour step\n",
    "    Lt_len = Nt * dt * (Pt/Nt)\n",
    "    lags_t = torch.arange(Pt, device=DEVICE, dtype=torch.float64) * dt\n",
    "    lags_t[Pt//2:] -= Lt_len\n",
    "\n",
    "    # Meshgrid & Covariance\n",
    "    L_x, L_y, L_t = torch.meshgrid(lags_x, lags_y, lags_t, indexing='ij')\n",
    "    C_vals = get_model_covariance_on_grid(L_x, L_y, L_t, params)\n",
    "\n",
    "    # FFT & Convolution\n",
    "    S = torch.fft.fftn(C_vals)\n",
    "    S.real = torch.clamp(S.real, min=0)\n",
    "\n",
    "    random_phase = torch.fft.fftn(torch.randn(Px, Py, Pt, device=DEVICE, dtype=torch.float64))\n",
    "    weighted_freq = torch.sqrt(S.real) * random_phase\n",
    "    field_sim = torch.fft.ifftn(weighted_freq).real\n",
    "    \n",
    "    # Crop to Exact Grid size\n",
    "    return field_sim[:Nx, :Ny, :Nt]\n",
    "\n",
    "# --- 4. EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # A. Define Exact Coordinates (Ascending for simulation)\n",
    "    lats_sim = torch.arange(0, 5.0 + 0.001, 0.044, device=DEVICE, dtype=torch.float64)\n",
    "    lons_sim = torch.arange(123.0, 133.0 + 0.001, 0.063, device=DEVICE, dtype=torch.float64)\n",
    "    t_def = 8\n",
    "    \n",
    "    print(\"1. Generating True Field via FFT...\")\n",
    "    sim_field = generate_exact_gems_field(lats_sim, lons_sim, t_def, TRUE_LOG_PARAMS)\n",
    "    \n",
    "    # B. Formatting Output\n",
    "    print(\"2. Formatting to Exact Regular Grid Tensors...\")\n",
    "    \n",
    "    input_map = {}\n",
    "    aggregated_list = [] # List to store tensors for concatenation\n",
    "    \n",
    "    nugget_std = torch.sqrt(torch.exp(TRUE_LOG_PARAMS[6]))\n",
    "    \n",
    "    # Flip for Descending Output order (Lat Max->0, Lon Max->123)\n",
    "    lats_flip = torch.flip(lats_sim, dims=[0])\n",
    "    lons_flip = torch.flip(lons_sim, dims=[0])\n",
    "    \n",
    "    # Meshgrid for Output\n",
    "    grid_lat, grid_lon = torch.meshgrid(lats_flip, lons_flip, indexing='ij')\n",
    "    flat_lats = grid_lat.flatten()\n",
    "    flat_lons = grid_lon.flatten()\n",
    "    \n",
    "    for t in range(t_def):\n",
    "        # Flip simulation field to match Descending coordinates\n",
    "        field_t = sim_field[:, :, t] # [Lat_Asc, Lon_Asc]\n",
    "        field_t_flipped = torch.flip(field_t, dims=[0, 1]) # [Lat_Desc, Lon_Desc]\n",
    "        \n",
    "        flat_vals = field_t_flipped.flatten()\n",
    "        \n",
    "        # Add Noise + Mean\n",
    "        obs_vals = flat_vals + (torch.randn_like(flat_vals) * nugget_std) + OZONE_MEAN\n",
    "        \n",
    "        # Time Column (Start 21.0)\n",
    "        time_val = 21.0 + t\n",
    "        flat_times = torch.full_like(flat_lats, time_val)\n",
    "        \n",
    "        # Stack [Lat, Lon, Val, Time]\n",
    "        row_tensor = torch.stack([flat_lats, flat_lons, obs_vals, flat_times], dim=1)\n",
    "        \n",
    "        # Generate Key\n",
    "        key_str = f'2024_07_y24m07day01_hm{t:02d}:53'\n",
    "        \n",
    "        # Store in Input Map (CPU)\n",
    "        input_map[key_str] = row_tensor.cpu()\n",
    "        \n",
    "        # Store for Aggregated Data (CPU)\n",
    "        aggregated_list.append(row_tensor.cpu())\n",
    "\n",
    "    # Create Single Big Tensor\n",
    "    aggregated_data = torch.cat(aggregated_list, dim=0)\n",
    "\n",
    "    # C. Verification\n",
    "    first_key = list(input_map.keys())[0]\n",
    "    tensor_ex = input_map[first_key]\n",
    "    \n",
    "    print(f\"\\nDone.\")\n",
    "    print(f\"Aggregated Tensor Shape: {aggregated_data.shape}\")\n",
    "    print(f\"Dictionary Size: {len(input_map)} keys.\")\n",
    "    print(f\"Sample Top Rows from '{first_key}':\")\n",
    "    print(tensor_ex[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e359cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "import os\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy                    # clone tensor\n",
    "import time\n",
    "\n",
    "# Custom imports\n",
    "\n",
    "\n",
    "from GEMS_TCO import kernels_reparam_space_time_gpu as kernels_reparam_space_time\n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "import typer\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "from GEMS_TCO import configuration as config\n",
    "from GEMS_TCO.data_loader import load_data2, exact_location_filter\n",
    "from GEMS_TCO import debiased_whittle\n",
    "from torch.nn import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0816f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GEMS_TCO import orderings as _orderings\n",
    "\n",
    "def get_spatial_ordering(input_maps, mm_cond_number):\n",
    "    # Extract the first timestamp's data\n",
    "    key_list = list(input_maps.keys())\n",
    "    data_for_coord = input_maps[key_list[0]]\n",
    "    \n",
    "    # --- FIX START ---\n",
    "    # Check if input is Tensor, if so convert to Numpy for KDTree processing\n",
    "    if isinstance(data_for_coord, torch.Tensor):\n",
    "        data_for_coord = data_for_coord.cpu().numpy()\n",
    "    # --- FIX END ---\n",
    "\n",
    "    x1 = data_for_coord[:, 0]\n",
    "    y1 = data_for_coord[:, 1]\n",
    "    \n",
    "    # Now this works because x1, y1 are numpy arrays\n",
    "    coords1 = np.stack((x1, y1), axis=-1)\n",
    "    \n",
    "    # ... rest of your code ...\n",
    "    # ord_mm = ...\n",
    "    # nns_map = _orderings.find_nns_l2(locs=coords1, max_nn=mm_cond_number)\n",
    "    # return ord_mm, nns_map\n",
    "\n",
    "\n",
    "def get_spatial_ordering(\n",
    "        \n",
    "        input_maps: torch.Tensor,\n",
    "        mm_cond_number: int = 10\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \n",
    "        key_list = list(input_maps.keys())\n",
    "        data_for_coord = input_maps[key_list[0]]\n",
    "        \n",
    "        # --- FIX START ---\n",
    "        # Check if input is Tensor, if so convert to Numpy for KDTree processing\n",
    "        if isinstance(data_for_coord, torch.Tensor):\n",
    "            data_for_coord = data_for_coord.cpu().numpy()\n",
    "        # --- FIX END ---\n",
    "\n",
    "        x1 = data_for_coord[:, 0]\n",
    "        y1 = data_for_coord[:, 1]\n",
    "        \n",
    "        # Now this works because x1, y1 are numpy arrays\n",
    "        coords1 = np.stack((x1, y1), axis=-1)\n",
    "\n",
    "        # Calculate MaxMin ordering\n",
    "        ord_mm = _orderings.maxmin_cpp(coords1)\n",
    "        \n",
    "        # Reorder coordinates to find nearest neighbors\n",
    "        data_for_coord_reordered = data_for_coord[ord_mm]\n",
    "        coords1_reordered = np.stack(\n",
    "            (data_for_coord_reordered[:, 0], data_for_coord_reordered[:, 1]), \n",
    "            axis=-1\n",
    "        )\n",
    "        \n",
    "        # Calculate nearest neighbors map\n",
    "        nns_map = _orderings.find_nns_l2(locs=coords1_reordered, max_nn=mm_cond_number)\n",
    "        \n",
    "        return ord_mm, nns_map\n",
    "\n",
    "ord_mm, nns_map = get_spatial_ordering(input_map, mm_cond_number=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004bac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa3990cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_input_map = {}\n",
    "for key in input_map:\n",
    "    mm_input_map[key] = input_map[key][ord_mm]  # Extract only Lat and Lon columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7191bead",
   "metadata": {},
   "source": [
    "# Fit vecchia max min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83dc5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "========================================\n",
      "--- Starting Processing for Day 2 (2024-07-2) ---\n",
      "========================================\n",
      "\n",
      "--- Starting Phase 2: Vecchia Optimization (Day 2) ---\n",
      "Pre-computing Batched Tensors (Padding Strategy)... Done. Heads: 0, Batched Tails: 145008\n",
      "--- Starting Batched L-BFGS Optimization (GPU) ---\n",
      "Warning: GPU Cholesky failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     82\u001b[39m start_time = time.time()\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# --- ðŸ’¥ Call the Batched Fit Method ---\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# REMOVED: model_instance.matern_cov_aniso_STABLE_log_reparam\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m out, steps_ran = \u001b[43mmodel_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_vecc_lbfgs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer_vecc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# covariance_function argument is GONE\u001b[39;49;00m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLBFGS_MAX_STEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_tol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-7\u001b[39;49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m end_time = time.time()\n\u001b[32m     95\u001b[39m epoch_time = end_time - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/kernels_reparam_space_time_gpu.py:434\u001b[39m, in \u001b[36mfit_vecchia_lbfgs.fit_vecc_lbfgs\u001b[39m\u001b[34m(self, params_list, optimizer, max_steps, grad_tol)\u001b[39m\n\u001b[32m    431\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m     loss = \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    436\u001b[39m     \u001b[38;5;66;03m# Monitoring\u001b[39;00m\n\u001b[32m    437\u001b[39m     max_abs_grad = \u001b[32m0.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gems_gpu/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    482\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    483\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    484\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    485\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gems_gpu/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gems_gpu/lib/python3.12/site-packages/torch/optim/lbfgs.py:457\u001b[39m, in \u001b[36mLBFGS.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_iter != max_iter:\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# re-evaluate function only if not in last iteration\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# the reason we do this: in a stochastic setting,\u001b[39;00m\n\u001b[32m    455\u001b[39m     \u001b[38;5;66;03m# no use to re-evaluate that function here\u001b[39;00m\n\u001b[32m    456\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m         loss = \u001b[38;5;28mfloat\u001b[39m(\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    458\u001b[39m     flat_grad = \u001b[38;5;28mself\u001b[39m._gather_flat_grad()\n\u001b[32m    459\u001b[39m     opt_cond = flat_grad.abs().max() <= tolerance_grad\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gems_gpu/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/kernels_reparam_space_time_gpu.py:430\u001b[39m, in \u001b[36mfit_vecchia_lbfgs.fit_vecc_lbfgs.<locals>.closure\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;66;03m# Batched Likelihood Calculation\u001b[39;00m\n\u001b[32m    429\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.vecchia_batched_likelihood(params)\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gems_gpu/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gems_gpu/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/gems_gpu/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "v=0.5\n",
    "mm_cond_number= 8\n",
    "nheads = 0\n",
    "lr = 0.1\n",
    "patience, factor = 5, 0.5\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Global L-BFGS Settings\n",
    "LBFGS_LR = 1.0\n",
    "LBFGS_MAX_STEPS = 10      # 10 to 20  \n",
    "LBFGS_HISTORY_SIZE = 100   \n",
    "LBFGS_MAX_EVAL = 100       # line search from 50 to 80\n",
    "       \n",
    "\n",
    "DELTA_LAT, DELTA_LON = 0.044, 0.063 \n",
    "LAT_COL, LON_COL, VAL_COL, TIME_COL = 0, 1, 2, 3\n",
    "\n",
    "days_list = [1]\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "\n",
    "for day_idx in days_list:  # 0-based\n",
    "\n",
    "    print(f'\\n{\"=\"*40}')\n",
    "    print(f'--- Starting Processing for Day {day_idx+1} (2024-07-{day_idx+1}) ---')\n",
    "    print(f'{\"=\"*40}')\n",
    "\n",
    "\n",
    "\n",
    "    if isinstance(aggregated_data, torch.Tensor):\n",
    "        aggregated_data = aggregated_data.to(DEVICE)\n",
    "\n",
    "\n",
    "    init_sigmasq   = 13.059\n",
    "    init_range_lat = 0.154 \n",
    "    init_range_lon = 0.195\n",
    "    init_advec_lat = 0.0218\n",
    "    init_range_time = 1.0\n",
    "    init_advec_lon = -0.1689\n",
    "    init_nugget    = 0.247\n",
    "    \n",
    "    # Map model parameters to the 'phi' reparameterization\n",
    "    init_phi2 = 1.0 / init_range_lon                # 1/range_lon\n",
    "    init_phi1 = init_sigmasq * init_phi2            # sigmasq / range_lon\n",
    "    init_phi3 = (init_range_lon / init_range_lat)**2  # (range_lon / range_lat)^2\n",
    "    init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "    # Create Initial Parameters (Float64, Requires Grad)\n",
    "    initial_vals = [np.log(init_phi1), np.log(init_phi2), np.log(init_phi3), \n",
    "                    np.log(init_phi4), init_advec_lat, init_advec_lon, np.log(init_nugget)]\n",
    "\n",
    "\n",
    "    params_list = [\n",
    "        torch.tensor([val], requires_grad=True, dtype=torch.float64, device=DEVICE)\n",
    "        for val in initial_vals\n",
    "    ]\n",
    "\n",
    "\n",
    "    # --- ðŸ’¥ Instantiate the GPU Batched Class ---\n",
    "    # NOTE: Ensure fit_vecchia_lbfgs is the NEW class we defined\n",
    "    model_instance = kernels_reparam_space_time.fit_vecchia_lbfgs(\n",
    "            smooth = v,\n",
    "            input_map = mm_input_map,\n",
    "            aggregated_data = aggregated_data,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "\n",
    "    # --- ðŸ’¥ Set L-BFGS Optimizer ---\n",
    "    optimizer_vecc = model_instance.set_optimizer(\n",
    "                params_list,     \n",
    "                lr=LBFGS_LR,            \n",
    "                max_iter=LBFGS_MAX_EVAL,        \n",
    "                history_size=LBFGS_HISTORY_SIZE \n",
    "            )\n",
    "\n",
    "    print(f\"\\n--- Starting Phase 2: Vecchia Optimization (Day {day_idx+1}) ---\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- ðŸ’¥ Call the Batched Fit Method ---\n",
    "    # REMOVED: model_instance.matern_cov_aniso_STABLE_log_reparam\n",
    "    out, steps_ran = model_instance.fit_vecc_lbfgs(\n",
    "            params_list,\n",
    "            optimizer_vecc,\n",
    "            # covariance_function argument is GONE\n",
    "            max_steps=LBFGS_MAX_STEPS, \n",
    "            grad_tol=1e-7\n",
    "        )\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Vecchia Optimization finished in {epoch_time:.2f}s. Results: {out}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gems_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
