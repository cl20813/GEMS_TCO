{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d975f044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy                    # clone tensor\n",
    "import time\n",
    "\n",
    "# Custom imports\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels\n",
    "from GEMS_TCO import data_preprocess \n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "import typer\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO.data_loader import load_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "00b39304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Generation ---\n",
      "--- Data Generation Complete ---\n",
      "\n",
      "--- Starting MLE Optimization (SciPy - Estimating SIGMA2 Only) ---\n",
      "\n",
      "Original Parameters (Used for Generation): Range (a)=2.0, Variance (ÏƒÂ²)=42, Nugget (Î·Â²)=0.0\n",
      "\n",
      "Fitted Parameter (SciPy MLE):\n",
      "  * Variance (ÏƒÂ²): 41.566 (Target: 42)\n",
      "  * Range (a): 2.0 (FIXED)\n",
      "  * Nugget (Î·Â²): 0.0 (FIXED)\n",
      "  * Optimization Success: True\n",
      "  * Final -LL Value: 2046.50\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from scipy.spatial.distance import cdist\n",
    "from numpy.linalg import det, inv\n",
    "\n",
    "# --- 0. Global Parameters and Utility Functions ---\n",
    "N_SPATIAL_POINTS = 1120\n",
    "N_DAYS = 31\n",
    "N_HOURS_PER_DAY = 8\n",
    "N_FEATURES = 4\n",
    "GRID_X = 40  \n",
    "GRID_Y = 28  \n",
    "LAT_MIN, LAT_MAX = 0, 25\n",
    "LON_MIN, LON_MAX = 113, 133\n",
    "BASE_DATE = '2024_07_y24m07day' \n",
    "\n",
    "# Exponential Kernel Parameters (Targets)\n",
    "SIGMA2 = 42  # TARGET Variance (Parameter to be estimated)\n",
    "RANGE_A = 2.0  # FIXED Range\n",
    "NUGGET = 0.0   # FIXED Nugget (Corrected from integer 0 to float 0.0)\n",
    "\n",
    "# Ozone Value Parameters\n",
    "OZONE_MEAN = 240.0\n",
    "\n",
    "def exponential_covariance(distances, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (Matern nu=0.5).\"\"\"\n",
    "    # Cov(h) = sigma^2 * exp(-h/a)\n",
    "    cov = sigma2 * np.exp(-distances / a)\n",
    "    # Add nugget effect to the diagonal (where distance h=0)\n",
    "    if distances.shape[0] == distances.shape[1]:\n",
    "        cov[np.diag_indices_from(distances)] += nugget\n",
    "    return cov\n",
    "\n",
    "# ðŸ’¡ CHANGE 1: Function now accepts only one parameter (sigma2)\n",
    "def neg_log_likelihood_sigma2(params_sigma2, distances, z, fixed_a, fixed_nugget):\n",
    "    \"\"\"\n",
    "    Calculates the negative log-likelihood for the single parameter (sigma2).\n",
    "    \"\"\"\n",
    "    sigma2 = params_sigma2[0]\n",
    "    a = fixed_a\n",
    "    nugget = fixed_nugget\n",
    "    N = z.size\n",
    "    \n",
    "    # Enforce bounds for the single parameter\n",
    "    if sigma2 <= 1e-6:\n",
    "        return 1e15 \n",
    "    \n",
    "    C = exponential_covariance(distances, sigma2, a, nugget)\n",
    "    C = C + np.eye(N) * 1e-6\n",
    "\n",
    "    try:\n",
    "        sign, log_det = np.linalg.slogdet(C)\n",
    "        if sign <= 0:\n",
    "             return 1e15 \n",
    "        \n",
    "        C_inv = inv(C)\n",
    "        quad_term = z.T @ C_inv @ z\n",
    "        \n",
    "        neg_LL = 0.5 * log_det + 0.5 * quad_term\n",
    "        return neg_LL\n",
    "        \n",
    "    except np.linalg.LinAlgError:\n",
    "        return 1e15\n",
    "\n",
    "def generate_ozone_data_map(coords, sigma2, a, nugget, mean, time_index):\n",
    "    \"\"\"Generates one hour of spatially correlated Gaussian data.\"\"\"\n",
    "    n_points = coords.shape[0]\n",
    "    distances = cdist(coords, coords, metric='euclidean')\n",
    "    Cov = exponential_covariance(distances, sigma2, a, nugget)\n",
    "    Cov = (Cov + Cov.T) / 2\n",
    "    \n",
    "    try:\n",
    "        L = np.linalg.cholesky(Cov + 1e-6 * np.eye(n_points))\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.zeros((n_points, N_FEATURES))\n",
    "\n",
    "    W = np.random.normal(0, 1, size=(n_points, 1))\n",
    "    Z_correlated = L @ W\n",
    "    \n",
    "    ozone_values = mean + Z_correlated\n",
    "    \n",
    "    data_np = np.zeros((n_points, N_FEATURES))\n",
    "    data_np[:, 0:1] = ozone_values             \n",
    "    data_np[:, 1] = coords[:, 1] * 10 + 2      \n",
    "    data_np[:, 2] = coords[:, 0] * 40 + 250    \n",
    "    data_np[:, 3] = time_index                 \n",
    "\n",
    "    return data_np\n",
    "\n",
    "# --- 1. Data Generation Execution ---\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "\n",
    "print(\"--- Starting Data Generation ---\")\n",
    "lat_coords = np.linspace(LAT_MIN, LAT_MAX, GRID_Y)\n",
    "lon_coords = np.linspace(LON_MIN, LON_MAX, GRID_X)\n",
    "coords_latlon = np.array([[lat, lon] for lat in lat_coords for lon in lon_coords])\n",
    "\n",
    "for i in range(N_DAYS):\n",
    "    cur_map = {}\n",
    "    cur_df_list = []\n",
    "    day_str = str(i+1).zfill(2)\n",
    "    \n",
    "    for j in range(N_HOURS_PER_DAY):\n",
    "        time_index = 21.0 + j\n",
    "        \n",
    "        data_np = generate_ozone_data_map(\n",
    "            coords_latlon, SIGMA2, RANGE_A, NUGGET, OZONE_MEAN, time_index\n",
    "        )\n",
    "        data_tensor = torch.tensor(data_np, dtype=torch.float)\n",
    "        \n",
    "        hour_str = str(j).zfill(2) \n",
    "        key = f'{BASE_DATE}{day_str}_hm{hour_str}:53'\n",
    "        \n",
    "        cur_map[key] = data_tensor\n",
    "        cur_df_list.append(data_tensor)\n",
    "\n",
    "    cur_df_aggregated = torch.cat(cur_df_list, dim=0)\n",
    "    df_day_aggregated_list.append(cur_df_aggregated)\n",
    "    df_day_map_list.append(cur_map)\n",
    "\n",
    "print(\"--- Data Generation Complete ---\")\n",
    "\n",
    "\n",
    "# --- 2. MLE Execution (Optimization for SIGMA2 only) ---\n",
    "\n",
    "# Extract data for fitting\n",
    "data_to_fit = df_day_aggregated_list[0][:N_SPATIAL_POINTS, :] \n",
    "z_data = data_to_fit[:, 0].numpy()\n",
    "coordinates = coords_latlon[:, [1, 0]] \n",
    "\n",
    "distances = cdist(coordinates, coordinates, metric='euclidean')\n",
    "z_centered = z_data - np.mean(z_data)\n",
    "\n",
    "# ðŸ’¡ CHANGE 2: Initial Guess (p0) contains only SIGMA2\n",
    "# We start slightly off the target value of 31.0 for a true optimization test.\n",
    "p0 = np.array([SIGMA2 + 5]) \n",
    "\n",
    "print(\"\\n--- Starting MLE Optimization (SciPy - Estimating SIGMA2 Only) ---\")\n",
    "\n",
    "# ðŸ’¡ CHANGE 3: Bounds contain only the bounds for SIGMA2\n",
    "bounds = opt.Bounds([0.1], [100.0])\n",
    "\n",
    "# Perform the minimization. Note the new function name and extra arguments (fixed_a, fixed_nugget).\n",
    "result = opt.minimize(\n",
    "    neg_log_likelihood_sigma2, \n",
    "    p0, \n",
    "    args=(distances, z_centered, RANGE_A, NUGGET), # Pass fixed parameters as args\n",
    "    method='L-BFGS-B', \n",
    "    bounds=bounds,\n",
    "    options={'disp': True, 'ftol': 1e-6}\n",
    ")\n",
    "\n",
    "\n",
    "# --- 3. Display Results ---\n",
    "\n",
    "if result.success:\n",
    "    fitted_sigma2 = result.x[0]\n",
    "\n",
    "    print(f\"\\nOriginal Parameters (Used for Generation): Range (a)={RANGE_A}, Variance (ÏƒÂ²)={SIGMA2}, Nugget (Î·Â²)={NUGGET}\")\n",
    "    print(\"\\nFitted Parameter (SciPy MLE):\")\n",
    "    print(f\"  * Variance (ÏƒÂ²): {fitted_sigma2:.3f} (Target: {SIGMA2})\")\n",
    "    print(f\"  * Range (a): {RANGE_A} (FIXED)\")\n",
    "    print(f\"  * Nugget (Î·Â²): {NUGGET} (FIXED)\")\n",
    "    print(f\"  * Optimization Success: {result.success}\")\n",
    "    print(f\"  * Final -LL Value: {result.fun:.2f}\")\n",
    "else:\n",
    "    print(\"\\nOptimization failed.\")\n",
    "    print(f\"Reason: {result.message}\")\n",
    "\n",
    "print(\"-------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ac47a6",
   "metadata": {},
   "source": [
    "# LBFGS VS ADAMS FOR SIGMA^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8bc3e5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Generation ---\n",
      "--- Data Generation Complete ---\n",
      "\n",
      "--- A. Starting MLE Optimization (PyTorch L-BFGS) ---\n",
      "LBFGS Step 5/50, NLL: 1706.40, Sigma2: 28.792, Grad: -0.0000\n",
      "LBFGS Step 10/50, NLL: 1706.40, Sigma2: 28.792, Grad: -0.0000\n",
      "LBFGS Step 15/50, NLL: 1706.40, Sigma2: 28.792, Grad: -0.0000\n",
      "LBFGS Step 20/50, NLL: 1706.40, Sigma2: 28.792, Grad: -0.0000\n",
      "LBFGS Step 25/50, NLL: 1706.40, Sigma2: 28.792, Grad: -0.0000\n",
      "LBFGS Step 30/50, NLL: 1706.40, Sigma2: 28.792, Grad: -0.0000\n",
      "LBFGS Step 35/50, NLL: 1706.40, Sigma2: 28.792, Grad: -0.0000\n",
      "LBFGS Step 40/50, NLL: 1706.40, Sigma2: 28.792, Grad: -0.0000\n",
      "LBFGS Step 45/50, NLL: 1706.40, Sigma2: 28.792, Grad: -0.0000\n",
      "LBFGS Step 50/50, NLL: 1706.40, Sigma2: 28.792, Grad: -0.0000\n",
      "\n",
      "--- B. Starting MLE Optimization (PyTorch Adam) ---\n",
      "Adam Epoch 50/500, NLL: 1706.66, Sigma2: 30.132, Grad: 3.9830\n",
      "Adam Epoch 100/500, NLL: 1706.40, Sigma2: 28.767, Grad: -0.0630\n",
      "Adam Epoch 150/500, NLL: 1706.40, Sigma2: 28.790, Grad: -0.0059\n",
      "Adam Epoch 200/500, NLL: 1706.40, Sigma2: 28.792, Grad: 0.0004\n",
      "Adam Epoch 250/500, NLL: 1706.40, Sigma2: 28.792, Grad: -0.0001\n",
      "Adam Epoch 300/500, NLL: 1706.40, Sigma2: 28.792, Grad: -0.0000\n",
      "Adam Epoch 350/500, NLL: 1706.40, Sigma2: 28.792, Grad: -0.0000\n",
      "Adam Epoch 400/500, NLL: 1706.40, Sigma2: 28.792, Grad: -0.0000\n",
      "Adam Epoch 450/500, NLL: 1706.40, Sigma2: 28.792, Grad: -0.0000\n",
      "Adam Epoch 500/500, NLL: 1706.40, Sigma2: 28.792, Grad: -0.0000\n",
      "\n",
      "==================================================\n",
      "TARGET PARAMETERS: Variance (ÏƒÂ²)=30.0, Range (a)=1, Nugget (Î·Â²)=3.0\n",
      "==================================================\n",
      "âœ¨ PyTorch L-BFGS Results:\n",
      "  * Fitted Variance (ÏƒÂ²): 28.792 (Target: 30.0)\n",
      "  * Final -LL Value: 1706.40\n",
      "  * Optimization Steps: 50 steps\n",
      "\n",
      "ðŸš€ PyTorch Adam Results:\n",
      "  * Fitted Variance (ÏƒÂ²): 28.792 (Target: 30.0)\n",
      "  * Final -LL Value: 1706.40\n",
      "  * Optimization Steps: 500 epochs\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from scipy.spatial.distance import cdist\n",
    "# Removed 'scipy.optimize as opt' and 'from numpy.linalg import inv'\n",
    "\n",
    "# --- 0. Global Parameters and Utility Functions ---\n",
    "N_SPATIAL_POINTS = 1000\n",
    "N_DAYS = 31\n",
    "N_HOURS_PER_DAY = 8\n",
    "N_FEATURES = 4\n",
    "GRID_X = 50 \n",
    "GRID_Y = 20  \n",
    "LAT_MIN, LAT_MAX = 0, 5\n",
    "LON_MIN, LON_MAX = 113, 123\n",
    "BASE_DATE = '2024_07_y24m07day' \n",
    "\n",
    "# Exponential Kernel Parameters (Targets)\n",
    "SIGMA2_TRUE = 30.0 # TARGET Variance\n",
    "RANGE_A = 1   # FIXED Range\n",
    "NUGGET = 3.0     # FIXED Nugget\n",
    "\n",
    "# Optimization Setup\n",
    "ADAM_ITERATIONS = 500\n",
    "ADAM_LEARNING_RATE = 0.01\n",
    "\n",
    "# L-BFGS Setup\n",
    "LBFGS_MAX_STEPS = 50 \n",
    "# ðŸ’¡ CORRECTED PARAMETER NAME: max_eval controls function evaluations including line search\n",
    "LBFGS_MAX_EVAL = 50 \n",
    "\n",
    "OZONE_MEAN = 240.0\n",
    "\n",
    "# --- COVARIANCE FUNCTIONS ---\n",
    "\n",
    "def exponential_covariance_numpy(distances, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (NumPy for Generation).\"\"\"\n",
    "    # Cov(h) = sigma^2 * exp(-h/a)\n",
    "    cov = sigma2 * np.exp(-distances / a)\n",
    "    if distances.shape[0] == distances.shape[1]:\n",
    "        cov[np.diag_indices_from(distances)] += (nugget + 1e-6)\n",
    "    return cov\n",
    "\n",
    "def exponential_covariance_torch(distances_torch, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (PyTorch for Optimization).\"\"\"\n",
    "    cov = sigma2 * torch.exp(-distances_torch / a)\n",
    "    \n",
    "    # Add nugget effect + jitter to the diagonal\n",
    "    if distances_torch.shape[0] == distances_torch.shape[1]:\n",
    "        jitter = 1e-6 \n",
    "        diag_mask = torch.eye(cov.shape[0], device=cov.device)\n",
    "        cov = cov + diag_mask * (nugget + jitter)\n",
    "    return cov\n",
    "\n",
    "# --- SHARED NLL Function for PyTorch (Adam and LBFGS) ---\n",
    "def neg_log_likelihood_torch(raw_params_sigma2, distances_torch, z_centered_torch, fixed_a, fixed_nugget):\n",
    "    \"\"\"Calculates -LL for PyTorch (using square reparameterization).\"\"\"\n",
    "    # Reparameterization: sigma2 = raw_params_sigma2^2\n",
    "    sigma2 = raw_params_sigma2.pow(2).squeeze()\n",
    "    \n",
    "    C = exponential_covariance_torch(distances_torch, sigma2, fixed_a, fixed_nugget)\n",
    "    \n",
    "    try:\n",
    "        # Use PyTorch linear algebra functions: Cholesky decomposition\n",
    "        L = torch.linalg.cholesky(C)\n",
    "        log_det = 2.0 * torch.sum(torch.log(torch.diag(L)))\n",
    "        alpha = torch.linalg.solve(C, z_centered_torch.unsqueeze(1))\n",
    "        quad_term = z_centered_torch.unsqueeze(0) @ alpha\n",
    "        neg_LL = 0.5 * log_det + 0.5 * quad_term.squeeze()\n",
    "        return neg_LL\n",
    "    except RuntimeError:\n",
    "        return torch.tensor(1e15, device=C.device)\n",
    "\n",
    "\n",
    "# --- Data Generation Function ---\n",
    "def generate_ozone_data_map(coords, sigma2, a, nugget, mean, time_index):\n",
    "    n_points = coords.shape[0]\n",
    "    distances = cdist(coords, coords, metric='euclidean')\n",
    "    Cov = exponential_covariance_numpy(distances, sigma2, a, nugget) \n",
    "    Cov = (Cov + Cov.T) / 2\n",
    "    \n",
    "    try:\n",
    "        L = np.linalg.cholesky(Cov)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.zeros((n_points, N_FEATURES))\n",
    "\n",
    "    W = np.random.normal(0, 1, size=(n_points, 1))\n",
    "    Z_correlated = L @ W\n",
    "    ozone_values = mean + Z_correlated\n",
    "    \n",
    "    data_np = np.zeros((n_points, N_FEATURES))\n",
    "    data_np[:, 0:1] = ozone_values             \n",
    "    data_np[:, 1] = coords[:, 1] * 10 + 2      \n",
    "    data_np[:, 2] = coords[:, 0] * 40 + 250    \n",
    "    data_np[:, 3] = time_index                 \n",
    "    return data_np\n",
    "\n",
    "\n",
    "# --- 1. Data Generation Execution ---\n",
    "df_day_aggregated_list = []\n",
    "print(\"--- Starting Data Generation ---\")\n",
    "lat_coords = np.linspace(LAT_MIN, LAT_MAX, GRID_Y)\n",
    "lon_coords = np.linspace(LON_MIN, LON_MAX, GRID_X)\n",
    "coords_latlon = np.array([[lat, lon] for lat in lat_coords for lon in lon_coords])\n",
    "\n",
    "# Generate only one hour of data for fitting the spatial model\n",
    "data_np = generate_ozone_data_map(\n",
    "    coords_latlon, SIGMA2_TRUE, RANGE_A, NUGGET, OZONE_MEAN, 21.0\n",
    ")\n",
    "df_day_aggregated_list.append(torch.tensor(data_np, dtype=torch.float))\n",
    "\n",
    "print(\"--- Data Generation Complete ---\")\n",
    "\n",
    "# --- 2. Data Preparation ---\n",
    "data_to_fit = df_day_aggregated_list[0][:N_SPATIAL_POINTS, :] \n",
    "z_data = data_to_fit[:, 0].numpy()\n",
    "coordinates = coords_latlon[:, [1, 0]] \n",
    "distances_np = cdist(coordinates, coordinates, metric='euclidean')\n",
    "z_centered_np = z_data - np.mean(z_data)\n",
    "\n",
    "# Convert to Torch Tensors\n",
    "distances_torch = torch.tensor(distances_np, dtype=torch.float)\n",
    "z_centered_torch = torch.tensor(z_centered_np, dtype=torch.float)\n",
    "\n",
    "# --- Initial Parameter Setup (Shared) ---\n",
    "# Initial guess for raw_sigma2: sqrt(target + 5) = sqrt(47)\n",
    "raw_sigma2_start = np.sqrt(SIGMA2_TRUE + 5) \n",
    "\n",
    "# ----------------------------------------------------\n",
    "# A. Optimization with L-BFGS (PyTorch)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Reset parameters for LBFGS\n",
    "raw_params_lbfgs = torch.tensor(\n",
    "    [raw_sigma2_start], \n",
    "    dtype=torch.float, \n",
    "    requires_grad=True\n",
    ")\n",
    "\n",
    "optimizer_lbfgs = optim.LBFGS(\n",
    "    [raw_params_lbfgs], \n",
    "    lr=1.0, \n",
    "    max_iter=LBFGS_MAX_STEPS,\n",
    "    max_eval=LBFGS_MAX_EVAL # ðŸ’¡ CORRECTED: Use max_eval instead of max_ls\n",
    ")\n",
    "\n",
    "final_loss_lbfgs = torch.tensor(0.0)\n",
    "print(\"\\n--- A. Starting MLE Optimization (PyTorch L-BFGS) ---\")\n",
    "\n",
    "# L-BFGS requires a \"closure\" function\n",
    "def closure_lbfgs():\n",
    "    optimizer_lbfgs.zero_grad()\n",
    "    loss = neg_log_likelihood_torch(raw_params_lbfgs, distances_torch, z_centered_torch, RANGE_A, NUGGET)\n",
    "    if not torch.isinf(loss) and not torch.isnan(loss):\n",
    "        loss.backward()\n",
    "    return loss\n",
    "\n",
    "# L-BFGS Optimization Loop\n",
    "for step in range(LBFGS_MAX_STEPS):\n",
    "    loss = optimizer_lbfgs.step(closure_lbfgs)\n",
    "    final_loss_lbfgs = loss\n",
    "    \n",
    "    if (step + 1) % 5 == 0: \n",
    "        current_sigma2 = raw_params_lbfgs.pow(2).item()\n",
    "        grad_value = raw_params_lbfgs.grad.item() if raw_params_lbfgs.grad is not None else 0.0\n",
    "        print(f\"LBFGS Step {step + 1}/{LBFGS_MAX_STEPS}, NLL: {loss.item():.2f}, Sigma2: {current_sigma2:.3f}, Grad: {grad_value:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# B. Optimization with Adam (PyTorch)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Reset parameters for Adam (Use the same start point)\n",
    "raw_params_adam = torch.tensor(\n",
    "    [raw_sigma2_start], \n",
    "    dtype=torch.float, \n",
    "    requires_grad=True\n",
    ")\n",
    "\n",
    "optimizer_adam = optim.Adam(\n",
    "    [raw_params_adam], \n",
    "    lr=ADAM_LEARNING_RATE\n",
    ")\n",
    "\n",
    "final_loss_adam = torch.tensor(0.0)\n",
    "print(f\"\\n--- B. Starting MLE Optimization (PyTorch Adam) ---\")\n",
    "\n",
    "# Adam Optimization Loop\n",
    "for epoch in range(ADAM_ITERATIONS):\n",
    "    optimizer_adam.zero_grad()\n",
    "    \n",
    "    loss = neg_log_likelihood_torch(raw_params_adam, distances_torch, z_centered_torch, RANGE_A, NUGGET)\n",
    "    \n",
    "    if torch.isinf(loss) or torch.isnan(loss):\n",
    "        loss = torch.tensor(1e15, device=loss.device)\n",
    "        break\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer_adam.step()\n",
    "    final_loss_adam = loss\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0: \n",
    "        current_sigma2 = raw_params_adam.pow(2).item()\n",
    "        grad_value = raw_params_adam.grad.item() if raw_params_adam.grad is not None else 0.0\n",
    "        print(f\"Adam Epoch {epoch + 1}/{ADAM_ITERATIONS}, NLL: {loss.item():.2f}, Sigma2: {current_sigma2:.3f}, Grad: {grad_value:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. Display Results\n",
    "# ----------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"TARGET PARAMETERS: Variance (ÏƒÂ²)={SIGMA2_TRUE}, Range (a)={RANGE_A}, Nugget (Î·Â²)={NUGGET}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# L-BFGS Results\n",
    "fitted_sigma2_lbfgs = raw_params_lbfgs.pow(2).detach().numpy().item()\n",
    "print(\"âœ¨ PyTorch L-BFGS Results:\")\n",
    "print(f\"  * Fitted Variance (ÏƒÂ²): {fitted_sigma2_lbfgs:.3f} (Target: {SIGMA2_TRUE})\")\n",
    "print(f\"  * Final -LL Value: {final_loss_lbfgs.item():.2f}\")\n",
    "print(f\"  * Optimization Steps: {LBFGS_MAX_STEPS} steps\")\n",
    "\n",
    "# Adam Results\n",
    "fitted_sigma2_adam = raw_params_adam.pow(2).detach().numpy().item()\n",
    "print(\"\\nðŸš€ PyTorch Adam Results:\")\n",
    "print(f\"  * Fitted Variance (ÏƒÂ²): {fitted_sigma2_adam:.3f} (Target: {SIGMA2_TRUE})\")\n",
    "print(f\"  * Final -LL Value: {final_loss_adam.item():.2f}\")\n",
    "print(f\"  * Optimization Steps: {ADAM_ITERATIONS} epochs\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de967d91",
   "metadata": {},
   "source": [
    "# LBFGS VS ADAMS FOR RANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "65123cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Generation ---\n",
      "--- Data Generation Complete ---\n",
      "\n",
      "--- A. Starting MLE Optimization (PyTorch L-BFGS) ---\n",
      "Fixed Variance (ÏƒÂ²): 42.0\n",
      "LBFGS Step 5/50, NLL: 1484.49, Range_a: 1.955, Grad: 0.0000\n",
      "LBFGS Step 10/50, NLL: 1484.49, Range_a: 1.955, Grad: 0.0000\n",
      "LBFGS Step 15/50, NLL: 1484.49, Range_a: 1.955, Grad: 0.0000\n",
      "LBFGS Step 20/50, NLL: 1484.49, Range_a: 1.955, Grad: 0.0000\n",
      "LBFGS Step 25/50, NLL: 1484.49, Range_a: 1.955, Grad: 0.0000\n",
      "LBFGS Step 30/50, NLL: 1484.49, Range_a: 1.955, Grad: 0.0000\n",
      "LBFGS Step 35/50, NLL: 1484.49, Range_a: 1.955, Grad: 0.0000\n",
      "LBFGS Step 40/50, NLL: 1484.49, Range_a: 1.955, Grad: 0.0000\n",
      "LBFGS Step 45/50, NLL: 1484.49, Range_a: 1.955, Grad: 0.0000\n",
      "LBFGS Step 50/50, NLL: 1484.49, Range_a: 1.955, Grad: 0.0000\n",
      "\n",
      "--- B. Starting MLE Optimization (PyTorch Adam) ---\n",
      "Fixed Variance (ÏƒÂ²): 42.0\n",
      "Adam Epoch 50/500, NLL: 1500.56, Range_a: 2.448, Grad: 182.9287\n",
      "Adam Epoch 100/500, NLL: 1484.51, Range_a: 1.971, Grad: 7.2815\n",
      "Adam Epoch 150/500, NLL: 1484.49, Range_a: 1.952, Grad: -1.1025\n",
      "Adam Epoch 200/500, NLL: 1484.48, Range_a: 1.955, Grad: 0.0795\n",
      "Adam Epoch 250/500, NLL: 1484.48, Range_a: 1.955, Grad: -0.0080\n",
      "Adam Epoch 300/500, NLL: 1484.49, Range_a: 1.955, Grad: 0.0008\n",
      "Adam Epoch 350/500, NLL: 1484.49, Range_a: 1.955, Grad: 0.0003\n",
      "Adam Epoch 400/500, NLL: 1484.49, Range_a: 1.955, Grad: -0.0011\n",
      "Adam Epoch 450/500, NLL: 1484.49, Range_a: 1.955, Grad: 0.0012\n",
      "Adam Epoch 500/500, NLL: 1484.49, Range_a: 1.955, Grad: 0.0012\n",
      "\n",
      "==================================================\n",
      "TARGET PARAMETERS: Variance (ÏƒÂ²)=42.0, Range (a)=2.0, Nugget (Î·Â²)=0.0\n",
      "**FIXED PARAMETER: Variance (ÏƒÂ²)=42.0**\n",
      "==================================================\n",
      "âœ¨ PyTorch L-BFGS Results:\n",
      "  * Fitted Range (a): 1.955 (Target: 2.0)\n",
      "  * Final -LL Value: 1484.49\n",
      "  * Optimization Steps: 50 steps\n",
      "\n",
      "ðŸš€ PyTorch Adam Results:\n",
      "  * Fitted Range (a): 1.955 (Target: 2.0)\n",
      "  * Final -LL Value: 1484.49\n",
      "  * Optimization Steps: 500 epochs\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from scipy.spatial.distance import cdist\n",
    "# Removed 'scipy.optimize as opt' and 'from numpy.linalg import inv'\n",
    "\n",
    "# --- 0. Global Parameters and Utility Functions ---\n",
    "N_SPATIAL_POINTS = 1120\n",
    "N_DAYS = 31\n",
    "N_HOURS_PER_DAY = 8\n",
    "N_FEATURES = 4\n",
    "GRID_X = 40  \n",
    "GRID_Y = 28  \n",
    "LAT_MIN, LAT_MAX = 0, 5\n",
    "LON_MIN, LON_MAX = 113, 123\n",
    "BASE_DATE = '2024_07_y24m07day' \n",
    "\n",
    "# Exponential Kernel Parameters (Targets)\n",
    "SIGMA2_TRUE = 42.0 # FIXED Variance (Used as target and fixed value)\n",
    "RANGE_A_TRUE = 2.0    # TARGET Range\n",
    "NUGGET = 0.0     # FIXED Nugget\n",
    "\n",
    "# Optimization Setup\n",
    "ADAM_ITERATIONS = 500\n",
    "ADAM_LEARNING_RATE = 0.01\n",
    "\n",
    "# L-BFGS Setup\n",
    "LBFGS_MAX_STEPS = 50 \n",
    "LBFGS_MAX_EVAL = 50 \n",
    "\n",
    "OZONE_MEAN = 240.0\n",
    "# ðŸ’¡ FIXED PARAMETER: Use the true value for the fixed parameter\n",
    "SIGMA2_FIXED = SIGMA2_TRUE \n",
    "\n",
    "# --- COVARIANCE FUNCTIONS ---\n",
    "\n",
    "def exponential_covariance_numpy(distances, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (NumPy for Generation).\"\"\"\n",
    "    # Cov(h) = sigma^2 * exp(-h/a)\n",
    "    cov = sigma2 * np.exp(-distances / a)\n",
    "    if distances.shape[0] == distances.shape[1]:\n",
    "        cov[np.diag_indices_from(distances)] += (nugget + 1e-6)\n",
    "    return cov\n",
    "\n",
    "def exponential_covariance_torch(distances_torch, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (PyTorch for Optimization).\"\"\"\n",
    "    cov = sigma2 * torch.exp(-distances_torch / a)\n",
    "    \n",
    "    # Add nugget effect + jitter to the diagonal\n",
    "    if distances_torch.shape[0] == distances_torch.shape[1]:\n",
    "        jitter = 1e-6 \n",
    "        diag_mask = torch.eye(cov.shape[0], device=cov.device)\n",
    "        cov = cov + diag_mask * (nugget + jitter)\n",
    "    return cov\n",
    "\n",
    "# --- SHARED NLL Function for PyTorch (Adam and LBFGS) ---\n",
    "# ðŸ’¡ MODIFIED: params is now raw_params_range_a. fixed_sigma2 replaces raw_params_sigma2\n",
    "def neg_log_likelihood_torch(raw_params_range_a, distances_torch, z_centered_torch, fixed_sigma2, fixed_nugget):\n",
    "    \"\"\"Calculates -LL for PyTorch (optimizing RANGE only, fixing SIGMA2).\"\"\"\n",
    "    \n",
    "    # Reparameterization: a = raw_range_a^2\n",
    "    range_a = raw_params_range_a.pow(2).squeeze()\n",
    "    \n",
    "    C = exponential_covariance_torch(distances_torch, fixed_sigma2, range_a, fixed_nugget)\n",
    "    \n",
    "    try:\n",
    "        # Use PyTorch linear algebra functions: Cholesky decomposition\n",
    "        L = torch.linalg.cholesky(C)\n",
    "        log_det = 2.0 * torch.sum(torch.log(torch.diag(L)))\n",
    "        alpha = torch.linalg.solve(C, z_centered_torch.unsqueeze(1))\n",
    "        quad_term = z_centered_torch.unsqueeze(0) @ alpha\n",
    "        neg_LL = 0.5 * log_det + 0.5 * quad_term.squeeze()\n",
    "        return neg_LL\n",
    "    except RuntimeError:\n",
    "        return torch.tensor(1e15, device=C.device)\n",
    "\n",
    "\n",
    "# --- Data Generation Function ---\n",
    "def generate_ozone_data_map(coords, sigma2, a, nugget, mean, time_index):\n",
    "    n_points = coords.shape[0]\n",
    "    distances = cdist(coords, coords, metric='euclidean')\n",
    "    Cov = exponential_covariance_numpy(distances, sigma2, a, nugget) \n",
    "    Cov = (Cov + Cov.T) / 2\n",
    "    \n",
    "    try:\n",
    "        L = np.linalg.cholesky(Cov)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.zeros((n_points, N_FEATURES))\n",
    "\n",
    "    W = np.random.normal(0, 1, size=(n_points, 1))\n",
    "    Z_correlated = L @ W\n",
    "    ozone_values = mean + Z_correlated\n",
    "    \n",
    "    data_np = np.zeros((n_points, N_FEATURES))\n",
    "    data_np[:, 0:1] = ozone_values             \n",
    "    data_np[:, 1] = coords[:, 1] * 10 + 2      \n",
    "    data_np[:, 2] = coords[:, 0] * 40 + 250    \n",
    "    data_np[:, 3] = time_index                 \n",
    "    return data_np\n",
    "\n",
    "\n",
    "# --- 1. Data Generation Execution ---\n",
    "df_day_aggregated_list = []\n",
    "print(\"--- Starting Data Generation ---\")\n",
    "lat_coords = np.linspace(LAT_MIN, LAT_MAX, GRID_Y)\n",
    "lon_coords = np.linspace(LON_MIN, LON_MAX, GRID_X)\n",
    "coords_latlon = np.array([[lat, lon] for lat in lat_coords for lon in lon_coords])\n",
    "\n",
    "# Generate only one hour of data for fitting the spatial model\n",
    "data_np = generate_ozone_data_map(\n",
    "    coords_latlon, SIGMA2_TRUE, RANGE_A_TRUE, NUGGET, OZONE_MEAN, 21.0\n",
    ")\n",
    "df_day_aggregated_list.append(torch.tensor(data_np, dtype=torch.float))\n",
    "\n",
    "print(\"--- Data Generation Complete ---\")\n",
    "\n",
    "# --- 2. Data Preparation ---\n",
    "data_to_fit = df_day_aggregated_list[0][:N_SPATIAL_POINTS, :] \n",
    "z_data = data_to_fit[:, 0].numpy()\n",
    "coordinates = coords_latlon[:, [1, 0]] \n",
    "distances_np = cdist(coordinates, coordinates, metric='euclidean')\n",
    "z_centered_np = z_data - np.mean(z_data)\n",
    "\n",
    "# Convert to Torch Tensors\n",
    "distances_torch = torch.tensor(distances_np, dtype=torch.float)\n",
    "z_centered_torch = torch.tensor(z_centered_np, dtype=torch.float)\n",
    "\n",
    "# --- Initial Parameter Setup (Shared) ---\n",
    "# ðŸ’¡ MODIFIED: Initial guess for raw_range_a: sqrt(target + 0.5) = sqrt(2.5)\n",
    "raw_range_a_start = np.sqrt(RANGE_A_TRUE + 2) \n",
    "\n",
    "# ----------------------------------------------------\n",
    "# A. Optimization with L-BFGS (PyTorch)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Reset parameters for LBFGS\n",
    "raw_params_lbfgs = torch.tensor(\n",
    "    [raw_range_a_start], \n",
    "    dtype=torch.float, \n",
    "    requires_grad=True\n",
    ")\n",
    "\n",
    "optimizer_lbfgs = optim.LBFGS(\n",
    "    [raw_params_lbfgs], \n",
    "    lr=1.0, \n",
    "    max_iter=LBFGS_MAX_STEPS,\n",
    "    max_eval=LBFGS_MAX_EVAL \n",
    ")\n",
    "\n",
    "final_loss_lbfgs = torch.tensor(0.0)\n",
    "print(\"\\n--- A. Starting MLE Optimization (PyTorch L-BFGS) ---\")\n",
    "print(f\"Fixed Variance (ÏƒÂ²): {SIGMA2_FIXED:.1f}\")\n",
    "\n",
    "# L-BFGS requires a \"closure\" function\n",
    "def closure_lbfgs():\n",
    "    optimizer_lbfgs.zero_grad()\n",
    "    # ðŸ’¡ MODIFIED: Pass fixed_sigma2 instead of raw_params_sigma2\n",
    "    loss = neg_log_likelihood_torch(raw_params_lbfgs, distances_torch, z_centered_torch, SIGMA2_FIXED, NUGGET)\n",
    "    if not torch.isinf(loss) and not torch.isnan(loss):\n",
    "        loss.backward()\n",
    "    return loss\n",
    "\n",
    "# L-BFGS Optimization Loop\n",
    "for step in range(LBFGS_MAX_STEPS):\n",
    "    loss = optimizer_lbfgs.step(closure_lbfgs)\n",
    "    final_loss_lbfgs = loss\n",
    "    \n",
    "    if (step + 1) % 5 == 0: \n",
    "        current_a = raw_params_lbfgs.pow(2).item()\n",
    "        grad_value = raw_params_lbfgs.grad.item() if raw_params_lbfgs.grad is not None else 0.0\n",
    "        print(f\"LBFGS Step {step + 1}/{LBFGS_MAX_STEPS}, NLL: {loss.item():.2f}, Range_a: {current_a:.3f}, Grad: {grad_value:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# B. Optimization with Adam (PyTorch)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Reset parameters for Adam (Use the same start point)\n",
    "raw_params_adam = torch.tensor(\n",
    "    [raw_range_a_start], \n",
    "    dtype=torch.float, \n",
    "    requires_grad=True\n",
    ")\n",
    "\n",
    "optimizer_adam = optim.Adam(\n",
    "    [raw_params_adam], \n",
    "    lr=ADAM_LEARNING_RATE\n",
    ")\n",
    "\n",
    "final_loss_adam = torch.tensor(0.0)\n",
    "print(f\"\\n--- B. Starting MLE Optimization (PyTorch Adam) ---\")\n",
    "print(f\"Fixed Variance (ÏƒÂ²): {SIGMA2_FIXED:.1f}\")\n",
    "\n",
    "# Adam Optimization Loop\n",
    "for epoch in range(ADAM_ITERATIONS):\n",
    "    optimizer_adam.zero_grad()\n",
    "    \n",
    "    # ðŸ’¡ MODIFIED: Pass fixed_sigma2 instead of raw_params_sigma2\n",
    "    loss = neg_log_likelihood_torch(raw_params_adam, distances_torch, z_centered_torch, SIGMA2_FIXED, NUGGET)\n",
    "    \n",
    "    if torch.isinf(loss) or torch.isnan(loss):\n",
    "        loss = torch.tensor(1e15, device=loss.device)\n",
    "        break\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer_adam.step()\n",
    "    final_loss_adam = loss\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0: \n",
    "        current_a = raw_params_adam.pow(2).item()\n",
    "        grad_value = raw_params_adam.grad.item() if raw_params_adam.grad is not None else 0.0\n",
    "        print(f\"Adam Epoch {epoch + 1}/{ADAM_ITERATIONS}, NLL: {loss.item():.2f}, Range_a: {current_a:.3f}, Grad: {grad_value:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. Display Results\n",
    "# ----------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"TARGET PARAMETERS: Variance (ÏƒÂ²)={SIGMA2_TRUE}, Range (a)={RANGE_A_TRUE}, Nugget (Î·Â²)={NUGGET}\")\n",
    "print(f\"**FIXED PARAMETER: Variance (ÏƒÂ²)={SIGMA2_FIXED}**\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# L-BFGS Results\n",
    "fitted_range_a_lbfgs = raw_params_lbfgs.pow(2).detach().numpy().item()\n",
    "print(\"âœ¨ PyTorch L-BFGS Results:\")\n",
    "print(f\"  * Fitted Range (a): {fitted_range_a_lbfgs:.3f} (Target: {RANGE_A_TRUE})\")\n",
    "print(f\"  * Final -LL Value: {final_loss_lbfgs.item():.2f}\")\n",
    "print(f\"  * Optimization Steps: {LBFGS_MAX_STEPS} steps\")\n",
    "\n",
    "# Adam Results\n",
    "fitted_range_a_adam = raw_params_adam.pow(2).detach().numpy().item()\n",
    "print(\"\\nðŸš€ PyTorch Adam Results:\")\n",
    "print(f\"  * Fitted Range (a): {fitted_range_a_adam:.3f} (Target: {RANGE_A_TRUE})\")\n",
    "print(f\"  * Final -LL Value: {final_loss_adam.item():.2f}\")\n",
    "print(f\"  * Optimization Steps: {ADAM_ITERATIONS} epochs\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6368fa5",
   "metadata": {},
   "source": [
    "# lbfgs vs adams fit both  sigma^2 and range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "727adbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Generation ---\n",
      "--- Data Generation Complete ---\n",
      "\n",
      "--- A. Starting MLE Optimization (PyTorch L-BFGS) ---\n",
      "LBFGS Step 5/50, NLL: 1444.80, Params: [ÏƒÂ²: 32.857, a: 1.622], Grads: [-0.0006, 0.0027]\n",
      "LBFGS Step 10/50, NLL: 1444.80, Params: [ÏƒÂ²: 32.857, a: 1.622], Grads: [-0.0006, 0.0027]\n",
      "LBFGS Step 15/50, NLL: 1444.80, Params: [ÏƒÂ²: 32.857, a: 1.622], Grads: [-0.0006, 0.0027]\n",
      "LBFGS Step 20/50, NLL: 1444.80, Params: [ÏƒÂ²: 32.857, a: 1.622], Grads: [-0.0006, 0.0027]\n",
      "LBFGS Step 25/50, NLL: 1444.80, Params: [ÏƒÂ²: 32.857, a: 1.622], Grads: [-0.0006, 0.0027]\n",
      "LBFGS Step 30/50, NLL: 1444.80, Params: [ÏƒÂ²: 32.857, a: 1.622], Grads: [-0.0006, 0.0027]\n",
      "LBFGS Step 35/50, NLL: 1444.80, Params: [ÏƒÂ²: 32.857, a: 1.622], Grads: [-0.0006, 0.0027]\n",
      "LBFGS Step 40/50, NLL: 1444.80, Params: [ÏƒÂ²: 32.857, a: 1.622], Grads: [-0.0006, 0.0027]\n",
      "LBFGS Step 45/50, NLL: 1444.80, Params: [ÏƒÂ²: 32.857, a: 1.622], Grads: [-0.0006, 0.0027]\n",
      "LBFGS Step 50/50, NLL: 1444.80, Params: [ÏƒÂ²: 32.857, a: 1.622], Grads: [-0.0006, 0.0027]\n",
      "\n",
      "--- B. Starting MLE Optimization (PyTorch Adam) ---\n",
      "Adam Epoch 50/500, NLL: 1445.19, Params: [ÏƒÂ²: 46.963, a: 2.334], Grads: [0.1978, 1.3968]\n",
      "Adam Epoch 100/500, NLL: 1445.14, Params: [ÏƒÂ²: 45.780, a: 2.274], Grads: [0.3768, 0.5666]\n",
      "Adam Epoch 150/500, NLL: 1445.09, Params: [ÏƒÂ²: 44.300, a: 2.199], Grads: [0.3979, 0.4069]\n",
      "Adam Epoch 200/500, NLL: 1445.03, Params: [ÏƒÂ²: 42.632, a: 2.115], Grads: [0.3827, 0.3679]\n",
      "Adam Epoch 250/500, NLL: 1444.97, Params: [ÏƒÂ²: 40.876, a: 2.027], Grads: [0.3537, 0.3376]\n",
      "Adam Epoch 300/500, NLL: 1444.91, Params: [ÏƒÂ²: 39.133, a: 1.939], Grads: [0.3140, 0.2945]\n",
      "Adam Epoch 350/500, NLL: 1444.87, Params: [ÏƒÂ²: 37.508, a: 1.857], Grads: [0.2619, 0.2504]\n",
      "Adam Epoch 400/500, NLL: 1444.83, Params: [ÏƒÂ²: 36.095, a: 1.786], Grads: [0.2027, 0.1985]\n",
      "Adam Epoch 450/500, NLL: 1444.81, Params: [ÏƒÂ²: 34.963, a: 1.729], Grads: [0.1452, 0.1391]\n",
      "Adam Epoch 500/500, NLL: 1444.80, Params: [ÏƒÂ²: 34.133, a: 1.687], Grads: [0.0943, 0.0911]\n",
      "\n",
      "==================================================\n",
      "TARGET PARAMETERS: Variance (ÏƒÂ²)=42.0, Range (a)=2.0, Nugget (Î·Â²)=0.0\n",
      "==================================================\n",
      "âœ¨ PyTorch L-BFGS Results:\n",
      "  * Fitted Variance (ÏƒÂ²): 32.857 (Target: 42.0)\n",
      "  * Fitted Range (a): 1.622 (Target: 2.0)\n",
      "  * Final -LL Value: 1444.80\n",
      "  * Optimization Steps: 50 steps\n",
      "\n",
      "ðŸš€ PyTorch Adam Results:\n",
      "  * Fitted Variance (ÏƒÂ²): 34.133 (Target: 42.0)\n",
      "  * Fitted Range (a): 1.687 (Target: 2.0)\n",
      "  * Final -LL Value: 1444.80\n",
      "  * Optimization Steps: 500 epochs\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from scipy.spatial.distance import cdist\n",
    "# Removed 'scipy.optimize as opt' and 'from numpy.linalg import inv'\n",
    "\n",
    "# --- 0. Global Parameters and Utility Functions ---\n",
    "N_SPATIAL_POINTS = 1120\n",
    "N_DAYS = 31\n",
    "N_HOURS_PER_DAY = 8\n",
    "N_FEATURES = 4\n",
    "GRID_X = 40  \n",
    "GRID_Y = 28  \n",
    "LAT_MIN, LAT_MAX = 0, 5\n",
    "LON_MIN, LON_MAX = 113, 123\n",
    "BASE_DATE = '2024_07_y24m07day' \n",
    "\n",
    "# Exponential Kernel Parameters (Targets)\n",
    "SIGMA2_TRUE = 42.0 # TARGET Variance\n",
    "RANGE_A_TRUE = 2.0    # TARGET Range\n",
    "NUGGET = 0.0     # FIXED Nugget\n",
    "\n",
    "# Optimization Setup\n",
    "ADAM_ITERATIONS = 500\n",
    "ADAM_LEARNING_RATE = 0.01\n",
    "\n",
    "# L-BFGS Setup\n",
    "LBFGS_MAX_STEPS = 50 \n",
    "LBFGS_MAX_EVAL = 50 \n",
    "\n",
    "OZONE_MEAN = 240.0\n",
    "\n",
    "# --- COVARIANCE FUNCTIONS ---\n",
    "\n",
    "def exponential_covariance_numpy(distances, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (NumPy for Generation).\"\"\"\n",
    "    # Cov(h) = sigma^2 * exp(-h/a)\n",
    "    cov = sigma2 * np.exp(-distances / a)\n",
    "    if distances.shape[0] == distances.shape[1]:\n",
    "        cov[np.diag_indices_from(distances)] += (nugget + 1e-6)\n",
    "    return cov\n",
    "\n",
    "def exponential_covariance_torch(distances_torch, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (PyTorch for Optimization).\"\"\"\n",
    "    cov = sigma2 * torch.exp(-distances_torch / a)\n",
    "    \n",
    "    # Add nugget effect + jitter to the diagonal\n",
    "    if distances_torch.shape[0] == distances_torch.shape[1]:\n",
    "        jitter = 1e-6 \n",
    "        diag_mask = torch.eye(cov.shape[0], device=cov.device)\n",
    "        cov = cov + diag_mask * (nugget + jitter)\n",
    "    return cov\n",
    "\n",
    "# --- SHARED NLL Function for PyTorch (Adam and LBFGS) ---\n",
    "# ðŸ’¡ MODIFIED: Accepts raw_params_all which contains [raw_sigma2, raw_range_a]\n",
    "def neg_log_likelihood_torch(raw_params_all, distances_torch, z_centered_torch, fixed_nugget):\n",
    "    \"\"\"Calculates -LL for PyTorch (optimizing SIGMA2 and RANGE).\"\"\"\n",
    "    \n",
    "    # Reparameterization: sigma2 = raw_sigma2^2, range_a = raw_range_a^2\n",
    "    sigma2 = raw_params_all[0].pow(2).squeeze()\n",
    "    range_a = raw_params_all[1].pow(2).squeeze()\n",
    "    \n",
    "    C = exponential_covariance_torch(distances_torch, sigma2, range_a, fixed_nugget)\n",
    "    \n",
    "    try:\n",
    "        # Use PyTorch linear algebra functions: Cholesky decomposition\n",
    "        L = torch.linalg.cholesky(C)\n",
    "        log_det = 2.0 * torch.sum(torch.log(torch.diag(L)))\n",
    "        alpha = torch.linalg.solve(C, z_centered_torch.unsqueeze(1))\n",
    "        quad_term = z_centered_torch.unsqueeze(0) @ alpha\n",
    "        neg_LL = 0.5 * log_det + 0.5 * quad_term.squeeze()\n",
    "        return neg_LL\n",
    "    except RuntimeError:\n",
    "        return torch.tensor(1e15, device=C.device)\n",
    "\n",
    "\n",
    "# --- Data Generation Function (Unchanged) ---\n",
    "def generate_ozone_data_map(coords, sigma2, a, nugget, mean, time_index):\n",
    "    n_points = coords.shape[0]\n",
    "    distances = cdist(coords, coords, metric='euclidean')\n",
    "    Cov = exponential_covariance_numpy(distances, sigma2, a, nugget) \n",
    "    Cov = (Cov + Cov.T) / 2\n",
    "    \n",
    "    try:\n",
    "        L = np.linalg.cholesky(Cov)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.zeros((n_points, N_FEATURES))\n",
    "\n",
    "    W = np.random.normal(0, 1, size=(n_points, 1))\n",
    "    Z_correlated = L @ W\n",
    "    ozone_values = mean + Z_correlated\n",
    "    \n",
    "    data_np = np.zeros((n_points, N_FEATURES))\n",
    "    data_np[:, 0:1] = ozone_values             \n",
    "    data_np[:, 1] = coords[:, 1] * 10 + 2      \n",
    "    data_np[:, 2] = coords[:, 0] * 40 + 250    \n",
    "    data_np[:, 3] = time_index                 \n",
    "    return data_np\n",
    "\n",
    "\n",
    "# --- 1. Data Generation Execution ---\n",
    "df_day_aggregated_list = []\n",
    "print(\"--- Starting Data Generation ---\")\n",
    "lat_coords = np.linspace(LAT_MIN, LAT_MAX, GRID_Y)\n",
    "lon_coords = np.linspace(LON_MIN, LON_MAX, GRID_X)\n",
    "coords_latlon = np.array([[lat, lon] for lat in lat_coords for lon in lon_coords])\n",
    "\n",
    "# Generate only one hour of data for fitting the spatial model\n",
    "data_np = generate_ozone_data_map(\n",
    "    coords_latlon, SIGMA2_TRUE, RANGE_A_TRUE, NUGGET, OZONE_MEAN, 21.0\n",
    ")\n",
    "df_day_aggregated_list.append(torch.tensor(data_np, dtype=torch.float))\n",
    "\n",
    "print(\"--- Data Generation Complete ---\")\n",
    "\n",
    "# --- 2. Data Preparation ---\n",
    "data_to_fit = df_day_aggregated_list[0][:N_SPATIAL_POINTS, :] \n",
    "z_data = data_to_fit[:, 0].numpy()\n",
    "coordinates = coords_latlon[:, [1, 0]] \n",
    "distances_np = cdist(coordinates, coordinates, metric='euclidean')\n",
    "z_centered_np = z_data - np.mean(z_data)\n",
    "\n",
    "# Convert to Torch Tensors\n",
    "distances_torch = torch.tensor(distances_np, dtype=torch.float)\n",
    "z_centered_torch = torch.tensor(z_centered_np, dtype=torch.float)\n",
    "\n",
    "# --- Initial Parameter Setup (Shared) ---\n",
    "# ðŸ’¡ NEW: Initial guess for raw_sigma2 and raw_range_a. Starting off target for both.\n",
    "raw_sigma2_start = np.sqrt(SIGMA2_TRUE + 5)     # Target: sqrt(42) â‰ˆ 6.48. Start: sqrt(47) â‰ˆ 6.86\n",
    "raw_range_a_start = np.sqrt(RANGE_A_TRUE + 0.5) # Target: sqrt(2) â‰ˆ 1.41. Start: sqrt(2.5) â‰ˆ 1.58\n",
    "\n",
    "initial_params = [raw_sigma2_start, raw_range_a_start]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# A. Optimization with L-BFGS (PyTorch)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Reset parameters for LBFGS\n",
    "raw_params_lbfgs = torch.tensor(\n",
    "    initial_params, \n",
    "    dtype=torch.float, \n",
    "    requires_grad=True\n",
    ")\n",
    "\n",
    "optimizer_lbfgs = optim.LBFGS(\n",
    "    [raw_params_lbfgs], \n",
    "    lr=1.0, \n",
    "    max_iter=LBFGS_MAX_STEPS,\n",
    "    max_eval=LBFGS_MAX_EVAL \n",
    ")\n",
    "\n",
    "final_loss_lbfgs = torch.tensor(0.0)\n",
    "print(\"\\n--- A. Starting MLE Optimization (PyTorch L-BFGS) ---\")\n",
    "\n",
    "# L-BFGS requires a \"closure\" function\n",
    "def closure_lbfgs():\n",
    "    optimizer_lbfgs.zero_grad()\n",
    "    # ðŸ’¡ MODIFIED: Pass raw_params_lbfgs and fixed_nugget\n",
    "    loss = neg_log_likelihood_torch(raw_params_lbfgs, distances_torch, z_centered_torch, NUGGET)\n",
    "    if not torch.isinf(loss) and not torch.isnan(loss):\n",
    "        loss.backward()\n",
    "    return loss\n",
    "\n",
    "# L-BFGS Optimization Loop\n",
    "for step in range(LBFGS_MAX_STEPS):\n",
    "    loss = optimizer_lbfgs.step(closure_lbfgs)\n",
    "    final_loss_lbfgs = loss\n",
    "    \n",
    "    if (step + 1) % 5 == 0: \n",
    "        current_sigma2 = raw_params_lbfgs[0].pow(2).item()\n",
    "        current_a = raw_params_lbfgs[1].pow(2).item()\n",
    "        grad_sigma2 = raw_params_lbfgs.grad[0].item() if raw_params_lbfgs.grad is not None else 0.0\n",
    "        grad_a = raw_params_lbfgs.grad[1].item() if raw_params_lbfgs.grad is not None else 0.0\n",
    "        print(f\"LBFGS Step {step + 1}/{LBFGS_MAX_STEPS}, NLL: {loss.item():.2f}, Params: [ÏƒÂ²: {current_sigma2:.3f}, a: {current_a:.3f}], Grads: [{grad_sigma2:.4f}, {grad_a:.4f}]\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# B. Optimization with Adam (PyTorch)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Reset parameters for Adam (Use the same start point)\n",
    "raw_params_adam = torch.tensor(\n",
    "    initial_params, \n",
    "    dtype=torch.float, \n",
    "    requires_grad=True\n",
    ")\n",
    "\n",
    "optimizer_adam = optim.Adam(\n",
    "    [raw_params_adam], \n",
    "    lr=ADAM_LEARNING_RATE\n",
    ")\n",
    "\n",
    "final_loss_adam = torch.tensor(0.0)\n",
    "print(f\"\\n--- B. Starting MLE Optimization (PyTorch Adam) ---\")\n",
    "\n",
    "# Adam Optimization Loop\n",
    "for epoch in range(ADAM_ITERATIONS):\n",
    "    optimizer_adam.zero_grad()\n",
    "    \n",
    "    # ðŸ’¡ MODIFIED: Pass raw_params_adam and fixed_nugget\n",
    "    loss = neg_log_likelihood_torch(raw_params_adam, distances_torch, z_centered_torch, NUGGET)\n",
    "    \n",
    "    if torch.isinf(loss) or torch.isnan(loss):\n",
    "        loss = torch.tensor(1e15, device=loss.device)\n",
    "        break\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer_adam.step()\n",
    "    final_loss_adam = loss\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0: \n",
    "        current_sigma2 = raw_params_adam[0].pow(2).item()\n",
    "        current_a = raw_params_adam[1].pow(2).item()\n",
    "        grad_sigma2 = raw_params_adam.grad[0].item() if raw_params_adam.grad is not None else 0.0\n",
    "        grad_a = raw_params_adam.grad[1].item() if raw_params_adam.grad is not None else 0.0\n",
    "        print(f\"Adam Epoch {epoch + 1}/{ADAM_ITERATIONS}, NLL: {loss.item():.2f}, Params: [ÏƒÂ²: {current_sigma2:.3f}, a: {current_a:.3f}], Grads: [{grad_sigma2:.4f}, {grad_a:.4f}]\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. Display Results\n",
    "# ----------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"TARGET PARAMETERS: Variance (ÏƒÂ²)={SIGMA2_TRUE}, Range (a)={RANGE_A_TRUE}, Nugget (Î·Â²)={NUGGET}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# L-BFGS Results\n",
    "fitted_sigma2_lbfgs = raw_params_lbfgs[0].pow(2).detach().numpy().item()\n",
    "fitted_range_a_lbfgs = raw_params_lbfgs[1].pow(2).detach().numpy().item()\n",
    "print(\"âœ¨ PyTorch L-BFGS Results:\")\n",
    "print(f\"  * Fitted Variance (ÏƒÂ²): {fitted_sigma2_lbfgs:.3f} (Target: {SIGMA2_TRUE})\")\n",
    "print(f\"  * Fitted Range (a): {fitted_range_a_lbfgs:.3f} (Target: {RANGE_A_TRUE})\")\n",
    "print(f\"  * Final -LL Value: {final_loss_lbfgs.item():.2f}\")\n",
    "print(f\"  * Optimization Steps: {LBFGS_MAX_STEPS} steps\")\n",
    "\n",
    "# Adam Results\n",
    "fitted_sigma2_adam = raw_params_adam[0].pow(2).detach().numpy().item()\n",
    "fitted_range_a_adam = raw_params_adam[1].pow(2).detach().numpy().item()\n",
    "print(\"\\nðŸš€ PyTorch Adam Results:\")\n",
    "print(f\"  * Fitted Variance (ÏƒÂ²): {fitted_sigma2_adam:.3f} (Target: {SIGMA2_TRUE})\")\n",
    "print(f\"  * Fitted Range (a): {fitted_range_a_adam:.3f} (Target: {RANGE_A_TRUE})\")\n",
    "print(f\"  * Final -LL Value: {final_loss_adam.item():.2f}\")\n",
    "print(f\"  * Optimization Steps: {ADAM_ITERATIONS} epochs\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116189cf",
   "metadata": {},
   "source": [
    "# square parameteriazation + stablize parametrization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f6e40941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Generation (4480 points) ---\n",
      "--- Data Generation Complete ---\n",
      "\n",
      "--- A. Starting MLE Optimization (PyTorch L-BFGS) - STABLE ---\n",
      "LBFGS Step 5/50, NLL: 4230.51, Params: [ÏƒÂ²: 54.321, a: 2.632], Grads: [Î¦1_raw: -0.0019, Î¦2_raw: -0.0053]\n",
      "LBFGS Step 10/50, NLL: 4230.51, Params: [ÏƒÂ²: 54.321, a: 2.632], Grads: [Î¦1_raw: -0.0019, Î¦2_raw: -0.0053]\n",
      "LBFGS Step 15/50, NLL: 4230.51, Params: [ÏƒÂ²: 54.321, a: 2.632], Grads: [Î¦1_raw: -0.0019, Î¦2_raw: -0.0053]\n",
      "LBFGS Step 20/50, NLL: 4230.51, Params: [ÏƒÂ²: 54.321, a: 2.632], Grads: [Î¦1_raw: -0.0019, Î¦2_raw: -0.0053]\n",
      "LBFGS Step 25/50, NLL: 4230.51, Params: [ÏƒÂ²: 54.321, a: 2.632], Grads: [Î¦1_raw: -0.0019, Î¦2_raw: -0.0053]\n",
      "LBFGS Step 30/50, NLL: 4230.51, Params: [ÏƒÂ²: 54.321, a: 2.632], Grads: [Î¦1_raw: -0.0019, Î¦2_raw: -0.0053]\n",
      "LBFGS Step 35/50, NLL: 4230.51, Params: [ÏƒÂ²: 54.321, a: 2.632], Grads: [Î¦1_raw: -0.0019, Î¦2_raw: -0.0053]\n",
      "LBFGS Step 40/50, NLL: 4230.51, Params: [ÏƒÂ²: 54.321, a: 2.632], Grads: [Î¦1_raw: -0.0019, Î¦2_raw: -0.0053]\n",
      "LBFGS Step 45/50, NLL: 4230.51, Params: [ÏƒÂ²: 54.321, a: 2.632], Grads: [Î¦1_raw: -0.0019, Î¦2_raw: -0.0053]\n",
      "LBFGS Step 50/50, NLL: 4230.51, Params: [ÏƒÂ²: 54.321, a: 2.632], Grads: [Î¦1_raw: -0.0019, Î¦2_raw: -0.0053]\n",
      "\n",
      "--- B. Starting MLE Optimization (PyTorch Adam) - STABLE ---\n",
      "Adam Epoch 50/500, NLL: 4231.12, Params: [ÏƒÂ²: 52.617, a: 2.495], Grads: [Î¦1_raw: 22.2927, Î¦2_raw: 0.0429]\n",
      "Adam Epoch 100/500, NLL: 4230.52, Params: [ÏƒÂ²: 54.420, a: 2.641], Grads: [Î¦1_raw: -1.9052, Î¦2_raw: 0.0205]\n",
      "Adam Epoch 150/500, NLL: 4230.48, Params: [ÏƒÂ²: 54.426, a: 2.637], Grads: [Î¦1_raw: 0.1456, Î¦2_raw: -0.0211]\n",
      "Adam Epoch 200/500, NLL: 4230.51, Params: [ÏƒÂ²: 54.378, a: 2.635], Grads: [Î¦1_raw: 0.0150, Î¦2_raw: -0.0108]\n",
      "Adam Epoch 250/500, NLL: 4230.51, Params: [ÏƒÂ²: 54.373, a: 2.634], Grads: [Î¦1_raw: 0.0142, Î¦2_raw: -0.0026]\n",
      "Adam Epoch 300/500, NLL: 4230.50, Params: [ÏƒÂ²: 54.374, a: 2.634], Grads: [Î¦1_raw: 0.0311, Î¦2_raw: -0.0048]\n",
      "Adam Epoch 350/500, NLL: 4230.52, Params: [ÏƒÂ²: 54.277, a: 2.630], Grads: [Î¦1_raw: -0.0134, Î¦2_raw: 0.0627]\n",
      "Adam Epoch 400/500, NLL: 4230.51, Params: [ÏƒÂ²: 54.323, a: 2.632], Grads: [Î¦1_raw: 0.0285, Î¦2_raw: -0.0169]\n",
      "Adam Epoch 450/500, NLL: 4230.52, Params: [ÏƒÂ²: 54.170, a: 2.624], Grads: [Î¦1_raw: 0.0038, Î¦2_raw: -0.0371]\n",
      "Adam Epoch 500/500, NLL: 4230.55, Params: [ÏƒÂ²: 54.243, a: 2.628], Grads: [Î¦1_raw: -0.0326, Î¦2_raw: -0.0073]\n",
      "\n",
      "==================================================\n",
      "TARGET PARAMETERS: Variance (ÏƒÂ²)=42.0, Range (a)=2, Nugget (Î·Â²)=0.0\n",
      "==================================================\n",
      "âœ¨ PyTorch L-BFGS Results (Stable Reparameterization):\n",
      "  * Fitted Variance (ÏƒÂ²): 54.321 (Target: 42.0)\n",
      "  * Fitted Range (a): 2.632 (Target: 2)\n",
      "  * Fitted Ratio (Î¦â‚=ÏƒÂ²/a): 20.640 (Target Ratio: 21.000)\n",
      "  * Final -LL Value: 4230.51\n",
      "  * Optimization Steps: 50 steps\n",
      "\n",
      "ðŸš€ PyTorch Adam Results (Stable Reparameterization):\n",
      "  * Fitted Variance (ÏƒÂ²): 54.243 (Target: 42.0)\n",
      "  * Fitted Range (a): 2.628 (Target: 2)\n",
      "  * Fitted Ratio (Î¦â‚=ÏƒÂ²/a): 20.640 (Target Ratio: 21.000)\n",
      "  * Final -LL Value: 4230.55\n",
      "  * Optimization Steps: 500 epochs\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# --- 0. Global Parameters and Utility Functions ---\n",
    "# CHANGED: Increased grid resolution to increase data points (40*28 -> 80*56)\n",
    "GRID_X = 80  \n",
    "GRID_Y = 56  \n",
    "N_SPATIAL_POINTS = GRID_X * GRID_Y # New total: 4480 points\n",
    "N_DAYS = 31\n",
    "N_HOURS_PER_DAY = 8\n",
    "N_FEATURES = 4\n",
    "LAT_MIN, LAT_MAX = 0, 5\n",
    "LON_MIN, LON_MAX = 113, 123\n",
    "BASE_DATE = '2024_07_y24m07day' \n",
    "\n",
    "# Exponential Kernel Parameters (Targets)\n",
    "SIGMA2_TRUE = 42.0 # TARGET Variance\n",
    "RANGE_A_TRUE = 2   # TARGET Range\n",
    "NUGGET = 0.0     # FIXED Nugget\n",
    "\n",
    "# Optimization Setup\n",
    "ADAM_ITERATIONS = 500\n",
    "ADAM_LEARNING_RATE = 0.01\n",
    "\n",
    "# L-BFGS Setup\n",
    "LBFGS_MAX_STEPS = 50 \n",
    "LBFGS_MAX_EVAL = 50 \n",
    "\n",
    "OZONE_MEAN = 240.0\n",
    "\n",
    "# Calculate target stable parameters for display\n",
    "PHI1_TARGET = SIGMA2_TRUE / RANGE_A_TRUE    # 42.0 / 1.5 = 28.0\n",
    "PHI2_TARGET = 1.0 / RANGE_A_TRUE            # 1.0 / 1.5 = 0.667\n",
    "\n",
    "# --- COVARIANCE FUNCTIONS ---\n",
    "\n",
    "def exponential_covariance_numpy(distances, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (NumPy for Generation).\"\"\"\n",
    "    # Cov(h) = sigma^2 * exp(-h/a)\n",
    "    cov = sigma2 * np.exp(-distances / a)\n",
    "    if distances.shape[0] == distances.shape[1]:\n",
    "        cov[np.diag_indices_from(distances)] += (nugget + 1e-6)\n",
    "    return cov\n",
    "\n",
    "def exponential_covariance_torch(distances_torch, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (PyTorch for Optimization).\"\"\"\n",
    "    cov = sigma2 * torch.exp(-distances_torch / a)\n",
    "    \n",
    "    # Add nugget effect + jitter to the diagonal\n",
    "    if distances_torch.shape[0] == distances_torch.shape[1]:\n",
    "        jitter = 1e-6 \n",
    "        diag_mask = torch.eye(cov.shape[0], device=cov.device)\n",
    "        cov = cov + diag_mask * (nugget + jitter)\n",
    "    return cov\n",
    "\n",
    "# --- SHARED NLL Function using STABLE REPARAMETERIZATION ---\n",
    "def neg_log_likelihood_torch_stable(raw_params_phi, distances_torch, z_centered_torch, fixed_nugget):\n",
    "    \"\"\"\n",
    "    Calculates -LL for PyTorch (optimizing Stable Reparameterization).\n",
    "    raw_params_phi[0] = raw_phi1_sqrt (for sigma2/a)\n",
    "    raw_params_phi[1] = raw_phi2_sqrt (for 1/a)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Apply Square Reparameterization to raw parameters to ensure positivity\n",
    "    phi1 = raw_params_phi[0].pow(2).squeeze() # Phi1 = sigma2 / a (TARGET RATIO)\n",
    "    phi2 = raw_params_phi[1].pow(2).squeeze() # Phi2 = 1 / a (INVERSE RANGE)\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # 2. Derive Original Parameters\n",
    "    range_a = 1.0 / (phi2 + epsilon)          # Range: a = 1 / Phi2\n",
    "    sigma2 = phi1 / (phi2 + epsilon)          # Variance: sigma2 = Phi1 / Phi2\n",
    "    \n",
    "    C = exponential_covariance_torch(distances_torch, sigma2, range_a, fixed_nugget)\n",
    "    \n",
    "    try:\n",
    "        # Use PyTorch linear algebra functions: Cholesky decomposition\n",
    "        L = torch.linalg.cholesky(C)\n",
    "        log_det = 2.0 * torch.sum(torch.log(torch.diag(L)))\n",
    "        alpha = torch.linalg.solve(C, z_centered_torch.unsqueeze(1))\n",
    "        quad_term = z_centered_torch.unsqueeze(0) @ alpha\n",
    "        neg_LL = 0.5 * log_det + 0.5 * quad_term.squeeze()\n",
    "        return neg_LL\n",
    "    except RuntimeError:\n",
    "        return torch.tensor(1e15, device=C.device)\n",
    "\n",
    "\n",
    "# --- Data Generation Function (Unchanged) ---\n",
    "def generate_ozone_data_map(coords, sigma2, a, nugget, mean, time_index):\n",
    "    n_points = coords.shape[0]\n",
    "    distances = cdist(coords, coords, metric='euclidean')\n",
    "    Cov = exponential_covariance_numpy(distances, sigma2, a, nugget) \n",
    "    Cov = (Cov + Cov.T) / 2\n",
    "    \n",
    "    try:\n",
    "        L = np.linalg.cholesky(Cov)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.zeros((n_points, N_FEATURES))\n",
    "\n",
    "    W = np.random.normal(0, 1, size=(n_points, 1))\n",
    "    Z_correlated = L @ W\n",
    "    ozone_values = mean + Z_correlated\n",
    "    \n",
    "    data_np = np.zeros((n_points, N_FEATURES))\n",
    "    data_np[:, 0:1] = ozone_values             \n",
    "    data_np[:, 1] = coords[:, 1] * 10 + 2      \n",
    "    data_np[:, 2] = coords[:, 0] * 40 + 250    \n",
    "    data_np[:, 3] = time_index                 \n",
    "    return data_np\n",
    "\n",
    "\n",
    "# --- 1. Data Generation Execution ---\n",
    "df_day_aggregated_list = []\n",
    "print(f\"--- Starting Data Generation ({N_SPATIAL_POINTS} points) ---\")\n",
    "lat_coords = np.linspace(LAT_MIN, LAT_MAX, GRID_Y)\n",
    "lon_coords = np.linspace(LON_MIN, LON_MAX, GRID_X)\n",
    "coords_latlon = np.array([[lat, lon] for lat in lat_coords for lon in lon_coords])\n",
    "\n",
    "# Generate only one hour of data for fitting the spatial model\n",
    "data_np = generate_ozone_data_map(\n",
    "    coords_latlon, SIGMA2_TRUE, RANGE_A_TRUE, NUGGET, OZONE_MEAN, 21.0\n",
    ")\n",
    "df_day_aggregated_list.append(torch.tensor(data_np, dtype=torch.float))\n",
    "\n",
    "print(\"--- Data Generation Complete ---\")\n",
    "\n",
    "# --- 2. Data Preparation ---\n",
    "# NOTE: This slice now takes the first 4480 points (all of them)\n",
    "data_to_fit = df_day_aggregated_list[0][:N_SPATIAL_POINTS, :] \n",
    "z_data = data_to_fit[:, 0].numpy()\n",
    "coordinates = coords_latlon[:, [1, 0]] \n",
    "distances_np = cdist(coordinates, coordinates, metric='euclidean')\n",
    "z_centered_np = z_data - np.mean(z_data)\n",
    "\n",
    "# Convert to Torch Tensors\n",
    "distances_torch = torch.tensor(distances_np, dtype=torch.float)\n",
    "z_centered_torch = torch.tensor(z_centered_np, dtype=torch.float)\n",
    "\n",
    "# --- Initial Parameter Setup (Shared) ---\n",
    "# Start Phi1 (Target 28.0) at 32.0 (sqrt 5.66)\n",
    "raw_phi1_sqrt_start = np.sqrt(PHI1_TARGET + 4.0) \n",
    "# Start Phi2 (Target 0.667) at 0.8 (sqrt 0.64)\n",
    "raw_phi2_sqrt_start = np.sqrt(PHI2_TARGET + 0.133) \n",
    "\n",
    "initial_params_stable = [raw_phi1_sqrt_start, raw_phi2_sqrt_start]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# A. Optimization with L-BFGS (PyTorch) - STABLE\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Reset parameters for LBFGS\n",
    "raw_params_lbfgs = torch.tensor(\n",
    "    initial_params_stable, \n",
    "    dtype=torch.float, \n",
    "    requires_grad=True\n",
    ")\n",
    "\n",
    "optimizer_lbfgs = optim.LBFGS(\n",
    "    [raw_params_lbfgs], \n",
    "    lr=1.0, \n",
    "    max_iter=LBFGS_MAX_STEPS,\n",
    "    max_eval=LBFGS_MAX_EVAL \n",
    ")\n",
    "\n",
    "final_loss_lbfgs = torch.tensor(0.0)\n",
    "print(\"\\n--- A. Starting MLE Optimization (PyTorch L-BFGS) - STABLE ---\")\n",
    "\n",
    "# L-BFGS requires a \"closure\" function\n",
    "def closure_lbfgs():\n",
    "    optimizer_lbfgs.zero_grad()\n",
    "    # Use the stable NLL function\n",
    "    loss = neg_log_likelihood_torch_stable(raw_params_lbfgs, distances_torch, z_centered_torch, NUGGET)\n",
    "    if not torch.isinf(loss) and not torch.isnan(loss):\n",
    "        loss.backward()\n",
    "    return loss\n",
    "\n",
    "# L-BFGS Optimization Loop\n",
    "for step in range(LBFGS_MAX_STEPS):\n",
    "    loss = optimizer_lbfgs.step(closure_lbfgs)\n",
    "    final_loss_lbfgs = loss\n",
    "    \n",
    "    if (step + 1) % 5 == 0: \n",
    "        phi1 = raw_params_lbfgs[0].pow(2).item()\n",
    "        phi2 = raw_params_lbfgs[1].pow(2).item()\n",
    "        current_sigma2 = phi1 / (phi2 + 1e-6)\n",
    "        current_a = 1.0 / (phi2 + 1e-6)\n",
    "        grad_phi1 = raw_params_lbfgs.grad[0].item() if raw_params_lbfgs.grad is not None else 0.0\n",
    "        grad_phi2 = raw_params_lbfgs.grad[1].item() if raw_params_lbfgs.grad is not None else 0.0\n",
    "        print(f\"LBFGS Step {step + 1}/{LBFGS_MAX_STEPS}, NLL: {loss.item():.2f}, Params: [ÏƒÂ²: {current_sigma2:.3f}, a: {current_a:.3f}], Grads: [Î¦1_raw: {grad_phi1:.4f}, Î¦2_raw: {grad_phi2:.4f}]\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# B. Optimization with Adam (PyTorch) - STABLE\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Reset parameters for Adam (Use the same start point)\n",
    "raw_params_adam = torch.tensor(\n",
    "    initial_params_stable, \n",
    "    dtype=torch.float, \n",
    "    requires_grad=True\n",
    ")\n",
    "\n",
    "optimizer_adam = optim.Adam(\n",
    "    [raw_params_adam], \n",
    "    lr=ADAM_LEARNING_RATE\n",
    ")\n",
    "\n",
    "final_loss_adam = torch.tensor(0.0)\n",
    "print(f\"\\n--- B. Starting MLE Optimization (PyTorch Adam) - STABLE ---\")\n",
    "\n",
    "# Adam Optimization Loop\n",
    "for epoch in range(ADAM_ITERATIONS):\n",
    "    optimizer_adam.zero_grad()\n",
    "    \n",
    "    # Use the stable NLL function\n",
    "    loss = neg_log_likelihood_torch_stable(raw_params_adam, distances_torch, z_centered_torch, NUGGET)\n",
    "    \n",
    "    if torch.isinf(loss) or torch.isnan(loss):\n",
    "        loss = torch.tensor(1e15, device=loss.device)\n",
    "        break\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer_adam.step()\n",
    "    final_loss_adam = loss\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0: \n",
    "        phi1 = raw_params_adam[0].pow(2).item()\n",
    "        phi2 = raw_params_adam[1].pow(2).item()\n",
    "        current_sigma2 = phi1 / (phi2 + 1e-6)\n",
    "        current_a = 1.0 / (phi2 + 1e-6)\n",
    "        grad_phi1 = raw_params_adam.grad[0].item() if raw_params_adam.grad is not None else 0.0\n",
    "        grad_phi2 = raw_params_adam.grad[1].item() if raw_params_adam.grad is not None else 0.0\n",
    "        print(f\"Adam Epoch {epoch + 1}/{ADAM_ITERATIONS}, NLL: {loss.item():.2f}, Params: [ÏƒÂ²: {current_sigma2:.3f}, a: {current_a:.3f}], Grads: [Î¦1_raw: {grad_phi1:.4f}, Î¦2_raw: {grad_phi2:.4f}]\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. Display Results\n",
    "# ----------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"TARGET PARAMETERS: Variance (ÏƒÂ²)={SIGMA2_TRUE}, Range (a)={RANGE_A_TRUE}, Nugget (Î·Â²)={NUGGET}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# L-BFGS Results\n",
    "phi1_lbfgs = raw_params_lbfgs[0].pow(2).detach().numpy().item()\n",
    "phi2_lbfgs = raw_params_lbfgs[1].pow(2).detach().numpy().item()\n",
    "fitted_sigma2_lbfgs = phi1_lbfgs / (phi2_lbfgs + 1e-6)\n",
    "fitted_range_a_lbfgs = 1.0 / (phi2_lbfgs + 1e-6)\n",
    "\n",
    "print(\"âœ¨ PyTorch L-BFGS Results (Stable Reparameterization):\")\n",
    "print(f\"  * Fitted Variance (ÏƒÂ²): {fitted_sigma2_lbfgs:.3f} (Target: {SIGMA2_TRUE})\")\n",
    "print(f\"  * Fitted Range (a): {fitted_range_a_lbfgs:.3f} (Target: {RANGE_A_TRUE})\")\n",
    "print(f\"  * Fitted Ratio (Î¦â‚=ÏƒÂ²/a): {phi1_lbfgs:.3f} (Target Ratio: {PHI1_TARGET:.3f})\")\n",
    "print(f\"  * Final -LL Value: {final_loss_lbfgs.item():.2f}\")\n",
    "print(f\"  * Optimization Steps: {LBFGS_MAX_STEPS} steps\")\n",
    "\n",
    "# Adam Results\n",
    "phi1_adam = raw_params_adam[0].pow(2).detach().numpy().item()\n",
    "phi2_adam = raw_params_adam[1].pow(2).detach().numpy().item()\n",
    "fitted_sigma2_adam = phi1_adam / (phi2_adam + 1e-6)\n",
    "fitted_range_a_adam = 1.0 / (phi2_adam + 1e-6)\n",
    "\n",
    "print(\"\\nðŸš€ PyTorch Adam Results (Stable Reparameterization):\")\n",
    "print(f\"  * Fitted Variance (ÏƒÂ²): {fitted_sigma2_adam:.3f} (Target: {SIGMA2_TRUE})\")\n",
    "print(f\"  * Fitted Range (a): {fitted_range_a_adam:.3f} (Target: {RANGE_A_TRUE})\")\n",
    "print(f\"  * Fitted Ratio (Î¦â‚=ÏƒÂ²/a): {phi1_adam:.3f} (Target Ratio: {PHI1_TARGET:.3f})\")\n",
    "print(f\"  * Final -LL Value: {final_loss_adam.item():.2f}\")\n",
    "print(f\"  * Optimization Steps: {ADAM_ITERATIONS} epochs\")\n",
    "print(\"=\"*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ccca2ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Generation ---\n",
      "--- Data Generation Complete ---\n",
      "\n",
      "--- A. Starting MLE Optimization (PyTorch L-BFGS) - STABLE ---\n",
      "LBFGS Step 5/50, NLL: 1812.62, Params: [ÏƒÂ²: 18.531, a: 0.779], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: 0.0001]\n",
      "LBFGS Step 10/50, NLL: 1812.62, Params: [ÏƒÂ²: 18.531, a: 0.779], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: 0.0001]\n",
      "LBFGS Step 15/50, NLL: 1812.62, Params: [ÏƒÂ²: 18.531, a: 0.779], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: 0.0001]\n",
      "LBFGS Step 20/50, NLL: 1812.62, Params: [ÏƒÂ²: 18.531, a: 0.779], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: 0.0001]\n",
      "LBFGS Step 25/50, NLL: 1812.62, Params: [ÏƒÂ²: 18.531, a: 0.779], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: 0.0001]\n",
      "LBFGS Step 30/50, NLL: 1812.62, Params: [ÏƒÂ²: 18.531, a: 0.779], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: 0.0001]\n",
      "LBFGS Step 35/50, NLL: 1812.62, Params: [ÏƒÂ²: 18.531, a: 0.779], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: 0.0001]\n",
      "LBFGS Step 40/50, NLL: 1812.62, Params: [ÏƒÂ²: 18.531, a: 0.779], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: 0.0001]\n",
      "LBFGS Step 45/50, NLL: 1812.62, Params: [ÏƒÂ²: 18.531, a: 0.779], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: 0.0001]\n",
      "LBFGS Step 50/50, NLL: 1812.62, Params: [ÏƒÂ²: 18.531, a: 0.779], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: 0.0001]\n",
      "\n",
      "--- B. Starting MLE Optimization (PyTorch Adam) - STABLE ---\n",
      "Adam Epoch 50/500, NLL: 1812.89, Params: [ÏƒÂ²: 18.326, a: 0.730], Grads: [Î¦1_raw: 3.7439, Î¦2_raw: 0.2424]\n",
      "Adam Epoch 100/500, NLL: 1812.62, Params: [ÏƒÂ²: 18.548, a: 0.778], Grads: [Î¦1_raw: 0.1449, Î¦2_raw: -0.0654]\n",
      "Adam Epoch 150/500, NLL: 1812.62, Params: [ÏƒÂ²: 18.536, a: 0.779], Grads: [Î¦1_raw: -0.0171, Î¦2_raw: -0.0126]\n",
      "Adam Epoch 200/500, NLL: 1812.62, Params: [ÏƒÂ²: 18.531, a: 0.779], Grads: [Î¦1_raw: 0.0013, Î¦2_raw: 0.0011]\n",
      "Adam Epoch 250/500, NLL: 1812.62, Params: [ÏƒÂ²: 18.531, a: 0.779], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: -0.0001]\n",
      "Adam Epoch 300/500, NLL: 1812.62, Params: [ÏƒÂ²: 18.531, a: 0.779], Grads: [Î¦1_raw: 0.0000, Î¦2_raw: -0.0001]\n",
      "Adam Epoch 350/500, NLL: 1812.62, Params: [ÏƒÂ²: 18.531, a: 0.779], Grads: [Î¦1_raw: -0.0000, Î¦2_raw: 0.0001]\n",
      "Adam Epoch 400/500, NLL: 1812.62, Params: [ÏƒÂ²: 18.531, a: 0.779], Grads: [Î¦1_raw: -0.0000, Î¦2_raw: 0.0001]\n",
      "Adam Epoch 450/500, NLL: 1812.62, Params: [ÏƒÂ²: 18.531, a: 0.779], Grads: [Î¦1_raw: 0.0000, Î¦2_raw: -0.0001]\n",
      "Adam Epoch 500/500, NLL: 1812.62, Params: [ÏƒÂ²: 18.531, a: 0.779], Grads: [Î¦1_raw: -0.0000, Î¦2_raw: 0.0001]\n",
      "\n",
      "==================================================\n",
      "TARGET PARAMETERS: Variance (ÏƒÂ²)=25.0, Range (a)=1, Nugget (Î·Â²)=3.0\n",
      "==================================================\n",
      "âœ¨ PyTorch L-BFGS Results (Stable Reparameterization):\n",
      "  * Fitted Variance (ÏƒÂ²): 18.531 (Target: 25.0)\n",
      "  * Fitted Range (a): 0.779 (Target: 1)\n",
      "  * Fitted Ratio (Î¦â‚=ÏƒÂ²/a): 23.797 (Target Ratio: 25.000)\n",
      "  * Final -LL Value: 1812.62\n",
      "  * Optimization Steps: 50 steps\n",
      "\n",
      "ðŸš€ PyTorch Adam Results (Stable Reparameterization):\n",
      "  * Fitted Variance (ÏƒÂ²): 18.531 (Target: 25.0)\n",
      "  * Fitted Range (a): 0.779 (Target: 1)\n",
      "  * Fitted Ratio (Î¦â‚=ÏƒÂ²/a): 23.797 (Target Ratio: 25.000)\n",
      "  * Final -LL Value: 1812.62\n",
      "  * Optimization Steps: 500 epochs\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# --- 0. Global Parameters and Utility Functions ---\n",
    "N_SPATIAL_POINTS = 1120\n",
    "N_DAYS = 31\n",
    "N_HOURS_PER_DAY = 8\n",
    "N_FEATURES = 4\n",
    "GRID_X = 40  \n",
    "GRID_Y = 28  \n",
    "LAT_MIN, LAT_MAX = 0, 5\n",
    "LON_MIN, LON_MAX = 113, 123\n",
    "BASE_DATE = '2024_07_y24m07day' \n",
    "\n",
    "# Exponential Kernel Parameters (Targets)\n",
    "SIGMA2_TRUE = 25.0 # TARGET Variance\n",
    "RANGE_A_TRUE = 1    # TARGET Range\n",
    "NUGGET = 3.0     # FIXED Nugget\n",
    "\n",
    "# Optimization Setup\n",
    "ADAM_ITERATIONS = 500\n",
    "ADAM_LEARNING_RATE = 0.01\n",
    "\n",
    "# L-BFGS Setup\n",
    "LBFGS_MAX_STEPS = 50 \n",
    "LBFGS_MAX_EVAL = 50 \n",
    "\n",
    "OZONE_MEAN = 240.0\n",
    "\n",
    "# --- COVARIANCE FUNCTIONS ---\n",
    "\n",
    "def exponential_covariance_numpy(distances, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (NumPy for Generation).\"\"\"\n",
    "    # Cov(h) = sigma^2 * exp(-h/a)\n",
    "    cov = sigma2 * np.exp(-distances / a)\n",
    "    if distances.shape[0] == distances.shape[1]:\n",
    "        cov[np.diag_indices_from(distances)] += (nugget + 1e-6)\n",
    "    return cov\n",
    "\n",
    "def exponential_covariance_torch(distances_torch, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (PyTorch for Optimization).\"\"\"\n",
    "    cov = sigma2 * torch.exp(-distances_torch / a)\n",
    "    \n",
    "    # Add nugget effect + jitter to the diagonal\n",
    "    if distances_torch.shape[0] == distances_torch.shape[1]:\n",
    "        jitter = 1e-6 \n",
    "        diag_mask = torch.eye(cov.shape[0], device=cov.device)\n",
    "        cov = cov + diag_mask * (nugget + jitter)\n",
    "    return cov\n",
    "\n",
    "# --- SHARED NLL Function using STABLE REPARAMETERIZATION ---\n",
    "def neg_log_likelihood_torch_stable(raw_params_phi, distances_torch, z_centered_torch, fixed_nugget):\n",
    "    \"\"\"\n",
    "    Calculates -LL for PyTorch (optimizing Stable Reparameterization).\n",
    "    raw_params_phi[0] = raw_phi1_sqrt (for sigma2/a)\n",
    "    raw_params_phi[1] = raw_phi2_sqrt (for 1/a)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Apply Square Reparameterization to raw parameters to ensure positivity\n",
    "    phi1 = raw_params_phi[0].pow(2).squeeze() # Phi1 = sigma2 / a (TARGET RATIO)\n",
    "    phi2 = raw_params_phi[1].pow(2).squeeze() # Phi2 = 1 / a (INVERSE RANGE)\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # 2. Derive Original Parameters\n",
    "    range_a = 1.0 / (phi2 + epsilon)          # Range: a = 1 / Phi2\n",
    "    sigma2 = phi1 / (phi2 + epsilon)          # Variance: sigma2 = Phi1 / Phi2\n",
    "    \n",
    "    C = exponential_covariance_torch(distances_torch, sigma2, range_a, fixed_nugget)\n",
    "    \n",
    "    try:\n",
    "        # Use PyTorch linear algebra functions: Cholesky decomposition\n",
    "        L = torch.linalg.cholesky(C)\n",
    "        log_det = 2.0 * torch.sum(torch.log(torch.diag(L)))\n",
    "        alpha = torch.linalg.solve(C, z_centered_torch.unsqueeze(1))\n",
    "        quad_term = z_centered_torch.unsqueeze(0) @ alpha\n",
    "        neg_LL = 0.5 * log_det + 0.5 * quad_term.squeeze()\n",
    "        return neg_LL\n",
    "    except RuntimeError:\n",
    "        return torch.tensor(1e15, device=C.device)\n",
    "\n",
    "\n",
    "# --- Data Generation Function (Unchanged) ---\n",
    "def generate_ozone_data_map(coords, sigma2, a, nugget, mean, time_index):\n",
    "    n_points = coords.shape[0]\n",
    "    distances = cdist(coords, coords, metric='euclidean')\n",
    "    Cov = exponential_covariance_numpy(distances, sigma2, a, nugget) \n",
    "    Cov = (Cov + Cov.T) / 2\n",
    "    \n",
    "    try:\n",
    "        L = np.linalg.cholesky(Cov)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.zeros((n_points, N_FEATURES))\n",
    "\n",
    "    W = np.random.normal(0, 1, size=(n_points, 1))\n",
    "    Z_correlated = L @ W\n",
    "    ozone_values = mean + Z_correlated\n",
    "    \n",
    "    data_np = np.zeros((n_points, N_FEATURES))\n",
    "    data_np[:, 0:1] = ozone_values             \n",
    "    data_np[:, 1] = coords[:, 1] * 10 + 2      \n",
    "    data_np[:, 2] = coords[:, 0] * 40 + 250    \n",
    "    data_np[:, 3] = time_index                 \n",
    "    return data_np\n",
    "\n",
    "\n",
    "# --- 1. Data Generation Execution ---\n",
    "df_day_aggregated_list = []\n",
    "print(\"--- Starting Data Generation ---\")\n",
    "lat_coords = np.linspace(LAT_MIN, LAT_MAX, GRID_Y)\n",
    "lon_coords = np.linspace(LON_MIN, LON_MAX, GRID_X)\n",
    "coords_latlon = np.array([[lat, lon] for lat in lat_coords for lon in lon_coords])\n",
    "\n",
    "# Generate only one hour of data for fitting the spatial model\n",
    "data_np = generate_ozone_data_map(\n",
    "    coords_latlon, SIGMA2_TRUE, RANGE_A_TRUE, NUGGET, OZONE_MEAN, 21.0\n",
    ")\n",
    "df_day_aggregated_list.append(torch.tensor(data_np, dtype=torch.float))\n",
    "\n",
    "print(\"--- Data Generation Complete ---\")\n",
    "\n",
    "# --- 2. Data Preparation ---\n",
    "data_to_fit = df_day_aggregated_list[0][:N_SPATIAL_POINTS, :] \n",
    "z_data = data_to_fit[:, 0].numpy()\n",
    "coordinates = coords_latlon[:, [1, 0]] \n",
    "distances_np = cdist(coordinates, coordinates, metric='euclidean')\n",
    "z_centered_np = z_data - np.mean(z_data)\n",
    "\n",
    "# Convert to Torch Tensors\n",
    "distances_torch = torch.tensor(distances_np, dtype=torch.float)\n",
    "z_centered_torch = torch.tensor(z_centered_np, dtype=torch.float)\n",
    "\n",
    "# --- Initial Parameter Setup (Shared) ---\n",
    "# Calculate target stable parameters\n",
    "# PHI1 is the Target Ratio: sigma2 / a\n",
    "PHI1_TARGET = SIGMA2_TRUE / RANGE_A_TRUE    # 42.0 / 1.5 = 28.0\n",
    "# PHI2 is the Inverse Range: 1 / a\n",
    "PHI2_TARGET = 1.0 / RANGE_A_TRUE            # 1.0 / 1.5 = 0.667\n",
    "\n",
    "# ðŸ’¡ NEW: Initial guess for raw_phi1_sqrt and raw_phi2_sqrt. \n",
    "# Start Phi1 (Target 28.0) at 32.0 (sqrt 5.66)\n",
    "raw_phi1_sqrt_start = np.sqrt(PHI1_TARGET + 4.0) \n",
    "# Start Phi2 (Target 0.667) at 0.8 (sqrt 0.64)\n",
    "raw_phi2_sqrt_start = np.sqrt(PHI2_TARGET + 0.133) \n",
    "\n",
    "initial_params_stable = [raw_phi1_sqrt_start, raw_phi2_sqrt_start]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# A. Optimization with L-BFGS (PyTorch) - STABLE\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Reset parameters for LBFGS\n",
    "raw_params_lbfgs = torch.tensor(\n",
    "    initial_params_stable, \n",
    "    dtype=torch.float, \n",
    "    requires_grad=True\n",
    ")\n",
    "\n",
    "optimizer_lbfgs = optim.LBFGS(\n",
    "    [raw_params_lbfgs], \n",
    "    lr=1.0, \n",
    "    max_iter=LBFGS_MAX_STEPS,\n",
    "    max_eval=LBFGS_MAX_EVAL \n",
    ")\n",
    "\n",
    "final_loss_lbfgs = torch.tensor(0.0)\n",
    "print(\"\\n--- A. Starting MLE Optimization (PyTorch L-BFGS) - STABLE ---\")\n",
    "\n",
    "# L-BFGS requires a \"closure\" function\n",
    "def closure_lbfgs():\n",
    "    optimizer_lbfgs.zero_grad()\n",
    "    # Use the stable NLL function\n",
    "    loss = neg_log_likelihood_torch_stable(raw_params_lbfgs, distances_torch, z_centered_torch, NUGGET)\n",
    "    if not torch.isinf(loss) and not torch.isnan(loss):\n",
    "        loss.backward()\n",
    "    return loss\n",
    "\n",
    "# L-BFGS Optimization Loop\n",
    "for step in range(LBFGS_MAX_STEPS):\n",
    "    loss = optimizer_lbfgs.step(closure_lbfgs)\n",
    "    final_loss_lbfgs = loss\n",
    "    \n",
    "    if (step + 1) % 5 == 0: \n",
    "        phi1 = raw_params_lbfgs[0].pow(2).item()\n",
    "        phi2 = raw_params_lbfgs[1].pow(2).item()\n",
    "        current_sigma2 = phi1 / (phi2 + 1e-6)\n",
    "        current_a = 1.0 / (phi2 + 1e-6)\n",
    "        grad_phi1 = raw_params_lbfgs.grad[0].item() if raw_params_lbfgs.grad is not None else 0.0\n",
    "        grad_phi2 = raw_params_lbfgs.grad[1].item() if raw_params_lbfgs.grad is not None else 0.0\n",
    "        print(f\"LBFGS Step {step + 1}/{LBFGS_MAX_STEPS}, NLL: {loss.item():.2f}, Params: [ÏƒÂ²: {current_sigma2:.3f}, a: {current_a:.3f}], Grads: [Î¦1_raw: {grad_phi1:.4f}, Î¦2_raw: {grad_phi2:.4f}]\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# B. Optimization with Adam (PyTorch) - STABLE\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Reset parameters for Adam (Use the same start point)\n",
    "raw_params_adam = torch.tensor(\n",
    "    initial_params_stable, \n",
    "    dtype=torch.float, \n",
    "    requires_grad=True\n",
    ")\n",
    "\n",
    "optimizer_adam = optim.Adam(\n",
    "    [raw_params_adam], \n",
    "    lr=ADAM_LEARNING_RATE\n",
    ")\n",
    "\n",
    "final_loss_adam = torch.tensor(0.0)\n",
    "print(f\"\\n--- B. Starting MLE Optimization (PyTorch Adam) - STABLE ---\")\n",
    "\n",
    "# Adam Optimization Loop\n",
    "for epoch in range(ADAM_ITERATIONS):\n",
    "    optimizer_adam.zero_grad()\n",
    "    \n",
    "    # Use the stable NLL function\n",
    "    loss = neg_log_likelihood_torch_stable(raw_params_adam, distances_torch, z_centered_torch, NUGGET)\n",
    "    \n",
    "    if torch.isinf(loss) or torch.isnan(loss):\n",
    "        loss = torch.tensor(1e15, device=loss.device)\n",
    "        break\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer_adam.step()\n",
    "    final_loss_adam = loss\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0: \n",
    "        phi1 = raw_params_adam[0].pow(2).item()\n",
    "        phi2 = raw_params_adam[1].pow(2).item()\n",
    "        current_sigma2 = phi1 / (phi2 + 1e-6)\n",
    "        current_a = 1.0 / (phi2 + 1e-6)\n",
    "        grad_phi1 = raw_params_adam.grad[0].item() if raw_params_adam.grad is not None else 0.0\n",
    "        grad_phi2 = raw_params_adam.grad[1].item() if raw_params_adam.grad is not None else 0.0\n",
    "        print(f\"Adam Epoch {epoch + 1}/{ADAM_ITERATIONS}, NLL: {loss.item():.2f}, Params: [ÏƒÂ²: {current_sigma2:.3f}, a: {current_a:.3f}], Grads: [Î¦1_raw: {grad_phi1:.4f}, Î¦2_raw: {grad_phi2:.4f}]\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. Display Results\n",
    "# ----------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"TARGET PARAMETERS: Variance (ÏƒÂ²)={SIGMA2_TRUE}, Range (a)={RANGE_A_TRUE}, Nugget (Î·Â²)={NUGGET}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# L-BFGS Results\n",
    "phi1_lbfgs = raw_params_lbfgs[0].pow(2).detach().numpy().item()\n",
    "phi2_lbfgs = raw_params_lbfgs[1].pow(2).detach().numpy().item()\n",
    "fitted_sigma2_lbfgs = phi1_lbfgs / (phi2_lbfgs + 1e-6)\n",
    "fitted_range_a_lbfgs = 1.0 / (phi2_lbfgs + 1e-6)\n",
    "\n",
    "print(\"âœ¨ PyTorch L-BFGS Results (Stable Reparameterization):\")\n",
    "print(f\"  * Fitted Variance (ÏƒÂ²): {fitted_sigma2_lbfgs:.3f} (Target: {SIGMA2_TRUE})\")\n",
    "print(f\"  * Fitted Range (a): {fitted_range_a_lbfgs:.3f} (Target: {RANGE_A_TRUE})\")\n",
    "print(f\"  * Fitted Ratio (Î¦â‚=ÏƒÂ²/a): {phi1_lbfgs:.3f} (Target Ratio: {PHI1_TARGET:.3f})\") # <-- NEW\n",
    "print(f\"  * Final -LL Value: {final_loss_lbfgs.item():.2f}\")\n",
    "print(f\"  * Optimization Steps: {LBFGS_MAX_STEPS} steps\")\n",
    "\n",
    "# Adam Results\n",
    "phi1_adam = raw_params_adam[0].pow(2).detach().numpy().item()\n",
    "phi2_adam = raw_params_adam[1].pow(2).detach().numpy().item()\n",
    "fitted_sigma2_adam = phi1_adam / (phi2_adam + 1e-6)\n",
    "fitted_range_a_adam = 1.0 / (phi2_adam + 1e-6)\n",
    "\n",
    "print(\"\\nðŸš€ PyTorch Adam Results (Stable Reparameterization):\")\n",
    "print(f\"  * Fitted Variance (ÏƒÂ²): {fitted_sigma2_adam:.3f} (Target: {SIGMA2_TRUE})\")\n",
    "print(f\"  * Fitted Range (a): {fitted_range_a_adam:.3f} (Target: {RANGE_A_TRUE})\")\n",
    "print(f\"  * Fitted Ratio (Î¦â‚=ÏƒÂ²/a): {phi1_adam:.3f} (Target Ratio: {PHI1_TARGET:.3f})\") # <-- NEW\n",
    "print(f\"  * Final -LL Value: {final_loss_adam.item():.2f}\")\n",
    "print(f\"  * Optimization Steps: {ADAM_ITERATIONS} epochs\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41734385",
   "metadata": {},
   "source": [
    "# L BFGS vs Adams  1120 reparametrization for anisotrpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "916aee43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Generation (Anisotropic) ---\n",
      "--- Data Generation Complete ---\n",
      "\n",
      "--- A. Starting MLE Optimization (PyTorch L-BFGS) - STABLE ---\n",
      "LBFGS Step 5/50, NLL: 1552.12, Params: [ÏƒÂ²: 18.863, a: 0.883, Î¸â‚ƒ-ratio: 2.336], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: 0.0000, Î¦3_raw: -0.0001]\n",
      "LBFGS Step 10/50, NLL: 1552.12, Params: [ÏƒÂ²: 18.863, a: 0.883, Î¸â‚ƒ-ratio: 2.336], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: 0.0000, Î¦3_raw: -0.0001]\n",
      "LBFGS Step 15/50, NLL: 1552.12, Params: [ÏƒÂ²: 18.863, a: 0.883, Î¸â‚ƒ-ratio: 2.336], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: 0.0000, Î¦3_raw: -0.0001]\n",
      "LBFGS Step 20/50, NLL: 1552.12, Params: [ÏƒÂ²: 18.863, a: 0.883, Î¸â‚ƒ-ratio: 2.336], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: 0.0000, Î¦3_raw: -0.0001]\n",
      "LBFGS Step 25/50, NLL: 1552.12, Params: [ÏƒÂ²: 18.863, a: 0.883, Î¸â‚ƒ-ratio: 2.336], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: 0.0000, Î¦3_raw: -0.0001]\n",
      "LBFGS Step 30/50, NLL: 1552.12, Params: [ÏƒÂ²: 18.863, a: 0.883, Î¸â‚ƒ-ratio: 2.336], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: 0.0000, Î¦3_raw: -0.0001]\n",
      "LBFGS Step 35/50, NLL: 1552.12, Params: [ÏƒÂ²: 18.863, a: 0.883, Î¸â‚ƒ-ratio: 2.336], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: 0.0000, Î¦3_raw: -0.0001]\n",
      "LBFGS Step 40/50, NLL: 1552.12, Params: [ÏƒÂ²: 18.863, a: 0.883, Î¸â‚ƒ-ratio: 2.336], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: 0.0000, Î¦3_raw: -0.0001]\n",
      "LBFGS Step 45/50, NLL: 1552.12, Params: [ÏƒÂ²: 18.863, a: 0.883, Î¸â‚ƒ-ratio: 2.336], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: 0.0000, Î¦3_raw: -0.0001]\n",
      "LBFGS Step 50/50, NLL: 1552.12, Params: [ÏƒÂ²: 18.863, a: 0.883, Î¸â‚ƒ-ratio: 2.336], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: 0.0000, Î¦3_raw: -0.0001]\n",
      "\n",
      "--- B. Starting MLE Optimization (PyTorch Adam) - STABLE ---\n",
      "Adam Epoch 50/500, NLL: 1552.64, Params: [ÏƒÂ²: 18.831, a: 0.925, Î¸â‚ƒ-ratio: 2.147], Grads: [Î¦1_raw: -0.4151, Î¦2_raw: 0.0672, Î¦3_raw: -5.3637]\n",
      "Adam Epoch 100/500, NLL: 1552.13, Params: [ÏƒÂ²: 18.892, a: 0.891, Î¸â‚ƒ-ratio: 2.305], Grads: [Î¦1_raw: -0.0841, Î¦2_raw: -0.0464, Î¦3_raw: -0.7663]\n",
      "Adam Epoch 150/500, NLL: 1552.12, Params: [ÏƒÂ²: 18.863, a: 0.883, Î¸â‚ƒ-ratio: 2.334], Grads: [Î¦1_raw: -0.0082, Î¦2_raw: -0.0003, Î¦3_raw: -0.0483]\n",
      "Adam Epoch 200/500, NLL: 1552.12, Params: [ÏƒÂ²: 18.863, a: 0.883, Î¸â‚ƒ-ratio: 2.336], Grads: [Î¦1_raw: 0.0005, Î¦2_raw: 0.0004, Î¦3_raw: 0.0022]\n",
      "Adam Epoch 250/500, NLL: 1552.12, Params: [ÏƒÂ²: 18.863, a: 0.883, Î¸â‚ƒ-ratio: 2.336], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: -0.0001, Î¦3_raw: 0.0003]\n",
      "Adam Epoch 300/500, NLL: 1552.11, Params: [ÏƒÂ²: 18.863, a: 0.883, Î¸â‚ƒ-ratio: 2.336], Grads: [Î¦1_raw: 0.0001, Î¦2_raw: 0.0002, Î¦3_raw: -0.0001]\n",
      "Adam Epoch 350/500, NLL: 1552.12, Params: [ÏƒÂ²: 18.863, a: 0.883, Î¸â‚ƒ-ratio: 2.336], Grads: [Î¦1_raw: 0.0000, Î¦2_raw: 0.0001, Î¦3_raw: 0.0000]\n",
      "Adam Epoch 400/500, NLL: 1552.12, Params: [ÏƒÂ²: 18.863, a: 0.883, Î¸â‚ƒ-ratio: 2.336], Grads: [Î¦1_raw: -0.0001, Î¦2_raw: 0.0003, Î¦3_raw: -0.0001]\n",
      "Adam Epoch 450/500, NLL: 1552.11, Params: [ÏƒÂ²: 18.863, a: 0.883, Î¸â‚ƒ-ratio: 2.336], Grads: [Î¦1_raw: 0.0000, Î¦2_raw: 0.0003, Î¦3_raw: -0.0000]\n",
      "Adam Epoch 500/500, NLL: 1552.12, Params: [ÏƒÂ²: 18.863, a: 0.883, Î¸â‚ƒ-ratio: 2.336], Grads: [Î¦1_raw: 0.0001, Î¦2_raw: 0.0002, Î¦3_raw: -0.0001]\n",
      "\n",
      "===========================================================================\n",
      "TARGET PARAMETERS: Variance (ÏƒÂ²)=30.0, Range (a)=1.5, Anisotropy (Î¸â‚ƒ-ratio)=2.0\n",
      "                 (Derived Targets: Î¸â‚=20.000, Î¸â‚‚=0.667, Ï†â‚ƒ (Î¸â‚ƒÂ²)=4.000)\n",
      "===========================================================================\n",
      "âœ¨ PyTorch L-BFGS Results (Stable Reparameterization):\n",
      "  * Fitted Variance (ÏƒÂ²): 18.863 (Target: 30.0)\n",
      "  * Fitted Range (a): 0.883 (Target: 1.5)\n",
      "  * Fitted Anisotropy (Î¸â‚ƒ-ratio): 2.336 (Target: 2.0)\n",
      "  ---\n",
      "  * Fitted Î¸â‚ (ÏƒÂ²/a): 21.372 (Target: 20.000)\n",
      "  * Fitted Î¸â‚‚ (1/a): 1.133 (Target: 0.667)\n",
      "  * Fitted Ï†â‚ƒ (Î¸â‚ƒÂ²): 5.457 (Target: 4.000)\n",
      "  ---\n",
      "  * Final -LL Value: 1552.12\n",
      "  * Optimization Steps: 50 steps\n",
      "\n",
      "ðŸš€ PyTorch Adam Results (Stable Reparameterization):\n",
      "  * Fitted Variance (ÏƒÂ²): 18.863 (Target: 30.0)\n",
      "  * Fitted Range (a): 0.883 (Target: 1.5)\n",
      "  * Fitted Anisotropy (Î¸â‚ƒ-ratio): 2.336 (Target: 2.0)\n",
      "  ---\n",
      "  * Fitted Î¸â‚ (ÏƒÂ²/a): 21.372 (Target: 20.000)\n",
      "  * Fitted Î¸â‚‚ (1/a): 1.133 (Target: 0.667)\n",
      "  * Fitted Ï†â‚ƒ (Î¸â‚ƒÂ²): 5.457 (Target: 4.000)\n",
      "  ---\n",
      "  * Final -LL Value: 1552.12\n",
      "  * Optimization Steps: 500 epochs\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# --- 0. Global Parameters and Utility Functions ---\n",
    "N_SPATIAL_POINTS = 1120\n",
    "N_DAYS = 31\n",
    "N_HOURS_PER_DAY = 8\n",
    "N_FEATURES = 4\n",
    "GRID_X = 40  \n",
    "GRID_Y = 28  \n",
    "LAT_MIN, LAT_MAX = 0, 5\n",
    "LON_MIN, LON_MAX = 113, 123\n",
    "BASE_DATE = '2024_07_y24m07day' \n",
    "\n",
    "# Exponential Kernel Parameters (Targets)\n",
    "SIGMA2_TRUE = 30.0      # TARGET Variance (theta_1 / theta_2)\n",
    "RANGE_A_TRUE = 1.5      # TARGET Range (1 / theta_2)\n",
    "ANISOTROPY_RATIO_TRUE = 2.0 # TARGET Anisotropy (theta_3)\n",
    "# ðŸ’¡ NEW: Define the target for the parameter we actually optimize (theta_3^2)\n",
    "PHI3_TARGET_SQ = ANISOTROPY_RATIO_TRUE**2 # Target = 4.0\n",
    "NUGGET = 2.0          # FIXED Nugget\n",
    "\n",
    "# Optimization Setup\n",
    "ADAM_ITERATIONS = 500\n",
    "ADAM_LEARNING_RATE = 0.01\n",
    "\n",
    "# L-BFGS Setup\n",
    "LBFGS_MAX_STEPS = 50 \n",
    "LBFGS_MAX_EVAL = 50 \n",
    "\n",
    "OZONE_MEAN = 240.0\n",
    "\n",
    "# --- COVARIANCE FUNCTIONS ---\n",
    "\n",
    "def exponential_covariance_numpy(distances, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (NumPy for Generation).\"\"\"\n",
    "    cov = sigma2 * np.exp(-distances / a)\n",
    "    if distances.shape[0] == distances.shape[1]:\n",
    "        cov[np.diag_indices_from(distances)] += (nugget + 1e-6)\n",
    "    return cov\n",
    "\n",
    "def exponential_covariance_torch(distances_torch, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (PyTorch for Optimization).\"\"\"\n",
    "    cov = sigma2 * torch.exp(-distances_torch / a)\n",
    "    \n",
    "    if distances_torch.shape[0] == distances_torch.shape[1]:\n",
    "        jitter = 1e-6 \n",
    "        diag_mask = torch.eye(cov.shape[0], device=cov.device)\n",
    "        cov = cov + diag_mask * (nugget + jitter)\n",
    "    return cov\n",
    "\n",
    "# --- ðŸ’¡ MODIFIED NLL Function (STABLE FIX) ---\n",
    "def neg_log_likelihood_torch_stable(raw_params_phi, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch, fixed_nugget):\n",
    "    \"\"\"\n",
    "    Calculates -LL for PyTorch (optimizing Stable Reparameterization).\n",
    "    raw_params_phi[0] = raw_phi1_sqrt (for theta_1 = sigma2/a)\n",
    "    raw_params_phi[1] = raw_phi2_sqrt (for theta_2 = 1/a)\n",
    "    raw_params_phi[2] = raw_phi3_sqrt (for phi_3 = theta_3^2)\n",
    "    \"\"\"\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # 1. ðŸ’¡ FIX: Apply epsilon *after* pow(2) to prevent parameters from being 0\n",
    "    phi1 = raw_params_phi[0].pow(2).squeeze() + epsilon # theta_1 = sigma2 / a\n",
    "    phi2 = raw_params_phi[1].pow(2).squeeze() + epsilon # theta_2 = 1 / a\n",
    "    phi3 = raw_params_phi[2].pow(2).squeeze() + epsilon # ðŸ’¡ phi_3 = theta_3^2 (the squared ratio)\n",
    "    \n",
    "    # 2. Derive Original Parameters (no epsilon needed here now)\n",
    "    range_a = 1.0 / phi2          # Range: a = 1 / theta_2\n",
    "    sigma2 = phi1 / phi2          # Variance: sigma2 = theta_1 / theta_2\n",
    "    \n",
    "    # 3. Compute Anisotropic Distance (no epsilon needed here now)\n",
    "    # d_aniso^2 = (d_lon / theta_3)^2 + d_lat^2 = d_lon^2 / theta_3^2 + d_lat^2\n",
    "    aniso_dist_sq = (d_lon_sq_torch / phi3) + d_lat_sq_torch\n",
    "    \n",
    "    # ðŸ’¡ Add small epsilon *inside* sqrt to prevent inf gradient at 0\n",
    "    aniso_dist = torch.sqrt(aniso_dist_sq + epsilon)\n",
    "    \n",
    "    # 4. Calculate Covariance Matrix C\n",
    "    C = exponential_covariance_torch(aniso_dist, sigma2, range_a, fixed_nugget)\n",
    "    \n",
    "    try:\n",
    "        L = torch.linalg.cholesky(C)\n",
    "        log_det = 2.0 * torch.sum(torch.log(torch.diag(L)))\n",
    "        alpha = torch.linalg.solve(C, z_centered_torch.unsqueeze(1))\n",
    "        quad_term = z_centered_torch.unsqueeze(0) @ alpha\n",
    "        neg_LL = 0.5 * log_det + 0.5 * quad_term.squeeze()\n",
    "        \n",
    "        # Check for nan/inf *before* returning\n",
    "        if torch.isnan(neg_LL) or torch.isinf(neg_LL):\n",
    "            return torch.tensor(1e15, device=C.device, dtype=torch.float) + raw_params_phi.sum() * 0.0\n",
    "\n",
    "        return neg_LL\n",
    "    except RuntimeError:\n",
    "        # This catches Cholesky failures\n",
    "        return torch.tensor(1e15, device=C.device, dtype=torch.float) + raw_params_phi.sum() * 0.0\n",
    "\n",
    "\n",
    "# --- Data Generation Function (Unchanged) ---\n",
    "def generate_ozone_data_map(coords, sigma2, a, nugget, mean, time_index, anisotropy_ratio):\n",
    "    n_points = coords.shape[0]\n",
    "    coords_transformed = coords.copy()\n",
    "    coords_transformed[:, 1] = coords_transformed[:, 1] / anisotropy_ratio\n",
    "    \n",
    "    distances = cdist(coords_transformed, coords_transformed, metric='euclidean')\n",
    "    \n",
    "    Cov = exponential_covariance_numpy(distances, sigma2, a, nugget) \n",
    "    Cov = (Cov + Cov.T) / 2\n",
    "    \n",
    "    try:\n",
    "        L = np.linalg.cholesky(Cov)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.zeros((n_points, N_FEATURES))\n",
    "\n",
    "    W = np.random.normal(0, 1, size=(n_points, 1))\n",
    "    Z_correlated = L @ W\n",
    "    ozone_values = mean + Z_correlated\n",
    "    \n",
    "    data_np = np.zeros((n_points, N_FEATURES))\n",
    "    data_np[:, 0:1] = ozone_values             \n",
    "    data_np[:, 1] = coords[:, 1] * 10 + 2      # Original lon\n",
    "    data_np[:, 2] = coords[:, 0] * 40 + 250    # Original lat\n",
    "    data_np[:, 3] = time_index                 \n",
    "    return data_np\n",
    "\n",
    "\n",
    "# --- 1. Data Generation Execution ---\n",
    "df_day_aggregated_list = []\n",
    "print(\"--- Starting Data Generation (Anisotropic) ---\")\n",
    "lat_coords = np.linspace(LAT_MIN, LAT_MAX, GRID_Y)\n",
    "lon_coords = np.linspace(LON_MIN, LON_MAX, GRID_X)\n",
    "coords_latlon = np.array([[lat, lon] for lat in lat_coords for lon in lon_coords]) # [lat, lon]\n",
    "\n",
    "data_np = generate_ozone_data_map(\n",
    "    coords_latlon, SIGMA2_TRUE, RANGE_A_TRUE, NUGGET, OZONE_MEAN, 21.0,\n",
    "    ANISOTROPY_RATIO_TRUE\n",
    ")\n",
    "df_day_aggregated_list.append(torch.tensor(data_np, dtype=torch.float))\n",
    "\n",
    "print(\"--- Data Generation Complete ---\")\n",
    "\n",
    "# --- 2. Data Preparation (Unchanged) ---\n",
    "data_to_fit = df_day_aggregated_list[0][:N_SPATIAL_POINTS, :] \n",
    "z_data = data_to_fit[:, 0].numpy()\n",
    "coordinates = coords_latlon[:, [1, 0]] # Switch to [lon, lat]\n",
    "z_centered_np = z_data - np.mean(z_data)\n",
    "z_centered_torch = torch.tensor(z_centered_np, dtype=torch.float)\n",
    "\n",
    "lons = coordinates[:, 0:1] # (N, 1)\n",
    "lats = coordinates[:, 1:2] # (N, 1)\n",
    "d_lon_np = cdist(lons, lons, metric='euclidean')\n",
    "d_lat_np = cdist(lats, lats, metric='euclidean')\n",
    "d_lon_sq_np = np.square(d_lon_np)\n",
    "d_lat_sq_np = np.square(d_lat_np)\n",
    "\n",
    "d_lon_sq_torch = torch.tensor(d_lon_sq_np, dtype=torch.float)\n",
    "d_lat_sq_torch = torch.tensor(d_lat_sq_np, dtype=torch.float)\n",
    "\n",
    "\n",
    "# --- ðŸ’¡ MODIFIED Initial Parameter Setup ---\n",
    "# Calculate target stable parameters\n",
    "PHI1_TARGET = SIGMA2_TRUE / RANGE_A_TRUE    # theta_1 = 28.0\n",
    "PHI2_TARGET = 1.0 / RANGE_A_TRUE            # theta_2 = 0.667\n",
    "# ðŸ’¡ PHI3 is now the SQUARED ratio\n",
    "PHI3_TARGET = PHI3_TARGET_SQ                # phi_3 = 4.0\n",
    "\n",
    "# Initial guess for raw_phi_sqrt\n",
    "raw_phi1_sqrt_start = np.sqrt(PHI1_TARGET - 3.0) # Start theta_1 at 25.0\n",
    "raw_phi2_sqrt_start = np.sqrt(PHI2_TARGET - 0.1) # Start theta_2 at ~0.57\n",
    "# ðŸ’¡ Start phi_3 at 3.0 (Target is 4.0)\n",
    "raw_phi3_sqrt_start = np.sqrt(3.0) \n",
    "\n",
    "initial_params_stable = [\n",
    "    raw_phi1_sqrt_start, \n",
    "    raw_phi2_sqrt_start,\n",
    "    raw_phi3_sqrt_start\n",
    "]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# A. Optimization with L-BFGS (PyTorch) - STABLE\n",
    "# ----------------------------------------------------\n",
    "\n",
    "raw_params_lbfgs = torch.tensor(\n",
    "    initial_params_stable, \n",
    "    dtype=torch.float, \n",
    "    requires_grad=True\n",
    ")\n",
    "\n",
    "optimizer_lbfgs = optim.LBFGS(\n",
    "    [raw_params_lbfgs], \n",
    "    lr=1.0, \n",
    "    max_iter=LBFGS_MAX_STEPS,\n",
    "    max_eval=LBFGS_MAX_EVAL \n",
    ")\n",
    "\n",
    "final_loss_lbfgs = torch.tensor(0.0)\n",
    "print(\"\\n--- A. Starting MLE Optimization (PyTorch L-BFGS) - STABLE ---\")\n",
    "\n",
    "# L-BFGS requires a \"closure\" function\n",
    "def closure_lbfgs():\n",
    "    optimizer_lbfgs.zero_grad()\n",
    "    loss = neg_log_likelihood_torch_stable(\n",
    "        raw_params_lbfgs, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch, NUGGET\n",
    "    )\n",
    "    # ðŸ’¡ The 'if requires_grad' check is no longer needed\n",
    "    if not torch.isinf(loss) and not torch.isnan(loss):\n",
    "        loss.backward()\n",
    "    return loss\n",
    "\n",
    "# L-BFGS Optimization Loop\n",
    "for step in range(LBFGS_MAX_STEPS):\n",
    "    loss = optimizer_lbfgs.step(closure_lbfgs)\n",
    "    final_loss_lbfgs = loss\n",
    "    \n",
    "    if (step + 1) % 5 == 0: \n",
    "        phi1 = raw_params_lbfgs[0].pow(2).item()\n",
    "        phi2 = raw_params_lbfgs[1].pow(2).item()\n",
    "        phi3 = raw_params_lbfgs[2].pow(2).item() # ðŸ’¡ This is theta_3^2\n",
    "        \n",
    "        current_sigma2 = phi1 / (phi2 + 1e-6)\n",
    "        current_a = 1.0 / (phi2 + 1e-6)\n",
    "        # ðŸ’¡ Show the ratio by taking the sqrt\n",
    "        current_theta_3_ratio = np.sqrt(phi3) \n",
    "        \n",
    "        grad_phi1 = raw_params_lbfgs.grad[0].item() if raw_params_lbfgs.grad is not None else 0.0\n",
    "        grad_phi2 = raw_params_lbfgs.grad[1].item() if raw_params_lbfgs.grad is not None else 0.0\n",
    "        grad_phi3 = raw_params_lbfgs.grad[2].item() if raw_params_lbfgs.grad is not None else 0.0\n",
    "        print(f\"LBFGS Step {step + 1}/{LBFGS_MAX_STEPS}, NLL: {loss.item():.2f}, Params: [ÏƒÂ²: {current_sigma2:.3f}, a: {current_a:.3f}, Î¸â‚ƒ-ratio: {current_theta_3_ratio:.3f}], Grads: [Î¦1_raw: {grad_phi1:.4f}, Î¦2_raw: {grad_phi2:.4f}, Î¦3_raw: {grad_phi3:.4f}]\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# B. Optimization with Adam (PyTorch) - STABLE\n",
    "# ----------------------------------------------------\n",
    "\n",
    "raw_params_adam = torch.tensor(\n",
    "    initial_params_stable, \n",
    "    dtype=torch.float, \n",
    "    requires_grad=True\n",
    ")\n",
    "\n",
    "optimizer_adam = optim.Adam(\n",
    "    [raw_params_adam], \n",
    "    lr=ADAM_LEARNING_RATE\n",
    ")\n",
    "\n",
    "final_loss_adam = torch.tensor(0.0)\n",
    "print(f\"\\n--- B. Starting MLE Optimization (PyTorch Adam) - STABLE ---\")\n",
    "\n",
    "# Adam Optimization Loop\n",
    "for epoch in range(ADAM_ITERATIONS):\n",
    "    optimizer_adam.zero_grad()\n",
    "    \n",
    "    loss = neg_log_likelihood_torch_stable(\n",
    "        raw_params_adam, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch, NUGGET\n",
    "    )\n",
    "    \n",
    "    # ðŸ’¡ The 'if not loss.requires_grad' check is no longer needed\n",
    "    if torch.isinf(loss) or torch.isnan(loss):\n",
    "        if (epoch + 1) % 50 == 0: \n",
    "            print(f\"Adam Epoch {epoch + 1}/{ADAM_ITERATIONS}, Invalid loss. Skipping step.\")\n",
    "        continue # Skip this step\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer_adam.step()\n",
    "    final_loss_adam = loss\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0: \n",
    "        phi1 = raw_params_adam[0].pow(2).item()\n",
    "        phi2 = raw_params_adam[1].pow(2).item()\n",
    "        phi3 = raw_params_adam[2].pow(2).item() # ðŸ’¡ This is theta_3^2\n",
    "        \n",
    "        current_sigma2 = phi1 / (phi2 + 1e-6)\n",
    "        current_a = 1.0 / (phi2 + 1e-6)\n",
    "        # ðŸ’¡ Show the ratio by taking the sqrt\n",
    "        current_theta_3_ratio = np.sqrt(phi3)\n",
    "        \n",
    "        grad_phi1 = raw_params_adam.grad[0].item() if raw_params_adam.grad is not None else 0.0\n",
    "        grad_phi2 = raw_params_adam.grad[1].item() if raw_params_adam.grad is not None else 0.0\n",
    "        grad_phi3 = raw_params_adam.grad[2].item() if raw_params_adam.grad is not None else 0.0\n",
    "        print(f\"Adam Epoch {epoch + 1}/{ADAM_ITERATIONS}, NLL: {loss.item():.2f}, Params: [ÏƒÂ²: {current_sigma2:.3f}, a: {current_a:.3f}, Î¸â‚ƒ-ratio: {current_theta_3_ratio:.3f}], Grads: [Î¦1_raw: {grad_phi1:.4f}, Î¦2_raw: {grad_phi2:.4f}, Î¦3_raw: {grad_phi3:.4f}]\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. ðŸ’¡ MODIFIED Display Results\n",
    "# ----------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*75)\n",
    "print(f\"TARGET PARAMETERS: Variance (ÏƒÂ²)={SIGMA2_TRUE}, Range (a)={RANGE_A_TRUE}, Anisotropy (Î¸â‚ƒ-ratio)={ANISOTROPY_RATIO_TRUE}\")\n",
    "print(f\"                 (Derived Targets: Î¸â‚={PHI1_TARGET:.3f}, Î¸â‚‚={PHI2_TARGET:.3f}, Ï†â‚ƒ (Î¸â‚ƒÂ²)={PHI3_TARGET:.3f})\")\n",
    "print(\"=\"*75)\n",
    "\n",
    "# L-BFGS Results\n",
    "phi1_lbfgs = raw_params_lbfgs[0].pow(2).detach().numpy().item()\n",
    "phi2_lbfgs = raw_params_lbfgs[1].pow(2).detach().numpy().item()\n",
    "phi3_lbfgs = raw_params_lbfgs[2].pow(2).detach().numpy().item() # ðŸ’¡ This is theta_3^2\n",
    "fitted_sigma2_lbfgs = phi1_lbfgs / (phi2_lbfgs + 1e-6)\n",
    "fitted_range_a_lbfgs = 1.0 / (phi2_lbfgs + 1e-6)\n",
    "fitted_ratio_lbfgs = np.sqrt(phi3_lbfgs) # ðŸ’¡ This is theta_3\n",
    "\n",
    "print(\"âœ¨ PyTorch L-BFGS Results (Stable Reparameterization):\")\n",
    "print(f\"  * Fitted Variance (ÏƒÂ²): {fitted_sigma2_lbfgs:.3f} (Target: {SIGMA2_TRUE})\")\n",
    "print(f\"  * Fitted Range (a): {fitted_range_a_lbfgs:.3f} (Target: {RANGE_A_TRUE})\")\n",
    "print(f\"  * Fitted Anisotropy (Î¸â‚ƒ-ratio): {fitted_ratio_lbfgs:.3f} (Target: {ANISOTROPY_RATIO_TRUE})\")\n",
    "print(f\"  ---\")\n",
    "print(f\"  * Fitted Î¸â‚ (ÏƒÂ²/a): {phi1_lbfgs:.3f} (Target: {PHI1_TARGET:.3f})\")\n",
    "print(f\"  * Fitted Î¸â‚‚ (1/a): {phi2_lbfgs:.3f} (Target: {PHI2_TARGET:.3f})\")\n",
    "print(f\"  * Fitted Ï†â‚ƒ (Î¸â‚ƒÂ²): {phi3_lbfgs:.3f} (Target: {PHI3_TARGET:.3f})\")\n",
    "print(f\"  ---\")\n",
    "print(f\"  * Final -LL Value: {final_loss_lbfgs.item():.2f}\")\n",
    "print(f\"  * Optimization Steps: {LBFGS_MAX_STEPS} steps\")\n",
    "\n",
    "# Adam Results\n",
    "phi1_adam = raw_params_adam[0].pow(2).detach().numpy().item()\n",
    "phi2_adam = raw_params_adam[1].pow(2).detach().numpy().item()\n",
    "phi3_adam = raw_params_adam[2].pow(2).detach().numpy().item() # ðŸ’¡ This is theta_3^2\n",
    "fitted_sigma2_adam = phi1_adam / (phi2_adam + 1e-6)\n",
    "fitted_range_a_adam = 1.0 / (phi2_adam + 1e-6)\n",
    "fitted_ratio_adam = np.sqrt(phi3_adam) # ðŸ’¡ This is theta_3\n",
    "\n",
    "print(\"\\nðŸš€ PyTorch Adam Results (Stable Reparameterization):\")\n",
    "print(f\"  * Fitted Variance (ÏƒÂ²): {fitted_sigma2_adam:.3f} (Target: {SIGMA2_TRUE})\")\n",
    "print(f\"  * Fitted Range (a): {fitted_range_a_adam:.3f} (Target: {RANGE_A_TRUE})\")\n",
    "print(f\"  * Fitted Anisotropy (Î¸â‚ƒ-ratio): {fitted_ratio_adam:.3f} (Target: {ANISOTROPY_RATIO_TRUE})\")\n",
    "print(f\"  ---\")\n",
    "print(f\"  * Fitted Î¸â‚ (ÏƒÂ²/a): {phi1_adam:.3f} (Target: {PHI1_TARGET:.3f})\")\n",
    "print(f\"  * Fitted Î¸â‚‚ (1/a): {phi2_adam:.3f} (Target: {PHI2_TARGET:.3f})\")\n",
    "print(f\"  * Fitted Ï†â‚ƒ (Î¸â‚ƒÂ²): {phi3_adam:.3f} (Target: {PHI3_TARGET:.3f})\")\n",
    "print(f\"  ---\")\n",
    "print(f\"  * Final -LL Value: {final_loss_adam.item():.2f}\")\n",
    "print(f\"  * Optimization Steps: {ADAM_ITERATIONS} epochs\")\n",
    "print(\"=\"*75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a4b0a",
   "metadata": {},
   "source": [
    "use log transformation instead of optimizing raw prams .pow()     torch.exp(raw_log_param) this guarantees positivity without epsilon hack and optimization more lobust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "814b7ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Generation (Anisotropic) ---\n",
      "--- Data Generation Complete ---\n",
      "\n",
      "--- A. Starting MLE Optimization (PyTorch L-BFGS) - LOG-STABLE ---\n",
      "LBFGS Step 5/50, NLL: 1728.45, Params: [ÏƒÂ²: 27.037, a: 1.182, Î¸â‚ƒ-ratio: 1.830, Î·Â²: 3.075], Grads: [logÎ¦1: 0.0002, logÎ¦2: -0.0001, logÎ¦3: 0.0000, logÎ—Â²: 0.0003]\n",
      "LBFGS Step 10/50, NLL: 1728.45, Params: [ÏƒÂ²: 27.037, a: 1.182, Î¸â‚ƒ-ratio: 1.830, Î·Â²: 3.075], Grads: [logÎ¦1: 0.0002, logÎ¦2: -0.0001, logÎ¦3: 0.0000, logÎ—Â²: 0.0003]\n",
      "LBFGS Step 15/50, NLL: 1728.45, Params: [ÏƒÂ²: 27.037, a: 1.182, Î¸â‚ƒ-ratio: 1.830, Î·Â²: 3.075], Grads: [logÎ¦1: 0.0002, logÎ¦2: -0.0001, logÎ¦3: 0.0000, logÎ—Â²: 0.0003]\n",
      "LBFGS Step 20/50, NLL: 1728.45, Params: [ÏƒÂ²: 27.037, a: 1.182, Î¸â‚ƒ-ratio: 1.830, Î·Â²: 3.075], Grads: [logÎ¦1: 0.0002, logÎ¦2: -0.0001, logÎ¦3: 0.0000, logÎ—Â²: 0.0003]\n",
      "LBFGS Step 25/50, NLL: 1728.45, Params: [ÏƒÂ²: 27.037, a: 1.182, Î¸â‚ƒ-ratio: 1.830, Î·Â²: 3.075], Grads: [logÎ¦1: 0.0002, logÎ¦2: -0.0001, logÎ¦3: 0.0000, logÎ—Â²: 0.0003]\n",
      "LBFGS Step 30/50, NLL: 1728.45, Params: [ÏƒÂ²: 27.037, a: 1.182, Î¸â‚ƒ-ratio: 1.830, Î·Â²: 3.075], Grads: [logÎ¦1: 0.0002, logÎ¦2: -0.0001, logÎ¦3: 0.0000, logÎ—Â²: 0.0003]\n",
      "LBFGS Step 35/50, NLL: 1728.45, Params: [ÏƒÂ²: 27.037, a: 1.182, Î¸â‚ƒ-ratio: 1.830, Î·Â²: 3.075], Grads: [logÎ¦1: 0.0002, logÎ¦2: -0.0001, logÎ¦3: 0.0000, logÎ—Â²: 0.0003]\n",
      "LBFGS Step 40/50, NLL: 1728.45, Params: [ÏƒÂ²: 27.037, a: 1.182, Î¸â‚ƒ-ratio: 1.830, Î·Â²: 3.075], Grads: [logÎ¦1: 0.0002, logÎ¦2: -0.0001, logÎ¦3: 0.0000, logÎ—Â²: 0.0003]\n",
      "LBFGS Step 45/50, NLL: 1728.45, Params: [ÏƒÂ²: 27.037, a: 1.182, Î¸â‚ƒ-ratio: 1.830, Î·Â²: 3.075], Grads: [logÎ¦1: 0.0002, logÎ¦2: -0.0001, logÎ¦3: 0.0000, logÎ—Â²: 0.0003]\n",
      "LBFGS Step 50/50, NLL: 1728.45, Params: [ÏƒÂ²: 27.037, a: 1.182, Î¸â‚ƒ-ratio: 1.830, Î·Â²: 3.075], Grads: [logÎ¦1: 0.0002, logÎ¦2: -0.0001, logÎ¦3: 0.0000, logÎ—Â²: 0.0003]\n",
      "\n",
      "--- B. Starting MLE Optimization (PyTorch Adam) - LOG-STABLE ---\n",
      "Adam Epoch 50/500, NLL: 1820.75, Params: [ÏƒÂ²: 14.552, a: 0.630, Î¸â‚ƒ-ratio: 2.182, Î·Â²: 1.050], Grads: [logÎ¦1: -243.3292, logÎ¦2: 24.2717, logÎ¦3: 95.7872, logÎ—Â²: -118.1958]\n",
      "Adam Epoch 100/500, NLL: 1800.43, Params: [ÏƒÂ²: 15.897, a: 0.657, Î¸â‚ƒ-ratio: 2.132, Î·Â²: 1.100], Grads: [logÎ¦1: -207.2993, logÎ¦2: 18.7555, logÎ¦3: 82.2243, logÎ—Â²: -101.6124]\n",
      "Adam Epoch 150/500, NLL: 1784.58, Params: [ÏƒÂ²: 17.200, a: 0.682, Î¸â‚ƒ-ratio: 2.088, Î·Â²: 1.147], Grads: [logÎ¦1: -176.0226, logÎ¦2: 14.5002, logÎ¦3: 70.4332, logÎ—Â²: -87.4978]\n",
      "Adam Epoch 200/500, NLL: 1772.27, Params: [ÏƒÂ²: 18.444, a: 0.704, Î¸â‚ƒ-ratio: 2.048, Î·Â²: 1.193], Grads: [logÎ¦1: -148.8455, logÎ¦2: 11.1798, logÎ¦3: 60.1919, logÎ—Â²: -75.4661]\n",
      "Adam Epoch 250/500, NLL: 1762.78, Params: [ÏƒÂ²: 19.610, a: 0.723, Î¸â‚ƒ-ratio: 2.012, Î·Â²: 1.236], Grads: [logÎ¦1: -125.2279, logÎ¦2: 8.5626, logÎ¦3: 51.3071, logÎ—Â²: -65.1989]\n",
      "Adam Epoch 300/500, NLL: 1755.50, Params: [ÏƒÂ²: 20.685, a: 0.740, Î¸â‚ƒ-ratio: 1.980, Î·Â²: 1.278], Grads: [logÎ¦1: -104.7293, logÎ¦2: 6.4846, logÎ¦3: 43.6115, logÎ—Â²: -56.4363]\n",
      "Adam Epoch 350/500, NLL: 1749.98, Params: [ÏƒÂ²: 21.657, a: 0.753, Î¸â‚ƒ-ratio: 1.952, Î·Â²: 1.317], Grads: [logÎ¦1: -86.9706, logÎ¦2: 4.8265, logÎ¦3: 36.9583, logÎ—Â²: -48.9575]\n",
      "Adam Epoch 400/500, NLL: 1745.81, Params: [ÏƒÂ²: 22.518, a: 0.764, Î¸â‚ƒ-ratio: 1.926, Î·Â²: 1.354], Grads: [logÎ¦1: -71.6293, logÎ¦2: 3.4998, logÎ¦3: 31.2187, logÎ—Â²: -42.5783]\n",
      "Adam Epoch 450/500, NLL: 1742.69, Params: [ÏƒÂ²: 23.262, a: 0.773, Î¸â‚ƒ-ratio: 1.904, Î·Â²: 1.389], Grads: [logÎ¦1: -58.4258, logÎ¦2: 2.4391, logÎ¦3: 26.2815, logÎ—Â²: -37.1436]\n",
      "Adam Epoch 500/500, NLL: 1740.38, Params: [ÏƒÂ²: 23.889, a: 0.779, Î¸â‚ƒ-ratio: 1.884, Î·Â²: 1.423], Grads: [logÎ¦1: -47.1122, logÎ¦2: 1.5945, logÎ¦3: 22.0462, logÎ—Â²: -32.5199]\n",
      "\n",
      "===========================================================================\n",
      "TARGET PARAMETERS: Variance (ÏƒÂ²)=30.0, Range (a)=1.5, Anisotropy (Î¸â‚ƒ-ratio)=2.0, Nugget (Î·Â²)=3.0\n",
      "                 (Derived Targets: Î¸â‚=20.000, Î¸â‚‚=0.667, Ï†â‚ƒ (Î¸â‚ƒÂ²)=4.000)\n",
      "===========================================================================\n",
      "âœ¨ PyTorch L-BFGS Results (Log-Stable Reparameterization):\n",
      "  * Fitted Variance (ÏƒÂ²): 27.037 (Target: 30.0)\n",
      "  * Fitted Range (a): 1.182 (Target: 1.5)\n",
      "  * Fitted Anisotropy (Î¸â‚ƒ-ratio): 1.830 (Target: 2.0)\n",
      "  * Fitted Nugget (Î·Â²): 3.075 (Target: 3.0)\n",
      "  ---\n",
      "  * Fitted Î¸â‚ (ÏƒÂ²/a): 22.870 (Target: 20.000)\n",
      "  * Fitted Î¸â‚‚ (1/a): 0.846 (Target: 0.667)\n",
      "  * Fitted Ï†â‚ƒ (Î¸â‚ƒÂ²): 3.351 (Target: 4.000)\n",
      "  ---\n",
      "  * Final -LL Value: 1728.45\n",
      "  * --- Sanity Check (ÏƒÂ²_hat + 1) ---\n",
      "  * NLL @ (ÏƒÂ²_hat + 1.0): 1728.57 (Change: 0.12)\n",
      "  * Optimization Steps: 50 steps\n",
      "\n",
      "ðŸš€ PyTorch Adam Results (Log-Stable Reparameterization):\n",
      "  * Fitted Variance (ÏƒÂ²): 23.889 (Target: 30.0)\n",
      "  * Fitted Range (a): 0.779 (Target: 1.5)\n",
      "  * Fitted Anisotropy (Î¸â‚ƒ-ratio): 1.884 (Target: 2.0)\n",
      "  * Fitted Nugget (Î·Â²): 1.423 (Target: 3.0)\n",
      "  ---\n",
      "  * Fitted Î¸â‚ (ÏƒÂ²/a): 30.649 (Target: 20.000)\n",
      "  * Fitted Î¸â‚‚ (1/a): 1.283 (Target: 0.667)\n",
      "  * Fitted Ï†â‚ƒ (Î¸â‚ƒÂ²): 3.551 (Target: 4.000)\n",
      "  ---\n",
      "  * Final -LL Value: 1740.38\n",
      "  * --- Sanity Check (ÏƒÂ²_hat + 1) ---\n",
      "  * NLL @ (ÏƒÂ²_hat + 1.0): 1738.69 (Change: -1.69)\n",
      "  * Optimization Steps: 500 epochs\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# --- 0. Global Parameters and Utility Functions ---\n",
    "N_SPATIAL_POINTS = 1120\n",
    "N_DAYS = 31\n",
    "N_HOURS_PER_DAY = 8\n",
    "N_FEATURES = 4\n",
    "GRID_X = 40  \n",
    "GRID_Y = 28  \n",
    "LAT_MIN, LAT_MAX = 0, 5\n",
    "LON_MIN, LON_MAX = 113, 123\n",
    "BASE_DATE = '2024_07_y24m07day' \n",
    "\n",
    "# Exponential Kernel Parameters (Targets)\n",
    "SIGMA2_TRUE = 30.0      # TARGET Variance (theta_1 / theta_2)\n",
    "RANGE_A_TRUE = 1.5      # TARGET Range (1 / theta_2)\n",
    "ANISOTROPY_RATIO_TRUE = 2.0 # TARGET Anisotropy (theta_3)\n",
    "PHI3_TARGET_SQ = ANISOTROPY_RATIO_TRUE**2 # Target = 4.0\n",
    "NUGGET_TRUE = 3.0       # ðŸ’¡ TARGET Nugget\n",
    "\n",
    "# Optimization Setup\n",
    "ADAM_ITERATIONS = 500\n",
    "ADAM_LEARNING_RATE = 0.001 \n",
    "\n",
    "# L-BFGS Setup\n",
    "LBFGS_MAX_STEPS = 50 \n",
    "LBFGS_MAX_EVAL = 50 \n",
    "\n",
    "OZONE_MEAN = 240.0\n",
    "\n",
    "# --- COVARIANCE FUNCTIONS (Unchanged) ---\n",
    "\n",
    "def exponential_covariance_numpy(distances, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (NumPy for Generation).\"\"\"\n",
    "    cov = sigma2 * np.exp(-distances / a)\n",
    "    if distances.shape[0] == distances.shape[1]:\n",
    "        cov[np.diag_indices_from(distances)] += (nugget + 1e-6)\n",
    "    return cov\n",
    "\n",
    "def exponential_covariance_torch(distances_torch, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (PyTorch for Optimization).\"\"\"\n",
    "    cov = sigma2 * torch.exp(-distances_torch / a)\n",
    "    \n",
    "    if distances_torch.shape[0] == distances_torch.shape[1]:\n",
    "        jitter = 1e-6 \n",
    "        diag_mask = torch.eye(cov.shape[0], device=cov.device)\n",
    "        cov = cov + diag_mask * (nugget + jitter)\n",
    "    return cov\n",
    "\n",
    "# --- ðŸ’¡ MODIFIED NLL Function (Log-Reparameterization) ---\n",
    "def neg_log_likelihood_torch_stable(raw_log_params, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch):\n",
    "    \"\"\"\n",
    "    Calculates -LL for PyTorch (optimizing Log-Reparameterization).\n",
    "    raw_log_params[0] = log(phi1) (for theta_1 = sigma2/a)\n",
    "    raw_log_params[1] = log(phi2) (for theta_2 = 1/a)\n",
    "    raw_log_params[2] = log(phi3) (for phi_3 = theta_3^2)\n",
    "    raw_log_params[3] = log(nugget) ðŸ’¡ NEW\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. ðŸ’¡ Recover parameters using torch.exp()\n",
    "    phi1   = torch.exp(raw_log_params[0]) # theta_1 = sigma2 / a\n",
    "    phi2   = torch.exp(raw_log_params[1]) # theta_2 = 1 / a\n",
    "    phi3   = torch.exp(raw_log_params[2]) # phi_3 = theta_3^2\n",
    "    nugget = torch.exp(raw_log_params[3]) # ðŸ’¡ NEW: Recover nugget\n",
    "    \n",
    "    # 2. Derive Original Parameters\n",
    "    range_a = 1.0 / phi2          # Range: a = 1 / theta_2\n",
    "    sigma2 = phi1 / phi2          # Variance: sigma2 = theta_1 / theta_2\n",
    "    \n",
    "    # 3. Compute Anisotropic Distance\n",
    "    aniso_dist_sq = (d_lon_sq_torch / phi3) + d_lat_sq_torch\n",
    "    \n",
    "    aniso_dist = torch.sqrt(aniso_dist_sq + 1e-6)\n",
    "    \n",
    "    # 4. Calculate Covariance Matrix C\n",
    "    # ðŸ’¡ Pass the optimized nugget, not a fixed one\n",
    "    C = exponential_covariance_torch(aniso_dist, sigma2, range_a, nugget) \n",
    "    \n",
    "    try:\n",
    "        L = torch.linalg.cholesky(C)\n",
    "        log_det = 2.0 * torch.sum(torch.log(torch.diag(L)))\n",
    "        alpha = torch.linalg.solve(C, z_centered_torch.unsqueeze(1))\n",
    "        quad_term = z_centered_torch.unsqueeze(0) @ alpha\n",
    "        neg_LL = 0.5 * log_det + 0.5 * quad_term.squeeze()\n",
    "        \n",
    "        if torch.isnan(neg_LL) or torch.isinf(neg_LL):\n",
    "            return torch.tensor(1e15, device=C.device, dtype=torch.float) + raw_log_params.sum() * 0.0\n",
    "\n",
    "        return neg_LL\n",
    "    except RuntimeError:\n",
    "        return torch.tensor(1e15, device=C.device, dtype=torch.float) + raw_log_params.sum() * 0.0\n",
    "\n",
    "\n",
    "# --- Data Generation Function (Unchanged) ---\n",
    "def generate_ozone_data_map(coords, sigma2, a, nugget, mean, time_index, anisotropy_ratio):\n",
    "    n_points = coords.shape[0]\n",
    "    coords_transformed = coords.copy()\n",
    "    coords_transformed[:, 1] = coords_transformed[:, 1] / anisotropy_ratio\n",
    "    distances = cdist(coords_transformed, coords_transformed, metric='euclidean')\n",
    "    Cov = exponential_covariance_numpy(distances, sigma2, a, nugget) \n",
    "    Cov = (Cov + Cov.T) / 2\n",
    "    try:\n",
    "        L = np.linalg.cholesky(Cov)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.zeros((n_points, N_FEATURES))\n",
    "    W = np.random.normal(0, 1, size=(n_points, 1))\n",
    "    Z_correlated = L @ W\n",
    "    ozone_values = mean + Z_correlated\n",
    "    data_np = np.zeros((n_points, N_FEATURES))\n",
    "    data_np[:, 0:1] = ozone_values             \n",
    "    data_np[:, 1] = coords[:, 1] * 10 + 2\n",
    "    data_np[:, 2] = coords[:, 0] * 40 + 250\n",
    "    data_np[:, 3] = time_index                 \n",
    "    return data_np\n",
    "\n",
    "\n",
    "# --- 1. Data Generation Execution ---\n",
    "df_day_aggregated_list = []\n",
    "print(\"--- Starting Data Generation (Anisotropic) ---\")\n",
    "lat_coords = np.linspace(LAT_MIN, LAT_MAX, GRID_Y)\n",
    "lon_coords = np.linspace(LON_MIN, LON_MAX, GRID_X)\n",
    "coords_latlon = np.array([[lat, lon] for lat in lat_coords for lon in lon_coords])\n",
    "\n",
    "data_np = generate_ozone_data_map(\n",
    "    coords_latlon, SIGMA2_TRUE, RANGE_A_TRUE, NUGGET_TRUE, OZONE_MEAN, 21.0, # ðŸ’¡ Use NUGGET_TRUE\n",
    "    ANISOTROPY_RATIO_TRUE\n",
    ")\n",
    "df_day_aggregated_list.append(torch.tensor(data_np, dtype=torch.float))\n",
    "print(\"--- Data Generation Complete ---\")\n",
    "\n",
    "# --- 2. Data Preparation (Unchanged) ---\n",
    "data_to_fit = df_day_aggregated_list[0][:N_SPATIAL_POINTS, :] \n",
    "z_data = data_to_fit[:, 0].numpy()\n",
    "coordinates = coords_latlon[:, [1, 0]]\n",
    "z_centered_np = z_data - np.mean(z_data)\n",
    "z_centered_torch = torch.tensor(z_centered_np, dtype=torch.float)\n",
    "\n",
    "lons = coordinates[:, 0:1]\n",
    "lats = coordinates[:, 1:2]\n",
    "d_lon_np = cdist(lons, lons, metric='euclidean')\n",
    "d_lat_np = cdist(lats, lats, metric='euclidean')\n",
    "d_lon_sq_np = np.square(d_lon_np)\n",
    "d_lat_sq_np = np.square(d_lat_np)\n",
    "\n",
    "d_lon_sq_torch = torch.tensor(d_lon_sq_np, dtype=torch.float)\n",
    "d_lat_sq_torch = torch.tensor(d_lat_sq_np, dtype=torch.float)\n",
    "\n",
    "\n",
    "# --- ðŸ’¡ MODIFIED Initial Parameter Setup (Log-space) ---\n",
    "# Calculate target stable parameters\n",
    "PHI1_TARGET = SIGMA2_TRUE / RANGE_A_TRUE    # theta_1 = 20.0\n",
    "PHI2_TARGET = 1.0 / RANGE_A_TRUE            # theta_2 = 0.667\n",
    "PHI3_TARGET = PHI3_TARGET_SQ                # phi_3 = 4.0\n",
    "\n",
    "# Initial guess is now the log() of the target values\n",
    "raw_log_phi1_start = np.log(PHI1_TARGET + 2.0) # Start off-target\n",
    "raw_log_phi2_start = np.log(PHI2_TARGET + 1.0) # Start off-target\n",
    "raw_log_phi3_start = np.log(PHI3_TARGET + 1.0) # Start off-target\n",
    "raw_log_nugget_start = np.log(1.0)             # ðŸ’¡ NEW: Initial guess for nugget (Target is 3.0)\n",
    "\n",
    "initial_params_stable = [\n",
    "    raw_log_phi1_start, \n",
    "    raw_log_phi2_start,\n",
    "    raw_log_phi3_start,\n",
    "    raw_log_nugget_start  # ðŸ’¡ NEW\n",
    "]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# A. Optimization with L-BFGS (PyTorch) - LOG-STABLE\n",
    "# ----------------------------------------------------\n",
    "\n",
    "raw_params_lbfgs = torch.tensor(\n",
    "    initial_params_stable, \n",
    "    dtype=torch.float, \n",
    "    requires_grad=True\n",
    ")\n",
    "\n",
    "optimizer_lbfgs = optim.LBFGS(\n",
    "    [raw_params_lbfgs], \n",
    "    lr=1.0, \n",
    "    max_iter=LBFGS_MAX_STEPS,\n",
    "    max_eval=LBFGS_MAX_EVAL \n",
    ")\n",
    "\n",
    "final_loss_lbfgs = torch.tensor(0.0)\n",
    "print(\"\\n--- A. Starting MLE Optimization (PyTorch L-BFGS) - LOG-STABLE ---\")\n",
    "\n",
    "def closure_lbfgs():\n",
    "    optimizer_lbfgs.zero_grad()\n",
    "    # ðŸ’¡ Call NLL function with 4 params (nugget is now inside)\n",
    "    loss = neg_log_likelihood_torch_stable(\n",
    "        raw_params_lbfgs, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch\n",
    "    )\n",
    "    if not torch.isinf(loss) and not torch.isnan(loss):\n",
    "        loss.backward()\n",
    "    return loss\n",
    "\n",
    "for step in range(LBFGS_MAX_STEPS):\n",
    "    loss = optimizer_lbfgs.step(closure_lbfgs)\n",
    "    final_loss_lbfgs = loss\n",
    "    \n",
    "    if (step + 1) % 5 == 0: \n",
    "        # ðŸ’¡ Recover parameters with .exp()\n",
    "        phi1 = raw_params_lbfgs[0].exp().item()\n",
    "        phi2 = raw_params_lbfgs[1].exp().item()\n",
    "        phi3 = raw_params_lbfgs[2].exp().item()\n",
    "        nugget = raw_params_lbfgs[3].exp().item() # ðŸ’¡ NEW\n",
    "        \n",
    "        current_sigma2 = phi1 / (phi2 + 1e-6)\n",
    "        current_a = 1.0 / (phi2 + 1e-6)\n",
    "        current_theta_3_ratio = np.sqrt(phi3) \n",
    "        \n",
    "        grad_phi1 = raw_params_lbfgs.grad[0].item() if raw_params_lbfgs.grad is not None else 0.0\n",
    "        grad_phi2 = raw_params_lbfgs.grad[1].item() if raw_params_lbfgs.grad is not None else 0.0\n",
    "        grad_phi3 = raw_params_lbfgs.grad[2].item() if raw_params_lbfgs.grad is not None else 0.0\n",
    "        grad_nugget = raw_params_lbfgs.grad[3].item() if raw_params_lbfgs.grad is not None else 0.0 # ðŸ’¡ NEW\n",
    "        \n",
    "        # ðŸ’¡ Updated print statement\n",
    "        print(f\"LBFGS Step {step + 1}/{LBFGS_MAX_STEPS}, NLL: {loss.item():.2f}, \"\n",
    "              f\"Params: [ÏƒÂ²: {current_sigma2:.3f}, a: {current_a:.3f}, Î¸â‚ƒ-ratio: {current_theta_3_ratio:.3f}, Î·Â²: {nugget:.3f}], \"\n",
    "              f\"Grads: [logÎ¦1: {grad_phi1:.4f}, logÎ¦2: {grad_phi2:.4f}, logÎ¦3: {grad_phi3:.4f}, logÎ—Â²: {grad_nugget:.4f}]\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# B. Optimization with Adam (PyTorch) - LOG-STABLE\n",
    "# ----------------------------------------------------\n",
    "\n",
    "raw_params_adam = torch.tensor(\n",
    "    initial_params_stable, \n",
    "    dtype=torch.float, \n",
    "    requires_grad=True\n",
    ")\n",
    "\n",
    "optimizer_adam = optim.Adam(\n",
    "    [raw_params_adam], \n",
    "    lr=ADAM_LEARNING_RATE\n",
    ")\n",
    "\n",
    "final_loss_adam = torch.tensor(0.0)\n",
    "print(f\"\\n--- B. Starting MLE Optimization (PyTorch Adam) - LOG-STABLE ---\")\n",
    "\n",
    "for epoch in range(ADAM_ITERATIONS):\n",
    "    optimizer_adam.zero_grad()\n",
    "    \n",
    "    # ðŸ’¡ Call NLL function with 4 params\n",
    "    loss = neg_log_likelihood_torch_stable(\n",
    "        raw_params_adam, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch\n",
    "    )\n",
    "    \n",
    "    if torch.isinf(loss) or torch.isnan(loss):\n",
    "        if (epoch + 1) % 50 == 0: \n",
    "            print(f\"Adam Epoch {epoch + 1}/{ADAM_ITERATIONS}, Invalid loss. Skipping step.\")\n",
    "        continue\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer_adam.step()\n",
    "    final_loss_adam = loss\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0: \n",
    "        # ðŸ’¡ Recover parameters with .exp()\n",
    "        phi1 = raw_params_adam[0].exp().item()\n",
    "        phi2 = raw_params_adam[1].exp().item()\n",
    "        phi3 = raw_params_adam[2].exp().item()\n",
    "        nugget = raw_params_adam[3].exp().item() # ðŸ’¡ NEW\n",
    "        \n",
    "        current_sigma2 = phi1 / (phi2 + 1e-6)\n",
    "        current_a = 1.0 / (phi2 + 1e-6)\n",
    "        current_theta_3_ratio = np.sqrt(phi3)\n",
    "        \n",
    "        grad_phi1 = raw_params_adam.grad[0].item() if raw_params_adam.grad is not None else 0.0\n",
    "        grad_phi2 = raw_params_adam.grad[1].item() if raw_params_adam.grad is not None else 0.0\n",
    "        grad_phi3 = raw_params_adam.grad[2].item() if raw_params_adam.grad is not None else 0.0\n",
    "        grad_nugget = raw_params_adam.grad[3].item() if raw_params_adam.grad is not None else 0.0 # ðŸ’¡ NEW\n",
    "        \n",
    "        # ðŸ’¡ Updated print statement\n",
    "        print(f\"Adam Epoch {epoch + 1}/{ADAM_ITERATIONS}, NLL: {loss.item():.2f}, \"\n",
    "              f\"Params: [ÏƒÂ²: {current_sigma2:.3f}, a: {current_a:.3f}, Î¸â‚ƒ-ratio: {current_theta_3_ratio:.3f}, Î·Â²: {nugget:.3f}], \"\n",
    "              f\"Grads: [logÎ¦1: {grad_phi1:.4f}, logÎ¦2: {grad_phi2:.4f}, logÎ¦3: {grad_phi3:.4f}, logÎ—Â²: {grad_nugget:.4f}]\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. ðŸ’¡ MODIFIED Display Results\n",
    "# ----------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*75)\n",
    "print(f\"TARGET PARAMETERS: Variance (ÏƒÂ²)={SIGMA2_TRUE}, Range (a)={RANGE_A_TRUE}, \"\n",
    "      f\"Anisotropy (Î¸â‚ƒ-ratio)={ANISOTROPY_RATIO_TRUE}, Nugget (Î·Â²)={NUGGET_TRUE}\") # ðŸ’¡ Updated\n",
    "print(f\"                 (Derived Targets: Î¸â‚={PHI1_TARGET:.3f}, Î¸â‚‚={PHI2_TARGET:.3f}, Ï†â‚ƒ (Î¸â‚ƒÂ²)={PHI3_TARGET:.3f})\")\n",
    "print(\"=\"*75)\n",
    "\n",
    "# L-BFGS Results\n",
    "# ðŸ’¡ Recover parameters with .exp()\n",
    "phi1_lbfgs = raw_params_lbfgs[0].exp().detach().numpy().item()\n",
    "phi2_lbfgs = raw_params_lbfgs[1].exp().detach().numpy().item()\n",
    "phi3_lbfgs = raw_params_lbfgs[2].exp().detach().numpy().item()\n",
    "fitted_nugget_lbfgs = raw_params_lbfgs[3].exp().detach().numpy().item() # ðŸ’¡ NEW\n",
    "\n",
    "fitted_sigma2_lbfgs = phi1_lbfgs / (phi2_lbfgs + 1e-6)\n",
    "fitted_range_a_lbfgs = 1.0 / (phi2_lbfgs + 1e-6)\n",
    "fitted_ratio_lbfgs = np.sqrt(phi3_lbfgs)\n",
    "\n",
    "print(\"âœ¨ PyTorch L-BFGS Results (Log-Stable Reparameterization):\")\n",
    "print(f\"  * Fitted Variance (ÏƒÂ²): {fitted_sigma2_lbfgs:.3f} (Target: {SIGMA2_TRUE})\")\n",
    "print(f\"  * Fitted Range (a): {fitted_range_a_lbfgs:.3f} (Target: {RANGE_A_TRUE})\")\n",
    "print(f\"  * Fitted Anisotropy (Î¸â‚ƒ-ratio): {fitted_ratio_lbfgs:.3f} (Target: {ANISOTROPY_RATIO_TRUE})\")\n",
    "print(f\"  * Fitted Nugget (Î·Â²): {fitted_nugget_lbfgs:.3f} (Target: {NUGGET_TRUE})\") # ðŸ’¡ NEW\n",
    "print(f\"  ---\")\n",
    "print(f\"  * Fitted Î¸â‚ (ÏƒÂ²/a): {phi1_lbfgs:.3f} (Target: {PHI1_TARGET:.3f})\")\n",
    "print(f\"  * Fitted Î¸â‚‚ (1/a): {phi2_lbfgs:.3f} (Target: {PHI2_TARGET:.3f})\")\n",
    "print(f\"  * Fitted Ï†â‚ƒ (Î¸â‚ƒÂ²): {phi3_lbfgs:.3f} (Target: {PHI3_TARGET:.3f})\")\n",
    "print(f\"  ---\")\n",
    "print(f\"  * Final -LL Value: {final_loss_lbfgs.item():.2f}\")\n",
    "\n",
    "# --- ðŸ’¡ NEW: Sanity Check for L-BFGS ---\n",
    "print(f\"  * --- Sanity Check (ÏƒÂ²_hat + 1) ---\")\n",
    "sigma2_perturbed_lbfgs = fitted_sigma2_lbfgs + 1.0\n",
    "phi1_perturbed_lbfgs = sigma2_perturbed_lbfgs * phi2_lbfgs\n",
    "\n",
    "# ðŸ’¡ Create the 4-element perturbed tensor\n",
    "raw_params_lbfgs_perturbed = torch.tensor([\n",
    "    np.log(phi1_perturbed_lbfgs),      # New log(phi1)\n",
    "    raw_params_lbfgs[1].item(),        # Old log(phi2)\n",
    "    raw_params_lbfgs[2].item(),        # Old log(phi3)\n",
    "    raw_params_lbfgs[3].item()         # ðŸ’¡ Old log(nugget)\n",
    "], dtype=torch.float)\n",
    "\n",
    "with torch.no_grad(): # Ensure no gradients are computed\n",
    "    # ðŸ’¡ Call NLL function without fixed nugget\n",
    "    nll_perturbed_lbfgs = neg_log_likelihood_torch_stable(\n",
    "        raw_params_lbfgs_perturbed, \n",
    "        d_lon_sq_torch, \n",
    "        d_lat_sq_torch, \n",
    "        z_centered_torch\n",
    "    )\n",
    "print(f\"  * NLL @ (ÏƒÂ²_hat + 1.0): {nll_perturbed_lbfgs.item():.2f} (Change: {nll_perturbed_lbfgs.item() - final_loss_lbfgs.item():.2f})\")\n",
    "# --- End Sanity Check ---\n",
    "\n",
    "print(f\"  * Optimization Steps: {LBFGS_MAX_STEPS} steps\")\n",
    "\n",
    "\n",
    "# Adam Results\n",
    "# ðŸ’¡ Recover parameters with .exp()\n",
    "phi1_adam = raw_params_adam[0].exp().detach().numpy().item()\n",
    "phi2_adam = raw_params_adam[1].exp().detach().numpy().item()\n",
    "phi3_adam = raw_params_adam[2].exp().detach().numpy().item()\n",
    "fitted_nugget_adam = raw_params_adam[3].exp().detach().numpy().item() # ðŸ’¡ NEW\n",
    "\n",
    "fitted_sigma2_adam = phi1_adam / (phi2_adam + 1e-6)\n",
    "fitted_range_a_adam = 1.0 / (phi2_adam + 1e-6)\n",
    "fitted_ratio_adam = np.sqrt(phi3_adam)\n",
    "\n",
    "print(\"\\nðŸš€ PyTorch Adam Results (Log-Stable Reparameterization):\")\n",
    "print(f\"  * Fitted Variance (ÏƒÂ²): {fitted_sigma2_adam:.3f} (Target: {SIGMA2_TRUE})\")\n",
    "print(f\"  * Fitted Range (a): {fitted_range_a_adam:.3f} (Target: {RANGE_A_TRUE})\")\n",
    "print(f\"  * Fitted Anisotropy (Î¸â‚ƒ-ratio): {fitted_ratio_adam:.3f} (Target: {ANISOTROPY_RATIO_TRUE})\")\n",
    "print(f\"  * Fitted Nugget (Î·Â²): {fitted_nugget_adam:.3f} (Target: {NUGGET_TRUE})\") # ðŸ’¡ NEW\n",
    "print(f\"  ---\")\n",
    "print(f\"  * Fitted Î¸â‚ (ÏƒÂ²/a): {phi1_adam:.3f} (Target: {PHI1_TARGET:.3f})\")\n",
    "print(f\"  * Fitted Î¸â‚‚ (1/a): {phi2_adam:.3f} (Target: {PHI2_TARGET:.3f})\")\n",
    "print(f\"  * Fitted Ï†â‚ƒ (Î¸â‚ƒÂ²): {phi3_adam:.3f} (Target: {PHI3_TARGET:.3f})\")\n",
    "print(f\"  ---\")\n",
    "print(f\"  * Final -LL Value: {final_loss_adam.item():.2f}\")\n",
    "\n",
    "# --- ðŸ’¡ NEW: Sanity Check for Adam ---\n",
    "print(f\"  * --- Sanity Check (ÏƒÂ²_hat + 1) ---\")\n",
    "sigma2_perturbed_adam = fitted_sigma2_adam + 1.0\n",
    "phi1_perturbed_adam = sigma2_perturbed_adam * phi2_adam\n",
    "\n",
    "# ðŸ’¡ Create the 4-element perturbed tensor\n",
    "raw_params_adam_perturbed = torch.tensor([\n",
    "    np.log(phi1_perturbed_adam),     # New log(phi1)\n",
    "    raw_params_adam[1].item(),       # Old log(phi2)\n",
    "    raw_params_adam[2].item(),       # Old log(phi3)\n",
    "    raw_params_adam[3].item()        # ðŸ’¡ Old log(nugget)\n",
    "], dtype=torch.float)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # ðŸ’¡ Call NLL function without fixed nugget\n",
    "    nll_perturbed_adam = neg_log_likelihood_torch_stable(\n",
    "        raw_params_adam_perturbed, \n",
    "        d_lon_sq_torch, \n",
    "        d_lat_sq_torch, \n",
    "        z_centered_torch\n",
    "    )\n",
    "print(f\"  * NLL @ (ÏƒÂ²_hat + 1.0): {nll_perturbed_adam.item():.2f} (Change: {nll_perturbed_adam.item() - final_loss_adam.item():.2f})\")\n",
    "# --- End Sanity Check ---\n",
    "\n",
    "print(f\"  * Optimization Steps: {ADAM_ITERATIONS} epochs\")\n",
    "print(\"=\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed9f4c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Generation (Anisotropic) ---\n",
      "--- Data Generation Complete ---\n",
      "\n",
      "--- A. Starting MLE Optimization (PyTorch L-BFGS) - LOG-STABLE ---\n",
      "LBFGS Step 5/50, NLL: 1712.44, Params: [ÏƒÂ²: 37.698, a: 1.468, Î¸â‚ƒ-ratio: 2.065, Î·Â²: 2.677], Grads: [logÎ¦1: -0.0004, logÎ¦2: -0.0000, logÎ¦3: 0.0000, logÎ—Â²: -0.0002]\n",
      "LBFGS Step 10/50, NLL: 1712.44, Params: [ÏƒÂ²: 37.698, a: 1.468, Î¸â‚ƒ-ratio: 2.065, Î·Â²: 2.677], Grads: [logÎ¦1: -0.0004, logÎ¦2: -0.0000, logÎ¦3: 0.0000, logÎ—Â²: -0.0002]\n",
      "LBFGS Step 15/50, NLL: 1712.44, Params: [ÏƒÂ²: 37.698, a: 1.468, Î¸â‚ƒ-ratio: 2.065, Î·Â²: 2.677], Grads: [logÎ¦1: -0.0004, logÎ¦2: -0.0000, logÎ¦3: 0.0000, logÎ—Â²: -0.0002]\n",
      "LBFGS Step 20/50, NLL: 1712.44, Params: [ÏƒÂ²: 37.698, a: 1.468, Î¸â‚ƒ-ratio: 2.065, Î·Â²: 2.677], Grads: [logÎ¦1: -0.0004, logÎ¦2: -0.0000, logÎ¦3: 0.0000, logÎ—Â²: -0.0002]\n",
      "LBFGS Step 25/50, NLL: 1712.44, Params: [ÏƒÂ²: 37.698, a: 1.468, Î¸â‚ƒ-ratio: 2.065, Î·Â²: 2.677], Grads: [logÎ¦1: -0.0004, logÎ¦2: -0.0000, logÎ¦3: 0.0000, logÎ—Â²: -0.0002]\n",
      "LBFGS Step 30/50, NLL: 1712.44, Params: [ÏƒÂ²: 37.698, a: 1.468, Î¸â‚ƒ-ratio: 2.065, Î·Â²: 2.677], Grads: [logÎ¦1: -0.0004, logÎ¦2: -0.0000, logÎ¦3: 0.0000, logÎ—Â²: -0.0002]\n",
      "LBFGS Step 35/50, NLL: 1712.44, Params: [ÏƒÂ²: 37.698, a: 1.468, Î¸â‚ƒ-ratio: 2.065, Î·Â²: 2.677], Grads: [logÎ¦1: -0.0004, logÎ¦2: -0.0000, logÎ¦3: 0.0000, logÎ—Â²: -0.0002]\n",
      "LBFGS Step 40/50, NLL: 1712.44, Params: [ÏƒÂ²: 37.698, a: 1.468, Î¸â‚ƒ-ratio: 2.065, Î·Â²: 2.677], Grads: [logÎ¦1: -0.0004, logÎ¦2: -0.0000, logÎ¦3: 0.0000, logÎ—Â²: -0.0002]\n",
      "LBFGS Step 45/50, NLL: 1712.44, Params: [ÏƒÂ²: 37.698, a: 1.468, Î¸â‚ƒ-ratio: 2.065, Î·Â²: 2.677], Grads: [logÎ¦1: -0.0004, logÎ¦2: -0.0000, logÎ¦3: 0.0000, logÎ—Â²: -0.0002]\n",
      "LBFGS Step 50/50, NLL: 1712.44, Params: [ÏƒÂ²: 37.698, a: 1.468, Î¸â‚ƒ-ratio: 2.065, Î·Â²: 2.677], Grads: [logÎ¦1: -0.0004, logÎ¦2: -0.0000, logÎ¦3: 0.0000, logÎ—Â²: -0.0002]\n",
      "\n",
      "--- B. Starting MLE Optimization (PyTorch Adam) - LOG-STABLE ---\n",
      "Adam Epoch 50/500, NLL: 1788.56, Params: [ÏƒÂ²: 14.554, a: 0.630, Î¸â‚ƒ-ratio: 2.182, Î·Â²: 1.050], Grads: [logÎ¦1: -229.7678, logÎ¦2: 36.8515, logÎ¦3: 70.5963, logÎ—Â²: -99.6156]\n",
      "Adam Epoch 100/500, NLL: 1770.38, Params: [ÏƒÂ²: 15.914, a: 0.658, Î¸â‚ƒ-ratio: 2.134, Î·Â²: 1.099], Grads: [logÎ¦1: -194.3450, logÎ¦2: 29.7312, logÎ¦3: 58.3198, logÎ—Â²: -84.7426]\n",
      "Adam Epoch 150/500, NLL: 1756.48, Params: [ÏƒÂ²: 17.249, a: 0.684, Î¸â‚ƒ-ratio: 2.091, Î·Â²: 1.146], Grads: [logÎ¦1: -163.8325, logÎ¦2: 24.1606, logÎ¦3: 47.7937, logÎ—Â²: -72.1851]\n",
      "Adam Epoch 200/500, NLL: 1745.94, Params: [ÏƒÂ²: 18.544, a: 0.709, Î¸â‚ƒ-ratio: 2.054, Î·Â²: 1.191], Grads: [logÎ¦1: -137.5355, logÎ¦2: 19.7543, logÎ¦3: 38.7934, logÎ—Â²: -61.5722]\n",
      "Adam Epoch 250/500, NLL: 1738.01, Params: [ÏƒÂ²: 19.786, a: 0.731, Î¸â‚ƒ-ratio: 2.022, Î·Â²: 1.233], Grads: [logÎ¦1: -114.8890, logÎ¦2: 16.2374, logÎ¦3: 31.1243, logÎ—Â²: -52.6022]\n",
      "Adam Epoch 300/500, NLL: 1732.10, Params: [ÏƒÂ²: 20.964, a: 0.751, Î¸â‚ƒ-ratio: 1.995, Î·Â²: 1.273], Grads: [logÎ¦1: -95.4217, logÎ¦2: 13.4090, logÎ¦3: 24.6163, logÎ—Â²: -45.0245]\n",
      "Adam Epoch 350/500, NLL: 1727.73, Params: [ÏƒÂ²: 22.068, a: 0.770, Î¸â‚ƒ-ratio: 1.972, Î·Â²: 1.310], Grads: [logÎ¦1: -78.7349, logÎ¦2: 11.1214, logÎ¦3: 19.1223, logÎ—Â²: -38.6311]\n",
      "Adam Epoch 400/500, NLL: 1724.55, Params: [ÏƒÂ²: 23.092, a: 0.787, Î¸â‚ƒ-ratio: 1.954, Î·Â²: 1.346], Grads: [logÎ¦1: -64.4815, logÎ¦2: 9.2634, logÎ¦3: 14.5117, logÎ—Â²: -33.2452]\n",
      "Adam Epoch 450/500, NLL: 1722.24, Params: [ÏƒÂ²: 24.030, a: 0.802, Î¸â‚ƒ-ratio: 1.939, Î·Â²: 1.379], Grads: [logÎ¦1: -52.3635, logÎ¦2: 7.7496, logÎ¦3: 10.6717, logÎ—Â²: -28.7196]\n",
      "Adam Epoch 500/500, NLL: 1720.58, Params: [ÏƒÂ²: 24.882, a: 0.816, Î¸â‚ƒ-ratio: 1.927, Î·Â²: 1.409], Grads: [logÎ¦1: -42.1148, logÎ¦2: 6.5139, logÎ¦3: 7.5012, logÎ—Â²: -24.9282]\n",
      "\n",
      "===========================================================================\n",
      "TARGET PARAMETERS: Variance (ÏƒÂ²)=30.0, Range (a)=1.5, Anisotropy (Î¸â‚ƒ-ratio)=2.0, Nugget (Î·Â²)=3.0\n",
      "                 (Derived Targets: Î¸â‚=20.000, Î¸â‚‚=0.667, Ï†â‚ƒ (Î¸â‚ƒÂ²)=4.000)\n",
      "===========================================================================\n",
      "âœ¨ PyTorch L-BFGS Results (Log-Stable Reparameterization):\n",
      "  * Fitted Variance (ÏƒÂ²): 37.698 (Target: 30.0)\n",
      "  * Fitted Range (a): 1.468 (Target: 1.5)\n",
      "  * Fitted Anisotropy (Î¸â‚ƒ-ratio): 2.065 (Target: 2.0)\n",
      "  * Fitted Nugget (Î·Â²): 2.677 (Target: 3.0)\n",
      "  ---\n",
      "  * Fitted Î¸â‚ (ÏƒÂ²/a): 25.681 (Target: 20.000)\n",
      "  * Fitted Î¸â‚‚ (1/a): 0.681 (Target: 0.667)\n",
      "  * Fitted Ï†â‚ƒ (Î¸â‚ƒÂ²): 4.264 (Target: 4.000)\n",
      "  ---\n",
      "  * Final -LL Value: 1712.44\n",
      "  * --- Sanity Check (L-BFGS) ---\n",
      "  * NLL @ ÏƒÂ²_hat + 1.0: 1712.51 (Change: 0.07)\n",
      "  * NLL @ ÏƒÂ²_hat - 1.0: 1712.52 (Change: 0.07)\n",
      "  * NLL @ a_hat + 0.1: 1712.84 (Change: 0.40)\n",
      "  * NLL @ a_hat - 0.1: 1712.91 (Change: 0.46)\n",
      "  * NLL @ Î¸â‚ƒ_hat + 0.1: 1712.66 (Change: 0.22)\n",
      "  * NLL @ Î¸â‚ƒ_hat - 0.1: 1712.68 (Change: 0.24)\n",
      "  * NLL @ Î·Â²_hat + 0.1: 1712.53 (Change: 0.08)\n",
      "  * NLL @ Î·Â²_hat - 0.1: 1712.53 (Change: 0.09)\n",
      "  * Optimization Steps: 50 steps\n",
      "\n",
      "ðŸš€ PyTorch Adam Results (Log-Stable Reparameterization):\n",
      "  * Fitted Variance (ÏƒÂ²): 24.882 (Target: 30.0)\n",
      "  * Fitted Range (a): 0.816 (Target: 1.5)\n",
      "  * Fitted Anisotropy (Î¸â‚ƒ-ratio): 1.927 (Target: 2.0)\n",
      "  * Fitted Nugget (Î·Â²): 1.409 (Target: 3.0)\n",
      "  ---\n",
      "  * Fitted Î¸â‚ (ÏƒÂ²/a): 30.481 (Target: 20.000)\n",
      "  * Fitted Î¸â‚‚ (1/a): 1.225 (Target: 0.667)\n",
      "  * Fitted Ï†â‚ƒ (Î¸â‚ƒÂ²): 3.714 (Target: 4.000)\n",
      "  ---\n",
      "  * Final -LL Value: 1720.58\n",
      "  * --- Sanity Check (Adam) ---\n",
      "  * NLL @ ÏƒÂ²_hat + 1.0: 1719.15 (Change: -1.43)\n",
      "  * NLL @ ÏƒÂ²_hat - 1.0: 1722.54 (Change: 1.97)\n",
      "  * NLL @ a_hat + 0.1: 1726.58 (Change: 6.01)\n",
      "  * NLL @ a_hat - 0.1: 1718.37 (Change: -2.21)\n",
      "  * NLL @ Î¸â‚ƒ_hat + 0.1: 1721.67 (Change: 1.09)\n",
      "  * NLL @ Î¸â‚ƒ_hat - 0.1: 1720.15 (Change: -0.42)\n",
      "  * NLL @ Î·Â²_hat + 0.1: 1718.95 (Change: -1.63)\n",
      "  * NLL @ Î·Â²_hat - 0.1: 1722.49 (Change: 1.91)\n",
      "  * Optimization Steps: 500 epochs\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# --- 0. Global Parameters and Utility Functions ---\n",
    "N_SPATIAL_POINTS = 1120\n",
    "N_DAYS = 31\n",
    "N_HOURS_PER_DAY = 8\n",
    "N_FEATURES = 4\n",
    "GRID_X = 40  \n",
    "GRID_Y = 28  \n",
    "LAT_MIN, LAT_MAX = 0, 5\n",
    "LON_MIN, LON_MAX = 113, 123\n",
    "BASE_DATE = '2024_07_y24m07day' \n",
    "\n",
    "# Exponential Kernel Parameters (Targets)\n",
    "SIGMA2_TRUE = 30.0      # TARGET Variance (theta_1 / theta_2)\n",
    "RANGE_A_TRUE = 1.5      # TARGET Range (1 / theta_2)\n",
    "ANISOTROPY_RATIO_TRUE = 2.0 # TARGET Anisotropy (theta_3)\n",
    "PHI3_TARGET_SQ = ANISOTROPY_RATIO_TRUE**2 # Target = 4.0\n",
    "NUGGET_TRUE = 3.0       # ðŸ’¡ TARGET Nugget\n",
    "\n",
    "# Optimization Setup\n",
    "ADAM_ITERATIONS = 500\n",
    "ADAM_LEARNING_RATE = 0.001 \n",
    "\n",
    "# L-BFGS Setup\n",
    "LBFGS_MAX_STEPS = 50 \n",
    "LBFGS_MAX_EVAL = 50 \n",
    "\n",
    "OZONE_MEAN = 240.0\n",
    "\n",
    "# --- COVARIANCE FUNCTIONS (Unchanged) ---\n",
    "\n",
    "def exponential_covariance_numpy(distances, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (NumPy for Generation).\"\"\"\n",
    "    cov = sigma2 * np.exp(-distances / a)\n",
    "    if distances.shape[0] == distances.shape[1]:\n",
    "        cov[np.diag_indices_from(distances)] += (nugget + 1e-6)\n",
    "    return cov\n",
    "\n",
    "def exponential_covariance_torch(distances_torch, sigma2, a, nugget):\n",
    "    \"\"\"Exponential covariance function (PyTorch for Optimization).\"\"\"\n",
    "    cov = sigma2 * torch.exp(-distances_torch / a)\n",
    "    \n",
    "    if distances_torch.shape[0] == distances_torch.shape[1]:\n",
    "        jitter = 1e-6 \n",
    "        diag_mask = torch.eye(cov.shape[0], device=cov.device)\n",
    "        cov = cov + diag_mask * (nugget + jitter)\n",
    "    return cov\n",
    "\n",
    "# --- ðŸ’¡ MODIFIED NLL Function (Log-Reparameterization) ---\n",
    "def neg_log_likelihood_torch_stable(raw_log_params, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch):\n",
    "    \"\"\"\n",
    "    Calculates -LL for PyTorch (optimizing Log-Reparameterization).\n",
    "    raw_log_params[0] = log(phi1) (for theta_1 = sigma2/a)\n",
    "    raw_log_params[1] = log(phi2) (for theta_2 = 1/a)\n",
    "    raw_log_params[2] = log(phi3) (for phi_3 = theta_3^2)\n",
    "    raw_log_params[3] = log(nugget) ðŸ’¡ NEW\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. ðŸ’¡ Recover parameters using torch.exp()\n",
    "    phi1   = torch.exp(raw_log_params[0]) # theta_1 = sigma2 / a\n",
    "    phi2   = torch.exp(raw_log_params[1]) # theta_2 = 1 / a\n",
    "    phi3   = torch.exp(raw_log_params[2]) # phi_3 = theta_3^2\n",
    "    nugget = torch.exp(raw_log_params[3]) # ðŸ’¡ NEW: Recover nugget\n",
    "    \n",
    "    # 2. Derive Original Parameters\n",
    "    range_a = 1.0 / phi2          # Range: a = 1 / theta_2\n",
    "    sigma2 = phi1 / phi2          # Variance: sigma2 = theta_1 / theta_2\n",
    "    \n",
    "    # 3. Compute Anisotropic Distance\n",
    "    aniso_dist_sq = (d_lon_sq_torch / phi3) + d_lat_sq_torch\n",
    "    \n",
    "    aniso_dist = torch.sqrt(aniso_dist_sq + 1e-6)\n",
    "    \n",
    "    # 4. Calculate Covariance Matrix C\n",
    "    # ðŸ’¡ Pass the optimized nugget, not a fixed one\n",
    "    C = exponential_covariance_torch(aniso_dist, sigma2, range_a, nugget) \n",
    "    \n",
    "    try:\n",
    "        L = torch.linalg.cholesky(C)\n",
    "        log_det = 2.0 * torch.sum(torch.log(torch.diag(L)))\n",
    "        alpha = torch.linalg.solve(C, z_centered_torch.unsqueeze(1))\n",
    "        quad_term = z_centered_torch.unsqueeze(0) @ alpha\n",
    "        neg_LL = 0.5 * log_det + 0.5 * quad_term.squeeze()\n",
    "        \n",
    "        if torch.isnan(neg_LL) or torch.isinf(neg_LL):\n",
    "            return torch.tensor(1e15, device=C.device, dtype=torch.float) + raw_log_params.sum() * 0.0\n",
    "\n",
    "        return neg_LL\n",
    "    except RuntimeError:\n",
    "        return torch.tensor(1e15, device=C.device, dtype=torch.float) + raw_log_params.sum() * 0.0\n",
    "\n",
    "\n",
    "# --- ðŸ’¡ NEW: Sanity Check Helper Function ---\n",
    "def check_nll_from_interpretable(sigma2, range_a, aniso_ratio, nugget, \n",
    "                                d_lon_sq_torch, d_lat_sq_torch, z_centered_torch):\n",
    "    \"\"\"\n",
    "    Helper function to calculate NLL from interpretable parameters.\n",
    "    Used for sanity checks.\n",
    "    \"\"\"\n",
    "    # 1. Convert interpretable params to phi-space\n",
    "    # Add small epsilon to prevent log(0) or divide by zero\n",
    "    range_a = max(range_a, 1e-6)\n",
    "    aniso_ratio = max(aniso_ratio, 1e-6)\n",
    "    nugget = max(nugget, 1e-6)\n",
    "\n",
    "    phi2 = 1.0 / range_a\n",
    "    phi1 = sigma2 * phi2\n",
    "    phi3 = aniso_ratio**2\n",
    "    \n",
    "    # 2. Create the raw_log_params tensor\n",
    "    raw_log_params = torch.tensor([\n",
    "        np.log(phi1), \n",
    "        np.log(phi2), \n",
    "        np.log(phi3), \n",
    "        np.log(nugget)\n",
    "    ], dtype=torch.float)\n",
    "    \n",
    "    # 3. Calculate NLL\n",
    "    with torch.no_grad():\n",
    "        nll = neg_log_likelihood_torch_stable(\n",
    "            raw_log_params, \n",
    "            d_lon_sq_torch, \n",
    "            d_lat_sq_torch, \n",
    "            z_centered_torch\n",
    "        )\n",
    "    return nll.item()\n",
    "\n",
    "\n",
    "# --- Data Generation Function (Unchanged) ---\n",
    "def generate_ozone_data_map(coords, sigma2, a, nugget, mean, time_index, anisotropy_ratio):\n",
    "    n_points = coords.shape[0]\n",
    "    coords_transformed = coords.copy()\n",
    "    coords_transformed[:, 1] = coords_transformed[:, 1] / anisotropy_ratio\n",
    "    distances = cdist(coords_transformed, coords_transformed, metric='euclidean')\n",
    "    Cov = exponential_covariance_numpy(distances, sigma2, a, nugget) \n",
    "    Cov = (Cov + Cov.T) / 2\n",
    "    try:\n",
    "        L = np.linalg.cholesky(Cov)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.zeros((n_points, N_FEATURES))\n",
    "    W = np.random.normal(0, 1, size=(n_points, 1))\n",
    "    Z_correlated = L @ W\n",
    "    ozone_values = mean + Z_correlated\n",
    "    data_np = np.zeros((n_points, N_FEATURES))\n",
    "    data_np[:, 0:1] = ozone_values             \n",
    "    data_np[:, 1] = coords[:, 1] * 10 + 2\n",
    "    data_np[:, 2] = coords[:, 0] * 40 + 250\n",
    "    data_np[:, 3] = time_index                 \n",
    "    return data_np\n",
    "\n",
    "\n",
    "# --- 1. Data Generation Execution ---\n",
    "df_day_aggregated_list = []\n",
    "print(\"--- Starting Data Generation (Anisotropic) ---\")\n",
    "lat_coords = np.linspace(LAT_MIN, LAT_MAX, GRID_Y)\n",
    "lon_coords = np.linspace(LON_MIN, LON_MAX, GRID_X)\n",
    "coords_latlon = np.array([[lat, lon] for lat in lat_coords for lon in lon_coords])\n",
    "\n",
    "data_np = generate_ozone_data_map(\n",
    "    coords_latlon, SIGMA2_TRUE, RANGE_A_TRUE, NUGGET_TRUE, OZONE_MEAN, 21.0, # ðŸ’¡ Use NUGGET_TRUE\n",
    "    ANISOTROPY_RATIO_TRUE\n",
    ")\n",
    "df_day_aggregated_list.append(torch.tensor(data_np, dtype=torch.float))\n",
    "print(\"--- Data Generation Complete ---\")\n",
    "\n",
    "# --- 2. Data Preparation (Unchanged) ---\n",
    "data_to_fit = df_day_aggregated_list[0][:N_SPATIAL_POINTS, :] \n",
    "z_data = data_to_fit[:, 0].numpy()\n",
    "coordinates = coords_latlon[:, [1, 0]]\n",
    "z_centered_np = z_data - np.mean(z_data)\n",
    "z_centered_torch = torch.tensor(z_centered_np, dtype=torch.float)\n",
    "\n",
    "lons = coordinates[:, 0:1]\n",
    "lats = coordinates[:, 1:2]\n",
    "d_lon_np = cdist(lons, lons, metric='euclidean')\n",
    "d_lat_np = cdist(lats, lats, metric='euclidean')\n",
    "d_lon_sq_np = np.square(d_lon_np)\n",
    "d_lat_sq_np = np.square(d_lat_np)\n",
    "\n",
    "d_lon_sq_torch = torch.tensor(d_lon_sq_np, dtype=torch.float)\n",
    "d_lat_sq_torch = torch.tensor(d_lat_sq_np, dtype=torch.float)\n",
    "\n",
    "\n",
    "# --- ðŸ’¡ MODIFIED Initial Parameter Setup (Log-space) ---\n",
    "# Calculate target stable parameters\n",
    "PHI1_TARGET = SIGMA2_TRUE / RANGE_A_TRUE    # theta_1 = 20.0\n",
    "PHI2_TARGET = 1.0 / RANGE_A_TRUE            # theta_2 = 0.667\n",
    "PHI3_TARGET = PHI3_TARGET_SQ                # phi_3 = 4.0\n",
    "\n",
    "# Initial guess is now the log() of the target values\n",
    "raw_log_phi1_start = np.log(PHI1_TARGET + 2.0) # Start off-target\n",
    "raw_log_phi2_start = np.log(PHI2_TARGET + 1.0) # Start off-target\n",
    "raw_log_phi3_start = np.log(PHI3_TARGET + 1.0) # Start off-target\n",
    "raw_log_nugget_start = np.log(1.0)             # ðŸ’¡ NEW: Initial guess for nugget (Target is 3.0)\n",
    "\n",
    "initial_params_stable = [\n",
    "    raw_log_phi1_start, \n",
    "    raw_log_phi2_start,\n",
    "    raw_log_phi3_start,\n",
    "    raw_log_nugget_start  # ðŸ’¡ NEW\n",
    "]\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# A. Optimization with L-BFGS (PyTorch) - LOG-STABLE\n",
    "# ----------------------------------------------------\n",
    "\n",
    "raw_params_lbfgs = torch.tensor(\n",
    "    initial_params_stable, \n",
    "    dtype=torch.float, \n",
    "    requires_grad=True\n",
    ")\n",
    "\n",
    "optimizer_lbfgs = optim.LBFGS(\n",
    "    [raw_params_lbfgs], \n",
    "    lr=1.0, \n",
    "    max_iter=LBFGS_MAX_STEPS,\n",
    "    max_eval=LBFGS_MAX_EVAL \n",
    ")\n",
    "\n",
    "final_loss_lbfgs = torch.tensor(0.0)\n",
    "print(\"\\n--- A. Starting MLE Optimization (PyTorch L-BFGS) - LOG-STABLE ---\")\n",
    "\n",
    "def closure_lbfgs():\n",
    "    optimizer_lbfgs.zero_grad()\n",
    "    # ðŸ’¡ Call NLL function with 4 params (nugget is now inside)\n",
    "    loss = neg_log_likelihood_torch_stable(\n",
    "        raw_params_lbfgs, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch\n",
    "    )\n",
    "    if not torch.isinf(loss) and not torch.isnan(loss):\n",
    "        loss.backward()\n",
    "    return loss\n",
    "\n",
    "for step in range(LBFGS_MAX_STEPS):\n",
    "    loss = optimizer_lbfgs.step(closure_lbfgs)\n",
    "    final_loss_lbfgs = loss\n",
    "    \n",
    "    if (step + 1) % 5 == 0: \n",
    "        # ðŸ’¡ Recover parameters with .exp()\n",
    "        phi1 = raw_params_lbfgs[0].exp().item()\n",
    "        phi2 = raw_params_lbfgs[1].exp().item()\n",
    "        phi3 = raw_params_lbfgs[2].exp().item()\n",
    "        nugget = raw_params_lbfgs[3].exp().item() # ðŸ’¡ NEW\n",
    "        \n",
    "        current_sigma2 = phi1 / (phi2 + 1e-6)\n",
    "        current_a = 1.0 / (phi2 + 1e-6)\n",
    "        current_theta_3_ratio = np.sqrt(phi3) \n",
    "        \n",
    "        grad_phi1 = raw_params_lbfgs.grad[0].item() if raw_params_lbfgs.grad is not None else 0.0\n",
    "        grad_phi2 = raw_params_lbfgs.grad[1].item() if raw_params_lbfgs.grad is not None else 0.0\n",
    "        grad_phi3 = raw_params_lbfgs.grad[2].item() if raw_params_lbfgs.grad is not None else 0.0\n",
    "        grad_nugget = raw_params_lbfgs.grad[3].item() if raw_params_lbfgs.grad is not None else 0.0 # ðŸ’¡ NEW\n",
    "        \n",
    "        # ðŸ’¡ Updated print statement\n",
    "        print(f\"LBFGS Step {step + 1}/{LBFGS_MAX_STEPS}, NLL: {loss.item():.2f}, \"\n",
    "              f\"Params: [ÏƒÂ²: {current_sigma2:.3f}, a: {current_a:.3f}, Î¸â‚ƒ-ratio: {current_theta_3_ratio:.3f}, Î·Â²: {nugget:.3f}], \"\n",
    "              f\"Grads: [logÎ¦1: {grad_phi1:.4f}, logÎ¦2: {grad_phi2:.4f}, logÎ¦3: {grad_phi3:.4f}, logÎ—Â²: {grad_nugget:.4f}]\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# B. Optimization with Adam (PyTorch) - LOG-STABLE\n",
    "# ----------------------------------------------------\n",
    "\n",
    "raw_params_adam = torch.tensor(\n",
    "    initial_params_stable, \n",
    "    dtype=torch.float, \n",
    "    requires_grad=True\n",
    ")\n",
    "\n",
    "optimizer_adam = optim.Adam(\n",
    "    [raw_params_adam], \n",
    "    lr=ADAM_LEARNING_RATE\n",
    ")\n",
    "\n",
    "final_loss_adam = torch.tensor(0.0)\n",
    "print(f\"\\n--- B. Starting MLE Optimization (PyTorch Adam) - LOG-STABLE ---\")\n",
    "\n",
    "for epoch in range(ADAM_ITERATIONS):\n",
    "    optimizer_adam.zero_grad()\n",
    "    \n",
    "    # ðŸ’¡ Call NLL function with 4 params\n",
    "    loss = neg_log_likelihood_torch_stable(\n",
    "        raw_params_adam, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch\n",
    "    )\n",
    "    \n",
    "    if torch.isinf(loss) or torch.isnan(loss):\n",
    "        if (epoch + 1) % 50 == 0: \n",
    "            print(f\"Adam Epoch {epoch + 1}/{ADAM_ITERATIONS}, Invalid loss. Skipping step.\")\n",
    "        continue\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer_adam.step()\n",
    "    final_loss_adam = loss\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0: \n",
    "        # ðŸ’¡ Recover parameters with .exp()\n",
    "        phi1 = raw_params_adam[0].exp().item()\n",
    "        phi2 = raw_params_adam[1].exp().item()\n",
    "        phi3 = raw_params_adam[2].exp().item()\n",
    "        nugget = raw_params_adam[3].exp().item() # ðŸ’¡ NEW\n",
    "        \n",
    "        current_sigma2 = phi1 / (phi2 + 1e-6)\n",
    "        current_a = 1.0 / (phi2 + 1e-6)\n",
    "        current_theta_3_ratio = np.sqrt(phi3)\n",
    "        \n",
    "        grad_phi1 = raw_params_adam.grad[0].item() if raw_params_adam.grad is not None else 0.0\n",
    "        grad_phi2 = raw_params_adam.grad[1].item() if raw_params_adam.grad is not None else 0.0\n",
    "        grad_phi3 = raw_params_adam.grad[2].item() if raw_params_adam.grad is not None else 0.0\n",
    "        grad_nugget = raw_params_adam.grad[3].item() if raw_params_adam.grad is not None else 0.0 # ðŸ’¡ NEW\n",
    "        \n",
    "        # ðŸ’¡ Updated print statement\n",
    "        print(f\"Adam Epoch {epoch + 1}/{ADAM_ITERATIONS}, NLL: {loss.item():.2f}, \"\n",
    "              f\"Params: [ÏƒÂ²: {current_sigma2:.3f}, a: {current_a:.3f}, Î¸â‚ƒ-ratio: {current_theta_3_ratio:.3f}, Î·Â²: {nugget:.3f}], \"\n",
    "              f\"Grads: [logÎ¦1: {grad_phi1:.4f}, logÎ¦2: {grad_phi2:.4f}, logÎ¦3: {grad_phi3:.4f}, logÎ—Â²: {grad_nugget:.4f}]\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. ðŸ’¡ MODIFIED Display Results\n",
    "# ----------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*75)\n",
    "print(f\"TARGET PARAMETERS: Variance (ÏƒÂ²)={SIGMA2_TRUE}, Range (a)={RANGE_A_TRUE}, \"\n",
    "      f\"Anisotropy (Î¸â‚ƒ-ratio)={ANISOTROPY_RATIO_TRUE}, Nugget (Î·Â²)={NUGGET_TRUE}\") # ðŸ’¡ Updated\n",
    "print(f\"                 (Derived Targets: Î¸â‚={PHI1_TARGET:.3f}, Î¸â‚‚={PHI2_TARGET:.3f}, Ï†â‚ƒ (Î¸â‚ƒÂ²)={PHI3_TARGET:.3f})\")\n",
    "print(\"=\"*75)\n",
    "\n",
    "# --- L-BFGS Results ---\n",
    "phi1_lbfgs = raw_params_lbfgs[0].exp().detach().numpy().item()\n",
    "phi2_lbfgs = raw_params_lbfgs[1].exp().detach().numpy().item()\n",
    "phi3_lbfgs = raw_params_lbfgs[2].exp().detach().numpy().item()\n",
    "fitted_nugget_lbfgs = raw_params_lbfgs[3].exp().detach().numpy().item() \n",
    "\n",
    "fitted_sigma2_lbfgs = phi1_lbfgs / (phi2_lbfgs + 1e-6)\n",
    "fitted_range_a_lbfgs = 1.0 / (phi2_lbfgs + 1e-6)\n",
    "fitted_ratio_lbfgs = np.sqrt(phi3_lbfgs)\n",
    "nll_final_lbfgs = final_loss_lbfgs.item()\n",
    "\n",
    "print(\"âœ¨ PyTorch L-BFGS Results (Log-Stable Reparameterization):\")\n",
    "print(f\"  * Fitted Variance (ÏƒÂ²): {fitted_sigma2_lbfgs:.3f} (Target: {SIGMA2_TRUE})\")\n",
    "print(f\"  * Fitted Range (a): {fitted_range_a_lbfgs:.3f} (Target: {RANGE_A_TRUE})\")\n",
    "print(f\"  * Fitted Anisotropy (Î¸â‚ƒ-ratio): {fitted_ratio_lbfgs:.3f} (Target: {ANISOTROPY_RATIO_TRUE})\")\n",
    "print(f\"  * Fitted Nugget (Î·Â²): {fitted_nugget_lbfgs:.3f} (Target: {NUGGET_TRUE})\") \n",
    "print(f\"  ---\")\n",
    "print(f\"  * Fitted Î¸â‚ (ÏƒÂ²/a): {phi1_lbfgs:.3f} (Target: {PHI1_TARGET:.3f})\")\n",
    "print(f\"  * Fitted Î¸â‚‚ (1/a): {phi2_lbfgs:.3f} (Target: {PHI2_TARGET:.3f})\")\n",
    "print(f\"  * Fitted Ï†â‚ƒ (Î¸â‚ƒÂ²): {phi3_lbfgs:.3f} (Target: {PHI3_TARGET:.3f})\")\n",
    "print(f\"  ---\")\n",
    "print(f\"  * Final -LL Value: {nll_final_lbfgs:.2f}\")\n",
    "\n",
    "# --- ðŸ’¡ NEW: Expanded Sanity Check for L-BFGS ---\n",
    "print(f\"  * --- Sanity Check (L-BFGS) ---\")\n",
    "# Check ÏƒÂ²\n",
    "s2_plus = check_nll_from_interpretable(fitted_sigma2_lbfgs + 1.0, fitted_range_a_lbfgs, fitted_ratio_lbfgs, fitted_nugget_lbfgs, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch)\n",
    "s2_minus = check_nll_from_interpretable(fitted_sigma2_lbfgs - 1.0, fitted_range_a_lbfgs, fitted_ratio_lbfgs, fitted_nugget_lbfgs, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch)\n",
    "print(f\"  * NLL @ ÏƒÂ²_hat + 1.0: {s2_plus:.2f} (Change: {s2_plus - nll_final_lbfgs:.2f})\")\n",
    "print(f\"  * NLL @ ÏƒÂ²_hat - 1.0: {s2_minus:.2f} (Change: {s2_minus - nll_final_lbfgs:.2f})\")\n",
    "# Check Range\n",
    "a_plus = check_nll_from_interpretable(fitted_sigma2_lbfgs, fitted_range_a_lbfgs + 0.1, fitted_ratio_lbfgs, fitted_nugget_lbfgs, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch)\n",
    "a_minus = check_nll_from_interpretable(fitted_sigma2_lbfgs, fitted_range_a_lbfgs - 0.1, fitted_ratio_lbfgs, fitted_nugget_lbfgs, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch)\n",
    "print(f\"  * NLL @ a_hat + 0.1: {a_plus:.2f} (Change: {a_plus - nll_final_lbfgs:.2f})\")\n",
    "print(f\"  * NLL @ a_hat - 0.1: {a_minus:.2f} (Change: {a_minus - nll_final_lbfgs:.2f})\")\n",
    "# Check Anisotropy\n",
    "r_plus = check_nll_from_interpretable(fitted_sigma2_lbfgs, fitted_range_a_lbfgs, fitted_ratio_lbfgs + 0.1, fitted_nugget_lbfgs, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch)\n",
    "r_minus = check_nll_from_interpretable(fitted_sigma2_lbfgs, fitted_range_a_lbfgs, fitted_ratio_lbfgs - 0.1, fitted_nugget_lbfgs, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch)\n",
    "print(f\"  * NLL @ Î¸â‚ƒ_hat + 0.1: {r_plus:.2f} (Change: {r_plus - nll_final_lbfgs:.2f})\")\n",
    "print(f\"  * NLL @ Î¸â‚ƒ_hat - 0.1: {r_minus:.2f} (Change: {r_minus - nll_final_lbfgs:.2f})\")\n",
    "# Check Nugget\n",
    "n_plus = check_nll_from_interpretable(fitted_sigma2_lbfgs, fitted_range_a_lbfgs, fitted_ratio_lbfgs, fitted_nugget_lbfgs + 0.1, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch)\n",
    "n_minus = check_nll_from_interpretable(fitted_sigma2_lbfgs, fitted_range_a_lbfgs, fitted_ratio_lbfgs, fitted_nugget_lbfgs - 0.1, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch)\n",
    "print(f\"  * NLL @ Î·Â²_hat + 0.1: {n_plus:.2f} (Change: {n_plus - nll_final_lbfgs:.2f})\")\n",
    "print(f\"  * NLL @ Î·Â²_hat - 0.1: {n_minus:.2f} (Change: {n_minus - nll_final_lbfgs:.2f})\")\n",
    "\n",
    "print(f\"  * Optimization Steps: {LBFGS_MAX_STEPS} steps\")\n",
    "\n",
    "\n",
    "# --- Adam Results ---\n",
    "phi1_adam = raw_params_adam[0].exp().detach().numpy().item()\n",
    "phi2_adam = raw_params_adam[1].exp().detach().numpy().item()\n",
    "phi3_adam = raw_params_adam[2].exp().detach().numpy().item()\n",
    "fitted_nugget_adam = raw_params_adam[3].exp().detach().numpy().item() \n",
    "\n",
    "fitted_sigma2_adam = phi1_adam / (phi2_adam + 1e-6)\n",
    "fitted_range_a_adam = 1.0 / (phi2_adam + 1e-6)\n",
    "fitted_ratio_adam = np.sqrt(phi3_adam)\n",
    "nll_final_adam = final_loss_adam.item()\n",
    "\n",
    "print(\"\\nðŸš€ PyTorch Adam Results (Log-Stable Reparameterization):\")\n",
    "print(f\"  * Fitted Variance (ÏƒÂ²): {fitted_sigma2_adam:.3f} (Target: {SIGMA2_TRUE})\")\n",
    "print(f\"  * Fitted Range (a): {fitted_range_a_adam:.3f} (Target: {RANGE_A_TRUE})\")\n",
    "print(f\"  * Fitted Anisotropy (Î¸â‚ƒ-ratio): {fitted_ratio_adam:.3f} (Target: {ANISOTROPY_RATIO_TRUE})\")\n",
    "print(f\"  * Fitted Nugget (Î·Â²): {fitted_nugget_adam:.3f} (Target: {NUGGET_TRUE})\") \n",
    "print(f\"  ---\")\n",
    "print(f\"  * Fitted Î¸â‚ (ÏƒÂ²/a): {phi1_adam:.3f} (Target: {PHI1_TARGET:.3f})\")\n",
    "print(f\"  * Fitted Î¸â‚‚ (1/a): {phi2_adam:.3f} (Target: {PHI2_TARGET:.3f})\")\n",
    "print(f\"  * Fitted Ï†â‚ƒ (Î¸â‚ƒÂ²): {phi3_adam:.3f} (Target: {PHI3_TARGET:.3f})\")\n",
    "print(f\"  ---\")\n",
    "print(f\"  * Final -LL Value: {nll_final_adam:.2f}\")\n",
    "\n",
    "# --- ðŸ’¡ NEW: Expanded Sanity Check for Adam ---\n",
    "print(f\"  * --- Sanity Check (Adam) ---\")\n",
    "# Check ÏƒÂ²\n",
    "s2_plus = check_nll_from_interpretable(fitted_sigma2_adam + 1.0, fitted_range_a_adam, fitted_ratio_adam, fitted_nugget_adam, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch)\n",
    "s2_minus = check_nll_from_interpretable(fitted_sigma2_adam - 1.0, fitted_range_a_adam, fitted_ratio_adam, fitted_nugget_adam, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch)\n",
    "print(f\"  * NLL @ ÏƒÂ²_hat + 1.0: {s2_plus:.2f} (Change: {s2_plus - nll_final_adam:.2f})\")\n",
    "print(f\"  * NLL @ ÏƒÂ²_hat - 1.0: {s2_minus:.2f} (Change: {s2_minus - nll_final_adam:.2f})\")\n",
    "# Check Range\n",
    "a_plus = check_nll_from_interpretable(fitted_sigma2_adam, fitted_range_a_adam + 0.1, fitted_ratio_adam, fitted_nugget_adam, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch)\n",
    "a_minus = check_nll_from_interpretable(fitted_sigma2_adam, fitted_range_a_adam - 0.1, fitted_ratio_adam, fitted_nugget_adam, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch)\n",
    "print(f\"  * NLL @ a_hat + 0.1: {a_plus:.2f} (Change: {a_plus - nll_final_adam:.2f})\")\n",
    "print(f\"  * NLL @ a_hat - 0.1: {a_minus:.2f} (Change: {a_minus - nll_final_adam:.2f})\")\n",
    "# Check Anisotropy\n",
    "r_plus = check_nll_from_interpretable(fitted_sigma2_adam, fitted_range_a_adam, fitted_ratio_adam + 0.1, fitted_nugget_adam, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch)\n",
    "r_minus = check_nll_from_interpretable(fitted_sigma2_adam, fitted_range_a_adam, fitted_ratio_adam - 0.1, fitted_nugget_adam, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch)\n",
    "print(f\"  * NLL @ Î¸â‚ƒ_hat + 0.1: {r_plus:.2f} (Change: {r_plus - nll_final_adam:.2f})\")\n",
    "print(f\"  * NLL @ Î¸â‚ƒ_hat - 0.1: {r_minus:.2f} (Change: {r_minus - nll_final_adam:.2f})\")\n",
    "# Check Nugget\n",
    "n_plus = check_nll_from_interpretable(fitted_sigma2_adam, fitted_range_a_adam, fitted_ratio_adam, fitted_nugget_adam + 0.1, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch)\n",
    "n_minus = check_nll_from_interpretable(fitted_sigma2_adam, fitted_range_a_adam, fitted_ratio_adam, fitted_nugget_adam - 0.1, d_lon_sq_torch, d_lat_sq_torch, z_centered_torch)\n",
    "print(f\"  * NLL @ Î·Â²_hat + 0.1: {n_plus:.2f} (Change: {n_plus - nll_final_adam:.2f})\")\n",
    "print(f\"  * NLL @ Î·Â²_hat - 0.1: {n_minus:.2f} (Change: {n_minus - nll_final_adam:.2f})\")\n",
    "\n",
    "print(f\"  * Optimization Steps: {ADAM_ITERATIONS} epochs\")\n",
    "print(\"=\"*75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
