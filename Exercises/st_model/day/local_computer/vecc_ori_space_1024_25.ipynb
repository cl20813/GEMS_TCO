{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a2d6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy                    # clone tensor\n",
    "import time\n",
    "\n",
    "# Custom imports\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels\n",
    "from GEMS_TCO import data_preprocess \n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "import typer\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO.data_loader import load_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a09d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting data to lat: [0.0, 5.0], lon: [123.0, 133.0]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "# Assuming 'config' and 'load_data' class are defined and imported elsewhere\n",
    "\n",
    "# --- Parameters derived from your framework ---\n",
    "v: float = 0.5\n",
    "space: List[str] = ['2', '2']\n",
    "days: List[str] = ['0', '31']\n",
    "mm_cond_number: int = 20\n",
    "# --- End of framework parameters ---\n",
    "\n",
    "lat_lon_resolution = [int(s) for s in space]\n",
    "days_s_e = [int(d) for d in days]\n",
    "days_list = list(range(days_s_e[0], days_s_e[1]))\n",
    "\n",
    "# These values were not in the framework, so they remain as set in your snippet\n",
    "years = ['2024']\n",
    "month_range = [7] \n",
    "\n",
    "# Assuming 'config' is available in your environment\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "\n",
    "## load ozone data from amarel\n",
    "data_load_instance = load_data2(config.mac_data_load_path)\n",
    "\n",
    "# Call the function using the variables from the framework\n",
    "df_map, ord_mm, nns_map = data_load_instance.load_maxmin_ordered_data_bymonthyear(\n",
    "lat_lon_resolution=lat_lon_resolution, \n",
    "mm_cond_number=mm_cond_number,\n",
    "years_=years, \n",
    "months_=month_range,\n",
    "lat_range=[0.0, 5.0],      # <-- Add this\n",
    "lon_range=[123.0, 133.0]   # <-- Add this\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9a2308b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36480, 6])\n",
      "tensor([[  4.9760, 132.9840, 267.0307,  45.0000,   4.9764, 132.9858],\n",
      "        [  4.9760, 132.8580, 258.8635,  45.0000,   4.9766, 132.8600],\n",
      "        [  4.9760, 132.7320, 272.0078,  45.0000,   4.9769, 132.7346],\n",
      "        [  4.9760, 132.6060, 267.8075,  45.0000,   4.9772, 132.6088],\n",
      "        [  4.9760, 132.4800, 266.1405,  45.0000,   4.9776, 132.4833],\n",
      "        [  4.9760, 132.3540, 261.8648,  45.0000,   4.9558, 132.3577],\n",
      "        [  4.9760, 132.2280, 265.3949,  45.0000,   4.9558, 132.2320],\n",
      "        [  4.9760, 132.1020, 260.7658,  45.0000,   4.9776, 132.1070],\n",
      "        [  4.9760, 131.9760, 260.2252,  45.0000,   4.9777, 131.9819],\n",
      "        [  4.9760, 131.8500, 259.0688,  45.0000,   4.9778, 131.8566]])\n"
     ]
    }
   ],
   "source": [
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "for i in range(31):\n",
    "    idx_for_datamap = [i*8, (i+1)*8]\n",
    "\n",
    "    cur_map, cur_df = analysis_map_no_mm, agg_data_no_mm = data_load_instance.load_working_data_keep_ori(\n",
    "    df_map, \n",
    "    idx_for_datamap, \n",
    "    ord_mm=None,  # or just omit it\n",
    "    dtype=torch.float # or just omit it\n",
    ")\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "print(df_day_aggregated_list[0].shape)\n",
    "print(df_day_aggregated_list[1][:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f575ba",
   "metadata": {},
   "source": [
    "# swap columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f43b444a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  4.9764, 132.9858, 267.0307,  45.0000,   4.9764, 132.9858],\n",
      "        [  4.9766, 132.8600, 258.8635,  45.0000,   4.9766, 132.8600],\n",
      "        [  4.9769, 132.7346, 272.0078,  45.0000,   4.9769, 132.7346],\n",
      "        [  4.9772, 132.6088, 267.8075,  45.0000,   4.9772, 132.6088],\n",
      "        [  4.9776, 132.4833, 266.1405,  45.0000,   4.9776, 132.4833],\n",
      "        [  4.9558, 132.3577, 261.8648,  45.0000,   4.9558, 132.3577],\n",
      "        [  4.9558, 132.2320, 265.3949,  45.0000,   4.9558, 132.2320],\n",
      "        [  4.9776, 132.1070, 260.7658,  45.0000,   4.9776, 132.1070],\n",
      "        [  4.9777, 131.9819, 260.2252,  45.0000,   4.9777, 131.9819],\n",
      "        [  4.9778, 131.8566, 259.0688,  45.0000,   4.9778, 131.8566]])\n"
     ]
    }
   ],
   "source": [
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "for i in range(31):\n",
    "    idx_for_datamap = [i*8, (i+1)*8]\n",
    "\n",
    "    cur_map, cur_df = analysis_map_no_mm, agg_data_no_mm = data_load_instance.load_working_data_keep_ori(\n",
    "    df_map, \n",
    "    idx_for_datamap, \n",
    "    ord_mm=None,  # or just omit it\n",
    "    dtype=torch.float # or just omit it \n",
    "    )\n",
    "    cur_df[:,0], cur_df[:,1] = cur_df[:,4] , cur_df[:,5]\n",
    "\n",
    "    for map in cur_map.values():\n",
    "        map[:,0], map[:,1] = map[:,4] , map[:,5]\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "print(df_day_aggregated_list[1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca78fe34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(64359.0249, dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance1 = kernels.vecchia_experiment(0.5, df_day_map_list[0], df_day_aggregated_list[0], nns_map, mm_cond_number, nheads=10)\n",
    "\n",
    "a = [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]\n",
    "#a = [30.2594, 0.665, 1.8981, 0.0, 0.1317, -0.0, 1.9785]\n",
    "#a = [45.1402, 0.6299, 0.7308, -0.0003, -0.0151, 0.0, 7.8922]\n",
    "#a = [21.7335, 1.2817, 1.5946, 0.042, -0.1241, 0.218, 4.8654]\n",
    "#a = [20.453542336448137, 1.4506118600616982, 2.43096923637867, -0.03476556019978718, -0.1559262606484541, 0.1254833595232136, 3.938183829354925]\n",
    "params = torch.tensor(a, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "cov_map = instance1.cov_structure_saver(params, instance1.matern_cov_anisotropy_v05)  \n",
    "instance1.vecchia_oct22( params, instance1.matern_cov_anisotropy_v05, cov_map )\n",
    "\n",
    "\n",
    "# 15994.2207 15995.6299\n",
    "# 64351   64359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46abc9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0.5 # smooth\n",
    "mm_cond_number = 20\n",
    "nheads = 300\n",
    "lr = 0.02\n",
    "lr_fast = lr_slow = lr\n",
    "step = 100\n",
    "gamma_par = 0.3\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cf2aaa",
   "metadata": {},
   "source": [
    "# fit just one hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0030ba9b",
   "metadata": {},
   "source": [
    "fit on center matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c38f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading data for all days...\n",
      "Pre-loading data for all days...\n",
      "Data loaded for 31 days.\n",
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 5000.0, smooth: 0.5\n",
      "mm_cond_number: 20,\n",
      "initial parameters: \n",
      " [tensor([28.7500], dtype=torch.float64, requires_grad=True), tensor([0.9800], dtype=torch.float64, requires_grad=True), tensor([1.0600], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([1.8900], dtype=torch.float64, requires_grad=True)]\n",
      "--- Epoch 1 / Loss: 8564.823608 ---\n",
      "  Param 0: Value=28.7500, Grad=-0.0993153032075611\n",
      "  Param 1: Value=0.9800, Grad=11.309863882310452\n",
      "  Param 2: Value=1.0600, Grad=-12.847192904026997\n",
      "  Param 3: Value=0.0000, Grad=-3.339550858072471e-13\n",
      "  Param 4: Value=0.0000, Grad=-5.684341886080801e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=1.8900, Grad=-3.4275484624484074\n",
      "------------------------------\n",
      "--- Epoch 51 / Loss: 8357.356144 ---\n",
      "  Param 0: Value=29.0119, Grad=-0.14121741100180785\n",
      "  Param 1: Value=1.1754, Grad=0.325078943641369\n",
      "  Param 2: Value=2.0453, Grad=0.19604647834535172\n",
      "  Param 3: Value=0.0000, Grad=6.963318810448982e-13\n",
      "  Param 4: Value=0.0000, Grad=1.3073986337985843e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9087, Grad=-1.1815353714862367\n",
      "------------------------------\n",
      "--- Epoch 101 / Loss: 8324.333578 ---\n",
      "  Param 0: Value=30.0133, Grad=-0.10258315940132823\n",
      "  Param 1: Value=1.2819, Grad=0.06277067774448142\n",
      "  Param 2: Value=2.2256, Grad=0.020882094927969774\n",
      "  Param 3: Value=0.0000, Grad=8.810729923425242e-13\n",
      "  Param 4: Value=-0.0000, Grad=3.979039320256561e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.1372, Grad=-0.17614890390998283\n",
      "------------------------------\n",
      "--- Epoch 151 / Loss: 8318.503123 ---\n",
      "  Param 0: Value=30.3156, Grad=-0.0945722933227322\n",
      "  Param 1: Value=1.3045, Grad=0.010014877831750368\n",
      "  Param 2: Value=2.2644, Grad=-0.004565895151813493\n",
      "  Param 3: Value=0.0000, Grad=-9.379164112033322e-13\n",
      "  Param 4: Value=0.0000, Grad=-2.1032064978498966e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.1761, Grad=-0.006079310865133802\n",
      "------------------------------\n",
      "--- Epoch 201 / Loss: 8315.916142 ---\n",
      "  Param 0: Value=30.6166, Grad=-0.09099891181593556\n",
      "  Param 1: Value=1.3161, Grad=-0.0022552663555046593\n",
      "  Param 2: Value=2.2855, Grad=-0.0005401919208907202\n",
      "  Param 3: Value=0.0000, Grad=8.526512829121202e-14\n",
      "  Param 4: Value=0.0000, Grad=-5.684341886080801e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.1760, Grad=5.428801041418296e-05\n",
      "------------------------------\n",
      "--- Epoch 251 / Loss: 8314.929025 ---\n",
      "  Param 0: Value=30.7068, Grad=-0.09023524001700389\n",
      "  Param 1: Value=1.3204, Grad=0.0008653593234804191\n",
      "  Param 2: Value=2.2924, Grad=0.0005658664121028778\n",
      "  Param 3: Value=0.0000, Grad=2.5579538487363607e-13\n",
      "  Param 4: Value=-0.0000, Grad=8.810729923425242e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.1761, Grad=-0.0006149093476306433\n",
      "------------------------------\n",
      "--- Epoch 301 / Loss: 8314.173855 ---\n",
      "  Param 0: Value=30.7969, Grad=-0.08926794229354451\n",
      "  Param 1: Value=1.3240, Grad=0.0003380439766758059\n",
      "  Param 2: Value=2.2983, Grad=0.0006913783525988038\n",
      "  Param 3: Value=0.0000, Grad=5.115907697472721e-13\n",
      "  Param 4: Value=-0.0000, Grad=-7.389644451905042e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.1756, Grad=-0.0011605860365562215\n",
      "------------------------------\n",
      "Converged at epoch 348\n",
      "Epoch 349,  \n",
      " vecc Parameters: [ 3.08228258e+01  1.32500993e+00  2.30001865e+00  5.19786119e-06\n",
      " -9.68318495e-07  0.00000000e+00  3.17560948e+00]\n",
      "FINAL STATE: Epoch 349, Loss: 8313.954087906111, \n",
      " vecc Parameters: [30.822825805680306, 1.3250099320608668, 2.3000186452826, 5.197861189672096e-06, -9.683184949324036e-07, 0.0, 3.1756094801297774]\n",
      "Day 1 optimization finished in 514.44s over 349 epochs.\n",
      "Day 1 final results: [30.822825805680306, 1.3250099320608668, 2.3000186452826, 5.197861189672096e-06, -9.683184949324036e-07, 0.0, 3.1756094801297774, 8313.954087906111]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------\n",
    "# Main Optimization Loop (Updated and Corrected)\n",
    "# ------------------------------------\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "num_days_to_load = 31 # From reference code\n",
    "\n",
    "for i in range(num_days_to_load):\n",
    "    idx_for_datamap = [i*8, i*8+1]\n",
    "    #idx_for_datamap = [i*8, (i+1)*8]    \n",
    "    # Using the new load function from the reference code\n",
    "    cur_map, cur_df = data_load_instance.load_working_data(\n",
    "        df_map, \n",
    "        idx_for_datamap, \n",
    "        ord_mm=None,\n",
    "        # Using float64 to ensure compatibility with kernels\n",
    "        dtype=torch.float64 \n",
    "    )\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "# ... (Data loading section unchanged and correct) ...\n",
    "print(f\"Data loaded for {len(df_day_map_list)} days.\")\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "days_list = [0]\n",
    "for day in days_list:  \n",
    "    \n",
    "    # Get the pre-loaded data for this day\n",
    "    analysis_data_map = df_day_map_list[day]\n",
    "    aggregated_data = df_day_aggregated_list[day]\n",
    "\n",
    "\n",
    "    # Initial parameters (full vector 'a' is for reference)\n",
    "    a = [28.75, 0.98, 1.5, 0, 0, 0, 1.890]\n",
    "    \n",
    "    # NEW: Define params as a list of 1-element tensors (one per parameter)\n",
    "    params_list = [\n",
    "        torch.tensor([val], dtype=torch.float64, requires_grad=True) for val in a\n",
    "    ]\n",
    "\n",
    "    # NEW: Define learning rates and groups\n",
    "    lr_slow, lr_fast = 0.02, 0.02 # Assuming these are defined elsewhere\n",
    "    slow_indices = [ 1, 2, 3, 4, 5, 6] # e.g., ranges, advection, beta, nugget\n",
    "    fast_indices = [0] # e.g., sigmasq\n",
    "    \n",
    "    # Define Parameter Groups for the optimizer\n",
    "    param_groups = [\n",
    "        {'params': [params_list[idx] for idx in slow_indices], 'lr': lr_slow, 'name': 'slow_group'},\n",
    "        {'params': [params_list[idx] for idx in fast_indices], 'lr': lr_fast, 'name': 'fast_group'}\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Calculate resolution for printing\n",
    "    res_calc = (200 / lat_lon_resolution[0]) * (100 / lat_lon_resolution[0])\n",
    "    print(f'\\n--- Starting Day {day+1} (2024-07-{day+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params_list}')\n",
    "            \n",
    "    # --- Data loading is now done *before* the loop ---\n",
    "\n",
    "    # Define device\n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model_instance = kernels.model_fitting(\n",
    "            smooth = v,\n",
    "            input_map = analysis_data_map,\n",
    "            aggregated_data = aggregated_data,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Adjusted optimizer call: Passes the list of parameter groups\n",
    "    optimizer, scheduler = model_instance.optimizer_fun(\n",
    "            param_groups,     # <--- Pass the list of groups\n",
    "            lr=lr,            # <--- Default LR (will be overridden by groups, but needed for function sig)\n",
    "            betas=(0.9, 0.8), \n",
    "            eps=1e-8, \n",
    "            step_size=step, \n",
    "            gamma=gamma_par\n",
    "        )\n",
    "\n",
    "    # Calling the robust training loop\n",
    "    out, epoch_ran = model_instance.run_vecc_scheduler_oct23(\n",
    "            params_list,     # <--- Pass the list of parameter tensors\n",
    "            optimizer,\n",
    "            scheduler, \n",
    "            model_instance.matern_cov_anisotropy_v05, \n",
    "            epochs=epochs\n",
    "        )\n",
    "    # --- End Correction ---\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"Day {day+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day+1} final results: {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08dc648",
   "metadata": {},
   "source": [
    "center mathicng but early stopping no scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7100194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading data for all days...\n",
      "Data loaded for 31 days.\n",
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 5000.0, smooth: 0.5\n",
      "mm_cond_number: 20,\n",
      "initial parameters: \n",
      " [35.0, 0.98, 1.06, 0.0, 0.0, 0.0, 1.89]\n",
      "Epoch 1/900 | NLL: 8622.638066 | Params: [35.    0.98  1.06  0.    0.    0.    1.89]\n",
      "Epoch 51/900 | NLL: 8405.685392 | Params: [3.40374906e+01 1.11234065e+00 1.91866560e+00 9.32178729e-06\n",
      " 2.84894614e-05 0.00000000e+00 2.49266061e+00]\n",
      "Epoch 101/900 | NLL: 8277.581800 | Params: [3.49132358e+01 1.53685422e+00 2.64780792e+00 1.99547338e-05\n",
      " 3.96079457e-05 0.00000000e+00 3.24166261e+00]\n",
      "Epoch 151/900 | NLL: 8279.748637 | Params: [3.74382832e+01 1.58731120e+00 2.73555859e+00 2.04456758e-05\n",
      " 2.47450605e-05 0.00000000e+00 3.14585079e+00]\n",
      "Epoch 201/900 | NLL: 8269.555044 | Params: [4.00126311e+01 1.71158269e+00 2.94600745e+00 1.44402536e-05\n",
      " 1.33050981e-05 0.00000000e+00 3.16034945e+00]\n",
      "Epoch 251/900 | NLL: 8265.056434 | Params: [4.25847621e+01 1.82102251e+00 3.13294301e+00 1.93305414e-05\n",
      " 2.46529293e-05 0.00000000e+00 3.15823918e+00]\n",
      "Epoch 301/900 | NLL: 8262.298709 | Params: [4.51578724e+01 1.93112559e+00 3.31888654e+00 1.77727193e-05\n",
      " 3.33758469e-05 0.00000000e+00 3.16002709e+00]\n",
      "Epoch 351/900 | NLL: 8260.079924 | Params: [4.77317546e+01 2.04595685e+00 3.50764402e+00 2.47960500e-05\n",
      " 3.05860279e-05 0.00000000e+00 3.15968528e+00]\n",
      "Epoch 401/900 | NLL: 8259.471383 | Params: [5.03095403e+01 2.15660273e+00 3.70067684e+00 3.87125002e-05\n",
      " 3.17851805e-05 0.00000000e+00 3.16110222e+00]\n",
      "Epoch 451/900 | NLL: 8259.302015 | Params: [5.28924634e+01 2.26925405e+00 3.89310929e+00 4.81266326e-05\n",
      " 3.05986580e-05 0.00000000e+00 3.16230822e+00]\n",
      "Epoch 501/900 | NLL: 8259.360693 | Params: [5.54845675e+01 2.38446142e+00 4.08779215e+00 2.69141086e-05\n",
      " 2.74885539e-05 0.00000000e+00 3.16398647e+00]\n",
      "*** Early stopping triggered after 514 epochs (Patience: 70, Best NLL: 8259.008468) ***\n",
      "Day 1 optimization finished in 577.47s over 514 epochs.\n",
      "Day 1 final results (Concatenated Tensor):\n",
      " [5.25307229e+01 2.25568639e+00 3.86505395e+00 5.23430145e-05\n",
      " 3.49769959e-05 0.00000000e+00 3.16205496e+00]\n",
      "\n",
      "--- All Days Processed ---\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------\n",
    "# Main Optimization Loop (Corrected for List Input & Early Stopping)\n",
    "# ------------------------------------\n",
    "import time \n",
    "import torch\n",
    "# Assuming necessary variables (lat_lon_resolution, v, mm_cond_number, \n",
    "# nheads, lr, step, gamma_par, epochs, data_load_instance, df_map, kernels, \n",
    "# nns_map) and the variables for groups (lr_slow, lr_fast, etc.) are defined.\n",
    "\n",
    "# --- 1. Pre-load all data (unchanged) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "num_days_to_load = 31 \n",
    "\n",
    "for i in range(num_days_to_load):\n",
    "    idx_for_datamap = [i*8, i*8+1]\n",
    "    cur_map, cur_df = data_load_instance.load_working_data(\n",
    "        df_map, \n",
    "        idx_for_datamap, \n",
    "        ord_mm=None,\n",
    "        dtype=torch.float64 \n",
    "    )\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "print(f\"Data loaded for {len(df_day_map_list)} days.\")\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "days_list = [0]\n",
    "# Placeholder values (assuming they are defined globally or outside the scope)\n",
    "lr_slow, lr_fast = 0.02, 0.05 \n",
    "slow_indices = [ 1, 2, 3, 4, 5, 6] \n",
    "fast_indices = [0] \n",
    "\n",
    "for day in days_list:  \n",
    "    \n",
    "    analysis_data_map = df_day_map_list[day]\n",
    "    aggregated_data = df_day_aggregated_list[day]\n",
    "\n",
    "    # Initial parameters (full vector 'a' is for reference)\n",
    "    a = [28.75, 0.98, 1.06, 0, 0, 0, 1.890]\n",
    "    a = [35, 0.98, 1.06, 0, 0, 0, 1.890]\n",
    "    \n",
    "    # ⭐ CRITICAL CHANGE 1: Define params as a list of 1-element tensors\n",
    "    params_list = [\n",
    "        torch.tensor([val], dtype=torch.float64, requires_grad=True) for val in a\n",
    "    ]\n",
    "\n",
    "    # ⭐ CRITICAL CHANGE 2: Define Parameter Groups for the optimizer\n",
    "    param_groups = [\n",
    "        {'params': [params_list[idx] for idx in slow_indices], 'lr': lr_slow, 'name': 'slow_group'},\n",
    "        {'params': [params_list[idx] for idx in fast_indices], 'lr': lr_fast, 'name': 'fast_group'}\n",
    "    ]\n",
    "\n",
    "    # Calculate resolution for printing\n",
    "    res_calc = (200 / lat_lon_resolution[0]) * (100 / lat_lon_resolution[0])\n",
    "    print(f'\\n--- Starting Day {day+1} (2024-07-{day+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    # Note: Cannot use params.detach().numpy() here as params is not yet concatenated\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {[p.item() for p in params_list]}')\n",
    "            \n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model_instance = kernels.model_fitting(\n",
    "            smooth = v,\n",
    "            input_map = analysis_data_map,\n",
    "            aggregated_data = aggregated_data,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ⭐ CRITICAL CHANGE 3: Adjusted optimizer call: Passes the list of parameter groups\n",
    "    # It must now use 'param_groups' as the first argument.\n",
    "    optimizer, _ = model_instance.optimizer_fun(\n",
    "            param_groups,     # <--- Pass the list of groups\n",
    "            lr=lr,            # <--- Default LR (overridden by groups)\n",
    "            betas=(0.9, 0.8), \n",
    "            eps=1e-8, \n",
    "            step_size=step, \n",
    "            gamma=gamma_par\n",
    "        )\n",
    "\n",
    "    # ⭐ CRITICAL CHANGE 4: Early Stopping Training Call\n",
    "    # New function name: run_vecc_early_stp_1028 (matching your code block)\n",
    "    # Input parameter is the list of tensors: params_list\n",
    "    out, epoch_ran = model_instance.run_vecc_early_stp_1028(\n",
    "        params_list,     # <--- Pass the list of parameter tensors\n",
    "        optimizer,\n",
    "        model_instance.matern_cov_anisotropy_v05, # The covariance function\n",
    "        epochs=epochs,\n",
    "        patience=70,    # Example Early Stopping parameter\n",
    "        min_delta=1e-3  # Example Early Stopping parameter\n",
    "    )\n",
    "    # --- End Correction ---\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    # The output 'out' from run_vecc_early_stp_1028 is a concatenated tensor (best_params)\n",
    "    # plus the final epoch number.\n",
    "    print(f\"Day {day+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day+1} final results (Concatenated Tensor):\\n {out.detach().numpy()}\")\n",
    "\n",
    "print(\"\\n--- All Days Processed ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78326624",
   "metadata": {},
   "source": [
    "use swap order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6984fb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading data for all days...\n",
      "Data loaded for 31 days.\n",
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 5000.0, smooth: 0.5\n",
      "mm_cond_number: 20,\n",
      "initial parameters: \n",
      " [28.75  0.98  1.06  0.    0.    0.    1.89]\n",
      "Epoch 1, Gradients: [-1.05690956e-01  1.14465942e+01 -1.27860107e+01 -1.22070312e-04\n",
      "  6.40869141e-04  0.00000000e+00 -3.60217286e+00]\n",
      " Loss: 8566.6640625, Parameters: [28.75  0.98  1.06  0.    0.    0.    1.89]\n",
      "Epoch 51, Gradients: [-1.37584448e-01  3.43799591e-01  1.42578125e-01 -7.62939453e-04\n",
      " -2.44140625e-04  0.00000000e+00 -1.18694783e+00]\n",
      " Loss: 8360.35546875, Parameters: [29.04513558  1.1734433   2.03553031  0.07186149 -0.06086177  0.\n",
      "  2.9143169 ]\n",
      "Epoch 101, Gradients: [-0.10074802  0.04925156  0.0145874   0.00018311 -0.00015259  0.\n",
      " -0.15212309]\n",
      " Loss: 8325.1630859375, Parameters: [ 3.00445305e+01  1.28615078e+00  2.23070790e+00 -3.06708430e-03\n",
      " -3.17577525e-02  0.00000000e+00  3.15776994e+00]\n",
      "Epoch 151, Gradients: [-0.09329809 -0.00375366 -0.0032959   0.00018311  0.00021362  0.\n",
      " -0.0004686 ]\n",
      " Loss: 8319.7451171875, Parameters: [30.34685896  1.30731968  2.26792338 -0.03378841 -0.05348533  0.\n",
      "  3.1924696 ]\n",
      "Epoch 201, Gradients: [-0.09018953  0.00040054 -0.00494385  0.00012207 -0.00079346  0.\n",
      " -0.0002285 ]\n",
      " Loss: 8316.7451171875, Parameters: [ 3.06478764e+01  1.32043399e+00  2.28913224e+00 -2.15062938e-02\n",
      " -1.93691679e-03  0.00000000e+00  3.19203577e+00]\n",
      "Epoch 251, Gradients: [-8.94369334e-02 -6.10351562e-05 -6.71386719e-04  5.49316406e-04\n",
      " -1.22070312e-04  0.00000000e+00 -9.91000794e-04]\n",
      " Loss: 8315.8505859375, Parameters: [ 3.07380382e+01  1.32443626e+00  2.29614542e+00 -4.27426179e-02\n",
      "  8.38798613e-03  0.00000000e+00  3.19208551e+00]\n",
      "Epoch 301, Gradients: [-0.08843087  0.00063324 -0.00061035 -0.00036621  0.00042725  0.\n",
      "  0.00024902]\n",
      " Loss: 8315.017578125, Parameters: [ 3.08281347e+01  1.32826749e+00  2.30224357e+00 -5.62444900e-02\n",
      " -1.50029938e-02  0.00000000e+00  3.19201226e+00]\n",
      "Epoch 351, Gradients: [-8.82462412e-02  1.29699707e-04  3.66210938e-04 -1.52587891e-04\n",
      " -3.05175781e-05  0.00000000e+00 -9.40983882e-05]\n",
      " Loss: 8314.75, Parameters: [ 3.08551495e+01  1.32950601e+00  2.30453555e+00 -5.56330974e-02\n",
      " -1.62730203e-02  0.00000000e+00  3.19213002e+00]\n",
      "Epoch 401, Gradients: [-0.08803163  0.00082397  0.00097656  0.00015259  0.00018311  0.\n",
      " -0.00133141]\n",
      " Loss: 8314.5, Parameters: [ 3.08821573e+01  1.33065113e+00  2.30641829e+00 -5.16880196e-02\n",
      " -2.05975481e-02  0.00000000e+00  3.19185235e+00]\n",
      "Converged at epoch 401\n",
      "Epoch 402,  \n",
      " vecc Parameters: [ 3.08824814e+01  1.33062876e+00  2.30639410e+00 -5.16675735e-02\n",
      " -2.05771004e-02  0.00000000e+00  3.19186239e+00]\n",
      "FINAL STATE: Epoch 402, Loss: 8314.5, \n",
      " vecc Parameters: [30.882481367893934, 1.3306287573904048, 2.3063941013205755, -0.05166757348340395, -0.020577100376109176, 0.0, 3.1918623924224208]\n",
      "Day 1 optimization finished in 468.35s over 402 epochs.\n",
      "Day 1 final results: [30.882481367893934, 1.3306287573904048, 2.3063941013205755, -0.05166757348340395, -0.020577100376109176, 0.0, 3.1918623924224208, 8314.5]\n",
      "\n",
      "--- All Days Processed ---\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------\n",
    "# Main Optimization Loop (Updated)\n",
    "# ------------------------------------\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "for i in range(31):\n",
    "    #idx_for_datamap = [i*8, (i+1)*8]\n",
    "    idx_for_datamap = [i*8, i*8 +1]\n",
    "\n",
    "    cur_map, cur_df = analysis_map_no_mm, agg_data_no_mm = data_load_instance.load_working_data_keep_ori(\n",
    "    df_map, \n",
    "    idx_for_datamap, \n",
    "    ord_mm=None,  # or just omit it\n",
    "    dtype=torch.float # or just omit it \n",
    "    )\n",
    "    cur_df[:,0], cur_df[:,1] = cur_df[:,4] , cur_df[:,5]\n",
    "\n",
    "    for map in cur_map.values():\n",
    "        map[:,0], map[:,1] = map[:,4] , map[:,5]\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "\n",
    "print(f\"Data loaded for {len(df_day_map_list)} days.\")\n",
    "# print(df_day_aggregated_list[0].shape) # From reference code\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "\n",
    "# This list is now just for iterating\n",
    "#days_list = range(len(df_day_map_list)) \n",
    "days_list = [0]\n",
    "for day in days_list:  \n",
    "    \n",
    "    # Get the pre-loaded data for this day\n",
    "    analysis_data_map = df_day_map_list[day]\n",
    "    aggregated_data = df_day_aggregated_list[day]\n",
    "\n",
    "    #a = [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]\n",
    "    a = [28.75, 0.98, 1.06, 0, 0, 0, 1.890]\n",
    "    #a = [28.75, 0.98, 1.06, 0.036, -0.155, 0.179, 1.890]\n",
    "    params = torch.tensor(a, dtype=torch.float64, requires_grad=True)\n",
    "    \n",
    "    # Calculate resolution for printing\n",
    "    res_calc = (200 / lat_lon_resolution[0]) * (100 / lat_lon_resolution[0])\n",
    "    print(f'\\n--- Starting Day {day+1} (2024-07-{day+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params.detach().numpy()}')\n",
    "            \n",
    "    # --- Data loading is now done *before* the loop ---\n",
    "\n",
    "    # We need to define the device (though we aren't passing it anymore)\n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model_instance = kernels.model_fitting(\n",
    "            smooth = v,\n",
    "            input_map = analysis_data_map,\n",
    "            aggregated_data = aggregated_data,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "            # device = device_str  <--- REMOVED: This was causing the TypeError\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Adjusted optimizer call based on expected return values (step size changed to step)\n",
    "    optimizer, scheduler = model_instance.optimizer_fun(\n",
    "        params, \n",
    "        lr=lr, \n",
    "        betas=(0.9, 0.8), \n",
    "        eps=1e-8, \n",
    "        step_size=step, # Using the 'step' variable here\n",
    "        gamma=gamma_par  # Using gamma_par\n",
    "    ) \n",
    "\n",
    "    # --- CRITICAL CORRECTION ---\n",
    "    # 1. We no longer need to create a separate 'instance_map'.\n",
    "    #    'model_instance' is already the correct instance.\n",
    "    # 2. We do NOT pre-calculate 'cov_map'. The optimized training loop\n",
    "    #    'run_vecc_may9' does this internally on each epoch\n",
    "    #    to ensure the gradients are correct.\n",
    "    # 3. We call 'run_vecc_may9' (the optimized loop) instead of 'run_vecc_grp9'.\n",
    "    #    This version does not take 'cov_map' as an argument.\n",
    "    \n",
    "    # Calling the optimized 'run_vecc_may9'\n",
    "    out, epoch_ran = model_instance.run_vecc_oct22(\n",
    "        params, \n",
    "        optimizer,\n",
    "        scheduler, \n",
    "        model_instance.matern_cov_anisotropy_v05, \n",
    "        epochs=epochs\n",
    "    )\n",
    "    # --- End Correction ---\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"Day {day+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day+1} final results: {out}\")\n",
    "\n",
    "print(\"\\n--- All Days Processed ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcea0601",
   "metadata": {},
   "source": [
    "## Full likelihood\n",
    "\n",
    "Vecchia Original\n",
    "[29.921775534973218, 1.8597906958307489, 3.4954059102213106, 0.22512272633105687, -0.0648045117355326, 0.0, 3.3287024509849537] 2036\n",
    "(data 1250) # 1 23641\n",
    "\n",
    "Day 1 final results: [30.882481367893934, 1.3306287573904048, 2.3063941013205755, -0.05166757348340395, -0.020577100376109176, 0.0, 3.1918623924224208] 8314. (data 5000). \n",
    "\n",
    "\n",
    "Center matching\n",
    "[29.867643794098317, 1.8170445928338301, 3.452060053856289, 9.346808116428221e-06, 1.3480203304274802e-05, 0.0, 3.278045234831132, 2038.106906954074]\n",
    "\n",
    "[30.823365978489328, 1.325000558988864, 2.300043515398019, 5.191119207004892e-06, -9.752316964224805e-07, 0.0, 3.1756405728254657, 8313.954087906111]\n",
    "\n",
    "# 1 23644\n",
    "\n",
    "\n",
    "If I fit 8 hours on original space--> numerically unstable and cholesky decomposition fails after hitting non positive matrix\n",
    "\n",
    "\n",
    "## Vecchia likleihood 8 hours using same parameters ()\n",
    "a = [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]\n",
    "\n",
    "original   15995.6299\n",
    "center matching # 15994.2207 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de556f",
   "metadata": {},
   "source": [
    "Pre-loading data for all days...\n",
    "Data loaded for 31 days.\n",
    "\n",
    "--- Starting Day 1 (2024-07-1) ---\n",
    "Data size per day: 1250.0, smooth: 0.5\n",
    "mm_cond_number: 20,\n",
    "initial parameters: \n",
    " [28.75   0.98   1.06   0.036 -0.155  0.179  1.89 ]\n",
    "Converged at epoch 535\n",
    "Epoch 536,  \n",
    " vecc Parameters: [31.00915505  1.1916293   1.84680497  0.03927127 -0.15666047  0.12771191\n",
    "  4.11954968]\n",
    "FINAL STATE: Epoch 536, Loss: 15746.237437383388, \n",
    " vecc Parameters: [31.009155050569746, 1.191629302554168, 1.846804972667287, 0.03927126531202711, -0.15666046904308123, 0.12771190935579058, 4.119549677954342]\n",
    "Day 1 optimization finished in 822.07s over 536 epochs.\n",
    "Day 1 final results: [31.009155050569746, 1.191629302554168, 1.846804972667287, 0.03927126531202711, -0.15666046904308123, 0.12771190935579058, 4.119549677954342, 15746.237437383388]\n",
    "\n",
    "--- All Days Processed ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caf2b92",
   "metadata": {},
   "source": [
    "Pre-loading data for all days...\n",
    "Data loaded for 31 days.\n",
    "\n",
    "--- Starting Day 1 (2024-07-1) ---\n",
    "Data size per day: 1250.0, smooth: 0.5\n",
    "mm_cond_number: 20,\n",
    "initial parameters: \n",
    " [28.75  0.98  1.06  0.    0.    0.    1.89]\n",
    "Converged at epoch 332\n",
    "Epoch 333,  \n",
    " vecc Parameters: [2.98676438e+01 1.81704459e+00 3.45206005e+00 9.34680812e-06\n",
    " 1.34802033e-05 0.00000000e+00 3.27804523e+00]\n",
    "FINAL STATE: Epoch 333, Loss: 2038.106906954074, \n",
    " vecc Parameters: [29.867643794098317, 1.8170445928338301, 3.452060053856289, 9.346808116428221e-06, 1.3480203304274802e-05, 0.0, 3.278045234831132]\n",
    "Day 1 optimization finished in 73.78s over 333 epochs.\n",
    "Day 1 final results: [29.867643794098317, 1.8170445928338301, 3.452060053856289, 9.346808116428221e-06, 1.3480203304274802e-05, 0.0, 3.278045234831132, 2038.106906954074]\n",
    "\n",
    "--- All Days Processed ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
