{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a2d6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy                    # clone tensor\n",
    "import time\n",
    "\n",
    "# Custom imports\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels\n",
    "from GEMS_TCO import data_preprocess \n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "import typer\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO.data_loader import load_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a09d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting data to lat: [0.0, 5.0], lon: [123.0, 133.0]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "# Assuming 'config' and 'load_data' class are defined and imported elsewhere\n",
    "\n",
    "# --- Parameters derived from your framework ---\n",
    "v: float = 0.5\n",
    "space: List[str] = ['2', '2']\n",
    "days: List[str] = ['0', '31']\n",
    "mm_cond_number: int = 20\n",
    "# --- End of framework parameters ---\n",
    "\n",
    "lat_lon_resolution = [int(s) for s in space]\n",
    "days_s_e = [int(d) for d in days]\n",
    "days_list = list(range(days_s_e[0], days_s_e[1]))\n",
    "\n",
    "# These values were not in the framework, so they remain as set in your snippet\n",
    "years = ['2024']\n",
    "month_range = [7] \n",
    "\n",
    "# Assuming 'config' is available in your environment\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "\n",
    "## load ozone data from amarel\n",
    "data_load_instance = load_data2(config.mac_data_load_path)\n",
    "\n",
    "# Call the function using the variables from the framework\n",
    "df_map, ord_mm, nns_map = data_load_instance.load_maxmin_ordered_data_bymonthyear(\n",
    "lat_lon_resolution=lat_lon_resolution, \n",
    "mm_cond_number=mm_cond_number,\n",
    "years_=years, \n",
    "months_=month_range,\n",
    "lat_range=[0.0, 5.0],      # <-- Add this\n",
    "lon_range=[123.0, 133.0]   # <-- Add this\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9a2308b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36480, 6])\n",
      "tensor([[  4.9760, 132.9840, 267.0307,  45.0000,   4.9764, 132.9858],\n",
      "        [  4.9760, 132.8580, 258.8635,  45.0000,   4.9766, 132.8600],\n",
      "        [  4.9760, 132.7320, 272.0078,  45.0000,   4.9769, 132.7346],\n",
      "        [  4.9760, 132.6060, 267.8075,  45.0000,   4.9772, 132.6088],\n",
      "        [  4.9760, 132.4800, 266.1405,  45.0000,   4.9776, 132.4833],\n",
      "        [  4.9760, 132.3540, 261.8648,  45.0000,   4.9558, 132.3577],\n",
      "        [  4.9760, 132.2280, 265.3949,  45.0000,   4.9558, 132.2320],\n",
      "        [  4.9760, 132.1020, 260.7658,  45.0000,   4.9776, 132.1070],\n",
      "        [  4.9760, 131.9760, 260.2252,  45.0000,   4.9777, 131.9819],\n",
      "        [  4.9760, 131.8500, 259.0688,  45.0000,   4.9778, 131.8566]])\n"
     ]
    }
   ],
   "source": [
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "for i in range(31):\n",
    "    idx_for_datamap = [i*8, (i+1)*8]\n",
    "\n",
    "    cur_map, cur_df = analysis_map_no_mm, agg_data_no_mm = data_load_instance.load_working_data_keep_ori(\n",
    "    df_map, \n",
    "    idx_for_datamap, \n",
    "    ord_mm=None,  # or just omit it\n",
    "    dtype=torch.float # or just omit it\n",
    ")\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "print(df_day_aggregated_list[0].shape)\n",
    "print(df_day_aggregated_list[1][:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af9d590e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04251644736842105"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "a= (df_day_aggregated_list[1][:,0] - df_day_aggregated_list[1][:,4]).numpy()\n",
    "\n",
    "len(a[a>0.01])/ len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f575ba",
   "metadata": {},
   "source": [
    "# swap columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f43b444a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  4.9764, 132.9858, 267.0307,  45.0000,   4.9764, 132.9858],\n",
      "        [  4.9766, 132.8600, 258.8635,  45.0000,   4.9766, 132.8600],\n",
      "        [  4.9769, 132.7346, 272.0078,  45.0000,   4.9769, 132.7346],\n",
      "        [  4.9772, 132.6088, 267.8075,  45.0000,   4.9772, 132.6088],\n",
      "        [  4.9776, 132.4833, 266.1405,  45.0000,   4.9776, 132.4833],\n",
      "        [  4.9558, 132.3577, 261.8648,  45.0000,   4.9558, 132.3577],\n",
      "        [  4.9558, 132.2320, 265.3949,  45.0000,   4.9558, 132.2320],\n",
      "        [  4.9776, 132.1070, 260.7658,  45.0000,   4.9776, 132.1070],\n",
      "        [  4.9777, 131.9819, 260.2252,  45.0000,   4.9777, 131.9819],\n",
      "        [  4.9778, 131.8566, 259.0688,  45.0000,   4.9778, 131.8566]])\n"
     ]
    }
   ],
   "source": [
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "for i in range(31):\n",
    "    idx_for_datamap = [i*8, (i+1)*8]\n",
    "\n",
    "    cur_map, cur_df = analysis_map_no_mm, agg_data_no_mm = data_load_instance.load_working_data_keep_ori(\n",
    "    df_map, \n",
    "    idx_for_datamap, \n",
    "    ord_mm=None,  # or just omit it\n",
    "    dtype=torch.float # or just omit it \n",
    "    )\n",
    "    cur_df[:,0], cur_df[:,1] = cur_df[:,4] , cur_df[:,5]\n",
    "\n",
    "    for map in cur_map.values():\n",
    "        map[:,0], map[:,1] = map[:,4] , map[:,5]\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "print(df_day_aggregated_list[1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca78fe34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(64351.7277, dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance1 = kernels.vecchia_experiment(0.5, df_day_map_list[0], df_day_aggregated_list[0], nns_map, mm_cond_number, nheads=10)\n",
    "\n",
    "a = [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]\n",
    "#a = [30.2594, 0.665, 1.8981, 0.0, 0.1317, -0.0, 1.9785]\n",
    "#a = [45.1402, 0.6299, 0.7308, -0.0003, -0.0151, 0.0, 7.8922]\n",
    "#a = [21.7335, 1.2817, 1.5946, 0.042, -0.1241, 0.218, 4.8654]\n",
    "#a = [20.453542336448137, 1.4506118600616982, 2.43096923637867, -0.03476556019978718, -0.1559262606484541, 0.1254833595232136, 3.938183829354925]\n",
    "params = torch.tensor(a, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "cov_map = instance1.cov_structure_saver(params, instance1.matern_cov_anisotropy_v05)  \n",
    "instance1.vecchia_oct22( params, instance1.matern_cov_anisotropy_v05, cov_map )\n",
    "\n",
    "\n",
    "# 15994.2207 15995.6299\n",
    "# 64351   64359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46abc9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0.5 # smooth\n",
    "mm_cond_number = 20\n",
    "nheads = 300\n",
    "lr = 0.02\n",
    "lr_fast = lr_slow = lr\n",
    "step = 100\n",
    "gamma_par = 0.3\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f043cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0.5 # smooth\n",
    "mm_cond_number = 20\n",
    "nheads = 600\n",
    "lr = 0.02\n",
    "lr_fast = lr_slow = lr\n",
    "step = 100\n",
    "gamma_par = 0.3\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "364fc41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading data for all days...\n",
      "Pre-loading data for all days...\n",
      "Data loaded for 31 days.\n",
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 5000.0, smooth: 0.5\n",
      "mm_cond_number: 20,\n",
      "initial parameters: \n",
      " [tensor([28.7500], dtype=torch.float64, requires_grad=True), tensor([0.9800], dtype=torch.float64, requires_grad=True), tensor([1.5000], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([1.8900], dtype=torch.float64, requires_grad=True)]\n",
      "--- Epoch 1 / Loss: 8363.102216 ---\n",
      "  Param 0: Value=28.7500, Grad=-0.37640853916042616\n",
      "  Param 1: Value=0.9800, Grad=11.524674971331365\n",
      "  Param 2: Value=1.5000, Grad=-3.7692176301393374\n",
      "  Param 3: Value=0.0000, Grad=4.547473508864641e-13\n",
      "  Param 4: Value=0.0000, Grad=-9.094947017729282e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=1.8900, Grad=-9.039175791333584\n",
      "------------------------------\n",
      "--- Epoch 11 / Loss: 8424.112753 ---\n",
      "  Param 0: Value=28.9533, Grad=-0.030204765734286543\n",
      "  Param 1: Value=0.8305, Grad=-6.631286968540024\n",
      "  Param 2: Value=1.6449, Grad=0.9682012922526155\n",
      "  Param 3: Value=-0.0000, Grad=6.366462912410498e-12\n",
      "  Param 4: Value=0.0000, Grad=2.7284841053187847e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.0938, Grad=-3.734608085865463\n",
      "------------------------------\n",
      "--- Epoch 21 / Loss: 8369.835149 ---\n",
      "  Param 0: Value=29.0896, Grad=0.05950786547132547\n",
      "  Param 1: Value=0.9338, Grad=-2.259289759950093\n",
      "  Param 2: Value=1.6789, Grad=-2.528528180213584\n",
      "  Param 3: Value=-0.0000, Grad=-1.3642420526593924e-12\n",
      "  Param 4: Value=-0.0000, Grad=-1.2789769243681803e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.3187, Grad=-1.410624500867872\n",
      "------------------------------\n",
      "--- Epoch 31 / Loss: 8296.451269 ---\n",
      "  Param 0: Value=29.0417, Grad=0.021010578658937407\n",
      "  Param 1: Value=1.0964, Grad=1.0339190887703182\n",
      "  Param 2: Value=1.8171, Grad=-3.3613344529265987\n",
      "  Param 3: Value=-0.0000, Grad=-3.865352482534945e-12\n",
      "  Param 4: Value=-0.0000, Grad=8.242295734817162e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.5749, Grad=-0.5712054174180552\n",
      "------------------------------\n",
      "--- Epoch 41 / Loss: 8267.510702 ---\n",
      "  Param 0: Value=28.9049, Grad=0.015621751647270088\n",
      "  Param 1: Value=1.1672, Grad=-0.40265097282191675\n",
      "  Param 2: Value=1.9934, Grad=-2.0621545340615057\n",
      "  Param 3: Value=-0.0000, Grad=1.1368683772161603e-13\n",
      "  Param 4: Value=-0.0000, Grad=-6.110667527536862e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.7999, Grad=0.7280025725832617\n",
      "------------------------------\n",
      "--- Epoch 51 / Loss: 8240.127214 ---\n",
      "  Param 0: Value=28.8668, Grad=-0.1461359690861359\n",
      "  Param 1: Value=1.2540, Grad=-0.012918286548810443\n",
      "  Param 2: Value=2.2148, Grad=0.17091061153149667\n",
      "  Param 3: Value=-0.0000, Grad=-2.6147972675971687e-12\n",
      "  Param 4: Value=-0.0000, Grad=9.663381206337363e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.8375, Grad=-0.7056190097953854\n",
      "------------------------------\n",
      "--- Epoch 61 / Loss: 8214.576171 ---\n",
      "  Param 0: Value=28.9928, Grad=-0.166996398994965\n",
      "  Param 1: Value=1.3512, Grad=0.18684124166276206\n",
      "  Param 2: Value=2.3546, Grad=0.3919388475548544\n",
      "  Param 3: Value=-0.0000, Grad=-9.094947017729282e-13\n",
      "  Param 4: Value=-0.0000, Grad=-1.5347723092418164e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9656, Grad=-0.2024386942640839\n",
      "------------------------------\n",
      "--- Epoch 71 / Loss: 8208.082301 ---\n",
      "  Param 0: Value=29.1704, Grad=-0.12500984734373843\n",
      "  Param 1: Value=1.3752, Grad=0.05758218487564193\n",
      "  Param 2: Value=2.3645, Grad=-0.017191303694971793\n",
      "  Param 3: Value=0.0000, Grad=6.821210263296962e-13\n",
      "  Param 4: Value=-0.0000, Grad=8.242295734817162e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.0299, Grad=0.5827501412991771\n",
      "------------------------------\n",
      "--- Epoch 81 / Loss: 8220.563464 ---\n",
      "  Param 0: Value=29.3658, Grad=-0.13966427156371664\n",
      "  Param 1: Value=1.3279, Grad=-0.015184840775447128\n",
      "  Param 2: Value=2.3271, Grad=0.22646053108729802\n",
      "  Param 3: Value=0.0000, Grad=-3.865352482534945e-12\n",
      "  Param 4: Value=-0.0000, Grad=7.389644451905042e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9274, Grad=-0.17856474297491998\n",
      "------------------------------\n",
      "--- Epoch 91 / Loss: 8227.701275 ---\n",
      "  Param 0: Value=29.5699, Grad=-0.10554138453119144\n",
      "  Param 1: Value=1.2996, Grad=-0.022153068062905845\n",
      "  Param 2: Value=2.2647, Grad=-0.20053354010821067\n",
      "  Param 3: Value=0.0000, Grad=0.0\n",
      "  Param 4: Value=-0.0000, Grad=6.536993168992922e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.8867, Grad=-0.09589009057808218\n",
      "------------------------------\n",
      "--- Epoch 101 / Loss: 8214.591805 ---\n",
      "  Param 0: Value=29.7774, Grad=-0.1192797387193103\n",
      "  Param 1: Value=1.3526, Grad=-0.021828641565576845\n",
      "  Param 2: Value=2.3550, Grad=0.05179594364892637\n",
      "  Param 3: Value=0.0000, Grad=2.2737367544323206e-13\n",
      "  Param 4: Value=-0.0000, Grad=2.8421709430404007e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9494, Grad=0.10425275110858001\n",
      "------------------------------\n",
      "--- Epoch 111 / Loss: 8214.788366 ---\n",
      "  Param 0: Value=29.8370, Grad=-0.12156907409315587\n",
      "  Param 1: Value=1.3522, Grad=0.018309632367746076\n",
      "  Param 2: Value=2.3550, Grad=0.06721825500909517\n",
      "  Param 3: Value=0.0000, Grad=-7.958078640513122e-13\n",
      "  Param 4: Value=-0.0000, Grad=3.339550858072471e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9394, Grad=0.009197556412380958\n",
      "------------------------------\n",
      "--- Epoch 121 / Loss: 8218.905036 ---\n",
      "  Param 0: Value=29.8971, Grad=-0.11074154738994935\n",
      "  Param 1: Value=1.3355, Grad=-0.005393106879793663\n",
      "  Param 2: Value=2.3262, Grad=-0.05933785732719343\n",
      "  Param 3: Value=0.0000, Grad=6.821210263296962e-13\n",
      "  Param 4: Value=-0.0000, Grad=-3.339550858072471e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9159, Grad=-0.022398096183092964\n",
      "------------------------------\n",
      "--- Epoch 131 / Loss: 8215.980770 ---\n",
      "  Param 0: Value=29.9582, Grad=-0.11471035013346897\n",
      "  Param 1: Value=1.3478, Grad=0.00566707826038737\n",
      "  Param 2: Value=2.3473, Grad=0.0035800167106003755\n",
      "  Param 3: Value=0.0000, Grad=3.865352482534945e-12\n",
      "  Param 4: Value=-0.0000, Grad=4.106937012693379e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9278, Grad=-0.0025298647060876256\n",
      "------------------------------\n",
      "--- Epoch 141 / Loss: 8215.435443 ---\n",
      "  Param 0: Value=30.0181, Grad=-0.11422861278457974\n",
      "  Param 1: Value=1.3505, Grad=-0.0019466861115802203\n",
      "  Param 2: Value=2.3528, Grad=0.013338873833561138\n",
      "  Param 3: Value=0.0000, Grad=-1.1368683772161603e-13\n",
      "  Param 4: Value=-0.0000, Grad=5.684341886080801e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9280, Grad=-0.0028666241799388814\n",
      "------------------------------\n",
      "--- Epoch 151 / Loss: 8215.312936 ---\n",
      "  Param 0: Value=30.0784, Grad=-0.11201826603718912\n",
      "  Param 1: Value=1.3513, Grad=0.0021803933068724746\n",
      "  Param 2: Value=2.3532, Grad=-0.007740791930530122\n",
      "  Param 3: Value=0.0000, Grad=6.821210263296962e-13\n",
      "  Param 4: Value=-0.0000, Grad=4.405364961712621e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9256, Grad=-0.0020880402027247946\n",
      "------------------------------\n",
      "--- Epoch 161 / Loss: 8214.808325 ---\n",
      "  Param 0: Value=30.1386, Grad=-0.11105291515229032\n",
      "  Param 1: Value=1.3537, Grad=0.0033074586925749827\n",
      "  Param 2: Value=2.3573, Grad=-0.009892542538409543\n",
      "  Param 3: Value=0.0000, Grad=1.0231815394945443e-12\n",
      "  Param 4: Value=-0.0000, Grad=3.581135388230905e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9253, Grad=-0.0016043716089422233\n",
      "------------------------------\n",
      "--- Epoch 171 / Loss: 8213.670171 ---\n",
      "  Param 0: Value=30.1989, Grad=-0.11170894642223006\n",
      "  Param 1: Value=1.3589, Grad=0.006468844002327501\n",
      "  Param 2: Value=2.3663, Grad=0.009326791540956947\n",
      "  Param 3: Value=0.0000, Grad=-5.115907697472721e-12\n",
      "  Param 4: Value=-0.0000, Grad=-2.4158453015843406e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9281, Grad=0.00041381065504086934\n",
      "------------------------------\n",
      "--- Epoch 181 / Loss: 8213.536870 ---\n",
      "  Param 0: Value=30.2591, Grad=-0.11018893767264126\n",
      "  Param 1: Value=1.3599, Grad=-0.007112188631680283\n",
      "  Param 2: Value=2.3691, Grad=0.007893562115384611\n",
      "  Param 3: Value=0.0000, Grad=1.1368683772161603e-13\n",
      "  Param 4: Value=-0.0001, Grad=6.394884621840902e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9268, Grad=0.0025301975230642704\n",
      "------------------------------\n",
      "--- Epoch 191 / Loss: 8213.102355 ---\n",
      "  Param 0: Value=30.3193, Grad=-0.10884870309698735\n",
      "  Param 1: Value=1.3621, Grad=-0.003953950161090347\n",
      "  Param 2: Value=2.3723, Grad=-0.0008259703843691568\n",
      "  Param 3: Value=0.0000, Grad=3.296918293926865e-12\n",
      "  Param 4: Value=-0.0001, Grad=1.2789769243681803e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9262, Grad=0.0052002247284246295\n",
      "------------------------------\n",
      "--- Epoch 201 / Loss: 8212.578757 ---\n",
      "  Param 0: Value=30.3795, Grad=-0.10799700066973283\n",
      "  Param 1: Value=1.3646, Grad=0.003130863001956641\n",
      "  Param 2: Value=2.3761, Grad=-0.005402978927691038\n",
      "  Param 3: Value=0.0000, Grad=-1.1368683772161603e-12\n",
      "  Param 4: Value=-0.0001, Grad=-9.379164112033322e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9256, Grad=0.002646017689981406\n",
      "------------------------------\n",
      "--- Epoch 211 / Loss: 8212.454680 ---\n",
      "  Param 0: Value=30.3976, Grad=-0.10824227476429993\n",
      "  Param 1: Value=1.3653, Grad=0.0008733398054943109\n",
      "  Param 2: Value=2.3780, Grad=0.0024018839780524104\n",
      "  Param 3: Value=0.0000, Grad=-3.410605131648481e-13\n",
      "  Param 4: Value=-0.0001, Grad=-1.0373923942097463e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9253, Grad=-0.004045547235223879\n",
      "------------------------------\n",
      "--- Epoch 221 / Loss: 8212.290822 ---\n",
      "  Param 0: Value=30.4156, Grad=-0.10793076513468947\n",
      "  Param 1: Value=1.3662, Grad=-0.0021105633664006973\n",
      "  Param 2: Value=2.3796, Grad=0.003384536243970615\n",
      "  Param 3: Value=0.0000, Grad=9.094947017729282e-13\n",
      "  Param 4: Value=-0.0001, Grad=-2.2595258997171186e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9256, Grad=-0.0008274364929596878\n",
      "------------------------------\n",
      "--- Epoch 231 / Loss: 8212.135752 ---\n",
      "  Param 0: Value=30.4337, Grad=-0.10757241343419377\n",
      "  Param 1: Value=1.3669, Grad=0.0001452609290311102\n",
      "  Param 2: Value=2.3806, Grad=0.0005365993915233958\n",
      "  Param 3: Value=0.0000, Grad=-2.3874235921539366e-12\n",
      "  Param 4: Value=-0.0001, Grad=-4.547473508864641e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9255, Grad=-0.00017160323797416055\n",
      "------------------------------\n",
      "--- Epoch 241 / Loss: 8211.988252 ---\n",
      "  Param 0: Value=30.4517, Grad=-0.10725854550903424\n",
      "  Param 1: Value=1.3677, Grad=0.001241375263745681\n",
      "  Param 2: Value=2.3817, Grad=-0.0010439756474625028\n",
      "  Param 3: Value=0.0000, Grad=4.092726157978177e-12\n",
      "  Param 4: Value=-0.0001, Grad=-2.984279490192421e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9254, Grad=0.00022022425974999038\n",
      "------------------------------\n",
      "--- Epoch 251 / Loss: 8211.868393 ---\n",
      "  Param 0: Value=30.4697, Grad=-0.1070454652001458\n",
      "  Param 1: Value=1.3683, Grad=0.00039892298777033375\n",
      "  Param 2: Value=2.3830, Grad=-0.0001817440211198118\n",
      "  Param 3: Value=0.0000, Grad=-2.2737367544323206e-13\n",
      "  Param 4: Value=-0.0001, Grad=5.400124791776761e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9251, Grad=-0.0008265563028162504\n",
      "------------------------------\n",
      "--- Epoch 261 / Loss: 8211.720210 ---\n",
      "  Param 0: Value=30.4877, Grad=-0.10675721576627728\n",
      "  Param 1: Value=1.3691, Grad=0.0005397419010080284\n",
      "  Param 2: Value=2.3842, Grad=-0.0008577165176006929\n",
      "  Param 3: Value=0.0000, Grad=6.821210263296962e-13\n",
      "  Param 4: Value=-0.0001, Grad=-1.2221335055073723e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9251, Grad=-0.0001899273261178891\n",
      "------------------------------\n",
      "--- Epoch 271 / Loss: 8211.590506 ---\n",
      "  Param 0: Value=30.5057, Grad=-0.10660026404169143\n",
      "  Param 1: Value=1.3698, Grad=-0.0008974566158030939\n",
      "  Param 2: Value=2.3856, Grad=0.001110185700895272\n",
      "  Param 3: Value=0.0000, Grad=-1.1368683772161603e-13\n",
      "  Param 4: Value=-0.0001, Grad=-1.6910917111090384e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9250, Grad=-0.0012578669549161425\n",
      "------------------------------\n",
      "--- Epoch 281 / Loss: 8211.443450 ---\n",
      "  Param 0: Value=30.5238, Grad=-0.10620266534979805\n",
      "  Param 1: Value=1.3705, Grad=-0.001979903728383192\n",
      "  Param 2: Value=2.3869, Grad=-0.00027925249065674507\n",
      "  Param 3: Value=0.0000, Grad=2.5011104298755527e-12\n",
      "  Param 4: Value=-0.0001, Grad=1.0942358130705543e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9251, Grad=0.0016152939432028113\n",
      "------------------------------\n",
      "--- Epoch 291 / Loss: 8211.268211 ---\n",
      "  Param 0: Value=30.5418, Grad=-0.10597357946224051\n",
      "  Param 1: Value=1.3714, Grad=0.0011588559582662583\n",
      "  Param 2: Value=2.3881, Grad=-0.001966667343936024\n",
      "  Param 3: Value=0.0000, Grad=-2.9558577807620168e-12\n",
      "  Param 4: Value=-0.0001, Grad=9.379164112033322e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9250, Grad=0.0009088027652448538\n",
      "------------------------------\n",
      "--- Epoch 301 / Loss: 8211.137019 ---\n",
      "  Param 0: Value=30.5598, Grad=-0.10596835182832498\n",
      "  Param 1: Value=1.3721, Grad=0.00018999335148350838\n",
      "  Param 2: Value=2.3896, Grad=0.001695255126946904\n",
      "  Param 3: Value=0.0000, Grad=-1.8189894035458565e-12\n",
      "  Param 4: Value=-0.0001, Grad=-4.263256414560601e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9248, Grad=-0.0024433460314230437\n",
      "------------------------------\n",
      "--- Epoch 311 / Loss: 8211.071288 ---\n",
      "  Param 0: Value=30.5652, Grad=-0.10572594435467009\n",
      "  Param 1: Value=1.3724, Grad=0.0008065453868795203\n",
      "  Param 2: Value=2.3899, Grad=-0.000877918587093518\n",
      "  Param 3: Value=0.0000, Grad=6.821210263296962e-13\n",
      "  Param 4: Value=-0.0001, Grad=1.7763568394002505e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9250, Grad=0.0005363689659323434\n",
      "------------------------------\n",
      "--- Epoch 321 / Loss: 8211.052798 ---\n",
      "  Param 0: Value=30.5706, Grad=-0.10572715200215876\n",
      "  Param 1: Value=1.3726, Grad=-0.00021642242606390028\n",
      "  Param 2: Value=2.3903, Grad=0.0006581825436455802\n",
      "  Param 3: Value=0.0000, Grad=-1.7053025658242404e-12\n",
      "  Param 4: Value=-0.0001, Grad=-9.094947017729282e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9248, Grad=-0.0009351164177946458\n",
      "------------------------------\n",
      "--- Epoch 331 / Loss: 8210.991750 ---\n",
      "  Param 0: Value=30.5760, Grad=-0.10559895221016014\n",
      "  Param 1: Value=1.3729, Grad=0.0005091192604282924\n",
      "  Param 2: Value=2.3907, Grad=-0.0004885364406277404\n",
      "  Param 3: Value=0.0000, Grad=-2.8421709430404007e-12\n",
      "  Param 4: Value=-0.0001, Grad=-1.4068746168049984e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9249, Grad=0.00017851721868833437\n",
      "------------------------------\n",
      "--- Epoch 341 / Loss: 8210.959367 ---\n",
      "  Param 0: Value=30.5814, Grad=-0.1055349747376324\n",
      "  Param 1: Value=1.3730, Grad=-0.00041673600627234464\n",
      "  Param 2: Value=2.3911, Grad=0.00015628749076768145\n",
      "  Param 3: Value=0.0000, Grad=-3.524291969370097e-12\n",
      "  Param 4: Value=-0.0001, Grad=-1.6342482922482304e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9249, Grad=0.00014522670596983644\n",
      "------------------------------\n",
      "--- Epoch 351 / Loss: 8210.910254 ---\n",
      "  Param 0: Value=30.5868, Grad=-0.1055120664214173\n",
      "  Param 1: Value=1.3733, Grad=0.00031677433869958804\n",
      "  Param 2: Value=2.3915, Grad=0.00036605987537541296\n",
      "  Param 3: Value=0.0000, Grad=-7.958078640513122e-13\n",
      "  Param 4: Value=-0.0001, Grad=1.9895196601282805e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9248, Grad=-0.0007195797887349364\n",
      "------------------------------\n",
      "--- Epoch 361 / Loss: 8210.871061 ---\n",
      "  Param 0: Value=30.5922, Grad=-0.10535017036895988\n",
      "  Param 1: Value=1.3735, Grad=-0.00028010197023320416\n",
      "  Param 2: Value=2.3919, Grad=-0.0004662178947114626\n",
      "  Param 3: Value=0.0000, Grad=-9.094947017729282e-13\n",
      "  Param 4: Value=-0.0001, Grad=7.389644451905042e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9249, Grad=0.0007351641061146275\n",
      "------------------------------\n",
      "Converged at epoch 367\n",
      "Epoch 368,  \n",
      " vecc Parameters: [ 3.05959975e+01  1.37364494e+00  2.39212313e+00  1.44753893e-06\n",
      " -6.20693187e-05  0.00000000e+00  2.92481524e+00]\n",
      "FINAL STATE: Epoch 368, Loss: 8210.844312040072, \n",
      " vecc Parameters: [30.59599753739114, 1.3736449365467722, 2.392123125548249, 1.4475389271016221e-06, -6.206931871974381e-05, 0.0, 2.9248152394498823]\n",
      "Day 1 optimization finished in 423.75s over 368 epochs.\n",
      "Day 1 final results: [30.59599753739114, 1.3736449365467722, 2.392123125548249, 1.4475389271016221e-06, -6.206931871974381e-05, 0.0, 2.9248152394498823, 8210.844312040072]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------\n",
    "# Main Optimization Loop (Updated and Corrected)\n",
    "# ------------------------------------\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "num_days_to_load = 31 # From reference code\n",
    "\n",
    "for i in range(num_days_to_load):\n",
    "    idx_for_datamap = [i*8, i*8+1]\n",
    "    #idx_for_datamap = [i*8, (i+1)*8]    \n",
    "    # Using the new load function from the reference code\n",
    "    cur_map, cur_df = data_load_instance.load_working_data(\n",
    "        df_map, \n",
    "        idx_for_datamap, \n",
    "        ord_mm=None,\n",
    "        # Using float64 to ensure compatibility with kernels\n",
    "        dtype=torch.float64 \n",
    "    )\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "# ... (Data loading section unchanged and correct) ...\n",
    "print(f\"Data loaded for {len(df_day_map_list)} days.\")\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "days_list = [0]\n",
    "for day in days_list:  \n",
    "    \n",
    "    # Get the pre-loaded data for this day\n",
    "    analysis_data_map = df_day_map_list[day]\n",
    "    aggregated_data = df_day_aggregated_list[day]\n",
    "\n",
    "\n",
    "    # Initial parameters (full vector 'a' is for reference)\n",
    "    a = [28.75, 0.98, 1.5, 0, 0, 0, 1.890]\n",
    "    \n",
    "    # NEW: Define params as a list of 1-element tensors (one per parameter)\n",
    "    params_list = [\n",
    "        torch.tensor([val], dtype=torch.float64, requires_grad=True) for val in a\n",
    "    ]\n",
    "\n",
    "    # NEW: Define learning rates and groups\n",
    "    lr_slow, lr_fast = 0.02, 0.02 # Assuming these are defined elsewhere\n",
    "    slow_indices = [ 1, 2, 3, 4, 5, 6] # e.g., ranges, advection, beta, nugget\n",
    "    fast_indices = [0] # e.g., sigmasq\n",
    "    \n",
    "    # Define Parameter Groups for the optimizer\n",
    "    param_groups = [\n",
    "        {'params': [params_list[idx] for idx in slow_indices], 'lr': lr_slow, 'name': 'slow_group'},\n",
    "        {'params': [params_list[idx] for idx in fast_indices], 'lr': lr_fast, 'name': 'fast_group'}\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Calculate resolution for printing\n",
    "    res_calc = (200 / lat_lon_resolution[0]) * (100 / lat_lon_resolution[0])\n",
    "    print(f'\\n--- Starting Day {day+1} (2024-07-{day+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params_list}')\n",
    "            \n",
    "    # --- Data loading is now done *before* the loop ---\n",
    "\n",
    "    # Define device\n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model_instance = kernels.model_fitting(\n",
    "            smooth = v,\n",
    "            input_map = analysis_data_map,\n",
    "            aggregated_data = aggregated_data,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Adjusted optimizer call: Passes the list of parameter groups\n",
    "    optimizer, scheduler = model_instance.optimizer_fun(\n",
    "            param_groups,     # <--- Pass the list of groups\n",
    "            lr=lr,            # <--- Default LR (will be overridden by groups, but needed for function sig)\n",
    "            betas=(0.9, 0.8), \n",
    "            eps=1e-8, \n",
    "            step_size=step, \n",
    "            gamma=gamma_par\n",
    "        )\n",
    "\n",
    "    # Calling the robust training loop\n",
    "    out, epoch_ran = model_instance.run_vecc_scheduler_oct23(\n",
    "            params_list,     # <--- Pass the list of parameter tensors\n",
    "            optimizer,\n",
    "            scheduler, \n",
    "            model_instance.matern_cov_anisotropy_v05, \n",
    "            epochs=epochs\n",
    "        )\n",
    "    # --- End Correction ---\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"Day {day+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day+1} final results: {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cf2aaa",
   "metadata": {},
   "source": [
    "# fit just one hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0030ba9b",
   "metadata": {},
   "source": [
    "fit on center matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c38f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading data for all days...\n",
      "Pre-loading data for all days...\n",
      "Data loaded for 31 days.\n",
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 5000.0, smooth: 0.5\n",
      "mm_cond_number: 20,\n",
      "initial parameters: \n",
      " [tensor([28.7500], dtype=torch.float64, requires_grad=True), tensor([0.9800], dtype=torch.float64, requires_grad=True), tensor([1.0600], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([1.8900], dtype=torch.float64, requires_grad=True)]\n",
      "--- Epoch 1 / Loss: 8564.823608 ---\n",
      "  Param 0: Value=28.7500, Grad=-0.0993153032075611\n",
      "  Param 1: Value=0.9800, Grad=11.309863882310452\n",
      "  Param 2: Value=1.0600, Grad=-12.847192904026997\n",
      "  Param 3: Value=0.0000, Grad=-3.339550858072471e-13\n",
      "  Param 4: Value=0.0000, Grad=-5.684341886080801e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=1.8900, Grad=-3.4275484624484074\n",
      "------------------------------\n",
      "--- Epoch 51 / Loss: 8357.356144 ---\n",
      "  Param 0: Value=29.0119, Grad=-0.14121741100180785\n",
      "  Param 1: Value=1.1754, Grad=0.325078943641369\n",
      "  Param 2: Value=2.0453, Grad=0.19604647834535172\n",
      "  Param 3: Value=0.0000, Grad=6.963318810448982e-13\n",
      "  Param 4: Value=0.0000, Grad=1.3073986337985843e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=2.9087, Grad=-1.1815353714862367\n",
      "------------------------------\n",
      "--- Epoch 101 / Loss: 8324.333578 ---\n",
      "  Param 0: Value=30.0133, Grad=-0.10258315940132823\n",
      "  Param 1: Value=1.2819, Grad=0.06277067774448142\n",
      "  Param 2: Value=2.2256, Grad=0.020882094927969774\n",
      "  Param 3: Value=0.0000, Grad=8.810729923425242e-13\n",
      "  Param 4: Value=-0.0000, Grad=3.979039320256561e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.1372, Grad=-0.17614890390998283\n",
      "------------------------------\n",
      "--- Epoch 151 / Loss: 8318.503123 ---\n",
      "  Param 0: Value=30.3156, Grad=-0.0945722933227322\n",
      "  Param 1: Value=1.3045, Grad=0.010014877831750368\n",
      "  Param 2: Value=2.2644, Grad=-0.004565895151813493\n",
      "  Param 3: Value=0.0000, Grad=-9.379164112033322e-13\n",
      "  Param 4: Value=0.0000, Grad=-2.1032064978498966e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.1761, Grad=-0.006079310865133802\n",
      "------------------------------\n",
      "--- Epoch 201 / Loss: 8315.916142 ---\n",
      "  Param 0: Value=30.6166, Grad=-0.09099891181593556\n",
      "  Param 1: Value=1.3161, Grad=-0.0022552663555046593\n",
      "  Param 2: Value=2.2855, Grad=-0.0005401919208907202\n",
      "  Param 3: Value=0.0000, Grad=8.526512829121202e-14\n",
      "  Param 4: Value=0.0000, Grad=-5.684341886080801e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.1760, Grad=5.428801041418296e-05\n",
      "------------------------------\n",
      "--- Epoch 251 / Loss: 8314.929025 ---\n",
      "  Param 0: Value=30.7068, Grad=-0.09023524001700389\n",
      "  Param 1: Value=1.3204, Grad=0.0008653593234804191\n",
      "  Param 2: Value=2.2924, Grad=0.0005658664121028778\n",
      "  Param 3: Value=0.0000, Grad=2.5579538487363607e-13\n",
      "  Param 4: Value=-0.0000, Grad=8.810729923425242e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.1761, Grad=-0.0006149093476306433\n",
      "------------------------------\n",
      "--- Epoch 301 / Loss: 8314.173855 ---\n",
      "  Param 0: Value=30.7969, Grad=-0.08926794229354451\n",
      "  Param 1: Value=1.3240, Grad=0.0003380439766758059\n",
      "  Param 2: Value=2.2983, Grad=0.0006913783525988038\n",
      "  Param 3: Value=0.0000, Grad=5.115907697472721e-13\n",
      "  Param 4: Value=-0.0000, Grad=-7.389644451905042e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.1756, Grad=-0.0011605860365562215\n",
      "------------------------------\n",
      "Converged at epoch 348\n",
      "Epoch 349,  \n",
      " vecc Parameters: [ 3.08228258e+01  1.32500993e+00  2.30001865e+00  5.19786119e-06\n",
      " -9.68318495e-07  0.00000000e+00  3.17560948e+00]\n",
      "FINAL STATE: Epoch 349, Loss: 8313.954087906111, \n",
      " vecc Parameters: [30.822825805680306, 1.3250099320608668, 2.3000186452826, 5.197861189672096e-06, -9.683184949324036e-07, 0.0, 3.1756094801297774]\n",
      "Day 1 optimization finished in 514.44s over 349 epochs.\n",
      "Day 1 final results: [30.822825805680306, 1.3250099320608668, 2.3000186452826, 5.197861189672096e-06, -9.683184949324036e-07, 0.0, 3.1756094801297774, 8313.954087906111]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------\n",
    "# Main Optimization Loop (Updated and Corrected)\n",
    "# ------------------------------------\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "num_days_to_load = 31 # From reference code\n",
    "\n",
    "for i in range(num_days_to_load):\n",
    "    idx_for_datamap = [i*8, i*8+1]\n",
    "    #idx_for_datamap = [i*8, (i+1)*8]    \n",
    "    # Using the new load function from the reference code\n",
    "    cur_map, cur_df = data_load_instance.load_working_data(\n",
    "        df_map, \n",
    "        idx_for_datamap, \n",
    "        ord_mm=None,\n",
    "        # Using float64 to ensure compatibility with kernels\n",
    "        dtype=torch.float64 \n",
    "    )\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "# ... (Data loading section unchanged and correct) ...\n",
    "print(f\"Data loaded for {len(df_day_map_list)} days.\")\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "days_list = [0]\n",
    "for day in days_list:  \n",
    "    \n",
    "    # Get the pre-loaded data for this day\n",
    "    analysis_data_map = df_day_map_list[day]\n",
    "    aggregated_data = df_day_aggregated_list[day]\n",
    "\n",
    "\n",
    "    # Initial parameters (full vector 'a' is for reference)\n",
    "    a = [28.75, 0.98, 1.5, 0, 0, 0, 1.890]\n",
    "    \n",
    "    # NEW: Define params as a list of 1-element tensors (one per parameter)\n",
    "    params_list = [\n",
    "        torch.tensor([val], dtype=torch.float64, requires_grad=True) for val in a\n",
    "    ]\n",
    "\n",
    "    # NEW: Define learning rates and groups\n",
    "    lr_slow, lr_fast = 0.02, 0.02 # Assuming these are defined elsewhere\n",
    "    slow_indices = [ 1, 2, 3, 4, 5, 6] # e.g., ranges, advection, beta, nugget\n",
    "    fast_indices = [0] # e.g., sigmasq\n",
    "    \n",
    "    # Define Parameter Groups for the optimizer\n",
    "    param_groups = [\n",
    "        {'params': [params_list[idx] for idx in slow_indices], 'lr': lr_slow, 'name': 'slow_group'},\n",
    "        {'params': [params_list[idx] for idx in fast_indices], 'lr': lr_fast, 'name': 'fast_group'}\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Calculate resolution for printing\n",
    "    res_calc = (200 / lat_lon_resolution[0]) * (100 / lat_lon_resolution[0])\n",
    "    print(f'\\n--- Starting Day {day+1} (2024-07-{day+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params_list}')\n",
    "            \n",
    "    # --- Data loading is now done *before* the loop ---\n",
    "\n",
    "    # Define device\n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model_instance = kernels.model_fitting(\n",
    "            smooth = v,\n",
    "            input_map = analysis_data_map,\n",
    "            aggregated_data = aggregated_data,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Adjusted optimizer call: Passes the list of parameter groups\n",
    "    optimizer, scheduler = model_instance.optimizer_fun(\n",
    "            param_groups,     # <--- Pass the list of groups\n",
    "            lr=lr,            # <--- Default LR (will be overridden by groups, but needed for function sig)\n",
    "            betas=(0.9, 0.8), \n",
    "            eps=1e-8, \n",
    "            step_size=step, \n",
    "            gamma=gamma_par\n",
    "        )\n",
    "\n",
    "    # Calling the robust training loop\n",
    "    out, epoch_ran = model_instance.run_vecc_scheduler_oct23(\n",
    "            params_list,     # <--- Pass the list of parameter tensors\n",
    "            optimizer,\n",
    "            scheduler, \n",
    "            model_instance.matern_cov_anisotropy_v05, \n",
    "            epochs=epochs\n",
    "        )\n",
    "    # --- End Correction ---\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"Day {day+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day+1} final results: {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08dc648",
   "metadata": {},
   "source": [
    "center mathicng but early stopping no scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7100194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading data for all days...\n",
      "Data loaded for 31 days.\n",
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 5000.0, smooth: 0.5\n",
      "mm_cond_number: 20,\n",
      "initial parameters: \n",
      " [35.0, 0.98, 1.06, 0.0, 0.0, 0.0, 1.89]\n",
      "Epoch 1/900 | NLL: 8622.638066 | Params: [35.    0.98  1.06  0.    0.    0.    1.89]\n",
      "Epoch 51/900 | NLL: 8405.685392 | Params: [3.40374906e+01 1.11234065e+00 1.91866560e+00 9.32178729e-06\n",
      " 2.84894614e-05 0.00000000e+00 2.49266061e+00]\n",
      "Epoch 101/900 | NLL: 8277.581800 | Params: [3.49132358e+01 1.53685422e+00 2.64780792e+00 1.99547338e-05\n",
      " 3.96079457e-05 0.00000000e+00 3.24166261e+00]\n",
      "Epoch 151/900 | NLL: 8279.748637 | Params: [3.74382832e+01 1.58731120e+00 2.73555859e+00 2.04456758e-05\n",
      " 2.47450605e-05 0.00000000e+00 3.14585079e+00]\n",
      "Epoch 201/900 | NLL: 8269.555044 | Params: [4.00126311e+01 1.71158269e+00 2.94600745e+00 1.44402536e-05\n",
      " 1.33050981e-05 0.00000000e+00 3.16034945e+00]\n",
      "Epoch 251/900 | NLL: 8265.056434 | Params: [4.25847621e+01 1.82102251e+00 3.13294301e+00 1.93305414e-05\n",
      " 2.46529293e-05 0.00000000e+00 3.15823918e+00]\n",
      "Epoch 301/900 | NLL: 8262.298709 | Params: [4.51578724e+01 1.93112559e+00 3.31888654e+00 1.77727193e-05\n",
      " 3.33758469e-05 0.00000000e+00 3.16002709e+00]\n",
      "Epoch 351/900 | NLL: 8260.079924 | Params: [4.77317546e+01 2.04595685e+00 3.50764402e+00 2.47960500e-05\n",
      " 3.05860279e-05 0.00000000e+00 3.15968528e+00]\n",
      "Epoch 401/900 | NLL: 8259.471383 | Params: [5.03095403e+01 2.15660273e+00 3.70067684e+00 3.87125002e-05\n",
      " 3.17851805e-05 0.00000000e+00 3.16110222e+00]\n",
      "Epoch 451/900 | NLL: 8259.302015 | Params: [5.28924634e+01 2.26925405e+00 3.89310929e+00 4.81266326e-05\n",
      " 3.05986580e-05 0.00000000e+00 3.16230822e+00]\n",
      "Epoch 501/900 | NLL: 8259.360693 | Params: [5.54845675e+01 2.38446142e+00 4.08779215e+00 2.69141086e-05\n",
      " 2.74885539e-05 0.00000000e+00 3.16398647e+00]\n",
      "*** Early stopping triggered after 514 epochs (Patience: 70, Best NLL: 8259.008468) ***\n",
      "Day 1 optimization finished in 577.47s over 514 epochs.\n",
      "Day 1 final results (Concatenated Tensor):\n",
      " [5.25307229e+01 2.25568639e+00 3.86505395e+00 5.23430145e-05\n",
      " 3.49769959e-05 0.00000000e+00 3.16205496e+00]\n",
      "\n",
      "--- All Days Processed ---\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------\n",
    "# Main Optimization Loop (Corrected for List Input & Early Stopping)\n",
    "# ------------------------------------\n",
    "import time \n",
    "import torch\n",
    "# Assuming necessary variables (lat_lon_resolution, v, mm_cond_number, \n",
    "# nheads, lr, step, gamma_par, epochs, data_load_instance, df_map, kernels, \n",
    "# nns_map) and the variables for groups (lr_slow, lr_fast, etc.) are defined.\n",
    "\n",
    "# --- 1. Pre-load all data (unchanged) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "num_days_to_load = 31 \n",
    "\n",
    "for i in range(num_days_to_load):\n",
    "    idx_for_datamap = [i*8, i*8+1]\n",
    "    cur_map, cur_df = data_load_instance.load_working_data(\n",
    "        df_map, \n",
    "        idx_for_datamap, \n",
    "        ord_mm=None,\n",
    "        dtype=torch.float64 \n",
    "    )\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "print(f\"Data loaded for {len(df_day_map_list)} days.\")\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "days_list = [0]\n",
    "# Placeholder values (assuming they are defined globally or outside the scope)\n",
    "lr_slow, lr_fast = 0.02, 0.05 \n",
    "slow_indices = [ 1, 2, 3, 4, 5, 6] \n",
    "fast_indices = [0] \n",
    "\n",
    "for day in days_list:  \n",
    "    \n",
    "    analysis_data_map = df_day_map_list[day]\n",
    "    aggregated_data = df_day_aggregated_list[day]\n",
    "\n",
    "    # Initial parameters (full vector 'a' is for reference)\n",
    "    a = [28.75, 0.98, 1.06, 0, 0, 0, 1.890]\n",
    "    a = [35, 0.98, 1.06, 0, 0, 0, 1.890]\n",
    "    \n",
    "    # ⭐ CRITICAL CHANGE 1: Define params as a list of 1-element tensors\n",
    "    params_list = [\n",
    "        torch.tensor([val], dtype=torch.float64, requires_grad=True) for val in a\n",
    "    ]\n",
    "\n",
    "    # ⭐ CRITICAL CHANGE 2: Define Parameter Groups for the optimizer\n",
    "    param_groups = [\n",
    "        {'params': [params_list[idx] for idx in slow_indices], 'lr': lr_slow, 'name': 'slow_group'},\n",
    "        {'params': [params_list[idx] for idx in fast_indices], 'lr': lr_fast, 'name': 'fast_group'}\n",
    "    ]\n",
    "\n",
    "    # Calculate resolution for printing\n",
    "    res_calc = (200 / lat_lon_resolution[0]) * (100 / lat_lon_resolution[0])\n",
    "    print(f'\\n--- Starting Day {day+1} (2024-07-{day+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    # Note: Cannot use params.detach().numpy() here as params is not yet concatenated\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {[p.item() for p in params_list]}')\n",
    "            \n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model_instance = kernels.model_fitting(\n",
    "            smooth = v,\n",
    "            input_map = analysis_data_map,\n",
    "            aggregated_data = aggregated_data,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ⭐ CRITICAL CHANGE 3: Adjusted optimizer call: Passes the list of parameter groups\n",
    "    # It must now use 'param_groups' as the first argument.\n",
    "    optimizer, _ = model_instance.optimizer_fun(\n",
    "            param_groups,     # <--- Pass the list of groups\n",
    "            lr=lr,            # <--- Default LR (overridden by groups)\n",
    "            betas=(0.9, 0.8), \n",
    "            eps=1e-8, \n",
    "            step_size=step, \n",
    "            gamma=gamma_par\n",
    "        )\n",
    "\n",
    "    # ⭐ CRITICAL CHANGE 4: Early Stopping Training Call\n",
    "    # New function name: run_vecc_early_stp_1028 (matching your code block)\n",
    "    # Input parameter is the list of tensors: params_list\n",
    "    out, epoch_ran = model_instance.run_vecc_early_stp_1028(\n",
    "        params_list,     # <--- Pass the list of parameter tensors\n",
    "        optimizer,\n",
    "        model_instance.matern_cov_anisotropy_v05, # The covariance function\n",
    "        epochs=epochs,\n",
    "        patience=70,    # Example Early Stopping parameter\n",
    "        min_delta=1e-3  # Example Early Stopping parameter\n",
    "    )\n",
    "    # --- End Correction ---\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    # The output 'out' from run_vecc_early_stp_1028 is a concatenated tensor (best_params)\n",
    "    # plus the final epoch number.\n",
    "    print(f\"Day {day+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day+1} final results (Concatenated Tensor):\\n {out.detach().numpy()}\")\n",
    "\n",
    "print(\"\\n--- All Days Processed ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78326624",
   "metadata": {},
   "source": [
    "use swap order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6984fb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading data for all days...\n",
      "Data loaded for 31 days.\n",
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 5000.0, smooth: 0.5\n",
      "mm_cond_number: 20,\n",
      "initial parameters: \n",
      " [28.75  0.98  1.06  0.    0.    0.    1.89]\n",
      "Epoch 1, Gradients: [-1.05690956e-01  1.14465942e+01 -1.27860107e+01 -1.22070312e-04\n",
      "  6.40869141e-04  0.00000000e+00 -3.60217286e+00]\n",
      " Loss: 8566.6640625, Parameters: [28.75  0.98  1.06  0.    0.    0.    1.89]\n",
      "Epoch 51, Gradients: [-1.37584448e-01  3.43799591e-01  1.42578125e-01 -7.62939453e-04\n",
      " -2.44140625e-04  0.00000000e+00 -1.18694783e+00]\n",
      " Loss: 8360.35546875, Parameters: [29.04513558  1.1734433   2.03553031  0.07186149 -0.06086177  0.\n",
      "  2.9143169 ]\n",
      "Epoch 101, Gradients: [-0.10074802  0.04925156  0.0145874   0.00018311 -0.00015259  0.\n",
      " -0.15212309]\n",
      " Loss: 8325.1630859375, Parameters: [ 3.00445305e+01  1.28615078e+00  2.23070790e+00 -3.06708430e-03\n",
      " -3.17577525e-02  0.00000000e+00  3.15776994e+00]\n",
      "Epoch 151, Gradients: [-0.09329809 -0.00375366 -0.0032959   0.00018311  0.00021362  0.\n",
      " -0.0004686 ]\n",
      " Loss: 8319.7451171875, Parameters: [30.34685896  1.30731968  2.26792338 -0.03378841 -0.05348533  0.\n",
      "  3.1924696 ]\n",
      "Epoch 201, Gradients: [-0.09018953  0.00040054 -0.00494385  0.00012207 -0.00079346  0.\n",
      " -0.0002285 ]\n",
      " Loss: 8316.7451171875, Parameters: [ 3.06478764e+01  1.32043399e+00  2.28913224e+00 -2.15062938e-02\n",
      " -1.93691679e-03  0.00000000e+00  3.19203577e+00]\n",
      "Epoch 251, Gradients: [-8.94369334e-02 -6.10351562e-05 -6.71386719e-04  5.49316406e-04\n",
      " -1.22070312e-04  0.00000000e+00 -9.91000794e-04]\n",
      " Loss: 8315.8505859375, Parameters: [ 3.07380382e+01  1.32443626e+00  2.29614542e+00 -4.27426179e-02\n",
      "  8.38798613e-03  0.00000000e+00  3.19208551e+00]\n",
      "Epoch 301, Gradients: [-0.08843087  0.00063324 -0.00061035 -0.00036621  0.00042725  0.\n",
      "  0.00024902]\n",
      " Loss: 8315.017578125, Parameters: [ 3.08281347e+01  1.32826749e+00  2.30224357e+00 -5.62444900e-02\n",
      " -1.50029938e-02  0.00000000e+00  3.19201226e+00]\n",
      "Epoch 351, Gradients: [-8.82462412e-02  1.29699707e-04  3.66210938e-04 -1.52587891e-04\n",
      " -3.05175781e-05  0.00000000e+00 -9.40983882e-05]\n",
      " Loss: 8314.75, Parameters: [ 3.08551495e+01  1.32950601e+00  2.30453555e+00 -5.56330974e-02\n",
      " -1.62730203e-02  0.00000000e+00  3.19213002e+00]\n",
      "Epoch 401, Gradients: [-0.08803163  0.00082397  0.00097656  0.00015259  0.00018311  0.\n",
      " -0.00133141]\n",
      " Loss: 8314.5, Parameters: [ 3.08821573e+01  1.33065113e+00  2.30641829e+00 -5.16880196e-02\n",
      " -2.05975481e-02  0.00000000e+00  3.19185235e+00]\n",
      "Converged at epoch 401\n",
      "Epoch 402,  \n",
      " vecc Parameters: [ 3.08824814e+01  1.33062876e+00  2.30639410e+00 -5.16675735e-02\n",
      " -2.05771004e-02  0.00000000e+00  3.19186239e+00]\n",
      "FINAL STATE: Epoch 402, Loss: 8314.5, \n",
      " vecc Parameters: [30.882481367893934, 1.3306287573904048, 2.3063941013205755, -0.05166757348340395, -0.020577100376109176, 0.0, 3.1918623924224208]\n",
      "Day 1 optimization finished in 468.35s over 402 epochs.\n",
      "Day 1 final results: [30.882481367893934, 1.3306287573904048, 2.3063941013205755, -0.05166757348340395, -0.020577100376109176, 0.0, 3.1918623924224208, 8314.5]\n",
      "\n",
      "--- All Days Processed ---\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------\n",
    "# Main Optimization Loop (Updated)\n",
    "# ------------------------------------\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "for i in range(31):\n",
    "    #idx_for_datamap = [i*8, (i+1)*8]\n",
    "    idx_for_datamap = [i*8, i*8 +1]\n",
    "\n",
    "    cur_map, cur_df = analysis_map_no_mm, agg_data_no_mm = data_load_instance.load_working_data_keep_ori(\n",
    "    df_map, \n",
    "    idx_for_datamap, \n",
    "    ord_mm=None,  # or just omit it\n",
    "    dtype=torch.float # or just omit it \n",
    "    )\n",
    "    cur_df[:,0], cur_df[:,1] = cur_df[:,4] , cur_df[:,5]\n",
    "\n",
    "    for map in cur_map.values():\n",
    "        map[:,0], map[:,1] = map[:,4] , map[:,5]\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "\n",
    "print(f\"Data loaded for {len(df_day_map_list)} days.\")\n",
    "# print(df_day_aggregated_list[0].shape) # From reference code\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "\n",
    "# This list is now just for iterating\n",
    "#days_list = range(len(df_day_map_list)) \n",
    "days_list = [0]\n",
    "for day in days_list:  \n",
    "    \n",
    "    # Get the pre-loaded data for this day\n",
    "    analysis_data_map = df_day_map_list[day]\n",
    "    aggregated_data = df_day_aggregated_list[day]\n",
    "\n",
    "    #a = [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]\n",
    "    a = [28.75, 0.98, 1.06, 0, 0, 0, 1.890]\n",
    "    #a = [28.75, 0.98, 1.06, 0.036, -0.155, 0.179, 1.890]\n",
    "    params = torch.tensor(a, dtype=torch.float64, requires_grad=True)\n",
    "    \n",
    "    # Calculate resolution for printing\n",
    "    res_calc = (200 / lat_lon_resolution[0]) * (100 / lat_lon_resolution[0])\n",
    "    print(f'\\n--- Starting Day {day+1} (2024-07-{day+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params.detach().numpy()}')\n",
    "            \n",
    "    # --- Data loading is now done *before* the loop ---\n",
    "\n",
    "    # We need to define the device (though we aren't passing it anymore)\n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model_instance = kernels.model_fitting(\n",
    "            smooth = v,\n",
    "            input_map = analysis_data_map,\n",
    "            aggregated_data = aggregated_data,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "            # device = device_str  <--- REMOVED: This was causing the TypeError\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Adjusted optimizer call based on expected return values (step size changed to step)\n",
    "    optimizer, scheduler = model_instance.optimizer_fun(\n",
    "        params, \n",
    "        lr=lr, \n",
    "        betas=(0.9, 0.8), \n",
    "        eps=1e-8, \n",
    "        step_size=step, # Using the 'step' variable here\n",
    "        gamma=gamma_par  # Using gamma_par\n",
    "    ) \n",
    "\n",
    "    # --- CRITICAL CORRECTION ---\n",
    "    # 1. We no longer need to create a separate 'instance_map'.\n",
    "    #    'model_instance' is already the correct instance.\n",
    "    # 2. We do NOT pre-calculate 'cov_map'. The optimized training loop\n",
    "    #    'run_vecc_may9' does this internally on each epoch\n",
    "    #    to ensure the gradients are correct.\n",
    "    # 3. We call 'run_vecc_may9' (the optimized loop) instead of 'run_vecc_grp9'.\n",
    "    #    This version does not take 'cov_map' as an argument.\n",
    "    \n",
    "    # Calling the optimized 'run_vecc_may9'\n",
    "    out, epoch_ran = model_instance.run_vecc_oct22(\n",
    "        params, \n",
    "        optimizer,\n",
    "        scheduler, \n",
    "        model_instance.matern_cov_anisotropy_v05, \n",
    "        epochs=epochs\n",
    "    )\n",
    "    # --- End Correction ---\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"Day {day+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day+1} final results: {out}\")\n",
    "\n",
    "print(\"\\n--- All Days Processed ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcea0601",
   "metadata": {},
   "source": [
    "## Full likelihood\n",
    "\n",
    "Vecchia Original\n",
    "[29.921775534973218, 1.8597906958307489, 3.4954059102213106, 0.22512272633105687, -0.0648045117355326, 0.0, 3.3287024509849537] 2036\n",
    "(data 1250) # 1 23641\n",
    "\n",
    "Day 1 final results: [30.882481367893934, 1.3306287573904048, 2.3063941013205755, -0.05166757348340395, -0.020577100376109176, 0.0, 3.1918623924224208] 8314. (data 5000). \n",
    "\n",
    "\n",
    "Center matching\n",
    "[29.867643794098317, 1.8170445928338301, 3.452060053856289, 9.346808116428221e-06, 1.3480203304274802e-05, 0.0, 3.278045234831132, 2038.106906954074]\n",
    "\n",
    "[30.823365978489328, 1.325000558988864, 2.300043515398019, 5.191119207004892e-06, -9.752316964224805e-07, 0.0, 3.1756405728254657, 8313.954087906111]\n",
    "\n",
    "# 1 23644\n",
    "\n",
    "\n",
    "If I fit 8 hours on original space--> numerically unstable and cholesky decomposition fails after hitting non positive matrix\n",
    "\n",
    "\n",
    "## Vecchia likleihood 8 hours using same parameters ()\n",
    "a = [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]\n",
    "\n",
    "original   15995.6299\n",
    "center matching # 15994.2207 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de556f",
   "metadata": {},
   "source": [
    "Pre-loading data for all days...\n",
    "Data loaded for 31 days.\n",
    "\n",
    "--- Starting Day 1 (2024-07-1) ---\n",
    "Data size per day: 1250.0, smooth: 0.5\n",
    "mm_cond_number: 20,\n",
    "initial parameters: \n",
    " [28.75   0.98   1.06   0.036 -0.155  0.179  1.89 ]\n",
    "Converged at epoch 535\n",
    "Epoch 536,  \n",
    " vecc Parameters: [31.00915505  1.1916293   1.84680497  0.03927127 -0.15666047  0.12771191\n",
    "  4.11954968]\n",
    "FINAL STATE: Epoch 536, Loss: 15746.237437383388, \n",
    " vecc Parameters: [31.009155050569746, 1.191629302554168, 1.846804972667287, 0.03927126531202711, -0.15666046904308123, 0.12771190935579058, 4.119549677954342]\n",
    "Day 1 optimization finished in 822.07s over 536 epochs.\n",
    "Day 1 final results: [31.009155050569746, 1.191629302554168, 1.846804972667287, 0.03927126531202711, -0.15666046904308123, 0.12771190935579058, 4.119549677954342, 15746.237437383388]\n",
    "\n",
    "--- All Days Processed ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caf2b92",
   "metadata": {},
   "source": [
    "Pre-loading data for all days...\n",
    "Data loaded for 31 days.\n",
    "\n",
    "--- Starting Day 1 (2024-07-1) ---\n",
    "Data size per day: 1250.0, smooth: 0.5\n",
    "mm_cond_number: 20,\n",
    "initial parameters: \n",
    " [28.75  0.98  1.06  0.    0.    0.    1.89]\n",
    "Converged at epoch 332\n",
    "Epoch 333,  \n",
    " vecc Parameters: [2.98676438e+01 1.81704459e+00 3.45206005e+00 9.34680812e-06\n",
    " 1.34802033e-05 0.00000000e+00 3.27804523e+00]\n",
    "FINAL STATE: Epoch 333, Loss: 2038.106906954074, \n",
    " vecc Parameters: [29.867643794098317, 1.8170445928338301, 3.452060053856289, 9.346808116428221e-06, 1.3480203304274802e-05, 0.0, 3.278045234831132]\n",
    "Day 1 optimization finished in 73.78s over 333 epochs.\n",
    "Day 1 final results: [29.867643794098317, 1.8170445928338301, 3.452060053856289, 9.346808116428221e-06, 1.3480203304274802e-05, 0.0, 3.278045234831132, 2038.106906954074]\n",
    "\n",
    "--- All Days Processed ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
