{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a2d6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy                    # clone tensor\n",
    "import time\n",
    "\n",
    "# Custom imports\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels\n",
    "from GEMS_TCO import data_preprocess \n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "import typer\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO.data_loader import load_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a09d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting data to lat: [0.0, 5.0], lon: [123.0, 133.0]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "# Assuming 'config' and 'load_data' class are defined and imported elsewhere\n",
    "\n",
    "# --- Parameters derived from your framework ---\n",
    "v: float = 0.5\n",
    "space: List[str] = ['2', '2']\n",
    "days: List[str] = ['0', '31']\n",
    "mm_cond_number: int = 20\n",
    "# --- End of framework parameters ---\n",
    "\n",
    "lat_lon_resolution = [int(s) for s in space]\n",
    "days_s_e = [int(d) for d in days]\n",
    "days_list = list(range(days_s_e[0], days_s_e[1]))\n",
    "\n",
    "# These values were not in the framework, so they remain as set in your snippet\n",
    "years = ['2024']\n",
    "month_range = [7] \n",
    "\n",
    "# Assuming 'config' is available in your environment\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "\n",
    "## load ozone data from amarel\n",
    "data_load_instance = load_data2(config.mac_data_load_path)\n",
    "\n",
    "# Call the function using the variables from the framework\n",
    "df_map, ord_mm, nns_map = data_load_instance.load_maxmin_ordered_data_bymonthyear(\n",
    "lat_lon_resolution=lat_lon_resolution, \n",
    "mm_cond_number=mm_cond_number,\n",
    "years_=years, \n",
    "months_=month_range,\n",
    "lat_range=[0.0, 5.0],      # <-- Add this\n",
    "lon_range=[123.0, 133.0]   # <-- Add this\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9a2308b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36480, 6])\n",
      "tensor([[4.9760e+00, 1.3298e+02, 2.6703e+02, 4.5000e+01, 4.9764e+00, 1.3299e+02],\n",
      "        [4.9760e+00, 1.3286e+02, 2.5886e+02, 4.5000e+01, 4.9766e+00, 1.3286e+02],\n",
      "        [4.9760e+00, 1.3273e+02, 2.7201e+02, 4.5000e+01, 4.9769e+00, 1.3273e+02],\n",
      "        ...,\n",
      "        [4.8000e-02, 1.2328e+02, 2.5685e+02, 5.2000e+01, 5.2389e-02, 1.2329e+02],\n",
      "        [4.8000e-02, 1.2316e+02, 2.5635e+02, 5.2000e+01, 5.2416e-02, 1.2316e+02],\n",
      "        [4.8000e-02, 1.2303e+02, 2.6741e+02, 5.2000e+01, 5.2566e-02, 1.2304e+02]])\n"
     ]
    }
   ],
   "source": [
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "for i in range(31):\n",
    "    idx_for_datamap = [i*8, (i+1)*8]\n",
    "\n",
    "    cur_map, cur_df = analysis_map_no_mm, agg_data_no_mm = data_load_instance.load_working_data_keep_ori(\n",
    "    df_map, \n",
    "    idx_for_datamap, \n",
    "    ord_mm=None,  # or just omit it\n",
    "    dtype=torch.float # or just omit it\n",
    ")\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "print(df_day_aggregated_list[0].shape)\n",
    "print(df_day_aggregated_list[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f575ba",
   "metadata": {},
   "source": [
    "# swap columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f43b444a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.9764e+00, 1.3299e+02, 2.6703e+02, 4.5000e+01, 4.9764e+00, 1.3299e+02],\n",
      "        [4.9766e+00, 1.3286e+02, 2.5886e+02, 4.5000e+01, 4.9766e+00, 1.3286e+02],\n",
      "        [4.9769e+00, 1.3273e+02, 2.7201e+02, 4.5000e+01, 4.9769e+00, 1.3273e+02],\n",
      "        ...,\n",
      "        [5.2389e-02, 1.2329e+02, 2.5685e+02, 5.2000e+01, 5.2389e-02, 1.2329e+02],\n",
      "        [5.2416e-02, 1.2316e+02, 2.5635e+02, 5.2000e+01, 5.2416e-02, 1.2316e+02],\n",
      "        [5.2566e-02, 1.2304e+02, 2.6741e+02, 5.2000e+01, 5.2566e-02, 1.2304e+02]])\n"
     ]
    }
   ],
   "source": [
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "for i in range(31):\n",
    "    idx_for_datamap = [i*8, (i+1)*8]\n",
    "\n",
    "    cur_map, cur_df = analysis_map_no_mm, agg_data_no_mm = data_load_instance.load_working_data_keep_ori(\n",
    "    df_map, \n",
    "    idx_for_datamap, \n",
    "    ord_mm=None,  # or just omit it\n",
    "    dtype=torch.float # or just omit it \n",
    "    )\n",
    "    cur_df[:,0], cur_df[:,1] = cur_df[:,4] , cur_df[:,5]\n",
    "\n",
    "    for map in cur_map.values():\n",
    "        map[:,0], map[:,1] = map[:,4] , map[:,5]\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "print(df_day_aggregated_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca78fe34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(64359.1719, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance1 = kernels.vecchia_experiment(0.5, df_day_map_list[0], df_day_aggregated_list[0], nns_map, mm_cond_number, nheads=10)\n",
    "\n",
    "a = [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]\n",
    "#a = [30.2594, 0.665, 1.8981, 0.0, 0.1317, -0.0, 1.9785]\n",
    "#a = [45.1402, 0.6299, 0.7308, -0.0003, -0.0151, 0.0, 7.8922]\n",
    "#a = [21.7335, 1.2817, 1.5946, 0.042, -0.1241, 0.218, 4.8654]\n",
    "#a = [20.453542336448137, 1.4506118600616982, 2.43096923637867, -0.03476556019978718, -0.1559262606484541, 0.1254833595232136, 3.938183829354925]\n",
    "params = torch.tensor(a, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "cov_map = instance1.cov_structure_saver(params, instance1.matern_cov_anisotropy_v05)  \n",
    "instance1.vecchia_oct22( params, instance1.matern_cov_anisotropy_v05, cov_map )\n",
    "\n",
    "\n",
    "# 15994.2207 15995.6299\n",
    "# 64351   64359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46abc9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0.5 # smooth\n",
    "mm_cond_number = 20\n",
    "nheads = 300\n",
    "lr = 0.02\n",
    "step = 100\n",
    "gamma_par = 0.3\n",
    "epochs = 900"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cf2aaa",
   "metadata": {},
   "source": [
    "# fit just one hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9d57a4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.8780e+00, 1.3298e+02, 2.4773e+02, 7.4100e+02, 4.8780e+00, 1.3298e+02],\n",
       "        [4.8784e+00, 1.3273e+02, 2.5376e+02, 7.4100e+02, 4.8784e+00, 1.3273e+02],\n",
       "        [4.8788e+00, 1.3248e+02, 2.5099e+02, 7.4100e+02, 4.8788e+00, 1.3248e+02],\n",
       "        ...,\n",
       "        [1.2842e-01, 1.2367e+02, 2.5461e+02, 7.4800e+02, 1.2842e-01, 1.2367e+02],\n",
       "        [1.2849e-01, 1.2342e+02, 2.5616e+02, 7.4800e+02, 1.2849e-01, 1.2342e+02],\n",
       "        [1.2889e-01, 1.2317e+02, 2.5457e+02, 7.4800e+02, 1.2889e-01, 1.2317e+02]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0030ba9b",
   "metadata": {},
   "source": [
    "fit on center matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "565c38f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading data for all days...\n",
      "Data loaded for 31 days.\n",
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 5000.0, smooth: 0.5\n",
      "mm_cond_number: 20,\n",
      "initial parameters: \n",
      " [28.75  0.98  1.06  0.    0.    0.    1.89]\n",
      "Epoch 1, Gradients: [-9.93153032e-02  1.13098639e+01 -1.28471929e+01 -3.33955086e-13\n",
      " -5.68434189e-13  0.00000000e+00 -3.42754846e+00]\n",
      " Loss: 8564.823607970236, Parameters: [28.75  0.98  1.06  0.    0.    0.    1.89]\n",
      "Epoch 51, Gradients: [-1.41217411e-01  3.25078944e-01  1.96046478e-01  6.96331881e-13\n",
      "  1.30739863e-12  0.00000000e+00 -1.18153537e+00]\n",
      " Loss: 8357.356143726023, Parameters: [2.90119149e+01 1.17540012e+00 2.04530883e+00 2.09668554e-05\n",
      " 1.46353349e-05 0.00000000e+00 2.90874622e+00]\n",
      "Epoch 101, Gradients: [-1.02583159e-01  6.27706777e-02  2.08820949e-02  8.81072992e-13\n",
      "  3.97903932e-13  0.00000000e+00 -1.76148904e-01]\n",
      " Loss: 8324.333578433401, Parameters: [ 3.00133200e+01  1.28192948e+00  2.22562651e+00  9.53212349e-06\n",
      " -2.31480545e-06  0.00000000e+00  3.13723545e+00]\n",
      "Epoch 151, Gradients: [-9.45722933e-02  1.00148778e-02 -4.56589515e-03 -9.37916411e-13\n",
      " -2.10320650e-12  0.00000000e+00 -6.07931087e-03]\n",
      " Loss: 8318.503123300665, Parameters: [3.03156217e+01 1.30448238e+00 2.26438789e+00 6.33924197e-06\n",
      " 1.90895534e-06 0.00000000e+00 3.17606200e+00]\n",
      "Epoch 201, Gradients: [-9.09989118e-02 -2.25526636e-03 -5.40191921e-04  8.52651283e-14\n",
      " -5.68434189e-13  0.00000000e+00  5.42880104e-05]\n",
      " Loss: 8315.91614232461, Parameters: [3.06166370e+01 1.31613655e+00 2.28553728e+00 5.02240352e-06\n",
      " 1.16797269e-06 0.00000000e+00 3.17598095e+00]\n",
      "Epoch 251, Gradients: [-9.02352400e-02  8.65359323e-04  5.65866412e-04  2.55795385e-13\n",
      "  8.81072992e-13  0.00000000e+00 -6.14909348e-04]\n",
      " Loss: 8314.929024664427, Parameters: [ 3.07067956e+01  1.32042800e+00  2.29237613e+00  4.50911576e-06\n",
      " -6.46034135e-07  0.00000000e+00  3.17609267e+00]\n",
      "Epoch 301, Gradients: [-8.92679423e-02  3.38043977e-04  6.91378353e-04  5.11590770e-13\n",
      " -7.38964445e-13  0.00000000e+00 -1.16058604e-03]\n",
      " Loss: 8314.173855163866, Parameters: [ 3.07968920e+01  1.32396711e+00  2.29834714e+00  4.81931406e-06\n",
      " -1.09548304e-06  0.00000000e+00  3.17555235e+00]\n",
      "Converged at epoch 348\n",
      "Epoch 349,  \n",
      " vecc Parameters: [ 3.08233660e+01  1.32500056e+00  2.30004352e+00  5.19111921e-06\n",
      " -9.75231696e-07  0.00000000e+00  3.17564057e+00]\n",
      "FINAL STATE: Epoch 349, Loss: 8313.954087906111, \n",
      " vecc Parameters: [30.823365978489328, 1.325000558988864, 2.300043515398019, 5.191119207004892e-06, -9.752316964224805e-07, 0.0, 3.1756405728254657]\n",
      "Day 1 optimization finished in 403.70s over 349 epochs.\n",
      "Day 1 final results: [30.823365978489328, 1.325000558988864, 2.300043515398019, 5.191119207004892e-06, -9.752316964224805e-07, 0.0, 3.1756405728254657, 8313.954087906111]\n",
      "\n",
      "--- All Days Processed ---\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------\n",
    "# Main Optimization Loop (Updated)\n",
    "# ------------------------------------\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "num_days_to_load = 31 # From reference code\n",
    "\n",
    "for i in range(num_days_to_load):\n",
    "    idx_for_datamap = [i*8, i*8+1]\n",
    "    #idx_for_datamap = [i*8, (i+1)*8]    \n",
    "    # Using the new load function from the reference code\n",
    "    cur_map, cur_df = data_load_instance.load_working_data(\n",
    "        df_map, \n",
    "        idx_for_datamap, \n",
    "        ord_mm=None,\n",
    "        # Using float64 to ensure compatibility with kernels\n",
    "        dtype=torch.float64 \n",
    "    )\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "print(f\"Data loaded for {len(df_day_map_list)} days.\")\n",
    "# print(df_day_aggregated_list[0].shape) # From reference code\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "\n",
    "# This list is now just for iterating\n",
    "#days_list = range(len(df_day_map_list)) \n",
    "days_list = [0]\n",
    "for day in days_list:  \n",
    "    \n",
    "    # Get the pre-loaded data for this day\n",
    "    analysis_data_map = df_day_map_list[day]\n",
    "    aggregated_data = df_day_aggregated_list[day]\n",
    "\n",
    "    #a = [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]\n",
    "    a = [28.75, 0.98, 1.06, 0, 0, 0, 1.890]\n",
    "    #a = [28.75, 0.98, 1.06, 0.036, -0.155, 0.179, 1.890]\n",
    "    params = torch.tensor(a, dtype=torch.float64, requires_grad=True)\n",
    "    \n",
    "    # Calculate resolution for printing\n",
    "    res_calc = (200 / lat_lon_resolution[0]) * (100 / lat_lon_resolution[0])\n",
    "    print(f'\\n--- Starting Day {day+1} (2024-07-{day+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params.detach().numpy()}')\n",
    "            \n",
    "    # --- Data loading is now done *before* the loop ---\n",
    "\n",
    "    # We need to define the device (though we aren't passing it anymore)\n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model_instance = kernels.model_fitting(\n",
    "            smooth = v,\n",
    "            input_map = analysis_data_map,\n",
    "            aggregated_data = aggregated_data,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "            # device = device_str  <--- REMOVED: This was causing the TypeError\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Adjusted optimizer call based on expected return values (step size changed to step)\n",
    "    optimizer, scheduler = model_instance.optimizer_fun(\n",
    "        params, \n",
    "        lr=lr, \n",
    "        betas=(0.9, 0.8), \n",
    "        eps=1e-8, \n",
    "        step_size=step, # Using the 'step' variable here\n",
    "        gamma=gamma_par  # Using gamma_par\n",
    "    ) \n",
    "\n",
    "    # --- CRITICAL CORRECTION ---\n",
    "    # 1. We no longer need to create a separate 'instance_map'.\n",
    "    #    'model_instance' is already the correct instance.\n",
    "    # 2. We do NOT pre-calculate 'cov_map'. The optimized training loop\n",
    "    #    'run_vecc_may9' does this internally on each epoch\n",
    "    #    to ensure the gradients are correct.\n",
    "    # 3. We call 'run_vecc_may9' (the optimized loop) instead of 'run_vecc_grp9'.\n",
    "    #    This version does not take 'cov_map' as an argument.\n",
    "    \n",
    "    # Calling the optimized 'run_vecc_may9'\n",
    "    out, epoch_ran = model_instance.run_vecc_oct22(\n",
    "        params, \n",
    "        optimizer,\n",
    "        scheduler, \n",
    "        model_instance.matern_cov_anisotropy_v05, \n",
    "        epochs=epochs\n",
    "    )\n",
    "    # --- End Correction ---\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"Day {day+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day+1} final results: {out}\")\n",
    "\n",
    "print(\"\\n--- All Days Processed ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78326624",
   "metadata": {},
   "source": [
    "use swap order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6984fb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading data for all days...\n",
      "Data loaded for 31 days.\n",
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 5000.0, smooth: 0.5\n",
      "mm_cond_number: 20,\n",
      "initial parameters: \n",
      " [28.75  0.98  1.06  0.    0.    0.    1.89]\n",
      "Epoch 1, Gradients: [-1.05690956e-01  1.14465942e+01 -1.27860107e+01 -1.22070312e-04\n",
      "  6.40869141e-04  0.00000000e+00 -3.60217286e+00]\n",
      " Loss: 8566.6640625, Parameters: [28.75  0.98  1.06  0.    0.    0.    1.89]\n",
      "Epoch 51, Gradients: [-1.37584448e-01  3.43799591e-01  1.42578125e-01 -7.62939453e-04\n",
      " -2.44140625e-04  0.00000000e+00 -1.18694783e+00]\n",
      " Loss: 8360.35546875, Parameters: [29.04513558  1.1734433   2.03553031  0.07186149 -0.06086177  0.\n",
      "  2.9143169 ]\n",
      "Epoch 101, Gradients: [-0.10074802  0.04925156  0.0145874   0.00018311 -0.00015259  0.\n",
      " -0.15212309]\n",
      " Loss: 8325.1630859375, Parameters: [ 3.00445305e+01  1.28615078e+00  2.23070790e+00 -3.06708430e-03\n",
      " -3.17577525e-02  0.00000000e+00  3.15776994e+00]\n",
      "Epoch 151, Gradients: [-0.09329809 -0.00375366 -0.0032959   0.00018311  0.00021362  0.\n",
      " -0.0004686 ]\n",
      " Loss: 8319.7451171875, Parameters: [30.34685896  1.30731968  2.26792338 -0.03378841 -0.05348533  0.\n",
      "  3.1924696 ]\n",
      "Epoch 201, Gradients: [-0.09018953  0.00040054 -0.00494385  0.00012207 -0.00079346  0.\n",
      " -0.0002285 ]\n",
      " Loss: 8316.7451171875, Parameters: [ 3.06478764e+01  1.32043399e+00  2.28913224e+00 -2.15062938e-02\n",
      " -1.93691679e-03  0.00000000e+00  3.19203577e+00]\n",
      "Epoch 251, Gradients: [-8.94369334e-02 -6.10351562e-05 -6.71386719e-04  5.49316406e-04\n",
      " -1.22070312e-04  0.00000000e+00 -9.91000794e-04]\n",
      " Loss: 8315.8505859375, Parameters: [ 3.07380382e+01  1.32443626e+00  2.29614542e+00 -4.27426179e-02\n",
      "  8.38798613e-03  0.00000000e+00  3.19208551e+00]\n",
      "Epoch 301, Gradients: [-0.08843087  0.00063324 -0.00061035 -0.00036621  0.00042725  0.\n",
      "  0.00024902]\n",
      " Loss: 8315.017578125, Parameters: [ 3.08281347e+01  1.32826749e+00  2.30224357e+00 -5.62444900e-02\n",
      " -1.50029938e-02  0.00000000e+00  3.19201226e+00]\n",
      "Epoch 351, Gradients: [-8.82462412e-02  1.29699707e-04  3.66210938e-04 -1.52587891e-04\n",
      " -3.05175781e-05  0.00000000e+00 -9.40983882e-05]\n",
      " Loss: 8314.75, Parameters: [ 3.08551495e+01  1.32950601e+00  2.30453555e+00 -5.56330974e-02\n",
      " -1.62730203e-02  0.00000000e+00  3.19213002e+00]\n",
      "Epoch 401, Gradients: [-0.08803163  0.00082397  0.00097656  0.00015259  0.00018311  0.\n",
      " -0.00133141]\n",
      " Loss: 8314.5, Parameters: [ 3.08821573e+01  1.33065113e+00  2.30641829e+00 -5.16880196e-02\n",
      " -2.05975481e-02  0.00000000e+00  3.19185235e+00]\n",
      "Converged at epoch 401\n",
      "Epoch 402,  \n",
      " vecc Parameters: [ 3.08824814e+01  1.33062876e+00  2.30639410e+00 -5.16675735e-02\n",
      " -2.05771004e-02  0.00000000e+00  3.19186239e+00]\n",
      "FINAL STATE: Epoch 402, Loss: 8314.5, \n",
      " vecc Parameters: [30.882481367893934, 1.3306287573904048, 2.3063941013205755, -0.05166757348340395, -0.020577100376109176, 0.0, 3.1918623924224208]\n",
      "Day 1 optimization finished in 468.35s over 402 epochs.\n",
      "Day 1 final results: [30.882481367893934, 1.3306287573904048, 2.3063941013205755, -0.05166757348340395, -0.020577100376109176, 0.0, 3.1918623924224208, 8314.5]\n",
      "\n",
      "--- All Days Processed ---\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------\n",
    "# Main Optimization Loop (Updated)\n",
    "# ------------------------------------\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "for i in range(31):\n",
    "    #idx_for_datamap = [i*8, (i+1)*8]\n",
    "    idx_for_datamap = [i*8, i*8 +1]\n",
    "\n",
    "    cur_map, cur_df = analysis_map_no_mm, agg_data_no_mm = data_load_instance.load_working_data_keep_ori(\n",
    "    df_map, \n",
    "    idx_for_datamap, \n",
    "    ord_mm=None,  # or just omit it\n",
    "    dtype=torch.float # or just omit it \n",
    "    )\n",
    "    cur_df[:,0], cur_df[:,1] = cur_df[:,4] , cur_df[:,5]\n",
    "\n",
    "    for map in cur_map.values():\n",
    "        map[:,0], map[:,1] = map[:,4] , map[:,5]\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "\n",
    "print(f\"Data loaded for {len(df_day_map_list)} days.\")\n",
    "# print(df_day_aggregated_list[0].shape) # From reference code\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "\n",
    "# This list is now just for iterating\n",
    "#days_list = range(len(df_day_map_list)) \n",
    "days_list = [0]\n",
    "for day in days_list:  \n",
    "    \n",
    "    # Get the pre-loaded data for this day\n",
    "    analysis_data_map = df_day_map_list[day]\n",
    "    aggregated_data = df_day_aggregated_list[day]\n",
    "\n",
    "    #a = [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]\n",
    "    a = [28.75, 0.98, 1.06, 0, 0, 0, 1.890]\n",
    "    #a = [28.75, 0.98, 1.06, 0.036, -0.155, 0.179, 1.890]\n",
    "    params = torch.tensor(a, dtype=torch.float64, requires_grad=True)\n",
    "    \n",
    "    # Calculate resolution for printing\n",
    "    res_calc = (200 / lat_lon_resolution[0]) * (100 / lat_lon_resolution[0])\n",
    "    print(f'\\n--- Starting Day {day+1} (2024-07-{day+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params.detach().numpy()}')\n",
    "            \n",
    "    # --- Data loading is now done *before* the loop ---\n",
    "\n",
    "    # We need to define the device (though we aren't passing it anymore)\n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model_instance = kernels.model_fitting(\n",
    "            smooth = v,\n",
    "            input_map = analysis_data_map,\n",
    "            aggregated_data = aggregated_data,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "            # device = device_str  <--- REMOVED: This was causing the TypeError\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Adjusted optimizer call based on expected return values (step size changed to step)\n",
    "    optimizer, scheduler = model_instance.optimizer_fun(\n",
    "        params, \n",
    "        lr=lr, \n",
    "        betas=(0.9, 0.8), \n",
    "        eps=1e-8, \n",
    "        step_size=step, # Using the 'step' variable here\n",
    "        gamma=gamma_par  # Using gamma_par\n",
    "    ) \n",
    "\n",
    "    # --- CRITICAL CORRECTION ---\n",
    "    # 1. We no longer need to create a separate 'instance_map'.\n",
    "    #    'model_instance' is already the correct instance.\n",
    "    # 2. We do NOT pre-calculate 'cov_map'. The optimized training loop\n",
    "    #    'run_vecc_may9' does this internally on each epoch\n",
    "    #    to ensure the gradients are correct.\n",
    "    # 3. We call 'run_vecc_may9' (the optimized loop) instead of 'run_vecc_grp9'.\n",
    "    #    This version does not take 'cov_map' as an argument.\n",
    "    \n",
    "    # Calling the optimized 'run_vecc_may9'\n",
    "    out, epoch_ran = model_instance.run_vecc_oct22(\n",
    "        params, \n",
    "        optimizer,\n",
    "        scheduler, \n",
    "        model_instance.matern_cov_anisotropy_v05, \n",
    "        epochs=epochs\n",
    "    )\n",
    "    # --- End Correction ---\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"Day {day+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day+1} final results: {out}\")\n",
    "\n",
    "print(\"\\n--- All Days Processed ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcea0601",
   "metadata": {},
   "source": [
    "## Full likelihood\n",
    "\n",
    "Vecchia Original\n",
    "[29.921775534973218, 1.8597906958307489, 3.4954059102213106, 0.22512272633105687, -0.0648045117355326, 0.0, 3.3287024509849537] 2036\n",
    "(data 1250) # 1 23641\n",
    "\n",
    "Day 1 final results: [30.882481367893934, 1.3306287573904048, 2.3063941013205755, -0.05166757348340395, -0.020577100376109176, 0.0, 3.1918623924224208] 8314. (data 5000). \n",
    "\n",
    "\n",
    "Center matching\n",
    "[29.867643794098317, 1.8170445928338301, 3.452060053856289, 9.346808116428221e-06, 1.3480203304274802e-05, 0.0, 3.278045234831132, 2038.106906954074]\n",
    "\n",
    "[30.823365978489328, 1.325000558988864, 2.300043515398019, 5.191119207004892e-06, -9.752316964224805e-07, 0.0, 3.1756405728254657, 8313.954087906111]\n",
    "\n",
    "# 1 23644\n",
    "\n",
    "\n",
    "If I fit 8 hours on original space--> numerically unstable and cholesky decomposition fails after hitting non positive matrix\n",
    "\n",
    "\n",
    "## Vecchia likleihood 8 hours using same parameters ()\n",
    "a = [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]\n",
    "\n",
    "original   15995.6299\n",
    "center matching # 15994.2207 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de556f",
   "metadata": {},
   "source": [
    "Pre-loading data for all days...\n",
    "Data loaded for 31 days.\n",
    "\n",
    "--- Starting Day 1 (2024-07-1) ---\n",
    "Data size per day: 1250.0, smooth: 0.5\n",
    "mm_cond_number: 20,\n",
    "initial parameters: \n",
    " [28.75   0.98   1.06   0.036 -0.155  0.179  1.89 ]\n",
    "Converged at epoch 535\n",
    "Epoch 536,  \n",
    " vecc Parameters: [31.00915505  1.1916293   1.84680497  0.03927127 -0.15666047  0.12771191\n",
    "  4.11954968]\n",
    "FINAL STATE: Epoch 536, Loss: 15746.237437383388, \n",
    " vecc Parameters: [31.009155050569746, 1.191629302554168, 1.846804972667287, 0.03927126531202711, -0.15666046904308123, 0.12771190935579058, 4.119549677954342]\n",
    "Day 1 optimization finished in 822.07s over 536 epochs.\n",
    "Day 1 final results: [31.009155050569746, 1.191629302554168, 1.846804972667287, 0.03927126531202711, -0.15666046904308123, 0.12771190935579058, 4.119549677954342, 15746.237437383388]\n",
    "\n",
    "--- All Days Processed ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caf2b92",
   "metadata": {},
   "source": [
    "Pre-loading data for all days...\n",
    "Data loaded for 31 days.\n",
    "\n",
    "--- Starting Day 1 (2024-07-1) ---\n",
    "Data size per day: 1250.0, smooth: 0.5\n",
    "mm_cond_number: 20,\n",
    "initial parameters: \n",
    " [28.75  0.98  1.06  0.    0.    0.    1.89]\n",
    "Converged at epoch 332\n",
    "Epoch 333,  \n",
    " vecc Parameters: [2.98676438e+01 1.81704459e+00 3.45206005e+00 9.34680812e-06\n",
    " 1.34802033e-05 0.00000000e+00 3.27804523e+00]\n",
    "FINAL STATE: Epoch 333, Loss: 2038.106906954074, \n",
    " vecc Parameters: [29.867643794098317, 1.8170445928338301, 3.452060053856289, 9.346808116428221e-06, 1.3480203304274802e-05, 0.0, 3.278045234831132]\n",
    "Day 1 optimization finished in 73.78s over 333 epochs.\n",
    "Day 1 final results: [29.867643794098317, 1.8170445928338301, 3.452060053856289, 9.346808116428221e-06, 1.3480203304274802e-05, 0.0, 3.278045234831132, 2038.106906954074]\n",
    "\n",
    "--- All Days Processed ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
