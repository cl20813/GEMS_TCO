{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f33ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reflected location error in ozone data simulation\n",
    "\n",
    "import torch\n",
    "import torch.fft\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import argparse \n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "from sklearn.neighbors import BallTree\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CUSTOM PATHS ---\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "# (필요 시 실제 GEMS_TCO 라이브러리 import)\n",
    "try:\n",
    "    from GEMS_TCO import kernels_reparam_space_time_gpu\n",
    "    from GEMS_TCO import kernels_reparam_space_time_gpu_copy_dummy_013126 as kernels_reparam_space_time_gpu_copy\n",
    "    from GEMS_TCO import kernels_columns as kernels_reparam_space_time_gpu_col\n",
    "    \n",
    "    from GEMS_TCO import orderings as _orderings\n",
    "    from GEMS_TCO import alg_optimization, BaseLogger\n",
    "\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Warning: GEMS_TCO modules not found. Ensure paths are correct.\")\n",
    "\n",
    "\n",
    "from GEMS_TCO import configuration as config\n",
    "from GEMS_TCO.data_loader import load_data2, exact_location_filter\n",
    "from GEMS_TCO import debiased_whittle\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d6ba3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22680, 4])\n"
     ]
    }
   ],
   "source": [
    "space: List[str] = ['1', '1']\n",
    "lat_lon_resolution = [int(s) for s in space]\n",
    "mm_cond_number: int = 8\n",
    "years = ['2024']\n",
    "month_range = [7] \n",
    "\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "data_load_instance = load_data2(config.mac_data_load_path)\n",
    "\n",
    "#lat_range_input = [1, 3]\n",
    "#lon_range_input = [125.0, 129.0]\n",
    "\n",
    "lat_range_input=[-3,-1]      \n",
    "lon_range_input=[121, 125] \n",
    "\n",
    "#lat_range_input=[-3,2]      \n",
    "#lon_range_input=[121, 131] \n",
    "\n",
    "df_map, ord_mm, nns_map = data_load_instance.load_maxmin_ordered_data_bymonthyear(\n",
    "lat_lon_resolution=lat_lon_resolution, \n",
    "mm_cond_number=mm_cond_number,\n",
    "years_=years, \n",
    "months_=month_range,\n",
    "\n",
    "lat_range=lat_range_input,   \n",
    "lon_range=lon_range_input\n",
    "  \n",
    ")\n",
    "\n",
    "\n",
    "daily_aggregated_reg_vecc_sim = [] \n",
    "daily_hourly_maps_reg_vecc_sim = []      \n",
    "\n",
    "daily_aggregated_irr_vecc_sim = [] \n",
    "daily_hourly_maps_irr_vecc_sim = []   \n",
    "\n",
    "\n",
    "for day_index in range(31):\n",
    "    hour_start_index = day_index * 8\n",
    "    \n",
    "    hour_end_index = (day_index + 1) * 8\n",
    "    #hour_end_index = day_index*8 + 1\n",
    "    hour_indices = [hour_start_index, hour_end_index]\n",
    "\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "    df_map, \n",
    "    hour_indices, \n",
    "    ord_mm= ord_mm,  # or just omit it\n",
    "    dtype=torch.float64, # or just omit it \n",
    "    keep_ori=False  #keep_exact_loc\n",
    "    )\n",
    "\n",
    "    daily_aggregated_reg_vecc_sim.append( day_aggregated_tensor )\n",
    "    daily_hourly_maps_reg_vecc_sim.append( day_hourly_map )\n",
    "\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "    df_map, \n",
    "    hour_indices, \n",
    "    ord_mm= ord_mm,  # or just omit it\n",
    "    dtype=torch.float64, # or just omit it \n",
    "    keep_ori=True  #keep_exact_loc\n",
    "    )\n",
    "\n",
    "    daily_aggregated_irr_vecc_sim.append( day_aggregated_tensor )\n",
    "    daily_hourly_maps_irr_vecc_sim.append( day_hourly_map )\n",
    "print(daily_aggregated_irr_vecc_sim[0].shape)\n",
    "\n",
    "nn = daily_aggregated_irr_vecc_sim[0].shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd23747a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating on: cpu\n",
      "Checking Inputs:\n",
      " -> Target SigmaSq: 13.059\n",
      "--- Generating Datasets: Vecc (Real Locs) & DW (Grid Locs) ---\n",
      "   [High-Res Gen] 160x/4x Field Created.\n",
      "   [Calibration] Raw Std: 3.5164 -> Fixed to: 3.6137\n",
      "  Processed Day 0 (Points: 2835)\n",
      "\n",
      "[Final Validation]\n",
      "Vecc Shape (Irregular): torch.Size([20723, 4])\n",
      "DW Shape   (Regular):   torch.Size([20723, 4])\n",
      "Variance Check: 13.2117 (Expected ~13.0 + Nugget)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "import sys\n",
    "# --- 1. CONFIGURATION ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.float32 if DEVICE.type == 'mps' else torch.float64\n",
    "print(f\"Simulating on: {DEVICE}\")\n",
    "\n",
    "# TRUE PARAMETERS\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lon = 0.195 \n",
    "init_range_lat = 0.154 \n",
    "init_advec_lat = 0.0418\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "# Map parameters\n",
    "init_phi2 = 1.0 / init_range_lon\n",
    "init_phi1 = init_sigmasq * init_phi2\n",
    "init_phi3 = (init_range_lon / init_range_lat)**2\n",
    "init_phi4 = (init_range_lon / init_range_time)**2\n",
    "\n",
    "initial_vals = [np.log(init_phi1), np.log(init_phi2), np.log(init_phi3), \n",
    "                np.log(init_phi4), init_advec_lat, init_advec_lon, np.log(init_nugget)]\n",
    "\n",
    "params_list = [torch.tensor([val], requires_grad=True, dtype=DTYPE, device=DEVICE) for val in initial_vals]\n",
    "OZONE_MEAN = 260.0\n",
    "\n",
    "# --- 2. EXACT COVARIANCE & FFT HELPERS ---\n",
    "def get_model_covariance_on_grid(lags_x, lags_y, lags_t, params):\n",
    "    phi1, phi2, phi3, phi4 = torch.exp(params[0]), torch.exp(params[1]), torch.exp(params[2]), torch.exp(params[3])\n",
    "    advec_lat, advec_lon = params[4], params[5]\n",
    "    sigmasq = phi1 / phi2\n",
    "\n",
    "    u_lat_eff = lags_x - advec_lat * lags_t\n",
    "    u_lon_eff = lags_y - advec_lon * lags_t\n",
    "    dist_sq = (u_lat_eff.pow(2) * phi3) + (u_lon_eff.pow(2)) + (lags_t.pow(2) * phi4)\n",
    "    return sigmasq * torch.exp(-torch.sqrt(dist_sq + 1e-8) * phi2)\n",
    "\n",
    "def make_target_grid(lat_start, lat_end, lat_step, lon_start, lon_end, lon_step, device, dtype):\n",
    "    \"\"\"\n",
    "    표준 분석용 레귤러 그리드 생성. \n",
    "    자동 방향 제어(아래 버전 장점) + 소수점 반올림 오차 방지(위 버전 장점) 통합.\n",
    "    \"\"\"\n",
    "    # 1. 방향 자동 교정 (Step 방향이 시작/끝점과 맞지 않을 경우)\n",
    "    if lat_start > lat_end and lat_step > 0: \n",
    "        lat_step = -lat_step\n",
    "    if lon_start > lon_end and lon_step > 0:\n",
    "        lon_step = -lon_step\n",
    "\n",
    "    # 2. Arange 생성\n",
    "    lats = torch.arange(lat_start, lat_end - 0.0001, lat_step, device=device, dtype=dtype)\n",
    "    lons = torch.arange(lon_start, lon_end + 0.0001, lon_step, device=device, dtype=dtype)\n",
    "\n",
    "    # 3. 수치적 안정성을 위한 Rounding (소수점 4자리)\n",
    "    # 이 과정이 없으면 나중에 모델 피팅 시 좌표 미세 오차로 인해 결과가 달라질 수 있습니다.\n",
    "    lats = torch.round(lats * 10000) / 10000\n",
    "    lons = torch.round(lons * 10000) / 10000\n",
    "\n",
    "    # 4. Meshgrid 생성\n",
    "    grid_lat, grid_lon = torch.meshgrid(lats, lons, indexing='ij')\n",
    "    center_points = torch.stack([grid_lat.flatten(), grid_lon.flatten()], dim=1)\n",
    "    \n",
    "    return center_points, len(lats), len(lons)\n",
    "\n",
    "\n",
    "def get_model_covariance_on_grid(lags_x, lags_y, lags_t, params):\n",
    "    phi1, phi2, phi3, phi4 = torch.exp(params[0]), torch.exp(params[1]), torch.exp(params[2]), torch.exp(params[3])\n",
    "    advec_lat, advec_lon = params[4], params[5]\n",
    "    sigmasq = phi1 / phi2\n",
    "\n",
    "    u_lat_eff = lags_x - advec_lat * lags_t\n",
    "    u_lon_eff = lags_y - advec_lon * lags_t\n",
    "    dist_sq = (u_lat_eff.pow(2) * phi3) + (u_lon_eff.pow(2)) + (lags_t.pow(2) * phi4)\n",
    "    return sigmasq * torch.exp(-torch.sqrt(dist_sq + 1e-8) * phi2)\n",
    "\n",
    "\n",
    "\n",
    "# --- 3. GRID & MAPPING FUNCTIONS ---\n",
    "\n",
    "\n",
    "def make_target_grid(lat_start, lat_end, lat_step, lon_start, lon_end, lon_step, device, dtype):\n",
    "    \"\"\"\n",
    "    표준 분석용 레귤러 그리드 생성. \n",
    "    자동 방향 제어(아래 버전 장점) + 소수점 반올림 오차 방지(위 버전 장점) 통합.\n",
    "    \"\"\"\n",
    "    # 1. 방향 자동 교정 (Step 방향이 시작/끝점과 맞지 않을 경우)\n",
    "    if lat_start > lat_end and lat_step > 0: \n",
    "        lat_step = -lat_step\n",
    "    if lon_start > lon_end and lon_step > 0:\n",
    "        lon_step = -lon_step\n",
    "\n",
    "    # 2. Arange 생성\n",
    "    lats = torch.arange(lat_start, lat_end - 0.0001, lat_step, device=device, dtype=dtype)\n",
    "    lons = torch.arange(lon_start, lon_end + 0.0001, lon_step, device=device, dtype=dtype)\n",
    "\n",
    "    # 3. 수치적 안정성을 위한 Rounding (소수점 4자리)\n",
    "    # 이 과정이 없으면 나중에 모델 피팅 시 좌표 미세 오차로 인해 결과가 달라질 수 있습니다.\n",
    "    lats = torch.round(lats * 10000) / 10000\n",
    "    lons = torch.round(lons * 10000) / 10000\n",
    "\n",
    "    # 4. Meshgrid 생성\n",
    "    grid_lat, grid_lon = torch.meshgrid(lats, lons, indexing='ij')\n",
    "    center_points = torch.stack([grid_lat.flatten(), grid_lon.flatten()], dim=1)\n",
    "    \n",
    "    return center_points, len(lats), len(lons)\n",
    "\n",
    "# --- 2. High-Res Field Generator (Lat x160 / Lon x4) ---\n",
    "def generate_high_res_field(target_lat_range, target_lon_range, t_steps, params, device, dtype):\n",
    "    \"\"\"\n",
    "    초고해상도(160배/4배) 필드를 생성하고, 분산을 참값(Target)에 맞게 보정(Calibration)합니다.\n",
    "    \"\"\"\n",
    "    # [설정] 해상도 확대 배율\n",
    "    lat_res_factor = 160.0  # 위도 160배\n",
    "    lon_res_factor = 4.0    # 경도 4배\n",
    "    \n",
    "    # 기본 해상도(0.044, 0.063)를 배율로 나눔\n",
    "    lat_res_high = 0.044 / lat_res_factor\n",
    "    lon_res_high = 0.063 / lon_res_factor\n",
    "    \n",
    "    t_lat_max = max(target_lat_range)\n",
    "    t_lat_min = min(target_lat_range)\n",
    "    \n",
    "    # Padding: 경계면 왜곡 방지를 위해 약간 넓게 생성\n",
    "    lats_high = torch.arange(t_lat_max + 0.1, t_lat_min - 0.1, -lat_res_high, device=device, dtype=dtype)\n",
    "    lons_high = torch.arange(target_lon_range[0] - 0.1, target_lon_range[1] + 0.1, lon_res_high, device=device, dtype=dtype)\n",
    "    \n",
    "    Nx, Ny, Nt = len(lats_high), len(lons_high), t_steps\n",
    "    dlat, dlon, dt = lat_res_high, lon_res_high, 1.0\n",
    "    \n",
    "    # FFT Grid Setup (Circulant Embedding)\n",
    "    Px, Py, Pt = 2*Nx, 2*Ny, 2*Nt\n",
    "    lags_x = torch.arange(Px, device=device, dtype=dtype) * dlat; lags_x[Px//2:] -= (Px * dlat)\n",
    "    lags_y = torch.arange(Py, device=device, dtype=dtype) * dlon; lags_y[Py//2:] -= (Py * dlon)\n",
    "    lags_t = torch.arange(Pt, device=device, dtype=dtype) * dt;   lags_t[Pt//2:] -= (Pt * dt)\n",
    "\n",
    "    L_x, L_y, L_t = torch.meshgrid(lags_x, lags_y, lags_t, indexing='ij')\n",
    "    \n",
    "    # 파라미터 해석\n",
    "    phi1, phi2 = torch.exp(params[0]), torch.exp(params[1])\n",
    "    phi3, phi4 = torch.exp(params[2]), torch.exp(params[3])\n",
    "    adv_lat, adv_lon = params[4], params[5]\n",
    "    \n",
    "    sigma_sq = phi1 / phi2  # 참값 분산 (13.059)\n",
    "    \n",
    "    u_x = L_x - adv_lat * L_t\n",
    "    u_y = L_y - adv_lon * L_t\n",
    "    dist_sq = (u_x * torch.sqrt(phi3) * phi2)**2 + (u_y * phi2)**2 + (L_t * torch.sqrt(phi4) * phi2)**2\n",
    "    C_vals = sigma_sq * torch.exp(-torch.sqrt(dist_sq + 1e-12))\n",
    "\n",
    "    # FFT Simulation\n",
    "    S = torch.fft.fftn(C_vals); S.real = torch.clamp(S.real, min=0)\n",
    "    random_phase = torch.fft.fftn(torch.randn(Px, Py, Pt, device=device, dtype=dtype))\n",
    "    field_sim_raw = torch.fft.ifftn(torch.sqrt(S.real) * random_phase).real\n",
    "    \n",
    "    # Crop\n",
    "    field_sim = field_sim_raw[:Nx, :Ny, :Nt]\n",
    "    \n",
    "    # [Calibration] 160배 해상도로 인해 흩어진 에너지를 모아 분산을 복원\n",
    "    current_std = field_sim.std()\n",
    "    target_std = torch.sqrt(sigma_sq)\n",
    "    \n",
    "    # (x - mean) * ratio\n",
    "    field_calibrated = (field_sim - field_sim.mean()) * (target_std / (current_std + 1e-9))\n",
    "    \n",
    "    print(f\"   [High-Res Gen] 160x/4x Field Created.\")\n",
    "    print(f\"   [Calibration] Raw Std: {current_std.item():.4f} -> Fixed to: {target_std.item():.4f}\")\n",
    "    \n",
    "    return field_calibrated, lats_high, lons_high\n",
    "\n",
    "# --- 3. Main Generator (Vecc & DW) ---\n",
    "def generate_exact_count_datasets(daily_maps_real, true_params_tensor, target_grid_info, device, dtype):\n",
    "    print(\"--- Generating Datasets: Vecc (Real Locs) & DW (Grid Locs) ---\")\n",
    "    \n",
    "    lat_s, lat_e, lat_step, lon_s, lon_e, lon_step = target_grid_info\n",
    "    \n",
    "    # 1. Target Grid 생성 (DW 매핑 목적지)\n",
    "    target_grid_coords, _, _ = make_target_grid(lat_s, lat_e, lat_step, lon_s, lon_e, lon_step, device, dtype)\n",
    "    target_locs_np = target_grid_coords.cpu().numpy()\n",
    "    target_tree = BallTree(np.radians(target_locs_np), metric='haversine') \n",
    "    \n",
    "    # 결과 저장 리스트\n",
    "    daily_aggregated_tensors_vecc_sim = []\n",
    "    daily_hourly_maps_vecc_sim = []\n",
    "    daily_aggregated_tensors_dw_sim = []\n",
    "    daily_hourly_maps_dw_sim = []\n",
    "    \n",
    "    # Nugget Noise (현실감 추가용)\n",
    "    # params[6] is log(nugget)\n",
    "    nugget_val = torch.exp(true_params_tensor[6])\n",
    "    noise_std = torch.sqrt(nugget_val)\n",
    "    \n",
    "    for day_idx, day_map_real in enumerate(daily_maps_real):\n",
    "        if day_idx >= 1: break \n",
    "        \n",
    "        # (A) High-Res Field 생성 (x160)\n",
    "        lat_range = (max(lat_s, lat_e), min(lat_s, lat_e))\n",
    "        lon_range = (lon_s, lon_e)\n",
    "        high_res_field, lats_high, lons_high = generate_high_res_field(\n",
    "            lat_range, lon_range, 8, true_params_tensor, device, dtype\n",
    "        )\n",
    "        \n",
    "        # (B) High-Res Sampling용 BallTree\n",
    "        hr_mesh_lat, hr_mesh_lon = torch.meshgrid(lats_high, lons_high, indexing='ij')\n",
    "        hr_flat_coords = torch.stack([hr_mesh_lat.flatten(), hr_mesh_lon.flatten()], dim=1).cpu().numpy()\n",
    "        hr_tree = BallTree(np.radians(hr_flat_coords), metric='haversine')\n",
    "        high_res_flat = high_res_field.reshape(-1, 8) \n",
    "\n",
    "        sorted_keys = sorted([k for k in day_map_real.keys() if 'hm' in k or 'time' in k])\n",
    "        \n",
    "        vecc_tensor_list = []\n",
    "        dw_tensor_list = []\n",
    "        current_day_vecc_map = {}\n",
    "        current_day_dw_map = {}\n",
    "        \n",
    "        for t_idx, key in enumerate(sorted_keys):\n",
    "            if t_idx >= 8: break\n",
    "            \n",
    "            real_tensor = day_map_real[key]\n",
    "            if not isinstance(real_tensor, torch.Tensor): real_tensor = torch.tensor(real_tensor, device=device, dtype=dtype)\n",
    "            else: real_tensor = real_tensor.to(device)\n",
    "            \n",
    "            # [Step 1] Unique: 중복 좌표 제거 (모델 안정성)\n",
    "            np_data = real_tensor.cpu().numpy()\n",
    "            _, unique_idx = np.unique(np_data[:, :2], axis=0, return_index=True)\n",
    "            unique_idx.sort()\n",
    "            clean_tensor = real_tensor[unique_idx]\n",
    "            \n",
    "            real_lats = clean_tensor[:, 0]\n",
    "            real_lons = clean_tensor[:, 1]\n",
    "            time_val = clean_tensor[:, 3] \n",
    "            \n",
    "            # [Step 2] Sampling: 현실 좌표 -> High-Res Field 값 추출\n",
    "            current_real_locs = torch.stack([real_lats, real_lons], dim=1).cpu().numpy()\n",
    "            _, vecc_hr_indices = hr_tree.query(np.radians(current_real_locs), k=1)\n",
    "            vecc_hr_indices_tensor = torch.tensor(vecc_hr_indices.flatten(), device=device, dtype=torch.long)\n",
    "            \n",
    "            # 값 생성: Signal + Noise + Mean(260)\n",
    "            # 160배 해상도이므로 Signal은 아주 매끄럽습니다. 여기에 Nugget을 더해 현실감을 줍니다.\n",
    "            signal = high_res_flat[vecc_hr_indices_tensor, t_idx]\n",
    "            noise = torch.randn_like(signal) * noise_std\n",
    "            sim_vals = signal + noise + 260.0\n",
    "            \n",
    "            # [Step 3] VECC SIM: 원래 위치(Original Locs) 유지\n",
    "            vecc_row = torch.stack([real_lats, real_lons, sim_vals, time_val], dim=1)\n",
    "            vecc_tensor_list.append(vecc_row.detach())\n",
    "            current_day_vecc_map[key] = vecc_row.detach()\n",
    "            \n",
    "            # [Step 4] DW SIM: 레귤러 그리드(Regular Grid)로 매핑\n",
    "            # 현실 좌표에서 가장 가까운 격자점을 찾습니다.\n",
    "            _, dw_indices = target_tree.query(np.radians(current_real_locs), k=1)\n",
    "            dw_indices_tensor = torch.tensor(dw_indices.flatten(), device=device, dtype=torch.long)\n",
    "            \n",
    "            mapped_lats = target_grid_coords[dw_indices_tensor, 0]\n",
    "            mapped_lons = target_grid_coords[dw_indices_tensor, 1]\n",
    "            \n",
    "            # 값(sim_vals)은 Vecc와 동일하게 가져갑니다.\n",
    "            dw_row = torch.stack([mapped_lats, mapped_lons, sim_vals, time_val], dim=1)\n",
    "            dw_tensor_list.append(dw_row.detach())\n",
    "            current_day_dw_map[key] = dw_row.detach()\n",
    "\n",
    "        # 결과 저장\n",
    "        daily_aggregated_tensors_vecc_sim.append(torch.cat(vecc_tensor_list, dim=0))\n",
    "        daily_hourly_maps_vecc_sim.append(current_day_vecc_map)\n",
    "        \n",
    "        daily_aggregated_tensors_dw_sim.append(torch.cat(dw_tensor_list, dim=0))\n",
    "        daily_hourly_maps_dw_sim.append(current_day_dw_map)\n",
    "        \n",
    "        print(f\"  Processed Day {day_idx} (Points: {len(vecc_tensor_list[0])})\")\n",
    "            \n",
    "    # 4개 모두 반환\n",
    "    return (daily_aggregated_tensors_dw_sim, daily_hourly_maps_dw_sim, \n",
    "            daily_aggregated_tensors_vecc_sim, daily_hourly_maps_vecc_sim)\n",
    "\n",
    "# --- 4. EXECUTION ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.float64\n",
    "\n",
    "target_grid_info = (-1.0, -3.0, 0.044, 121.0, 125.0, 0.063)\n",
    "\n",
    "# 1. 참값 정의\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lon = 0.195 \n",
    "init_range_lat = 0.154 \n",
    "init_range_time = 1.0\n",
    "init_nugget    = 0.247\n",
    "\n",
    "# 2. 파라미터 변환 (정답 로직)\n",
    "true_phi2 = 1.0 / init_range_lon              \n",
    "true_phi1 = init_sigmasq * true_phi2          \n",
    "true_phi3 = (init_range_lon / init_range_lat)**2\n",
    "true_phi4 = (init_range_lon / init_range_time)**2\n",
    "\n",
    "true_params_tensor = [\n",
    "    torch.tensor([np.log(true_phi1)], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([np.log(true_phi2)], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([np.log(true_phi3)], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([np.log(true_phi4)], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([0.0418], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([-0.1689], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([np.log(init_nugget)], device=DEVICE, dtype=DTYPE)\n",
    "]\n",
    "\n",
    "print(f\"Checking Inputs:\")\n",
    "print(f\" -> Target SigmaSq: {init_sigmasq}\")\n",
    "\n",
    "if 'daily_hourly_maps_irr_vecc_sim' in locals() and len(daily_hourly_maps_irr_vecc_sim) > 0:\n",
    "    (daily_aggregated_reg_vecc_sim, daily_hourly_maps_reg_vecc_sim,\n",
    "     daily_aggregated_irr_vecc_sim, daily_hourly_maps_irr_vecc_sim) = generate_exact_count_datasets(\n",
    "        daily_hourly_maps_irr_vecc_sim, true_params_tensor, target_grid_info, DEVICE, DTYPE\n",
    "    )\n",
    "    \n",
    "    # 검증: Vecc vs DW\n",
    "    v_agg = daily_aggregated_irr_vecc_sim[0]\n",
    "    dw_agg = daily_aggregated_reg_vecc_sim[0]\n",
    "    \n",
    "    print(f\"\\n[Final Validation]\")\n",
    "    print(f\"Vecc Shape (Irregular): {v_agg.shape}\")\n",
    "    print(f\"DW Shape   (Regular):   {dw_agg.shape}\")\n",
    "    print(f\"Variance Check: {v_agg[:, 2].var().item():.4f} (Expected ~13.0 + Nugget)\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: Load data first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ed5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating on: cpu\n",
      "Checking Inputs:\n",
      " -> Target SigmaSq: 13.059\n",
      "--- Generating Datasets: Vecc (Real Locs) & DW (Grid Locs) ---\n",
      "   [High-Res Gen] 160x/4x Field Created.\n",
      "   [Calibration] Raw Std: 3.6624 -> Fixed to: 3.6137\n",
      "  Processed Day 0 (Points: 2835)\n",
      "\n",
      "[Final Validation]\n",
      "Vecc Shape (Irregular): torch.Size([20723, 4])\n",
      "DW Shape   (Regular):   torch.Size([20723, 4])\n",
      "Variance Check: 13.2481 (Expected ~13.0 + Nugget)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "import sys\n",
    "# --- 1. CONFIGURATION ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.float32 if DEVICE.type == 'mps' else torch.float64\n",
    "print(f\"Simulating on: {DEVICE}\")\n",
    "\n",
    "# TRUE PARAMETERS\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lon = 0.195 \n",
    "init_range_lat = 0.154 \n",
    "init_advec_lat = 0.0418\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "# Map parameters\n",
    "init_phi2 = 1.0 / init_range_lon\n",
    "init_phi1 = init_sigmasq * init_phi2\n",
    "init_phi3 = (init_range_lon / init_range_lat)**2\n",
    "init_phi4 = (init_range_lon / init_range_time)**2\n",
    "\n",
    "initial_vals = [np.log(init_phi1), np.log(init_phi2), np.log(init_phi3), \n",
    "                np.log(init_phi4), init_advec_lat, init_advec_lon, np.log(init_nugget)]\n",
    "\n",
    "params_list = [torch.tensor([val], requires_grad=True, dtype=DTYPE, device=DEVICE) for val in initial_vals]\n",
    "OZONE_MEAN = 260.0\n",
    "\n",
    "# --- 2. EXACT COVARIANCE & FFT HELPERS ---\n",
    "def get_model_covariance_on_grid(lags_x, lags_y, lags_t, params):\n",
    "    phi1, phi2, phi3, phi4 = torch.exp(params[0]), torch.exp(params[1]), torch.exp(params[2]), torch.exp(params[3])\n",
    "    advec_lat, advec_lon = params[4], params[5]\n",
    "    sigmasq = phi1 / phi2\n",
    "\n",
    "    u_lat_eff = lags_x - advec_lat * lags_t\n",
    "    u_lon_eff = lags_y - advec_lon * lags_t\n",
    "    dist_sq = (u_lat_eff.pow(2) * phi3) + (u_lon_eff.pow(2)) + (lags_t.pow(2) * phi4)\n",
    "    return sigmasq * torch.exp(-torch.sqrt(dist_sq + 1e-8) * phi2)\n",
    "\n",
    "def generate_exact_gems_field(lat_coords, lon_coords, t_steps, params):\n",
    "    Nx, Ny, Nt = len(lat_coords), len(lon_coords), t_steps\n",
    "    dlat = float(lat_coords[1] - lat_coords[0])\n",
    "    dlon = float(lon_coords[1] - lon_coords[0])\n",
    "    dt = 1.0 \n",
    "    \n",
    "    # 2x Padding logic for FFT\n",
    "    Px, Py, Pt = 2*Nx, 2*Ny, 2*Nt\n",
    "    \n",
    "    lags_x = torch.arange(Px, device=DEVICE, dtype=DTYPE) * dlat\n",
    "    lags_x[Px//2:] -= (Px * dlat) # Wrap logic\n",
    "    \n",
    "    lags_y = torch.arange(Py, device=DEVICE, dtype=DTYPE) * dlon\n",
    "    lags_y[Py//2:] -= (Py * dlon)\n",
    "\n",
    "    lags_t = torch.arange(Pt, device=DEVICE, dtype=DTYPE) * dt\n",
    "    lags_t[Pt//2:] -= (Pt * dt)\n",
    "\n",
    "    L_x, L_y, L_t = torch.meshgrid(lags_x, lags_y, lags_t, indexing='ij')\n",
    "    C_vals = get_model_covariance_on_grid(L_x, L_y, L_t, params)\n",
    "\n",
    "    S = torch.fft.fftn(C_vals)\n",
    "    S.real = torch.clamp(S.real, min=0)\n",
    "    \n",
    "    # Random phase\n",
    "    random_phase = torch.fft.fftn(torch.randn(Px, Py, Pt, device=DEVICE, dtype=DTYPE))\n",
    "    field_sim = torch.fft.ifftn(torch.sqrt(S.real) * random_phase).real\n",
    "    \n",
    "    return field_sim[:Nx, :Ny, :Nt]\n",
    "\n",
    "# --- 3. GRID & MAPPING FUNCTIONS ---\n",
    "\n",
    "\n",
    "\n",
    "def coarse_by_center_tensor(\n",
    "    input_map_tensors: dict, \n",
    "    target_grid_tensor: torch.Tensor, \n",
    "    use_regular_coords: bool = True \n",
    "):\n",
    "    \"\"\"\n",
    "    Maps irregular data to target grid.\n",
    "    use_regular_coords=True : Output [RegLat, RegLon, Val, Time, IrregLat, IrregLon]\n",
    "    use_regular_coords=False: Output [IrregLat, IrregLon, Val, Time, RegLat, RegLon]\n",
    "    \"\"\"\n",
    "    coarse_map = {}\n",
    "    query_points_np = target_grid_tensor.cpu().numpy()\n",
    "    query_points_rad = np.radians(query_points_np)\n",
    "    \n",
    "    for key, val_tensor in input_map_tensors.items():\n",
    "        source_locs_np = val_tensor[:, :2].cpu().numpy()\n",
    "        source_locs_rad = np.radians(source_locs_np)\n",
    "        \n",
    "        # 1. Find Nearest Irregular Point for each Grid Point\n",
    "        tree = BallTree(source_locs_rad, metric='haversine')\n",
    "        _, ind = tree.query(query_points_rad, k=1)\n",
    "        nearest_indices = ind.flatten()\n",
    "        indices_tensor = torch.tensor(nearest_indices, device=val_tensor.device, dtype=torch.long)\n",
    "        \n",
    "        # 2. Extract Data\n",
    "        val = val_tensor[indices_tensor, 2]\n",
    "        time = val_tensor[indices_tensor, 3]\n",
    "        \n",
    "        # 3. Coordinate Swapping Logic\n",
    "        # (A) Regular Coordinates (from Target Grid)\n",
    "        reg_lat, reg_lon = target_grid_tensor[:, 0], target_grid_tensor[:, 1]\n",
    "        # (B) Irregular Coordinates (from Mapped Source)\n",
    "        irreg_lat, irreg_lon = val_tensor[indices_tensor, 0], val_tensor[indices_tensor, 1]\n",
    "        \n",
    "        if use_regular_coords:\n",
    "            # Main: Regular / Aux: Irregular\n",
    "            cols = [reg_lat, reg_lon, val, time, irreg_lat, irreg_lon]\n",
    "        else:\n",
    "            # Main: Irregular / Aux: Regular\n",
    "            cols = [irreg_lat, irreg_lon, val, time, reg_lat, reg_lon]\n",
    "\n",
    "        coarse_map[key] = torch.stack(cols, dim=1)\n",
    "\n",
    "    return coarse_map\n",
    "\n",
    "# --- 4. ORDERING HELPER (Updated) ---\n",
    "\n",
    "def get_spatial_ordering(\n",
    "        input_maps: dict,\n",
    "        mm_cond_number: int = 10,\n",
    "        coord_cols: Tuple[int, int] = (0, 1) # [NEW] Column indices to use for ordering\n",
    "    ) -> Tuple[np.ndarray, list]:\n",
    "        \n",
    "        key_list = list(input_maps.keys())\n",
    "        data_for_coord = input_maps[key_list[0]]\n",
    "        \n",
    "        if isinstance(data_for_coord, torch.Tensor):\n",
    "            data_for_coord = data_for_coord.cpu().numpy()\n",
    "\n",
    "        # [NEW] Use specified columns (Regular Grid) for robust ordering\n",
    "        x1 = data_for_coord[:, coord_cols[0]]\n",
    "        y1 = data_for_coord[:, coord_cols[1]]\n",
    "        coords1 = np.stack((x1, y1), axis=-1)\n",
    "\n",
    "        # 1. MaxMin Ordering\n",
    "        ord_mm = _orderings.maxmin_cpp(coords1)\n",
    "        \n",
    "        # 2. NN Search (based on reordered regular coords)\n",
    "        coords1_reordered = coords1[ord_mm]\n",
    "        nns_map_dict = _orderings.find_nns_l2(locs=coords1_reordered, max_nn=mm_cond_number)\n",
    "        nns_map_list = [nns_map_dict[i] for i in range(len(nns_map_dict))]\n",
    "        \n",
    "        return ord_mm, nns_map_list\n",
    "\n",
    "\n",
    "def make_target_grid(lat_start, lat_end, lat_step, lon_start, lon_end, lon_step, device, dtype):\n",
    "    \"\"\"\n",
    "    표준 분석용 레귤러 그리드 생성. \n",
    "    자동 방향 제어(아래 버전 장점) + 소수점 반올림 오차 방지(위 버전 장점) 통합.\n",
    "    \"\"\"\n",
    "    # 1. 방향 자동 교정 (Step 방향이 시작/끝점과 맞지 않을 경우)\n",
    "    if lat_start > lat_end and lat_step > 0: \n",
    "        lat_step = -lat_step\n",
    "    if lon_start > lon_end and lon_step > 0:\n",
    "        lon_step = -lon_step\n",
    "\n",
    "    # 2. Arange 생성\n",
    "    lats = torch.arange(lat_start, lat_end - 0.0001, lat_step, device=device, dtype=dtype)\n",
    "    lons = torch.arange(lon_start, lon_end + 0.0001, lon_step, device=device, dtype=dtype)\n",
    "\n",
    "    # 3. 수치적 안정성을 위한 Rounding (소수점 4자리)\n",
    "    # 이 과정이 없으면 나중에 모델 피팅 시 좌표 미세 오차로 인해 결과가 달라질 수 있습니다.\n",
    "    lats = torch.round(lats * 10000) / 10000\n",
    "    lons = torch.round(lons * 10000) / 10000\n",
    "\n",
    "    # 4. Meshgrid 생성\n",
    "    grid_lat, grid_lon = torch.meshgrid(lats, lons, indexing='ij')\n",
    "    center_points = torch.stack([grid_lat.flatten(), grid_lon.flatten()], dim=1)\n",
    "    \n",
    "    return center_points, len(lats), len(lons)\n",
    "\n",
    "\n",
    "def get_model_covariance_on_grid(lags_x, lags_y, lags_t, params):\n",
    "    phi1, phi2, phi3, phi4 = torch.exp(params[0]), torch.exp(params[1]), torch.exp(params[2]), torch.exp(params[3])\n",
    "    advec_lat, advec_lon = params[4], params[5]\n",
    "    sigmasq = phi1 / phi2\n",
    "\n",
    "    u_lat_eff = lags_x - advec_lat * lags_t\n",
    "    u_lon_eff = lags_y - advec_lon * lags_t\n",
    "    dist_sq = (u_lat_eff.pow(2) * phi3) + (u_lon_eff.pow(2)) + (lags_t.pow(2) * phi4)\n",
    "    return sigmasq * torch.exp(-torch.sqrt(dist_sq + 1e-8) * phi2)\n",
    "\n",
    "\n",
    "\n",
    "def make_target_grid(lat_start, lat_end, lat_step, lon_start, lon_end, lon_step, device, dtype):\n",
    "    \"\"\"\n",
    "    표준 분석용 레귤러 그리드 생성. \n",
    "    자동 방향 제어(아래 버전 장점) + 소수점 반올림 오차 방지(위 버전 장점) 통합.\n",
    "    \"\"\"\n",
    "    # 1. 방향 자동 교정 (Step 방향이 시작/끝점과 맞지 않을 경우)\n",
    "    if lat_start > lat_end and lat_step > 0: \n",
    "        lat_step = -lat_step\n",
    "    if lon_start > lon_end and lon_step > 0:\n",
    "        lon_step = -lon_step\n",
    "\n",
    "    # 2. Arange 생성\n",
    "    lats = torch.arange(lat_start, lat_end - 0.0001, lat_step, device=device, dtype=dtype)\n",
    "    lons = torch.arange(lon_start, lon_end + 0.0001, lon_step, device=device, dtype=dtype)\n",
    "\n",
    "    # 3. 수치적 안정성을 위한 Rounding (소수점 4자리)\n",
    "    # 이 과정이 없으면 나중에 모델 피팅 시 좌표 미세 오차로 인해 결과가 달라질 수 있습니다.\n",
    "    lats = torch.round(lats * 10000) / 10000\n",
    "    lons = torch.round(lons * 10000) / 10000\n",
    "\n",
    "    # 4. Meshgrid 생성\n",
    "    grid_lat, grid_lon = torch.meshgrid(lats, lons, indexing='ij')\n",
    "    center_points = torch.stack([grid_lat.flatten(), grid_lon.flatten()], dim=1)\n",
    "    \n",
    "    return center_points, len(lats), len(lons)\n",
    "\n",
    "# --- 2. High-Res Field Generator (Lat x160 / Lon x4) ---\n",
    "def generate_high_res_field(target_lat_range, target_lon_range, t_steps, params, device, dtype):\n",
    "    \"\"\"\n",
    "    초고해상도(160배/4배) 필드를 생성하고, 분산을 참값(Target)에 맞게 보정(Calibration)합니다.\n",
    "    \"\"\"\n",
    "    # [설정] 해상도 확대 배율\n",
    "    lat_res_factor = 160.0  # 위도 160배\n",
    "    lon_res_factor = 4.0    # 경도 4배\n",
    "    \n",
    "    # 기본 해상도(0.044, 0.063)를 배율로 나눔\n",
    "    lat_res_high = 0.044 / lat_res_factor\n",
    "    lon_res_high = 0.063 / lon_res_factor\n",
    "    \n",
    "    t_lat_max = max(target_lat_range)\n",
    "    t_lat_min = min(target_lat_range)\n",
    "    \n",
    "    # Padding: 경계면 왜곡 방지를 위해 약간 넓게 생성\n",
    "    lats_high = torch.arange(t_lat_max + 0.1, t_lat_min - 0.1, -lat_res_high, device=device, dtype=dtype)\n",
    "    lons_high = torch.arange(target_lon_range[0] - 0.1, target_lon_range[1] + 0.1, lon_res_high, device=device, dtype=dtype)\n",
    "    \n",
    "    Nx, Ny, Nt = len(lats_high), len(lons_high), t_steps\n",
    "    dlat, dlon, dt = lat_res_high, lon_res_high, 1.0\n",
    "    \n",
    "    # FFT Grid Setup (Circulant Embedding)\n",
    "    Px, Py, Pt = 2*Nx, 2*Ny, 2*Nt\n",
    "    lags_x = torch.arange(Px, device=device, dtype=dtype) * dlat; lags_x[Px//2:] -= (Px * dlat)\n",
    "    lags_y = torch.arange(Py, device=device, dtype=dtype) * dlon; lags_y[Py//2:] -= (Py * dlon)\n",
    "    lags_t = torch.arange(Pt, device=device, dtype=dtype) * dt;   lags_t[Pt//2:] -= (Pt * dt)\n",
    "\n",
    "    L_x, L_y, L_t = torch.meshgrid(lags_x, lags_y, lags_t, indexing='ij')\n",
    "    \n",
    "    # 파라미터 해석\n",
    "    phi1, phi2 = torch.exp(params[0]), torch.exp(params[1])\n",
    "    phi3, phi4 = torch.exp(params[2]), torch.exp(params[3])\n",
    "    adv_lat, adv_lon = params[4], params[5]\n",
    "    \n",
    "    sigma_sq = phi1 / phi2  # 참값 분산 (13.059)\n",
    "    \n",
    "    u_x = L_x - adv_lat * L_t\n",
    "    u_y = L_y - adv_lon * L_t\n",
    "    dist_sq = (u_x * torch.sqrt(phi3) * phi2)**2 + (u_y * phi2)**2 + (L_t * torch.sqrt(phi4) * phi2)**2\n",
    "    C_vals = sigma_sq * torch.exp(-torch.sqrt(dist_sq + 1e-12))\n",
    "\n",
    "    # FFT Simulation\n",
    "    S = torch.fft.fftn(C_vals); S.real = torch.clamp(S.real, min=0)\n",
    "    random_phase = torch.fft.fftn(torch.randn(Px, Py, Pt, device=device, dtype=dtype))\n",
    "    field_sim_raw = torch.fft.ifftn(torch.sqrt(S.real) * random_phase).real\n",
    "    \n",
    "    # Crop\n",
    "    field_sim = field_sim_raw[:Nx, :Ny, :Nt]\n",
    "    \n",
    "    # [Calibration] 160배 해상도로 인해 흩어진 에너지를 모아 분산을 복원\n",
    "    current_std = field_sim.std()\n",
    "    target_std = torch.sqrt(sigma_sq)\n",
    "    \n",
    "    # (x - mean) * ratio\n",
    "    field_calibrated = (field_sim - field_sim.mean()) * (target_std / (current_std + 1e-9))\n",
    "    \n",
    "    print(f\"   [High-Res Gen] 160x/4x Field Created.\")\n",
    "    print(f\"   [Calibration] Raw Std: {current_std.item():.4f} -> Fixed to: {target_std.item():.4f}\")\n",
    "    \n",
    "    return field_calibrated, lats_high, lons_high\n",
    "\n",
    "# --- 3. Main Generator (Vecc & DW) ---\n",
    "def generate_exact_count_datasets(daily_maps_real, true_params_tensor, target_grid_info, device, dtype):\n",
    "    print(\"--- Generating Datasets: Vecc (Real Locs) & DW (Grid Locs) ---\")\n",
    "    \n",
    "    lat_s, lat_e, lat_step, lon_s, lon_e, lon_step = target_grid_info\n",
    "    \n",
    "    # 1. Target Grid 생성 (DW 매핑 목적지)\n",
    "    target_grid_coords, _, _ = make_target_grid(lat_s, lat_e, lat_step, lon_s, lon_e, lon_step, device, dtype)\n",
    "    target_locs_np = target_grid_coords.cpu().numpy()\n",
    "    target_tree = BallTree(np.radians(target_locs_np), metric='haversine') \n",
    "    \n",
    "    # 결과 저장 리스트\n",
    "    daily_aggregated_tensors_vecc_sim = []\n",
    "    daily_hourly_maps_vecc_sim = []\n",
    "    daily_aggregated_tensors_dw_sim = []\n",
    "    daily_hourly_maps_dw_sim = []\n",
    "    \n",
    "    # Nugget Noise (현실감 추가용)\n",
    "    # params[6] is log(nugget)\n",
    "    nugget_val = torch.exp(true_params_tensor[6])\n",
    "    noise_std = torch.sqrt(nugget_val)\n",
    "    \n",
    "    for day_idx, day_map_real in enumerate(daily_maps_real):\n",
    "        if day_idx >= 1: break \n",
    "        \n",
    "        # (A) High-Res Field 생성 (x160)\n",
    "        lat_range = (max(lat_s, lat_e), min(lat_s, lat_e))\n",
    "        lon_range = (lon_s, lon_e)\n",
    "        high_res_field, lats_high, lons_high = generate_high_res_field(\n",
    "            lat_range, lon_range, 8, true_params_tensor, device, dtype\n",
    "        )\n",
    "        \n",
    "        # (B) High-Res Sampling용 BallTree\n",
    "        hr_mesh_lat, hr_mesh_lon = torch.meshgrid(lats_high, lons_high, indexing='ij')\n",
    "        hr_flat_coords = torch.stack([hr_mesh_lat.flatten(), hr_mesh_lon.flatten()], dim=1).cpu().numpy()\n",
    "        hr_tree = BallTree(np.radians(hr_flat_coords), metric='haversine')\n",
    "        high_res_flat = high_res_field.reshape(-1, 8) \n",
    "\n",
    "        sorted_keys = sorted([k for k in day_map_real.keys() if 'hm' in k or 'time' in k])\n",
    "        \n",
    "        vecc_tensor_list = []\n",
    "        dw_tensor_list = []\n",
    "        current_day_vecc_map = {}\n",
    "        current_day_dw_map = {}\n",
    "        \n",
    "        for t_idx, key in enumerate(sorted_keys):\n",
    "            if t_idx >= 8: break\n",
    "            \n",
    "            real_tensor = day_map_real[key]\n",
    "            if not isinstance(real_tensor, torch.Tensor): real_tensor = torch.tensor(real_tensor, device=device, dtype=dtype)\n",
    "            else: real_tensor = real_tensor.to(device)\n",
    "            \n",
    "            # [Step 1] Unique: 중복 좌표 제거 (모델 안정성)\n",
    "            np_data = real_tensor.cpu().numpy()\n",
    "            _, unique_idx = np.unique(np_data[:, :2], axis=0, return_index=True)\n",
    "            unique_idx.sort()\n",
    "            clean_tensor = real_tensor[unique_idx]\n",
    "            \n",
    "            real_lats = clean_tensor[:, 0]\n",
    "            real_lons = clean_tensor[:, 1]\n",
    "            time_val = clean_tensor[:, 3] \n",
    "            \n",
    "            # [Step 2] Sampling: 현실 좌표 -> High-Res Field 값 추출\n",
    "            current_real_locs = torch.stack([real_lats, real_lons], dim=1).cpu().numpy()\n",
    "            _, vecc_hr_indices = hr_tree.query(np.radians(current_real_locs), k=1)\n",
    "            vecc_hr_indices_tensor = torch.tensor(vecc_hr_indices.flatten(), device=device, dtype=torch.long)\n",
    "            \n",
    "            # 값 생성: Signal + Noise + Mean(260)\n",
    "            # 160배 해상도이므로 Signal은 아주 매끄럽습니다. 여기에 Nugget을 더해 현실감을 줍니다.\n",
    "            signal = high_res_flat[vecc_hr_indices_tensor, t_idx]\n",
    "            noise = torch.randn_like(signal) * noise_std\n",
    "            sim_vals = signal + noise + 260.0\n",
    "            \n",
    "            # [Step 3] VECC SIM: 원래 위치(Original Locs) 유지\n",
    "            vecc_row = torch.stack([real_lats, real_lons, sim_vals, time_val], dim=1)\n",
    "            vecc_tensor_list.append(vecc_row.detach())\n",
    "            current_day_vecc_map[key] = vecc_row.detach()\n",
    "            \n",
    "            # [Step 4] DW SIM: 레귤러 그리드(Regular Grid)로 매핑\n",
    "            # 현실 좌표에서 가장 가까운 격자점을 찾습니다.\n",
    "            _, dw_indices = target_tree.query(np.radians(current_real_locs), k=1)\n",
    "            dw_indices_tensor = torch.tensor(dw_indices.flatten(), device=device, dtype=torch.long)\n",
    "            \n",
    "            mapped_lats = target_grid_coords[dw_indices_tensor, 0]\n",
    "            mapped_lons = target_grid_coords[dw_indices_tensor, 1]\n",
    "            \n",
    "            # 값(sim_vals)은 Vecc와 동일하게 가져갑니다.\n",
    "            dw_row = torch.stack([mapped_lats, mapped_lons, sim_vals, time_val], dim=1)\n",
    "            dw_tensor_list.append(dw_row.detach())\n",
    "            current_day_dw_map[key] = dw_row.detach()\n",
    "\n",
    "        # 결과 저장\n",
    "        daily_aggregated_tensors_vecc_sim.append(torch.cat(vecc_tensor_list, dim=0))\n",
    "        daily_hourly_maps_vecc_sim.append(current_day_vecc_map)\n",
    "        \n",
    "        daily_aggregated_tensors_dw_sim.append(torch.cat(dw_tensor_list, dim=0))\n",
    "        daily_hourly_maps_dw_sim.append(current_day_dw_map)\n",
    "        \n",
    "        print(f\"  Processed Day {day_idx} (Points: {len(vecc_tensor_list[0])})\")\n",
    "            \n",
    "    # 4개 모두 반환\n",
    "    return (daily_aggregated_tensors_dw_sim, daily_hourly_maps_dw_sim, \n",
    "            daily_aggregated_tensors_vecc_sim, daily_hourly_maps_vecc_sim)\n",
    "\n",
    "# --- 4. EXECUTION ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.float64\n",
    "\n",
    "target_grid_info = (-1.0, -3.0, 0.044, 121.0, 125.0, 0.063)\n",
    "\n",
    "# 1. 참값 정의\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lon = 0.195 \n",
    "init_range_lat = 0.154 \n",
    "init_range_time = 1.0\n",
    "init_nugget    = 0.247\n",
    "\n",
    "# 2. 파라미터 변환 (정답 로직)\n",
    "true_phi2 = 1.0 / init_range_lon              \n",
    "true_phi1 = init_sigmasq * true_phi2          \n",
    "true_phi3 = (init_range_lon / init_range_lat)**2\n",
    "true_phi4 = (init_range_lon / init_range_time)**2\n",
    "\n",
    "true_params_tensor = [\n",
    "    torch.tensor([np.log(true_phi1)], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([np.log(true_phi2)], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([np.log(true_phi3)], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([np.log(true_phi4)], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([0.0418], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([-0.1689], device=DEVICE, dtype=DTYPE),\n",
    "    torch.tensor([np.log(init_nugget)], device=DEVICE, dtype=DTYPE)\n",
    "]\n",
    "\n",
    "print(f\"Checking Inputs:\")\n",
    "print(f\" -> Target SigmaSq: {init_sigmasq}\")\n",
    "\n",
    "if 'daily_hourly_maps_irr_vecc_sim' in locals() and len(daily_hourly_maps_irr_vecc_sim) > 0:\n",
    "    (daily_aggregated_reg_vecc_sim, daily_hourly_maps_reg_vecc_sim,\n",
    "     daily_aggregated_irr_vecc_sim, daily_hourly_maps_irr_vecc_sim) = generate_exact_count_datasets(\n",
    "        daily_hourly_maps_irr_vecc_sim, true_params_tensor, target_grid_info, DEVICE, DTYPE\n",
    "    )\n",
    "    \n",
    "    # 검증: Vecc vs DW\n",
    "    v_agg = daily_aggregated_irr_vecc_sim[0]\n",
    "    dw_agg = daily_aggregated_reg_vecc_sim[0]\n",
    "    \n",
    "    print(f\"\\n[Final Validation]\")\n",
    "    print(f\"Vecc Shape (Irregular): {v_agg.shape}\")\n",
    "    print(f\"DW Shape   (Regular):   {dw_agg.shape}\")\n",
    "    print(f\"Variance Check: {v_agg[:, 2].var().item():.4f} (Expected ~13.0 + Nugget)\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: Load data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87cb81b",
   "metadata": {},
   "source": [
    "dummy time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e15c017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Datasets (12 Cols: Lat, Lon, Val, Time, H0..H7) ---\n",
      "  Processed Day 5\n",
      "  Processed Day 10\n",
      "  Processed Day 15\n",
      "  Processed Day 20\n",
      "  Processed Day 25\n",
      "  Processed Day 30\n",
      "\n",
      "[Verification]\n",
      "Dataset Shape: torch.Size([22680, 12])\n",
      "✅ Success: Output has 12 columns (Lat, Lon, Val, Time, + 8 Dummies)\n",
      "Sample Row: \n",
      "tensor([ -1.9987, 122.9810, 258.8952,  21.0000,   1.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "import time\n",
    "import sys\n",
    "import typer\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def make_target_grid(lat_start, lat_end, lat_step, lon_start, lon_end, lon_step, device, dtype):\n",
    "    \"\"\"Target Regular Grid 생성\"\"\"\n",
    "    if lat_start > lat_end and lat_step > 0:\n",
    "        lat_step = -lat_step\n",
    "        \n",
    "    lats = torch.arange(lat_start, lat_end - 0.0001, lat_step, device=device, dtype=dtype)\n",
    "    lons = torch.arange(lon_start, lon_end - 0.0001, lon_step, device=device, dtype=dtype)\n",
    "    \n",
    "    grid_lat, grid_lon = torch.meshgrid(lats, lons, indexing='ij')\n",
    "    center_points = torch.stack([grid_lat.flatten(), grid_lon.flatten()], dim=1)\n",
    "    \n",
    "    return center_points, len(lats), len(lons)\n",
    "\n",
    "def generate_high_res_field(target_lat_range, target_lon_range, t_steps, params, device, dtype):\n",
    "    \"\"\"3배 고해상도 True Field 생성\"\"\"\n",
    "    res_factor = 3.0\n",
    "    lat_res_high = 0.044 / res_factor\n",
    "    lon_res_high = 0.063 / res_factor\n",
    "    \n",
    "    pad = 0.2\n",
    "    lats_high = torch.arange(target_lat_range[0] + pad, target_lat_range[1] - pad, -lat_res_high, device=device, dtype=dtype)\n",
    "    lons_high = torch.arange(target_lon_range[0] - pad, target_lon_range[1] + pad, lon_res_high, device=device, dtype=dtype)\n",
    "    \n",
    "    Nx, Ny, Nt = len(lats_high), len(lons_high), t_steps\n",
    "    dlat, dlon, dt = lat_res_high, lon_res_high, 1.0\n",
    "    \n",
    "    Px, Py, Pt = 2*Nx, 2*Ny, 2*Nt\n",
    "    lags_x = torch.arange(Px, device=device, dtype=dtype) * dlat\n",
    "    lags_x[Px//2:] -= (Px * dlat)\n",
    "    lags_y = torch.arange(Py, device=device, dtype=dtype) * dlon\n",
    "    lags_y[Py//2:] -= (Py * dlon)\n",
    "    lags_t = torch.arange(Pt, device=device, dtype=dtype) * dt\n",
    "    lags_t[Pt//2:] -= (Pt * dt)\n",
    "\n",
    "    L_x, L_y, L_t = torch.meshgrid(lags_x, lags_y, lags_t, indexing='ij')\n",
    "\n",
    "    phi1, phi2 = torch.exp(params[0]), torch.exp(params[1])\n",
    "    phi3, phi4 = torch.exp(params[2]), torch.exp(params[3])\n",
    "    adv_lat, adv_lon = params[4], params[5]\n",
    "    sigmasq = phi1 / phi2\n",
    "    \n",
    "    u_x = L_x - adv_lat * L_t\n",
    "    u_y = L_y - adv_lon * L_t\n",
    "    dist_sq = (u_x * torch.sqrt(phi3) * phi2)**2 + (u_y * phi2)**2 + (L_t * torch.sqrt(phi4) * phi2)**2\n",
    "    C_vals = sigmasq * torch.exp(-torch.sqrt(dist_sq + 1e-12))\n",
    "\n",
    "    S = torch.fft.fftn(C_vals)\n",
    "    S.real = torch.clamp(S.real, min=0)\n",
    "    random_phase = torch.fft.fftn(torch.randn(Px, Py, Pt, device=device, dtype=dtype))\n",
    "    field_sim = torch.fft.ifftn(torch.sqrt(S.real) * random_phase).real\n",
    "    \n",
    "    return field_sim[:Nx, :Ny, :Nt], lats_high, lons_high\n",
    "\n",
    "# --- Core Generator (12 Columns Logic) ---\n",
    "\n",
    "def generate_exact_count_datasets(\n",
    "    daily_maps_real,    \n",
    "    true_params_tensor, \n",
    "    target_grid_info,   \n",
    "    device, dtype\n",
    "):\n",
    "    print(\"--- Generating Datasets (12 Cols: Lat, Lon, Val, Time, H0..H7) ---\")\n",
    "    \n",
    "    lat_s, lat_e, lat_step, lon_s, lon_e, lon_step = target_grid_info\n",
    "    \n",
    "    # 1. Target Grid Coords\n",
    "    target_grid_coords, _, _ = make_target_grid(\n",
    "        lat_s, lat_e, lat_step, lon_s, lon_e, lon_step, device, dtype\n",
    "    )\n",
    "    \n",
    "    target_locs_np = target_grid_coords.cpu().numpy()\n",
    "    target_locs_rad = np.radians(target_locs_np)\n",
    "    \n",
    "    def normalize(val, min_v, max_v):\n",
    "        return 2 * (val - min_v) / (max_v - min_v) - 1\n",
    "\n",
    "    daily_aggregated_tensors_vecc_sim = []\n",
    "    daily_hourly_maps_vecc_sim = []\n",
    "    daily_aggregated_tensors_dw_sim = []\n",
    "    daily_hourly_maps_dw_sim = []\n",
    "    \n",
    "    # --- Day Loop ---\n",
    "    for day_idx, day_map_real in enumerate(daily_maps_real):\n",
    "        \n",
    "        # (A) High-Res Truth\n",
    "        high_res_field, lats_high, lons_high = generate_high_res_field(\n",
    "            (lat_s, lat_e), (lon_s, lon_e), 8, true_params_tensor, device, dtype\n",
    "        )\n",
    "        \n",
    "        lat_max_b, lat_min_b = lats_high[0], lats_high[-1]\n",
    "        lon_min_b, lon_max_b = lons_high[0], lons_high[-1]\n",
    "        \n",
    "        # (B) Hour Loop\n",
    "        sorted_keys = sorted([k for k in day_map_real.keys() if 'hm' in k or 'time' in k])\n",
    "        \n",
    "        current_day_vecc_map = {}\n",
    "        current_day_dw_map = {}\n",
    "        \n",
    "        vecc_tensor_list = []\n",
    "        dw_tensor_list = []\n",
    "        \n",
    "        for t_idx, key in enumerate(sorted_keys):\n",
    "            if t_idx >= 8: break\n",
    "            \n",
    "            # 1. Extract Real Info\n",
    "            real_tensor = day_map_real[key]\n",
    "            if not isinstance(real_tensor, torch.Tensor):\n",
    "                real_tensor = torch.tensor(real_tensor, device=device, dtype=dtype)\n",
    "            else:\n",
    "                real_tensor = real_tensor.to(device)\n",
    "            \n",
    "            real_lats = real_tensor[:, 0]\n",
    "            real_lons = real_tensor[:, 1]\n",
    "            time_val_continuous = real_tensor[:, 3] # [N] (e.g., 21.0, 22.0)\n",
    "            \n",
    "            N_points = real_lats.shape[0]\n",
    "            \n",
    "            # [NEW] Create 8 Dummy Variables\n",
    "            dummy_time = torch.zeros((N_points, 8), device=device, dtype=dtype)\n",
    "            dummy_time[:, t_idx] = 1.0 \n",
    "            \n",
    "            # 2. Sample Values\n",
    "            norm_lons = normalize(real_lons, lon_min_b, lon_max_b)\n",
    "            norm_lats = normalize(real_lats, lat_min_b, lat_max_b)\n",
    "            \n",
    "            grid_sample_input = torch.stack([norm_lons, norm_lats], dim=-1).unsqueeze(0).unsqueeze(0)\n",
    "            field_slice = high_res_field[:, :, t_idx].unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            sim_vals = F.grid_sample(field_slice, grid_sample_input, mode='bilinear', align_corners=True).squeeze()\n",
    "            sim_vals = sim_vals + 260.0 \n",
    "            \n",
    "            # 3. [Set 1: VECC SIM] - Irregular\n",
    "            # Col 0: Lat\n",
    "            # Col 1: Lon\n",
    "            # Col 2: Val\n",
    "            # Col 3: Time (Continuous) -> For Kernel\n",
    "            # Col 4-11: H0~H7 (Dummy) -> For Mean\n",
    "            vecc_row = torch.cat([\n",
    "                real_lats.unsqueeze(1), \n",
    "                real_lons.unsqueeze(1), \n",
    "                sim_vals.unsqueeze(1), \n",
    "                time_val_continuous.unsqueeze(1), \n",
    "                dummy_time\n",
    "            ], dim=1)\n",
    "            \n",
    "            current_day_vecc_map[key] = vecc_row.detach()\n",
    "            vecc_tensor_list.append(vecc_row.detach())\n",
    "            \n",
    "            # 4. [Set 2: DW SIM] - Regular Mapping\n",
    "            real_locs_np = torch.stack([real_lats, real_lons], dim=1).cpu().numpy()\n",
    "            \n",
    "            tree = BallTree(target_locs_rad, metric='haversine') \n",
    "            _, indices = tree.query(np.radians(real_locs_np), k=1)\n",
    "            indices = torch.tensor(indices.flatten(), device=device, dtype=torch.long)\n",
    "            \n",
    "            mapped_lats = target_grid_coords[indices, 0]\n",
    "            mapped_lons = target_grid_coords[indices, 1]\n",
    "            \n",
    "            # Time & Dummies 복사 (Mapped된 위치에도 동일하게 적용)\n",
    "            mapped_time_val = time_val_continuous # 시간은 모두 같음\n",
    "            mapped_dummies = dummy_time # 더미도 모두 같음\n",
    "            \n",
    "            dw_row = torch.cat([\n",
    "                mapped_lats.unsqueeze(1), \n",
    "                mapped_lons.unsqueeze(1), \n",
    "                sim_vals.unsqueeze(1), \n",
    "                mapped_time_val.unsqueeze(1),\n",
    "                mapped_dummies\n",
    "            ], dim=1)\n",
    "            \n",
    "            current_day_dw_map[key] = dw_row.detach()\n",
    "            dw_tensor_list.append(dw_row.detach())\n",
    "\n",
    "        # (C) Aggregate\n",
    "        daily_hourly_maps_vecc_sim.append(current_day_vecc_map)\n",
    "        daily_aggregated_tensors_vecc_sim.append(torch.cat(vecc_tensor_list, dim=0))\n",
    "        \n",
    "        daily_hourly_maps_dw_sim.append(current_day_dw_map)\n",
    "        daily_aggregated_tensors_dw_sim.append(torch.cat(dw_tensor_list, dim=0))\n",
    "        \n",
    "        if (day_idx + 1) % 5 == 0:\n",
    "            print(f\"  Processed Day {day_idx+1}\")\n",
    "            \n",
    "    return (daily_aggregated_tensors_dw_sim, daily_hourly_maps_dw_sim,\n",
    "            daily_aggregated_tensors_vecc_sim, daily_hourly_maps_vecc_sim)\n",
    "\n",
    "# --- Execution ---\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.float64\n",
    "\n",
    "target_grid_info = (2.0, -3.0, 0.044, 121.0, 131.0, 0.063)\n",
    "true_params_tensor = [torch.tensor([np.log(10.0)], device=DEVICE, dtype=DTYPE) for _ in range(7)]\n",
    "\n",
    "if 'daily_hourly_maps_vecc' in locals() and len(daily_hourly_maps_vecc) > 0:\n",
    "    \n",
    "    (daily_aggregated_tensors_dw_sim, \n",
    "     daily_hourly_maps_dw_sim,\n",
    "     daily_aggregated_tensors_vecc_sim, \n",
    "     daily_hourly_maps_vecc_sim) = generate_exact_count_datasets(\n",
    "        daily_hourly_maps_vecc,\n",
    "        true_params_tensor,\n",
    "        target_grid_info,\n",
    "        DEVICE, DTYPE\n",
    "    )\n",
    "    \n",
    "    # [Final Verification]\n",
    "    v_tensor = daily_aggregated_tensors_vecc_sim[0]\n",
    "    print(\"\\n[Verification]\")\n",
    "    print(f\"Dataset Shape: {v_tensor.shape}\")\n",
    "    \n",
    "    if v_tensor.shape[1] == 12:\n",
    "        print(\"✅ Success: Output has 12 columns (Lat, Lon, Val, Time, + 8 Dummies)\")\n",
    "        # 예시: 3번째 컬럼은 Continuous Time, 4번째부터는 Dummy\n",
    "        print(f\"Sample Row: \\n{v_tensor[0]}\")\n",
    "    else:\n",
    "        print(f\"❌ Error: Expected 12 columns, got {v_tensor.shape[1]}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Error: 'daily_hourly_maps_vecc' is not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cacde8",
   "metadata": {},
   "source": [
    "set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51634ad",
   "metadata": {},
   "source": [
    "Likelihood vs. Truth: A model with the wrong parameters (short range) might produce a higher Vecchia likelihood because it fits the high-frequency noise better, but it will be terrible at prediction (Kriging) away from data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7191bead",
   "metadata": {},
   "source": [
    "# Fit vecchia max min time 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44251197",
   "metadata": {},
   "source": [
    "vecc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b800f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "========================================\n",
      "--- Initializing VecchiaBatched Model ---\n",
      "========================================\n",
      "\n",
      "========================================\n",
      "--- Running L-BFGS Optimization ---\n",
      "========================================\n",
      "🚀 Pre-computing (Corrected Vectorization)... ✅ Done in 0.1728s. (Heads: 2400, Tails: 18323)\n",
      "--- Starting Batched L-BFGS Optimization (GPU) ---\n",
      "--- Step 1/3 / Loss: 1.531434 ---\n",
      "  Param 0: Value=4.0697, Grad=-1.7000002959879391e-07\n",
      "  Param 1: Value=1.6210, Grad=2.6377622508266894e-06\n",
      "  Param 2: Value=0.4661, Grad=-2.71106819701403e-06\n",
      "  Param 3: Value=-3.1744, Grad=1.0194285628546901e-06\n",
      "  Param 4: Value=-0.0437, Grad=-5.556265572511342e-06\n",
      "  Param 5: Value=-0.1745, Grad=-6.1534333991614554e-06\n",
      "  Param 6: Value=0.0276, Grad=-1.6677198637020775e-06\n",
      "  Max Abs Grad: 6.153433e-06\n",
      "------------------------------\n",
      "--- Step 2/3 / Loss: 1.512535 ---\n",
      "  Param 0: Value=4.0697, Grad=-1.7000002959879391e-07\n",
      "  Param 1: Value=1.6210, Grad=2.6377622508266894e-06\n",
      "  Param 2: Value=0.4661, Grad=-2.71106819701403e-06\n",
      "  Param 3: Value=-3.1744, Grad=1.0194285628546901e-06\n",
      "  Param 4: Value=-0.0437, Grad=-5.556265572511342e-06\n",
      "  Param 5: Value=-0.1745, Grad=-6.1534333991614554e-06\n",
      "  Param 6: Value=0.0276, Grad=-1.6677198637020775e-06\n",
      "  Max Abs Grad: 6.153433e-06\n",
      "------------------------------\n",
      "--- Step 3/3 / Loss: 1.512535 ---\n",
      "  Param 0: Value=4.0697, Grad=-1.7000002959879391e-07\n",
      "  Param 1: Value=1.6210, Grad=2.6377622508266894e-06\n",
      "  Param 2: Value=0.4661, Grad=-2.71106819701403e-06\n",
      "  Param 3: Value=-3.1744, Grad=1.0194285628546901e-06\n",
      "  Param 4: Value=-0.0437, Grad=-5.556265572511342e-06\n",
      "  Param 5: Value=-0.1745, Grad=-6.1534333991614554e-06\n",
      "  Param 6: Value=0.0276, Grad=-1.6677198637020775e-06\n",
      "  Max Abs Grad: 6.153433e-06\n",
      "------------------------------\n",
      "Final Interpretable Params: {'sigma_sq': 11.572638362471023, 'range_lon': 0.19769379793386327, 'range_lat': 0.15659784664041235, 'range_time': 0.9667142950234774, 'advec_lat': -0.0436988060271742, 'advec_lon': -0.1744892602049481, 'nugget': 1.0279906907148557}\n",
      "\n",
      "Optimization finished in 10.58s.\n",
      "Results after 2 steps: [4.06967947039131, 1.6210359203959632, 0.4660766516476125, -3.1743672764409245, -0.0436988060271742, -0.1744892602049481, 0.027606111267121487, 1.512535081910656]\n",
      "Final Params: [ 4.06967947  1.62103592  0.46607665 -3.17436728 -0.04369881 -0.17448926\n",
      "  0.02760611]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "v = 0.5              # Smoothness\n",
    "mm_cond_number = 8   # Neighbors\n",
    "#mm_cond_number = 16   # Neighbors\n",
    "nheads = 300           # 0 = Pure Vecchia\n",
    "lr = 1.0             # LBFGS learning rate\n",
    "LBFGS_MAX_STEPS = 3\n",
    "LBFGS_HISTORY_SIZE = 100 # 100\n",
    "LBFGS_LR = 1.0\n",
    "LBFGS_MAX_EVAL = 30    \n",
    "\n",
    "#DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- 1. SETUP PARAMETERS (List of Scalars) ---\n",
    "# Truth: [4.18, 1.94, 0.24, -3.97, 0.014, -0.20, -0.85]\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lat = 0.154 \n",
    "init_range_lon = 0.195\n",
    "init_advec_lat = 0.0218\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "# Map model parameters to the 'phi' reparameterization\n",
    "init_phi2 = 1.0 / init_range_lon                # 1/range_lon\n",
    "init_phi1 = init_sigmasq * init_phi2            # sigmasq / range_lon\n",
    "init_phi3 = (init_range_lon / init_range_lat)**2  # (range_lon / range_lat)^2\n",
    "init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "# Create Initial Parameters (Float64, Requires Grad)\n",
    "initial_vals = [np.log(init_phi1), np.log(init_phi2), np.log(init_phi3), \n",
    "                np.log(init_phi4), init_advec_lat, init_advec_lon, np.log(init_nugget)]\n",
    "\n",
    "# [4.2042, 1.6348, 0.4721, -3.2695, 0.0218, -0.1689, -1.3984]\n",
    "params_list = [\n",
    "    torch.tensor([val], requires_grad=True, dtype=torch.float64, device=DEVICE)\n",
    "    for val in initial_vals\n",
    "]\n",
    "\n",
    "# --- 2. INSTANTIATE MODEL ---\n",
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'--- Initializing VecchiaBatched Model ---')\n",
    "print(f'{\"=\"*40}')\n",
    "\n",
    "if isinstance(daily_aggregated_irr_vecc_sim, torch.Tensor):\n",
    "    daily_aggregated_irr_vecc_sim = daily_aggregated_irr_vecc_sim.to(DEVICE)\n",
    "\n",
    "# Instantiate\n",
    "model_instance = kernels_reparam_space_time_gpu_copy.fit_vecchia_lbfgs(\n",
    "    smooth=v,\n",
    "    #input_map=daily_hourly_maps_vecc_sim[0],\n",
    "    #aggregated_data= daily_aggregated_tensors_vecc_sim[0],\n",
    "\n",
    "    input_map=daily_hourly_maps_reg_vecc_sim[0],\n",
    "    aggregated_data= daily_aggregated_reg_vecc_sim[0],\n",
    "    nns_map=nns_map,\n",
    "    mm_cond_number=mm_cond_number,\n",
    "    nheads=nheads\n",
    ")\n",
    "\n",
    "'''\n",
    "model_instance = kernels_reparam_space_time_gpu_col.fit_vecchia_lbfgs(\n",
    "    smooth=v,\n",
    "    #input_map=daily_hourly_maps_vecc_sim[0],\n",
    "    #aggregated_data= daily_aggregated_tensors_vecc_sim[0],\n",
    "\n",
    "    input_map=daily_hourly_maps_irr_vecc_sim[0],\n",
    "    aggregated_data= daily_aggregated_irr_vecc_sim[0],\n",
    "\n",
    "    nns_map=None,\n",
    "    mm_cond_number=mm_cond_number\n",
    ")\n",
    "''' \n",
    "\n",
    "# --- 3. OPTIMIZATION LOOP ---\n",
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'--- Running L-BFGS Optimization ---')\n",
    "print(f'{\"=\"*40}')\n",
    "\n",
    "# Optimizer takes the LIST of scalars\n",
    "optimizer_vecc = model_instance.set_optimizer(\n",
    "            params_list,     \n",
    "            lr=LBFGS_LR,            \n",
    "            max_iter=LBFGS_MAX_EVAL,        \n",
    "            history_size=LBFGS_HISTORY_SIZE \n",
    "        )\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "out, steps_ran = model_instance.fit_vecc_lbfgs(\n",
    "        params_list,\n",
    "        optimizer_vecc,\n",
    "        # covariance_function argument is GONE\n",
    "        max_steps=LBFGS_MAX_STEPS, \n",
    "        grad_tol=1e-7\n",
    "    )\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "epoch_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nOptimization finished in {epoch_time:.2f}s.\")\n",
    "print(f\"Results after {steps_ran} steps: {out}\")\n",
    "print(f\"Final Params: {torch.cat(params_list).detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b08c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final Interpretable Params: {'sigma_sq': 13.048806443143716, 'range_lon': 0.2759775584138689, 'range_lat': 0.26948439045514627, 'range_time': 1.2123116662337672, 'advec_lat': 0.003713422682726235, 'advec_lon': -0.18035976962575426, 'nugget': 4.6861836446362183e-08}\n",
    "Final Interpretable Params: {'sigma_sq': 13.719100200518179, 'range_lon': 0.32214277436468963, 'range_lat': 0.32239904147765264, 'range_time': 1.4623993761954481, 'advec_lat': -0.03314564565869727, 'advec_lon': -0.25639209965916504, 'nugget': 0.38030028721574305}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce4fe1b",
   "metadata": {},
   "source": [
    "기존 (Naive)\t점 하나하나마다 매번 행렬을 만들고 분해함\t20,000번\t🐢 매우 느림\n",
    "현재 (Grouped)\t같은 모양을 가진 점끼리 모아서, 대표로 1번만 분해함\t50번\t🚀 400배"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63028aaf",
   "metadata": {},
   "source": [
    "dw\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 13.048806443143716, 'range_lon': 0.2759775584138689, 'range_lat': 0.26948439045514627, 'range_time': 1.2123116662337672, 'advec_lat': 0.003713422682726235, 'advec_lon': -0.18035976962575426, 'nugget': 4.6861836446362183e-08}\n",
    "\n",
    "\n",
    "dw_sim\n",
    "Final Interpretable Params: {'sigma_sq': 11.817838113249758, 'range_lon': 0.22621206038334832, 'range_lat': 0.17422723320268027, 'range_time': 0.997943651842861, 'advec_lat': -0.050699303814975866, 'advec_lon': -0.18116598528943043, 'nugget': 1.1984570483507146}\n",
    "\n",
    "\n",
    "vecc\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 13.719100200518179, 'range_lon': 0.32214277436468963, 'range_lat': 0.32239904147765264, 'range_time': 1.4623993761954481, 'advec_lat': -0.03314564565869727, 'advec_lon': -0.25639209965916504, 'nugget': 0.38030028721574305}\n",
    "\n",
    "\n",
    "vecc_sim\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 12.967334679138654, 'range_lon': 0.1829644675670931, 'range_lat': 0.14094812906918186, 'range_time': 0.9219567362979819, 'advec_lat': -0.04561716644679645, 'advec_lon': -0.17524213405286115, 'nugget': 0.1315329566135699}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c765af78",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "Final Interpretable Params: {\n",
    "   'sigma_sq': 12.982146243206834, '\n",
    "   range_lon': 0.1774660571793038, \n",
    "   'range_lat': 0.1377164149996857, \n",
    "   'range_time': 0.8960320754550594, \n",
    "   'advec_lat': 0.03668090767382201, \n",
    "   'advec_lon': -0.16896621297828324, \n",
    "   'nugget': 3.0318146866074696e-14}\n",
    "\n",
    "\n",
    "Final Interpretable Params: {\n",
    "   'sigma_sq': 12.652739121404919, \n",
    "   'range_lon': 0.17664470485305778, \n",
    "   'range_lat': 0.14565426376210802, \n",
    "   'range_time': 0.8720031476394872, \n",
    "   'advec_lat': 0.04466510371597738, \n",
    "   'advec_lon': -0.16447983825003892, 'nugget': 0.31139273714919946}\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "Final Interpretable Params: {\n",
    "   'sigma_sq': 12.600402314947898, \n",
    "   'range_lon': 0.15632064225879247, \n",
    "   'range_lat': 0.12850412363793687, \n",
    "   'range_time': 0.420237386856305, \n",
    "   'advec_lat': 3.646629197697307, \n",
    "   'advec_lon': -1.82731716498955, \n",
    "   'nugget': 3.3125395377840276e-12}\n",
    "\n",
    "\n",
    "   Final Interpretable Params: {\n",
    "      'sigma_sq': 12.465255157603531,\n",
    "       'range_lon': 0.16088131264500707, \n",
    "       'range_lat': 0.13795064901185095, \n",
    "       'range_time': 0.25642913066711404, \n",
    "       'advec_lat': 0.34121830847670426, \n",
    "       'advec_lon': -0.9086603984853497, 'nugget': 0.3413704158367853}\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "Final Interpretable Params: {\n",
    "   'sigma_sq': 12.84409971671357, \n",
    "   'range_lon': 0.17337138953730877, \n",
    "   'range_lat': 0.13617925813922688, \n",
    "   'range_time': 0.08597495864893002,\n",
    "    'advec_lat': -2.7365555385103155, \n",
    "    'advec_lon': -1.3125995387207232, \n",
    "    'nugget': 1.4388537434800932e-15}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a644f0",
   "metadata": {},
   "source": [
    "# TRUE PARAMETERS\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lon = 0.195 \n",
    "init_range_lat = 0.154 \n",
    "init_advec_lat = 0.0418\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "\n",
    "irregular grid\n",
    "\n",
    "Final Interpretable Params: {\n",
    "   'sigma_sq': 13.1581147614216, \n",
    "   'range_lon': 0.1670857937279293, \n",
    "   'range_lat': 0.13532624419142394, \n",
    "   'range_time': 0.662083817877393, \n",
    "   'advec_lat': 4.485362408040999, \n",
    "   'advec_lon': -0.05054465623325784, \n",
    "   'nugget': 3.202037263471046e-05}\n",
    "\n",
    "regular grid\n",
    "\n",
    "Final Interpretable Params: {\n",
    "   'sigma_sq': 13.547646279613172, \n",
    "   'range_lon': 0.2063608511650159, \n",
    "   'range_lat': 0.1698378442441089, \n",
    "   'range_time': 1.122662499886297, \n",
    "   'advec_lat': 0.05415059834603563, \n",
    "   'advec_lon': -0.16640862287821778, \n",
    "   'nugget': 0.48795387825462566}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c961fa4",
   "metadata": {},
   "source": [
    "# fit dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9b3e3",
   "metadata": {},
   "source": [
    "difference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d43435c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([142832, 4])\n",
      "142832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.8000e-02,  1.2305e+02,  0.0000e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2311e+02,  1.9113e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2317e+02,  3.1846e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2323e+02,  9.3044e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2330e+02,  7.5384e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2336e+02, -8.2040e-01,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2342e+02, -1.2852e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2349e+02, -3.2619e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2355e+02, -6.2629e-01,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2361e+02,  1.3124e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2368e+02,  4.2001e-01,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2374e+02, -4.3216e-01,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2380e+02, -5.5636e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2386e+02, -2.0360e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2393e+02,  4.5638e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2399e+02, -5.2148e-02,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2405e+02, -3.7626e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2412e+02,  3.4245e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2418e+02,  5.3648e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2424e+02,  1.9309e+00,  2.1000e+01]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [11.0474, 0.0623, 0.2445, 1.0972, 0.0101, -0.1671, 1.1825]\n",
    "day = 0 # 0 index\n",
    "lat_range= [0,5]\n",
    "lon_range= [123.0, 133.0]\n",
    "#lat_range= [1,3]\n",
    "#lon_range= [125, 129.0]\n",
    "\n",
    "daily_aggregated_tensors_dw = [aggregated_data]\n",
    "daily_hourly_maps_dw = [input_map]\n",
    "\n",
    "db = debiased_whittle.debiased_whittle_preprocess(daily_aggregated_tensors_dw, daily_hourly_maps_dw, day_idx=day, params_list=a, lat_range=lat_range, lon_range=lon_range)\n",
    "\n",
    "\n",
    "subsetted_aggregated_day = db.generate_spatially_filtered_days(0,5,123,133)\n",
    "print(subsetted_aggregated_day.shape)\n",
    "N2= subsetted_aggregated_day.shape[0]\n",
    "print(N2)\n",
    "subsetted_aggregated_day[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2497b8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Pre-computing J-vector (Hamming taper)...\n",
      "Pre-computing sample periodogram...\n",
      "Pre-computing Hamming taper autocorrelation...\n",
      "Data grid: 113x158, 8 time points. J-vector, Periodogram, Taper Autocorr on cpu.\n",
      "\n",
      "============================== Initialization Run 1/1 ==============================\n",
      "Starting with FIXED params (raw log-scale): [4.2042, 1.6348, 0.4721, -2.5562, 0.0218, -0.1689, -1.3984]\n",
      "Starting optimization run 1 on device cpu (Hamming, 7-param ST kernel, L-BFGS)...\n",
      "--- Step 1/20 ---\n",
      " Loss: 1.906929 | Max Grad: 6.274846e-04\n",
      "  Params (Raw Log): log_phi1: 4.3182, log_phi2: 1.7745, log_phi3: 0.0776, log_phi4: -3.5323, advec_lat: 0.0363, advec_lon: -0.1510, log_nugget: -1.0186\n",
      "--- Step 2/20 ---\n",
      " Loss: 1.817385 | Max Grad: 4.095367e-05\n",
      "  Params (Raw Log): log_phi1: 4.3182, log_phi2: 1.7744, log_phi3: 0.0775, log_phi4: -3.5320, advec_lat: 0.0363, advec_lon: -0.1510, log_nugget: -1.0177\n",
      "--- Step 3/20 ---\n",
      " Loss: 1.817385 | Max Grad: 4.095367e-05\n",
      "  Params (Raw Log): log_phi1: 4.3182, log_phi2: 1.7744, log_phi3: 0.0775, log_phi4: -3.5320, advec_lat: 0.0363, advec_lon: -0.1510, log_nugget: -1.0177\n",
      "--- Step 4/20 ---\n",
      " Loss: 1.817385 | Max Grad: 4.095367e-05\n",
      "  Params (Raw Log): log_phi1: 4.3182, log_phi2: 1.7744, log_phi3: 0.0775, log_phi4: -3.5320, advec_lat: 0.0363, advec_lon: -0.1510, log_nugget: -1.0177\n",
      "\n",
      "--- Converged on loss change (change < 1e-12) at step 4 ---\n",
      "\n",
      "--- Training Complete ---\n",
      "\n",
      "FINAL BEST STATE ACHIEVED (during training):\n",
      "Best Loss: 1.817\n",
      "\n",
      "\n",
      "========================= Overall Result from Run ========================= =========================\n",
      "Best Run Loss: 1.817 (after 4 steps)\n",
      "Final Parameters (Natural Scale): sigmasq: 12.7277, range_lat: 0.1631, range_lon: 0.1696, range_time: 0.9917, advec_lat: 0.0363, advec_lon: -0.1510, nugget: 0.3614\n",
      "Final Parameters (Phi Scale)    : phi1: 75.0512, phi2: 5.8967, phi3: 1.0806, phi4: 0.0292, advec_lat: 0.0363, advec_lon: -0.1510, nugget: 0.3614\n",
      "Final Parameters (Raw Log Scale): log_phi1: 4.3182, log_phi2: 1.7744, log_phi3: 0.0775, log_phi4: -3.5320, advec_lat: 0.0363, advec_lon: -0.1510, log_nugget: -1.0177\n",
      "\n",
      "Total execution time: 40.40 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dwl = debiased_whittle.debiased_whittle_likelihood()\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    DAY_TO_RUN = 3 # data is decided above\n",
    "    TAPERING_FUNC = dwl.cgn_hamming # Use Hamming taper\n",
    "    NUM_RUNS = 1\n",
    "    MAX_STEPS = 20 # L-BFGS usually converges in far fewer steps\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063 \n",
    "\n",
    "    LAT_COL, LON_COL = 0, 1\n",
    "    VAL_COL = 2 # Spatially differenced value\n",
    "    TIME_COL = 3\n",
    "\n",
    "\n",
    "    cur_df =subsetted_aggregated_day\n",
    "    \n",
    "    if cur_df.numel() == 0 or cur_df.shape[1] <= max(LAT_COL, LON_COL, VAL_COL, TIME_COL):\n",
    "        print(f\"Error: Data for Day {DAY_TO_RUN} is empty or invalid.\")\n",
    "        exit()\n",
    "\n",
    "    unique_times = torch.unique(cur_df[:, TIME_COL])\n",
    "    time_slices_list = [cur_df[cur_df[:, TIME_COL] == t_val] for t_val in unique_times]\n",
    "\n",
    "    # --- 1. Pre-compute J-vector, Taper Grid, and Taper Autocorrelation ---\n",
    "    print(\"Pre-computing J-vector (Hamming taper)...\")\n",
    "    \n",
    "    # --- 💥 REVISED: Renamed 'p' to 'p_time' 💥 ---\n",
    "    J_vec, n1, n2, p_time, taper_grid = dwl.generate_Jvector_tapered( \n",
    "        time_slices_list,\n",
    "        tapering_func=TAPERING_FUNC, \n",
    "        lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if J_vec is None or J_vec.numel() == 0 or n1 == 0 or n2 == 0 or p_time == 0:\n",
    "       print(f\"Error: J-vector generation failed for Day {DAY_TO_RUN}.\")\n",
    "       exit()\n",
    "       \n",
    "    print(\"Pre-computing sample periodogram...\")\n",
    "    I_sample = dwl.calculate_sample_periodogram_vectorized(J_vec)\n",
    "\n",
    "    print(\"Pre-computing Hamming taper autocorrelation...\")\n",
    "    taper_autocorr_grid = dwl.calculate_taper_autocorrelation_fft(taper_grid, n1, n2, DEVICE)\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Error: NaN/Inf in sample periodogram.\")\n",
    "        exit()\n",
    "    if torch.isnan(taper_autocorr_grid).any() or torch.isinf(taper_autocorr_grid).any():\n",
    "        print(\"Error: NaN/Inf in taper autocorrelation.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Data grid: {n1}x{n2}, {p_time} time points. J-vector, Periodogram, Taper Autocorr on {DEVICE}.\")\n",
    "    # --- END REVISION ---\n",
    "\n",
    "    # --- 2. Optimization Loop ---\n",
    "    all_final_results = []\n",
    "    all_final_losses = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        print(f\"\\n{'='*30} Initialization Run {i+1}/{NUM_RUNS} {'='*30}\")\n",
    "\n",
    "        # --- 7-PARAMETER initialization ---\n",
    "        ''' \n",
    "        init_sigmasq   = 15.0\n",
    "        init_range_lat = 0.66 \n",
    "        init_range_lon = 0.7 \n",
    "        init_nugget    = 1.5\n",
    "        init_beta      = 0.1  # Temporal range ratio\n",
    "        init_advec_lat = 0.02\n",
    "        init_advec_lon = -0.08\n",
    "        '''\n",
    "        init_sigmasq   = 13.059\n",
    "        init_range_lat = 0.154 \n",
    "        init_range_lon = 0.195\n",
    "        init_advec_lat = 0.0218\n",
    "        init_range_time = 0.7\n",
    "        init_advec_lon = -0.1689\n",
    "        init_nugget    = 0.247\n",
    "\n",
    "        init_phi2 = 1.0 / init_range_lon\n",
    "        init_phi1 = init_sigmasq * init_phi2\n",
    "        init_phi3 = (init_range_lon / init_range_lat)**2\n",
    "        # Change needed to match the spatial-temporal distance formula:\n",
    "        init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "        initial_params_values = [\n",
    "            np.log(init_phi1),    # [0] log_phi1\n",
    "            np.log(init_phi2),    # [1] log_phi2\n",
    "            np.log(init_phi3),    # [2] log_phi3\n",
    "            np.log(init_phi4),    # [3] log_phi4\n",
    "            init_advec_lat,       # [4] advec_lat (NOT log)\n",
    "            init_advec_lon,       # [5] advec_lon (NOT log)\n",
    "            np.log(init_nugget)   # [6] log_nugget\n",
    "        ]\n",
    "        \n",
    "        print(f\"Starting with FIXED params (raw log-scale): {[round(p, 4) for p in initial_params_values]}\")\n",
    "\n",
    "        params_list = [\n",
    "            Parameter(torch.tensor([val], dtype=torch.float64))\n",
    "            for val in initial_params_values\n",
    "        ]\n",
    "\n",
    "        # Helper to define the boundary globally for clarity\n",
    "        NUGGET_LOWER_BOUND = 0.05\n",
    "        LOG_NUGGET_LOWER_BOUND = np.log(NUGGET_LOWER_BOUND) # Approx -2.9957\n",
    "\n",
    "        # --- 💥 REVISED: Use L-BFGS Optimizer 💥 ---\n",
    "        optimizer = torch.optim.LBFGS(\n",
    "            params_list,\n",
    "            lr=1.0,           # Initial step length for line search\n",
    "            max_iter=20,      # Iterations per step\n",
    "            history_size=100,\n",
    "            line_search_fn=\"strong_wolfe\", # Often more robust\n",
    "            tolerance_grad=1e-5\n",
    "        )\n",
    "        # --- END REVISION ---\n",
    "\n",
    "        print(f\"Starting optimization run {i+1} on device {DEVICE} (Hamming, 7-param ST kernel, L-BFGS)...\")\n",
    "        \n",
    "        # --- 💥 REVISED: Call L-BFGS trainer, pass p_time 💥 ---\n",
    "        nat_params_str, phi_params_str, raw_params_str, loss, steps_run = dwl.run_lbfgs_tapered(\n",
    "            params_list=params_list,\n",
    "            optimizer=optimizer,\n",
    "            I_sample=I_sample,\n",
    "            n1=n1, n2=n2, p_time=p_time,\n",
    "            taper_autocorr_grid=taper_autocorr_grid, \n",
    "            max_steps=MAX_STEPS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        # --- END REVISION ---\n",
    "        \n",
    "        if loss is not None:\n",
    "            all_final_results.append((nat_params_str, phi_params_str, raw_params_str))\n",
    "            all_final_losses.append(loss)\n",
    "        else:\n",
    "            all_final_losses.append(float('inf'))\n",
    "\n",
    "    print(f\"\\n\\n{'='*25} Overall Result from Run {'='*25} {'='*25}\")\n",
    "    \n",
    "    valid_losses = [l for l in all_final_losses if l is not None and l != float('inf')]\n",
    "\n",
    "    if not valid_losses:\n",
    "        print(f\"The run failed or resulted in an invalid loss for Day {DAY_TO_RUN}.\")\n",
    "    else:\n",
    "        best_loss = min(valid_losses)\n",
    "        best_run_index = all_final_losses.index(best_loss)\n",
    "        best_results = all_final_results[best_run_index]\n",
    "        \n",
    "        print(f\"Best Run Loss: {best_loss} (after {steps_run} steps)\")\n",
    "        print(f\"Final Parameters (Natural Scale): {best_results[0]}\")\n",
    "        print(f\"Final Parameters (Phi Scale)    : {best_results[1]}\")\n",
    "        print(f\"Final Parameters (Raw Log Scale): {best_results[2]}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61060411",
   "metadata": {},
   "source": [
    "init_sigmasq   = 13.059\n",
    "init_range_lon = 0.195 \n",
    "init_range_lat = 0.154 \n",
    "init_advec_lat = 0.0418\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "1 st simulation (1 vs 3)\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 12.939260896610579, 'range_lon': 0.17242543281507933, 'range_lat': 0.1631231346200311, 'range_time': 1.1358209227858689, 'advec_lat': 0.045119029109988974, 'advec_lon': -0.17809677784942035, 'nugget': 0.3009582276680578}\n",
    "\n",
    "Final Parameters (Natural Scale): sigmasq: 13.2415, range_lat: 0.1685, range_lon: 0.1739, range_time: 0.9681, advec_lat: 0.0395, advec_lon: -0.1732, nugget: 0.3216\n",
    "\n",
    "2nd simulation (1 vs 3)\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 12.765293287952144, 'range_lon': 0.17039368470743024, 'range_lat': 0.16152132799710625, 'range_time': 1.081124889959751, 'advec_lat': 0.04635511983762563, 'advec_lon': -0.1775715292039452, 'nugget': 0.31503884896742074}\n",
    "\n",
    "\n",
    "sigmasq: 13.2415, range_lat: 0.1685, range_lon: 0.1739, range_time: 0.9681, advec_lat: 0.0395, advec_lon: -0.1732, nugget: 0.3216\n",
    "\n",
    "3nd simulation (mm 15 two times)\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 12.292570063933866, 'range_lon': 0.16170186578939172, 'range_lat': 0.1537053192245325, 'range_time': 0.9671689003322674, 'advec_lat': 0.04155171168950867, 'advec_lon': -0.16218073366456207, 'nugget': 0.29945872751676345}\n",
    "\n",
    "Final Parameters (Natural Scale): sigmasq: 12.7277, range_lat: 0.1631, range_lon: 0.1696, range_time: 0.9917, advec_lat: 0.0363, advec_lon: -0.1510, nugget: 0.3614\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
