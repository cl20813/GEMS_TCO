{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f33ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reflected location error in ozone data simulation\n",
    "\n",
    "import torch\n",
    "import torch.fft\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import argparse \n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "from sklearn.neighbors import BallTree\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CUSTOM PATHS ---\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "# (ÌïÑÏöî Ïãú Ïã§Ï†ú GEMS_TCO ÎùºÏù¥Î∏åÎü¨Î¶¨ import)\n",
    "try:\n",
    "    from GEMS_TCO import kernels_for_simulation_020626 as kernels_simulation\n",
    "    \n",
    "    # from GEMS_TCO import kernels_reparam_space_time_gpu_copy as kernels_reparam\n",
    "    \n",
    "    from GEMS_TCO import orderings as _orderings\n",
    "    from GEMS_TCO import alg_optimization, BaseLogger\n",
    "\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Warning: GEMS_TCO modules not found. Ensure paths are correct.\")\n",
    "\n",
    "from GEMS_TCO import configuration as config\n",
    "from GEMS_TCO.data_loader import load_data2, exact_location_filter\n",
    "from GEMS_TCO import debiased_whittle\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d6ba3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Global Monthly Mean for 2024-7: 257.0667 ---\n",
      "Global Monthly Mean: 257.0666873781436\n",
      "torch.Size([22680, 11])\n"
     ]
    }
   ],
   "source": [
    "space: List[str] = ['1', '1']\n",
    "lat_lon_resolution = [int(s) for s in space]\n",
    "mm_cond_number: int = 8\n",
    "years = ['2024']\n",
    "month_range = [7] \n",
    "\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "data_load_instance = load_data2(config.mac_data_load_path)\n",
    "\n",
    "#lat_range_input = [1, 3]\n",
    "#lon_range_input = [125.0, 129.0]\n",
    "\n",
    "lat_range_input=[-3,-1]      \n",
    "lon_range_input=[121, 125] \n",
    "\n",
    "#lat_range_input=[-3,2]      \n",
    "#lon_range_input=[121, 131] \n",
    "\n",
    "# Í∏∞Ï°¥: df_map, ord_mm, nns_map, day_offsets = ...\n",
    "# ÏàòÏ†ï ÌõÑ: Î≥ÄÏàòÎ™ÖÏùÑ monthly_meanÏúºÎ°ú Î≥ÄÍ≤Ω\n",
    "\n",
    "df_map, ord_mm, nns_map, monthly_mean = data_load_instance.load_maxmin_ordered_data_bymonthyear(\n",
    "    lat_lon_resolution=lat_lon_resolution, \n",
    "    mm_cond_number=mm_cond_number,\n",
    "    years_=years, \n",
    "    months_=month_range,\n",
    "    lat_range=lat_range_input,   \n",
    "    lon_range=lon_range_input\n",
    ")\n",
    "\n",
    "print(f\"Global Monthly Mean: {monthly_mean}\") # ÌôïÏù∏Ïö© Ï∂úÎ†•\n",
    "\n",
    "\n",
    "daily_aggregated_reg_vecc = [] \n",
    "daily_hourly_maps_reg_vecc = []      \n",
    "\n",
    "daily_aggregated_irr_vecc = [] \n",
    "daily_hourly_maps_irr_vecc = []   \n",
    "\n",
    "\n",
    "for day_index in range(31):\n",
    "    hour_start_index = day_index * 8\n",
    "    \n",
    "    hour_end_index = (day_index + 1) * 8\n",
    "    #hour_end_index = day_index*8 + 1\n",
    "    hour_indices = [hour_start_index, hour_end_index]\n",
    "\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "    df_map, \n",
    "    monthly_mean,  # <--- Ïù¥ Î∂ÄÎ∂ÑÏù¥ Ï∂îÍ∞ÄÎêòÏñ¥Ïïº Ìï©ÎãàÎã§\n",
    "    hour_indices, \n",
    "    ord_mm=ord_mm,\n",
    "    dtype=torch.float64, \n",
    "    keep_ori=False\n",
    "    )\n",
    "\n",
    "    daily_aggregated_reg_vecc.append( day_aggregated_tensor )\n",
    "    daily_hourly_maps_reg_vecc.append( day_hourly_map )\n",
    "\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "    df_map, \n",
    "    monthly_mean,  # <--- Ïù¥ Î∂ÄÎ∂ÑÏù¥ Ï∂îÍ∞ÄÎêòÏñ¥Ïïº Ìï©ÎãàÎã§\n",
    "    hour_indices, \n",
    "    ord_mm=ord_mm,\n",
    "    dtype=torch.float64, \n",
    "    keep_ori= True\n",
    "    )\n",
    "\n",
    "    daily_aggregated_irr_vecc.append( day_aggregated_tensor )\n",
    "    daily_hourly_maps_irr_vecc.append( day_hourly_map )\n",
    "print(daily_aggregated_irr_vecc[0].shape)\n",
    "\n",
    "nn = daily_aggregated_irr_vecc[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bceeca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "import sys\n",
    "\n",
    "# --- [ÏÑ§Ï†ï] Ïò§Ï°¥ ÎÜçÎèÑ ÏãúÎÆ¨Î†àÏù¥ÏÖò ÌååÎùºÎØ∏ÌÑ∞ ---\n",
    "# 1. U-Shape Diurnal Cycle (8ÏãúÍ∞Ñ: 08:00 ~ 15:00 Í∞ÄÏ†ï)\n",
    "# Ïòà: ÏïÑÏπ®Ïóê ÎÜíÏïòÎã§Í∞Ä(10), ÎÇÆÏóê Îñ®Ïñ¥ÏßÄÍ≥†(-5), Ïò§ÌõÑÏóê Îã§Ïãú Ïò§Î•¥Îäî Ìå®ÌÑ¥\n",
    "# Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞Ïùò beta Í≥ÑÏàòÏôÄ ÎπÑÏä∑Ìïú Ïä§ÏºÄÏùºÎ°ú ÏÑ§Ï†ïÌïòÏÑ∏Ïöî.\n",
    "HOURLY_TREND = torch.tensor([10.0, 5.0, 0.0, -5.0, -8.0, -5.0, 0.0, 5.0], dtype=torch.float64)\n",
    "\n",
    "# 2. Daily Baseline (Ïõî ÌèâÍ∑†)\n",
    "MONTHLY_MEAN = 260.0\n",
    "\n",
    "# --- 2. EXACT COVARIANCE & FFT HELPERS (Í∏∞Ï°¥ Ïú†ÏßÄ) ---\n",
    "def make_target_grid(lat_start, lat_end, lat_step, lon_start, lon_end, lon_step, device, dtype):\n",
    "    if lat_start > lat_end and lat_step > 0: lat_step = -lat_step\n",
    "    if lon_start > lon_end and lon_step > 0: lon_step = -lon_step\n",
    "    lats = torch.arange(lat_start, lat_end - 0.0001, lat_step, device=device, dtype=dtype)\n",
    "    lons = torch.arange(lon_start, lon_end + 0.0001, lon_step, device=device, dtype=dtype)\n",
    "    lats = torch.round(lats * 10000) / 10000\n",
    "    lons = torch.round(lons * 10000) / 10000\n",
    "    grid_lat, grid_lon = torch.meshgrid(lats, lons, indexing='ij')\n",
    "    center_points = torch.stack([grid_lat.flatten(), grid_lon.flatten()], dim=1)\n",
    "    return center_points, len(lats), len(lons)\n",
    "\n",
    "# --- 2. High-Res Field Generator (Í∏∞Ï°¥ Ïú†ÏßÄ) ---\n",
    "def generate_high_res_field(target_lat_range, target_lon_range, t_steps, params, device, dtype):\n",
    "    # ... (FFT Î°úÏßÅ Í∏∞Ï°¥Í≥º ÎèôÏùºÌïòÎØÄÎ°ú ÏÉùÎûµ, ÏúÑ ÏΩîÎìú Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©) ...\n",
    "    # [ÏÑ§Ï†ï] Ìï¥ÏÉÅÎèÑ ÌôïÎåÄ Î∞∞Ïú®\n",
    "    lat_res_factor = 160.0 \n",
    "    lon_res_factor = 4.0   \n",
    "    lat_res_high = 0.044 / lat_res_factor\n",
    "    lon_res_high = 0.063 / lon_res_factor\n",
    "    \n",
    "    t_lat_max = max(target_lat_range)\n",
    "    t_lat_min = min(target_lat_range)\n",
    "    \n",
    "    lats_high = torch.arange(t_lat_max + 0.1, t_lat_min - 0.1, -lat_res_high, device=device, dtype=dtype)\n",
    "    lons_high = torch.arange(target_lon_range[0] - 0.1, target_lon_range[1] + 0.1, lon_res_high, device=device, dtype=dtype)\n",
    "    \n",
    "    Nx, Ny, Nt = len(lats_high), len(lons_high), t_steps\n",
    "    dlat, dlon, dt = lat_res_high, lon_res_high, 1.0\n",
    "    \n",
    "    Px, Py, Pt = 2*Nx, 2*Ny, 2*Nt\n",
    "    lags_x = torch.arange(Px, device=device, dtype=dtype) * dlat; lags_x[Px//2:] -= (Px * dlat)\n",
    "    lags_y = torch.arange(Py, device=device, dtype=dtype) * dlon; lags_y[Py//2:] -= (Py * dlon)\n",
    "    lags_t = torch.arange(Pt, device=device, dtype=dtype) * dt;   lags_t[Pt//2:] -= (Pt * dt)\n",
    "\n",
    "    L_x, L_y, L_t = torch.meshgrid(lags_x, lags_y, lags_t, indexing='ij')\n",
    "    \n",
    "    phi1, phi2 = torch.exp(params[0]), torch.exp(params[1])\n",
    "    phi3, phi4 = torch.exp(params[2]), torch.exp(params[3])\n",
    "    adv_lat, adv_lon = params[4], params[5]\n",
    "    \n",
    "    sigma_sq = phi1 / phi2 \n",
    "    \n",
    "    u_x = L_x - adv_lat * L_t\n",
    "    u_y = L_y - adv_lon * L_t\n",
    "    dist_sq = (u_x * torch.sqrt(phi3) * phi2)**2 + (u_y * phi2)**2 + (L_t * torch.sqrt(phi4) * phi2)**2\n",
    "    C_vals = sigma_sq * torch.exp(-torch.sqrt(dist_sq + 1e-12))\n",
    "\n",
    "    S = torch.fft.fftn(C_vals); S.real = torch.clamp(S.real, min=0)\n",
    "    random_phase = torch.fft.fftn(torch.randn(Px, Py, Pt, device=device, dtype=dtype))\n",
    "    field_sim_raw = torch.fft.ifftn(torch.sqrt(S.real) * random_phase).real\n",
    "    \n",
    "    field_sim = field_sim_raw[:Nx, :Ny, :Nt]\n",
    "    current_std = field_sim.std()\n",
    "    target_std = torch.sqrt(sigma_sq)\n",
    "    field_calibrated = (field_sim - field_sim.mean()) * (target_std / (current_std + 1e-9))\n",
    "    \n",
    "    return field_calibrated, lats_high, lons_high\n",
    "\n",
    "# --- 3. Main Generator (Vecc & DW) - [ÌïµÏã¨ ÏàòÏ†ï Î∂ÄÎ∂Ñ] ---\n",
    "def generate_exact_count_datasets_fixed(daily_maps_real, true_params_tensor, target_grid_info, device, dtype):\n",
    "    \"\"\"\n",
    "    Step 1 (Daily AR1) + Step 2 (Hourly Trend) + Step 3 (GP)Î•º Î™®Îëê Í≤∞Ìï©Ìï©ÎãàÎã§.\n",
    "    \"\"\"\n",
    "    lat_s, lat_e, lat_step, lon_s, lon_e, lon_step = target_grid_info\n",
    "    \n",
    "    # [A] ÌÉÄÍ≤ü Í≤©Ïûê Î∞è High-Res Field Ï§ÄÎπÑ\n",
    "    target_grid_coords, Nx, Ny = make_target_grid(lat_s, lat_e, lat_step, lon_s, lon_e, lon_step, device, dtype)\n",
    "    target_tree = BallTree(np.radians(target_grid_coords.cpu().numpy()), metric='haversine') \n",
    "    \n",
    "    # 8ÏãúÍ∞ÑÏπò GP ÏÉùÏÑ±\n",
    "    high_res_field, lats_high, lons_high = generate_high_res_field((lat_s, lat_e), (lon_s, lon_e), 8, true_params_tensor, device, dtype)\n",
    "    \n",
    "    hr_mesh_lat, hr_mesh_lon = torch.meshgrid(lats_high, lons_high, indexing='ij')\n",
    "    hr_tree = BallTree(np.radians(torch.stack([hr_mesh_lat.flatten(), hr_mesh_lon.flatten()], dim=1).cpu().numpy()), metric='haversine')\n",
    "    high_res_flat = high_res_field.reshape(-1, 8) \n",
    "\n",
    "    irr_map_dict, dw_map_dict = {}, {}\n",
    "    irr_list, dw_list = [], []\n",
    "    noise_std = torch.sqrt(torch.exp(true_params_tensor[6]))\n",
    "\n",
    "    # [Step 1] Daily Baseline ÏÉùÏÑ± (AR1 Process Í∞ÄÏ†ï)\n",
    "    # Ïó¨Í∏∞ÏÑúÎäî ÌïòÎ£®ÏπòÎßå ÏÉùÏÑ±ÌïòÎØÄÎ°ú, AR(1)Ïùò ÌäπÏ†ï ÏãúÏ†ê Í∞í ÌïòÎÇòÎ•º ÎûúÎç§ ÏÉùÏÑ±ÌïòÍ±∞ÎÇò Í≥†Ï†ïÌï©ÎãàÎã§.\n",
    "    # ÎßåÏïΩ Ïó¨Îü¨ ÎÇ†ÏßúÎ•º Î£®ÌîÑ ÎèàÎã§Î©¥ e_d = phi * e_{d-1} + noise Î°ú Í∞±Ïã†Ìï¥Ïïº Ìï©ÎãàÎã§.\n",
    "    # ÏòàÏãú: Ïò§Îäò ÎÇ†Ïî®Ïóê Îî∞Î•∏ Ìé∏Ï∞® (Ïòà: +5.0)\n",
    "    daily_weather_noise = torch.randn(1, device=device, dtype=dtype) * 2.0 # AR processÏùò sigma\n",
    "    daily_base_val = MONTHLY_MEAN + daily_weather_noise # Ïò§ÎäòÏùò Í∏∞Ï†Ä ÎÜçÎèÑ\n",
    "    \n",
    "    print(f\"   [Simulation Info] Daily Base: {daily_base_val.item():.2f} (MonthMean {MONTHLY_MEAN} + Noise {daily_weather_noise.item():.2f})\")\n",
    "\n",
    "    # [C] Îç∞Ïù¥ÌÑ∞ Îß§Ìïë\n",
    "    day0_dict = daily_maps_real[0]\n",
    "    sorted_keys = sorted([k for k in day0_dict.keys() if 'hm' in k or 'time' in k])\n",
    "    \n",
    "    # GPUÎ°ú ÎØ∏Î¶¨ Trend Ïù¥Îèô\n",
    "    hourly_trend_gpu = HOURLY_TREND.to(device)\n",
    "\n",
    "    for t_idx, key in enumerate(sorted_keys):\n",
    "        if t_idx >= 8: break\n",
    "        real_tensor = day0_dict[key].to(device)\n",
    "        \n",
    "        clean_tensor = real_tensor \n",
    "        real_locs = clean_tensor[:, :2]\n",
    "        N_points = len(real_locs)\n",
    "        \n",
    "        # 1. High-Res GP Í∞í ÏÉòÌîåÎßÅ (Step 3)\n",
    "        _, hr_indices = hr_tree.query(np.radians(real_locs.cpu().numpy()), k=1)\n",
    "        gp_signal = high_res_flat[torch.tensor(hr_indices.flatten(), device=device), t_idx]\n",
    "        \n",
    "        # 2. Hourly Trend Í∞ÄÏ†∏Ïò§Í∏∞ (Step 2)\n",
    "        current_hour_trend = hourly_trend_gpu[t_idx]\n",
    "        \n",
    "        # 3. [ÌïµÏã¨] ÏãúÎÆ¨Î†àÏù¥ÏÖò Í∞í Ìï©ÏÑ±\n",
    "        # Value = (Daily Base) + (Hourly Trend) + (GP Residual) + (Nugget Noise)\n",
    "        sim_vals = daily_base_val + current_hour_trend + gp_signal + (torch.randn_like(gp_signal) * noise_std)\n",
    "        \n",
    "        # 4. Time Dummies ÏÉùÏÑ± (Feature)\n",
    "        dummies = F.one_hot(torch.tensor([t_idx]), num_classes=8).repeat(N_points, 1)\n",
    "        dummies = dummies[:, 1:].to(device=device, dtype=dtype) # D1 ~ D7 (InterceptÍ∞Ä D0 Ïó≠Ìï† ÌòπÏùÄ Î≥ÑÎèÑ)\n",
    "        \n",
    "        # 5. Offset Column ÏÉùÏÑ± (Step 1Îßå Ìè¨Ìï®!)\n",
    "        # Î™®Îç∏ÏóêÍ≤å \"Hourly Trend\"Îäî Ïà®Í∏∞Í≥†(Time DummyÎ°ú ÎßûÏ∂îÍ≤å ÌïòÍ≥†), \"Daily Base\"Îäî ÏïåÎ†§Ï§å(OffsetÏúºÎ°ú Î∫å)\n",
    "        # ÎßåÏïΩ Î™®Îç∏Ïù¥ InterceptÎèÑ ÌïôÏäµÌïòÍ≤å ÌïòÎ†§Î©¥ offsetÏùÑ 0ÏúºÎ°ú ÎëêÍ±∞ÎÇò MONTHLY_MEANÎßå Ï§òÎèÑ Îê®.\n",
    "        # Ïó¨Í∏∞ÏÑúÎäî \"Step 1(ÏùºÎ≥Ñ Î≥ÄÎèô)ÏùÄ Î¨ºÎ¶¨ Î™®Îç∏Ïù¥ Ïû°ÏïÑÏ§¨Îã§\"Í≥† Í∞ÄÏ†ïÌïòÍ≥† OffsetÏóê ÎÑ£Ïùå.\n",
    "        offset_col = torch.full((N_points, 1), daily_base_val.item(), device=device, dtype=dtype)\n",
    "        \n",
    "        # --- Í≤∞Ìï© ---\n",
    "        # (1) Vecc\n",
    "        base_vecc = torch.stack([real_locs[:,0], real_locs[:,1], sim_vals, clean_tensor[:,3]], dim=1)\n",
    "        vecc_row = torch.cat([base_vecc, dummies, offset_col], dim=1).detach() \n",
    "        irr_map_dict[key] = vecc_row\n",
    "        irr_list.append(vecc_row)\n",
    "        \n",
    "        # (2) DW\n",
    "        current_grid_points = target_grid_coords[:N_points]\n",
    "        base_dw = torch.stack([current_grid_points[:,0], current_grid_points[:,1], sim_vals, clean_tensor[:,3]], dim=1)\n",
    "        dw_row = torch.cat([base_dw, dummies, offset_col], dim=1).detach() \n",
    "        dw_map_dict[key] = dw_row\n",
    "        dw_list.append(dw_row)\n",
    "\n",
    "    return [torch.cat(dw_list)], [dw_map_dict], [torch.cat(irr_list)], [irr_map_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f16e60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Simulation Configuration ---\n",
      "Target Sigma^2: 13.059\n",
      "Hourly Trend applied: [10.  5.  0. -5. -8. -5.  0.  5.]\n",
      "Daily Base applied: AR(1) logic inside function\n",
      "\n",
      "üöÄ Generating Simulated Data (Step 1+2+3)...\n",
      "   [Simulation Info] Daily Base: 261.75 (MonthMean 260.0 + Noise 1.75)\n",
      "\n",
      "‚úÖ Simulation Complete!\n",
      "   - Generated Tensor Shape: torch.Size([22680, 12])\n",
      "     (Cols: Lat, Lon, Val, Time, D1..D7, Offset)\n",
      "   - Residual Stats (Val - Offset):\n",
      "     Mean: 0.2174 (Should be close to Mean of Hourly Trend)\n",
      "     Std : 7.0562  (Should contain Sigma_sq + Trend Var)\n"
     ]
    }
   ],
   "source": [
    "# --- 4. EXECUTION ---\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 1. ÌôòÍ≤Ω ÏÑ§Ï†ï\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DTYPE = torch.float64\n",
    "\n",
    "# ÌÉÄÍ≤ü Í∑∏Î¶¨Îìú Ï†ïÎ≥¥ (Lat Start, End, Step, Lon Start, End, Step)\n",
    "target_grid_info = (-1.0, -3.0, 0.044, 121.0, 125.0, 0.063)\n",
    "\n",
    "# 2. True Parameter Ï†ïÏùò (Ï†ïÎãµÏßÄ)\n",
    "# Ïù¥ ÌååÎùºÎØ∏ÌÑ∞Î°ú High-Res GPÎ•º ÏÉùÏÑ±Ìï©ÎãàÎã§.\n",
    "init_sigmasq    = 13.059\n",
    "init_range_lon  = 0.195 \n",
    "init_range_lat  = 0.154 \n",
    "init_range_time = 1.0\n",
    "init_nugget     = 0.247\n",
    "init_advec_lat  = 0.0418\n",
    "init_advec_lon  = -0.1689\n",
    "\n",
    "# Ïª§ÎÑê ÎÇ¥Î∂Ä ÌååÎùºÎØ∏ÌÑ∞(Phi)Î°ú Î≥ÄÌôò\n",
    "true_phi2 = 1.0 / init_range_lon              \n",
    "true_phi1 = init_sigmasq * true_phi2          \n",
    "true_phi3 = (init_range_lon / init_range_lat)**2\n",
    "true_phi4 = (init_range_lon / init_range_time)**2\n",
    "\n",
    "# ÌÖêÏÑú ÌòïÌÉúÎ°ú Î≥ÄÌôò (Log Scale for positive constraints)\n",
    "true_params_tensor = [\n",
    "    torch.tensor([np.log(true_phi1)], device=DEVICE, dtype=DTYPE), # Log Phi1\n",
    "    torch.tensor([np.log(true_phi2)], device=DEVICE, dtype=DTYPE), # Log Phi2\n",
    "    torch.tensor([np.log(true_phi3)], device=DEVICE, dtype=DTYPE), # Log Phi3\n",
    "    torch.tensor([np.log(true_phi4)], device=DEVICE, dtype=DTYPE), # Log Phi4\n",
    "    torch.tensor([init_advec_lat],    device=DEVICE, dtype=DTYPE), # Advec Lat\n",
    "    torch.tensor([init_advec_lon],    device=DEVICE, dtype=DTYPE), # Advec Lon\n",
    "    torch.tensor([np.log(init_nugget)], device=DEVICE, dtype=DTYPE)# Log Nugget\n",
    "]\n",
    "\n",
    "print(f\"--- Simulation Configuration ---\")\n",
    "print(f\"Target Sigma^2: {init_sigmasq}\")\n",
    "print(f\"Hourly Trend applied: {HOURLY_TREND.cpu().numpy()}\")\n",
    "print(f\"Daily Base applied: AR(1) logic inside function\")\n",
    "\n",
    "# 3. Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Ìï®Ïàò Ìò∏Ï∂ú\n",
    "# Ï£ºÏùò: 'daily_hourly_maps_vecc'Îäî Ïù¥Ï†Ñ ÏÖÄ(Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∂ÄÎ∂Ñ)ÏóêÏÑú Ïù¥ÎØ∏ Î°úÎìúÎêòÏñ¥ ÏûàÏñ¥Ïïº Ìï©ÎãàÎã§.\n",
    "if 'daily_hourly_maps_irr_vecc' in locals() and len(daily_hourly_maps_irr_vecc) > 0:\n",
    "    print(\"\\nüöÄ Generating Simulated Data (Step 1+2+3)...\")\n",
    "    \n",
    "    (daily_aggregated_reg_vecc_sim, daily_hourly_maps_reg_vecc_sim,\n",
    "     daily_aggregated_irr_vecc_sim, daily_hourly_maps_irr_vecc_sim) = generate_exact_count_datasets_fixed(\n",
    "        daily_hourly_maps_irr_vecc,  # ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞(ÏúÑÏπò Ï†ïÎ≥¥Ïö©)\n",
    "        true_params_tensor,      # GP ÌååÎùºÎØ∏ÌÑ∞\n",
    "        target_grid_info,        # Í∑∏Î¶¨Îìú Ï†ïÎ≥¥\n",
    "        DEVICE, \n",
    "        DTYPE\n",
    "    )\n",
    "    \n",
    "    # 4. Í≤∞Í≥º Í≤ÄÏ¶ù\n",
    "    sim_tensor = daily_aggregated_irr_vecc_sim[0] # Ï≤´Ïß∏ ÎÇ† Îç∞Ïù¥ÌÑ∞\n",
    "    \n",
    "    print(f\"\\n‚úÖ Simulation Complete!\")\n",
    "    print(f\"   - Generated Tensor Shape: {sim_tensor.shape}\")\n",
    "    print(f\"     (Cols: Lat, Lon, Val, Time, D1..D7, Offset)\")\n",
    "    \n",
    "    # Í∞í ÌôïÏù∏ (Simulated Val - Offset)Ïù¥ GP+Trend ÏûîÏ∞®ÏôÄ ÎπÑÏä∑ÌïúÏßÄ\n",
    "    # Val(col 2) - Offset(col 11)\n",
    "    # Col 11ÏùÄ Ïù∏Îç±Ïä§ 11 (12Î≤àÏß∏ Ïª¨Îüº)\n",
    "    if sim_tensor.shape[1] >= 12:\n",
    "        residuals = sim_tensor[:, 2] - sim_tensor[:, 11]\n",
    "        print(f\"   - Residual Stats (Val - Offset):\")\n",
    "        print(f\"     Mean: {residuals.mean().item():.4f} (Should be close to Mean of Hourly Trend)\")\n",
    "        print(f\"     Std : {residuals.std().item():.4f}  (Should contain Sigma_sq + Trend Var)\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Warning: Column count is less than 12. Check dummy/offset creation.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ùå Error: 'daily_hourly_maps_vecc' variable not found.\")\n",
    "    print(\"   Please run the Data Loading cell (Cell 3 or 7 in your notebook) first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7191bead",
   "metadata": {},
   "source": [
    "# Fit vecchia max min time 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b800f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "========================================\n",
      "--- Initializing VecchiaBatched Model ---\n",
      "========================================\n",
      "\n",
      "========================================\n",
      "--- Running L-BFGS Optimization ---\n",
      "========================================\n",
      "üöÄ Pre-computing (Time Dummies & Latitude Centering)... [Mean Lat: -1.9526] ‚úÖ Done. (Heads: 2400, Tails: 20280)\n",
      "--- Starting Batched L-BFGS Optimization (GPU) ---\n",
      "--- Step 1/3 / Loss: 1.806781 ---\n",
      "  Param 0: Value=4.9324, Grad=-0.0012334454438898586\n",
      "  Param 1: Value=2.5006, Grad=0.001210990819328973\n",
      "  Param 2: Value=0.2170, Grad=-0.0008502358196047664\n",
      "  Param 3: Value=-4.9371, Grad=-0.0013332741630110228\n",
      "  Param 4: Value=0.0975, Grad=-0.002917004343360237\n",
      "  Param 5: Value=-0.0322, Grad=0.0004220682113979697\n",
      "  Param 6: Value=1.2546, Grad=-0.0016858415066192982\n",
      "  Max Abs Grad: 2.917004e-03\n",
      "------------------------------\n",
      "--- Step 2/3 / Loss: 1.765995 ---\n",
      "  Param 0: Value=-7.7786, Grad=0.00015935633782689912\n",
      "  Param 1: Value=-13.1935, Grad=-0.00015932134294813235\n",
      "  Param 2: Value=-26.5020, Grad=2.4468541666580601e-26\n",
      "  Param 3: Value=29.3370, Grad=0.000259285064588584\n",
      "  Param 4: Value=1.0990, Grad=-7.318086182258873e-41\n",
      "  Param 5: Value=0.0392, Grad=8.449205165896378e-25\n",
      "  Param 6: Value=1.7322, Grad=0.007771739490119833\n",
      "  Max Abs Grad: 7.771739e-03\n",
      "------------------------------\n",
      "--- Step 3/3 / Loss: 1.420125 ---\n",
      "  Param 0: Value=-7.6115, Grad=3.612600338346965e-06\n",
      "  Param 1: Value=-13.1908, Grad=1.2734386688246667e-06\n",
      "  Param 2: Value=-26.9896, Grad=2.182541049582416e-15\n",
      "  Param 3: Value=29.0942, Grad=1.109325627507005e-06\n",
      "  Param 4: Value=1.0934, Grad=-4.372646753176335e-28\n",
      "  Param 5: Value=0.1587, Grad=-4.159264231450412e-16\n",
      "  Param 6: Value=1.7317, Grad=-3.274335960123942e-05\n",
      "  Max Abs Grad: 3.274336e-05\n",
      "------------------------------\n",
      "Final Interpretable Params: {'sigma_sq': 264.88689818169775, 'range_lon': 535433.6357208949, 'range_lat': 388530952641.3444, 'range_time': 0.2576248551531725, 'advec_lat': 1.0933911654395319, 'advec_lon': 0.15865488480666928, 'nugget': 5.650241491650876}\n",
      "\n",
      "Optimization finished in 54.26s.\n",
      "Results after 2 steps: [-7.611529296063428, -13.190832231630552, -26.98961288825679, 29.094166067282487, 1.0933911654395319, 0.15865488480666928, 1.731698286130052, 1.4201254288543559]\n",
      "Final Params: [ -7.6115293  -13.19083223 -26.98961289  29.09416607   1.09339117\n",
      "   0.15865488   1.73169829]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "v = 0.5              # Smoothness\n",
    "mm_cond_number = 8   # Neighbors\n",
    "#mm_cond_number = 16   # Neighbors\n",
    "nheads = 300           # 0 = Pure Vecchia\n",
    "lr = 1.0             # LBFGS learning rate\n",
    "LBFGS_MAX_STEPS = 3\n",
    "LBFGS_HISTORY_SIZE = 100 # 100\n",
    "LBFGS_LR = 1.0\n",
    "LBFGS_MAX_EVAL = 30    \n",
    "\n",
    "#DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- 1. SETUP PARAMETERS (List of Scalars) ---\n",
    "# Truth: [4.18, 1.94, 0.24, -3.97, 0.014, -0.20, -0.85]\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lat = 0.154 \n",
    "init_range_lon = 0.195\n",
    "init_advec_lat = 0.0218\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "# Map model parameters to the 'phi' reparameterization\n",
    "init_phi2 = 1.0 / init_range_lon                # 1/range_lon\n",
    "init_phi1 = init_sigmasq * init_phi2            # sigmasq / range_lon\n",
    "init_phi3 = (init_range_lon / init_range_lat)**2  # (range_lon / range_lat)^2\n",
    "init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "# Create Initial Parameters (Float64, Requires Grad)\n",
    "initial_vals = [np.log(init_phi1), np.log(init_phi2), np.log(init_phi3), \n",
    "                np.log(init_phi4), init_advec_lat, init_advec_lon, np.log(init_nugget)]\n",
    "\n",
    "# [4.2042, 1.6348, 0.4721, -3.2695, 0.0218, -0.1689, -1.3984]\n",
    "params_list = [\n",
    "    torch.tensor([val], requires_grad=True, dtype=torch.float64, device=DEVICE)\n",
    "    for val in initial_vals\n",
    "]\n",
    "\n",
    "# --- 2. INSTANTIATE MODEL ---\n",
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'--- Initializing VecchiaBatched Model ---')\n",
    "print(f'{\"=\"*40}')\n",
    "\n",
    "if isinstance(daily_aggregated_irr_vecc_sim, torch.Tensor):\n",
    "    daily_aggregated_irr_vecc_sim = daily_aggregated_irr_vecc_sim.to(DEVICE)\n",
    "\n",
    "# Instantiate\n",
    "model_instance = kernels_ar.fit_vecchia_lbfgs(\n",
    "    smooth=v,\n",
    "    #input_map=daily_hourly_maps_vecc_sim[0],\n",
    "    #aggregated_data= daily_aggregated_tensors_vecc_sim[0],\n",
    "\n",
    "    input_map=daily_hourly_maps_reg_vecc_sim[0],\n",
    "    aggregated_data= daily_aggregated_reg_vecc_sim[0],\n",
    "    nns_map=nns_map,\n",
    "    mm_cond_number=mm_cond_number,\n",
    "    nheads=nheads\n",
    ")\n",
    "\n",
    "'''\n",
    "model_instance = kernels_reparam_space_time_gpu_col.fit_vecchia_lbfgs(\n",
    "    smooth=v,\n",
    "    #input_map=daily_hourly_maps_vecc_sim[0],\n",
    "    #aggregated_data= daily_aggregated_tensors_vecc_sim[0],\n",
    "\n",
    "    input_map=daily_hourly_maps_irr_vecc_sim[0],\n",
    "    aggregated_data= daily_aggregated_irr_vecc_sim[0],\n",
    "\n",
    "    nns_map=None,\n",
    "    mm_cond_number=mm_cond_number\n",
    ")\n",
    "''' \n",
    "\n",
    "# --- 3. OPTIMIZATION LOOP ---\n",
    "print(f'\\n{\"=\"*40}')\n",
    "print(f'--- Running L-BFGS Optimization ---')\n",
    "print(f'{\"=\"*40}')\n",
    "\n",
    "# Optimizer takes the LIST of scalars\n",
    "optimizer_vecc = model_instance.set_optimizer(\n",
    "            params_list,     \n",
    "            lr=LBFGS_LR,            \n",
    "            max_iter=LBFGS_MAX_EVAL,        \n",
    "            history_size=LBFGS_HISTORY_SIZE \n",
    "        )\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "out, steps_ran = model_instance.fit_vecc_lbfgs(\n",
    "        params_list,\n",
    "        optimizer_vecc,\n",
    "        # covariance_function argument is GONE\n",
    "        max_steps=LBFGS_MAX_STEPS, \n",
    "        grad_tol=1e-7\n",
    "    )\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "epoch_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nOptimization finished in {epoch_time:.2f}s.\")\n",
    "print(f\"Results after {steps_ran} steps: {out}\")\n",
    "print(f\"Final Params: {torch.cat(params_list).detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b08c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "irr\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 13.190530313067597, 'range_lon': 0.19707111772283428, 'range_lat': 0.1527527611913852, 'range_time': 0.9929672518527729, 'advec_lat': -0.02957260581397776, 'advec_lon': -0.16218858835449518, 'nugget': 0.23616033677518194}\n",
    "\n",
    "Optimization finished in 12.21s.\n",
    "Results after 2 steps: [4.203689783333332, 1.6241906117210854, 0.5094879856667579, -3.234266034480146, -0.02957260581397776, -0.16218858835449518, -1.4432443115243747, 1.1447534519147522]\n",
    "Final Params: [ 4.20368978  1.62419061  0.50948799 -3.23426603 -0.02957261 -0.16218859\n",
    " -1.44324431]\n",
    "\n",
    "reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63028aaf",
   "metadata": {},
   "source": [
    "dw\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 13.048806443143716, 'range_lon': 0.2759775584138689, 'range_lat': 0.26948439045514627, 'range_time': 1.2123116662337672, 'advec_lat': 0.003713422682726235, 'advec_lon': -0.18035976962575426, 'nugget': 4.6861836446362183e-08}\n",
    "\n",
    "\n",
    "dw_sim\n",
    "Final Interpretable Params: {'sigma_sq': 11.817838113249758, 'range_lon': 0.22621206038334832, 'range_lat': 0.17422723320268027, 'range_time': 0.997943651842861, 'advec_lat': -0.050699303814975866, 'advec_lon': -0.18116598528943043, 'nugget': 1.1984570483507146}\n",
    "\n",
    "\n",
    "vecc\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 13.719100200518179, 'range_lon': 0.32214277436468963, 'range_lat': 0.32239904147765264, 'range_time': 1.4623993761954481, 'advec_lat': -0.03314564565869727, 'advec_lon': -0.25639209965916504, 'nugget': 0.38030028721574305}\n",
    "\n",
    "\n",
    "vecc_sim\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 12.967334679138654, 'range_lon': 0.1829644675670931, 'range_lat': 0.14094812906918186, 'range_time': 0.9219567362979819, 'advec_lat': -0.04561716644679645, 'advec_lon': -0.17524213405286115, 'nugget': 0.1315329566135699}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a644f0",
   "metadata": {},
   "source": [
    "# TRUE PARAMETERS\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lon = 0.195 \n",
    "init_range_lat = 0.154 \n",
    "init_advec_lat = 0.0418\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "\n",
    "irregular grid\n",
    "\n",
    "Final Interpretable Params: {\n",
    "   'sigma_sq': 13.1581147614216, \n",
    "   'range_lon': 0.1670857937279293, \n",
    "   'range_lat': 0.13532624419142394, \n",
    "   'range_time': 0.662083817877393, \n",
    "   'advec_lat': 4.485362408040999, \n",
    "   'advec_lon': -0.05054465623325784, \n",
    "   'nugget': 3.202037263471046e-05}\n",
    "\n",
    "regular grid\n",
    "\n",
    "Final Interpretable Params: {\n",
    "   'sigma_sq': 13.547646279613172, \n",
    "   'range_lon': 0.2063608511650159, \n",
    "   'range_lat': 0.1698378442441089, \n",
    "   'range_time': 1.122662499886297, \n",
    "   'advec_lat': 0.05415059834603563, \n",
    "   'advec_lon': -0.16640862287821778, \n",
    "   'nugget': 0.48795387825462566}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c961fa4",
   "metadata": {},
   "source": [
    "# fit dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9b3e3",
   "metadata": {},
   "source": [
    "difference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d43435c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([142832, 4])\n",
      "142832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.8000e-02,  1.2305e+02,  0.0000e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2311e+02,  1.9113e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2317e+02,  3.1846e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2323e+02,  9.3044e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2330e+02,  7.5384e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2336e+02, -8.2040e-01,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2342e+02, -1.2852e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2349e+02, -3.2619e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2355e+02, -6.2629e-01,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2361e+02,  1.3124e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2368e+02,  4.2001e-01,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2374e+02, -4.3216e-01,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2380e+02, -5.5636e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2386e+02, -2.0360e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2393e+02,  4.5638e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2399e+02, -5.2148e-02,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2405e+02, -3.7626e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2412e+02,  3.4245e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2418e+02,  5.3648e+00,  2.1000e+01],\n",
       "        [ 2.8000e-02,  1.2424e+02,  1.9309e+00,  2.1000e+01]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [11.0474, 0.0623, 0.2445, 1.0972, 0.0101, -0.1671, 1.1825]\n",
    "day = 0 # 0 index\n",
    "lat_range= [0,5]\n",
    "lon_range= [123.0, 133.0]\n",
    "#lat_range= [1,3]\n",
    "#lon_range= [125, 129.0]\n",
    "\n",
    "daily_aggregated_tensors_dw = [aggregated_data]\n",
    "daily_hourly_maps_dw = [input_map]\n",
    "\n",
    "db = debiased_whittle.debiased_whittle_preprocess(daily_aggregated_tensors_dw, daily_hourly_maps_dw, day_idx=day, params_list=a, lat_range=lat_range, lon_range=lon_range)\n",
    "\n",
    "\n",
    "subsetted_aggregated_day = db.generate_spatially_filtered_days(0,5,123,133)\n",
    "print(subsetted_aggregated_day.shape)\n",
    "N2= subsetted_aggregated_day.shape[0]\n",
    "print(N2)\n",
    "subsetted_aggregated_day[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2497b8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Pre-computing J-vector (Hamming taper)...\n",
      "Pre-computing sample periodogram...\n",
      "Pre-computing Hamming taper autocorrelation...\n",
      "Data grid: 113x158, 8 time points. J-vector, Periodogram, Taper Autocorr on cpu.\n",
      "\n",
      "============================== Initialization Run 1/1 ==============================\n",
      "Starting with FIXED params (raw log-scale): [4.2042, 1.6348, 0.4721, -2.5562, 0.0218, -0.1689, -1.3984]\n",
      "Starting optimization run 1 on device cpu (Hamming, 7-param ST kernel, L-BFGS)...\n",
      "--- Step 1/20 ---\n",
      " Loss: 1.906929 | Max Grad: 6.274846e-04\n",
      "  Params (Raw Log): log_phi1: 4.3182, log_phi2: 1.7745, log_phi3: 0.0776, log_phi4: -3.5323, advec_lat: 0.0363, advec_lon: -0.1510, log_nugget: -1.0186\n",
      "--- Step 2/20 ---\n",
      " Loss: 1.817385 | Max Grad: 4.095367e-05\n",
      "  Params (Raw Log): log_phi1: 4.3182, log_phi2: 1.7744, log_phi3: 0.0775, log_phi4: -3.5320, advec_lat: 0.0363, advec_lon: -0.1510, log_nugget: -1.0177\n",
      "--- Step 3/20 ---\n",
      " Loss: 1.817385 | Max Grad: 4.095367e-05\n",
      "  Params (Raw Log): log_phi1: 4.3182, log_phi2: 1.7744, log_phi3: 0.0775, log_phi4: -3.5320, advec_lat: 0.0363, advec_lon: -0.1510, log_nugget: -1.0177\n",
      "--- Step 4/20 ---\n",
      " Loss: 1.817385 | Max Grad: 4.095367e-05\n",
      "  Params (Raw Log): log_phi1: 4.3182, log_phi2: 1.7744, log_phi3: 0.0775, log_phi4: -3.5320, advec_lat: 0.0363, advec_lon: -0.1510, log_nugget: -1.0177\n",
      "\n",
      "--- Converged on loss change (change < 1e-12) at step 4 ---\n",
      "\n",
      "--- Training Complete ---\n",
      "\n",
      "FINAL BEST STATE ACHIEVED (during training):\n",
      "Best Loss: 1.817\n",
      "\n",
      "\n",
      "========================= Overall Result from Run ========================= =========================\n",
      "Best Run Loss: 1.817 (after 4 steps)\n",
      "Final Parameters (Natural Scale): sigmasq: 12.7277, range_lat: 0.1631, range_lon: 0.1696, range_time: 0.9917, advec_lat: 0.0363, advec_lon: -0.1510, nugget: 0.3614\n",
      "Final Parameters (Phi Scale)    : phi1: 75.0512, phi2: 5.8967, phi3: 1.0806, phi4: 0.0292, advec_lat: 0.0363, advec_lon: -0.1510, nugget: 0.3614\n",
      "Final Parameters (Raw Log Scale): log_phi1: 4.3182, log_phi2: 1.7744, log_phi3: 0.0775, log_phi4: -3.5320, advec_lat: 0.0363, advec_lon: -0.1510, log_nugget: -1.0177\n",
      "\n",
      "Total execution time: 40.40 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dwl = debiased_whittle.debiased_whittle_likelihood()\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    DAY_TO_RUN = 3 # data is decided above\n",
    "    TAPERING_FUNC = dwl.cgn_hamming # Use Hamming taper\n",
    "    NUM_RUNS = 1\n",
    "    MAX_STEPS = 20 # L-BFGS usually converges in far fewer steps\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063 \n",
    "\n",
    "    LAT_COL, LON_COL = 0, 1\n",
    "    VAL_COL = 2 # Spatially differenced value\n",
    "    TIME_COL = 3\n",
    "\n",
    "\n",
    "    cur_df =subsetted_aggregated_day\n",
    "    \n",
    "    if cur_df.numel() == 0 or cur_df.shape[1] <= max(LAT_COL, LON_COL, VAL_COL, TIME_COL):\n",
    "        print(f\"Error: Data for Day {DAY_TO_RUN} is empty or invalid.\")\n",
    "        exit()\n",
    "\n",
    "    unique_times = torch.unique(cur_df[:, TIME_COL])\n",
    "    time_slices_list = [cur_df[cur_df[:, TIME_COL] == t_val] for t_val in unique_times]\n",
    "\n",
    "    # --- 1. Pre-compute J-vector, Taper Grid, and Taper Autocorrelation ---\n",
    "    print(\"Pre-computing J-vector (Hamming taper)...\")\n",
    "    \n",
    "    # --- üí• REVISED: Renamed 'p' to 'p_time' üí• ---\n",
    "    J_vec, n1, n2, p_time, taper_grid = dwl.generate_Jvector_tapered( \n",
    "        time_slices_list,\n",
    "        tapering_func=TAPERING_FUNC, \n",
    "        lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if J_vec is None or J_vec.numel() == 0 or n1 == 0 or n2 == 0 or p_time == 0:\n",
    "       print(f\"Error: J-vector generation failed for Day {DAY_TO_RUN}.\")\n",
    "       exit()\n",
    "       \n",
    "    print(\"Pre-computing sample periodogram...\")\n",
    "    I_sample = dwl.calculate_sample_periodogram_vectorized(J_vec)\n",
    "\n",
    "    print(\"Pre-computing Hamming taper autocorrelation...\")\n",
    "    taper_autocorr_grid = dwl.calculate_taper_autocorrelation_fft(taper_grid, n1, n2, DEVICE)\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Error: NaN/Inf in sample periodogram.\")\n",
    "        exit()\n",
    "    if torch.isnan(taper_autocorr_grid).any() or torch.isinf(taper_autocorr_grid).any():\n",
    "        print(\"Error: NaN/Inf in taper autocorrelation.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Data grid: {n1}x{n2}, {p_time} time points. J-vector, Periodogram, Taper Autocorr on {DEVICE}.\")\n",
    "    # --- END REVISION ---\n",
    "\n",
    "    # --- 2. Optimization Loop ---\n",
    "    all_final_results = []\n",
    "    all_final_losses = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        print(f\"\\n{'='*30} Initialization Run {i+1}/{NUM_RUNS} {'='*30}\")\n",
    "\n",
    "        # --- 7-PARAMETER initialization ---\n",
    "        ''' \n",
    "        init_sigmasq   = 15.0\n",
    "        init_range_lat = 0.66 \n",
    "        init_range_lon = 0.7 \n",
    "        init_nugget    = 1.5\n",
    "        init_beta      = 0.1  # Temporal range ratio\n",
    "        init_advec_lat = 0.02\n",
    "        init_advec_lon = -0.08\n",
    "        '''\n",
    "        init_sigmasq   = 13.059\n",
    "        init_range_lat = 0.154 \n",
    "        init_range_lon = 0.195\n",
    "        init_advec_lat = 0.0218\n",
    "        init_range_time = 0.7\n",
    "        init_advec_lon = -0.1689\n",
    "        init_nugget    = 0.247\n",
    "\n",
    "        init_phi2 = 1.0 / init_range_lon\n",
    "        init_phi1 = init_sigmasq * init_phi2\n",
    "        init_phi3 = (init_range_lon / init_range_lat)**2\n",
    "        # Change needed to match the spatial-temporal distance formula:\n",
    "        init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "        initial_params_values = [\n",
    "            np.log(init_phi1),    # [0] log_phi1\n",
    "            np.log(init_phi2),    # [1] log_phi2\n",
    "            np.log(init_phi3),    # [2] log_phi3\n",
    "            np.log(init_phi4),    # [3] log_phi4\n",
    "            init_advec_lat,       # [4] advec_lat (NOT log)\n",
    "            init_advec_lon,       # [5] advec_lon (NOT log)\n",
    "            np.log(init_nugget)   # [6] log_nugget\n",
    "        ]\n",
    "        \n",
    "        print(f\"Starting with FIXED params (raw log-scale): {[round(p, 4) for p in initial_params_values]}\")\n",
    "\n",
    "        params_list = [\n",
    "            Parameter(torch.tensor([val], dtype=torch.float64))\n",
    "            for val in initial_params_values\n",
    "        ]\n",
    "\n",
    "        # Helper to define the boundary globally for clarity\n",
    "        NUGGET_LOWER_BOUND = 0.05\n",
    "        LOG_NUGGET_LOWER_BOUND = np.log(NUGGET_LOWER_BOUND) # Approx -2.9957\n",
    "\n",
    "        # --- üí• REVISED: Use L-BFGS Optimizer üí• ---\n",
    "        optimizer = torch.optim.LBFGS(\n",
    "            params_list,\n",
    "            lr=1.0,           # Initial step length for line search\n",
    "            max_iter=20,      # Iterations per step\n",
    "            history_size=100,\n",
    "            line_search_fn=\"strong_wolfe\", # Often more robust\n",
    "            tolerance_grad=1e-5\n",
    "        )\n",
    "        # --- END REVISION ---\n",
    "\n",
    "        print(f\"Starting optimization run {i+1} on device {DEVICE} (Hamming, 7-param ST kernel, L-BFGS)...\")\n",
    "        \n",
    "        # --- üí• REVISED: Call L-BFGS trainer, pass p_time üí• ---\n",
    "        nat_params_str, phi_params_str, raw_params_str, loss, steps_run = dwl.run_lbfgs_tapered(\n",
    "            params_list=params_list,\n",
    "            optimizer=optimizer,\n",
    "            I_sample=I_sample,\n",
    "            n1=n1, n2=n2, p_time=p_time,\n",
    "            taper_autocorr_grid=taper_autocorr_grid, \n",
    "            max_steps=MAX_STEPS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        # --- END REVISION ---\n",
    "        \n",
    "        if loss is not None:\n",
    "            all_final_results.append((nat_params_str, phi_params_str, raw_params_str))\n",
    "            all_final_losses.append(loss)\n",
    "        else:\n",
    "            all_final_losses.append(float('inf'))\n",
    "\n",
    "    print(f\"\\n\\n{'='*25} Overall Result from Run {'='*25} {'='*25}\")\n",
    "    \n",
    "    valid_losses = [l for l in all_final_losses if l is not None and l != float('inf')]\n",
    "\n",
    "    if not valid_losses:\n",
    "        print(f\"The run failed or resulted in an invalid loss for Day {DAY_TO_RUN}.\")\n",
    "    else:\n",
    "        best_loss = min(valid_losses)\n",
    "        best_run_index = all_final_losses.index(best_loss)\n",
    "        best_results = all_final_results[best_run_index]\n",
    "        \n",
    "        print(f\"Best Run Loss: {best_loss} (after {steps_run} steps)\")\n",
    "        print(f\"Final Parameters (Natural Scale): {best_results[0]}\")\n",
    "        print(f\"Final Parameters (Phi Scale)    : {best_results[1]}\")\n",
    "        print(f\"Final Parameters (Raw Log Scale): {best_results[2]}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61060411",
   "metadata": {},
   "source": [
    "init_sigmasq   = 13.059\n",
    "init_range_lon = 0.195 \n",
    "init_range_lat = 0.154 \n",
    "init_advec_lat = 0.0418\n",
    "init_range_time = 1.0\n",
    "init_advec_lon = -0.1689\n",
    "init_nugget    = 0.247\n",
    "\n",
    "1 st simulation (1 vs 3)\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 12.939260896610579, 'range_lon': 0.17242543281507933, 'range_lat': 0.1631231346200311, 'range_time': 1.1358209227858689, 'advec_lat': 0.045119029109988974, 'advec_lon': -0.17809677784942035, 'nugget': 0.3009582276680578}\n",
    "\n",
    "Final Parameters (Natural Scale): sigmasq: 13.2415, range_lat: 0.1685, range_lon: 0.1739, range_time: 0.9681, advec_lat: 0.0395, advec_lon: -0.1732, nugget: 0.3216\n",
    "\n",
    "2nd simulation (1 vs 3)\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 12.765293287952144, 'range_lon': 0.17039368470743024, 'range_lat': 0.16152132799710625, 'range_time': 1.081124889959751, 'advec_lat': 0.04635511983762563, 'advec_lon': -0.1775715292039452, 'nugget': 0.31503884896742074}\n",
    "\n",
    "\n",
    "sigmasq: 13.2415, range_lat: 0.1685, range_lon: 0.1739, range_time: 0.9681, advec_lat: 0.0395, advec_lon: -0.1732, nugget: 0.3216\n",
    "\n",
    "3nd simulation (mm 15 two times)\n",
    "\n",
    "Final Interpretable Params: {'sigma_sq': 12.292570063933866, 'range_lon': 0.16170186578939172, 'range_lat': 0.1537053192245325, 'range_time': 0.9671689003322674, 'advec_lat': 0.04155171168950867, 'advec_lon': -0.16218073366456207, 'nugget': 0.29945872751676345}\n",
    "\n",
    "Final Parameters (Natural Scale): sigmasq: 12.7277, range_lat: 0.1631, range_lon: 0.1696, range_time: 0.9917, advec_lat: 0.0363, advec_lon: -0.1510, nugget: 0.3614\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
