{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b8b1cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy                    # clone tensor\n",
    "import time\n",
    "\n",
    "# Custom imports\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels\n",
    "from GEMS_TCO import data_preprocess \n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "import typer\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO.data_loader import load_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03c9da5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting data to lat: [0.0, 5.0], lon: [123.0, 133.0]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "# Assuming 'config' and 'load_data' class are defined and imported elsewhere\n",
    "\n",
    "# --- Parameters derived from your framework ---\n",
    "v: float = 0.5\n",
    "space: List[str] = ['4', '4']\n",
    "days: List[str] = ['0', '31']\n",
    "mm_cond_number: int = 20\n",
    "# --- End of framework parameters ---\n",
    "\n",
    "lat_lon_resolution = [int(s) for s in space]\n",
    "days_s_e = [int(d) for d in days]\n",
    "days_list = list(range(days_s_e[0], days_s_e[1]))\n",
    "\n",
    "# These values were not in the framework, so they remain as set in your snippet\n",
    "years = ['2024']\n",
    "month_range = [7] \n",
    "\n",
    "# Assuming 'config' is available in your environment\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "\n",
    "## load ozone data from amarel\n",
    "data_load_instance = load_data2(config.mac_data_load_path)\n",
    "\n",
    "# Call the function using the variables from the framework\n",
    "df_map, ord_mm, nns_map = data_load_instance.load_maxmin_ordered_data_bymonthyear(\n",
    "lat_lon_resolution=lat_lon_resolution, \n",
    "mm_cond_number=mm_cond_number,\n",
    "years_=years, \n",
    "months_=month_range,\n",
    "lat_range=[0.0, 5.0],      # <-- Add this\n",
    "lon_range=[123.0, 133.0]   # <-- Add this\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e09bdf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8960, 4])\n"
     ]
    }
   ],
   "source": [
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "for i in range(31):\n",
    "    idx_for_datamap = [i*8, (i+1)*8]\n",
    "    cur_map, cur_df = analysis_map_no_mm, agg_data_no_mm = data_load_instance.load_working_data(\n",
    "    df_map, \n",
    "    idx_for_datamap, \n",
    "    ord_mm=None,  # or just omit it\n",
    "    dtype=torch.float # or just omit it\n",
    ")\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "print(df_day_aggregated_list[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c8a0ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15995.6299, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance1 = kernels.vecchia_experiment(0.5, df_day_map_list[0], df_day_aggregated_list[0], nns_map, mm_cond_number, nheads=10)\n",
    "\n",
    "a = [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]\n",
    "#a = [30.2594, 0.665, 1.8981, 0.0, 0.1317, -0.0, 1.9785]\n",
    "#a = [45.1402, 0.6299, 0.7308, -0.0003, -0.0151, 0.0, 7.8922]\n",
    "#a = [21.7335, 1.2817, 1.5946, 0.042, -0.1241, 0.218, 4.8654]\n",
    "#a = [20.453542336448137, 1.4506118600616982, 2.43096923637867, -0.03476556019978718, -0.1559262606484541, 0.1254833595232136, 3.938183829354925]\n",
    "params = torch.tensor(a, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "cov_map = instance1.cov_structure_saver(params, instance1.matern_cov_anisotropy_v05)  \n",
    "instance1.vecchia_oct22( params, instance1.matern_cov_anisotropy_v05, cov_map )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7499a13",
   "metadata": {},
   "source": [
    "v = 0.5 # smooth\n",
    "mm_cond_number = 20\n",
    "nheads = 20\n",
    "lr = 0.01\n",
    "step = 80\n",
    "gamma_par = 0.9\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c05cf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0.5 # smooth\n",
    "mm_cond_number = 20\n",
    "nheads = 300\n",
    "lr = 0.02\n",
    "step = 100\n",
    "gamma_par = 0.3\n",
    "epochs = 900"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afd1495",
   "metadata": {},
   "source": [
    "# fit 8 hours of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d2d747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Main Optimization Loop (Updated)\n",
    "# ------------------------------------\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "num_days_to_load = 31 # From reference code\n",
    "\n",
    "for i in range(num_days_to_load):\n",
    "    idx_for_datamap = [i*8, (i+1)*8]\n",
    "    # Using the new load function from the reference code\n",
    "    cur_map, cur_df = data_load_instance.load_working_data(\n",
    "        df_map, \n",
    "        idx_for_datamap, \n",
    "        ord_mm=None,\n",
    "        # Using float64 to ensure compatibility with kernels\n",
    "        dtype=torch.float64 \n",
    "    )\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "print(f\"Data loaded for {len(df_day_map_list)} days.\")\n",
    "# print(df_day_aggregated_list[0].shape) # From reference code\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "\n",
    "# This list is now just for iterating\n",
    "#days_list = range(len(df_day_map_list)) \n",
    "days_list = [0]\n",
    "for day in days_list:  \n",
    "    \n",
    "    # Get the pre-loaded data for this day\n",
    "    analysis_data_map = df_day_map_list[day]\n",
    "    aggregated_data = df_day_aggregated_list[day]\n",
    "\n",
    "    a = [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]\n",
    "    a = [28, 0.65, 0.75, 0.022, -0.144, 0.198, 2.5]\n",
    "    params = torch.tensor(a, dtype=torch.float64, requires_grad=True)\n",
    "    \n",
    "    # Calculate resolution for printing\n",
    "    res_calc = (200 / lat_lon_resolution[0]) * (100 / lat_lon_resolution[0])\n",
    "    print(f'\\n--- Starting Day {day+1} (2024-07-{day+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params.detach().numpy()}')\n",
    "            \n",
    "    # --- Data loading is now done *before* the loop ---\n",
    "\n",
    "    # We need to define the device (though we aren't passing it anymore)\n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model_instance = kernels.model_fitting(\n",
    "            smooth = v,\n",
    "            input_map = analysis_data_map,\n",
    "            aggregated_data = aggregated_data,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "            # device = device_str  <--- REMOVED: This was causing the TypeError\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Adjusted optimizer call based on expected return values (step size changed to step)\n",
    "    optimizer, scheduler = model_instance.optimizer_fun(\n",
    "        params, \n",
    "        lr=lr, \n",
    "        betas=(0.9, 0.8), \n",
    "        eps=1e-8, \n",
    "        step_size=step, # Using the 'step' variable here\n",
    "        gamma=gamma_par  # Using gamma_par\n",
    "    ) \n",
    "\n",
    "    # --- CRITICAL CORRECTION ---\n",
    "    # 1. We no longer need to create a separate 'instance_map'.\n",
    "    #    'model_instance' is already the correct instance.\n",
    "    # 2. We do NOT pre-calculate 'cov_map'. The optimized training loop\n",
    "    #    'run_vecc_may9' does this internally on each epoch\n",
    "    #    to ensure the gradients are correct.\n",
    "    # 3. We call 'run_vecc_may9' (the optimized loop) instead of 'run_vecc_grp9'.\n",
    "    #    This version does not take 'cov_map' as an argument.\n",
    "    \n",
    "    # Calling the optimized 'run_vecc_may9'\n",
    "    out, epoch_ran = model_instance.run_vecc_oct22(\n",
    "        params, \n",
    "        optimizer,\n",
    "        scheduler, \n",
    "        model_instance.matern_cov_anisotropy_v05, \n",
    "        epochs=epochs\n",
    "    )\n",
    "    # --- End Correction ---\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"Day {day+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day+1} final results: {out}\")\n",
    "\n",
    "print(\"\\n--- All Days Processed ---\")\n",
    "\n",
    "\n",
    "# 5000 takes 47 mn\n",
    "# Day 1 final results: [29.7507942474575, 0.9895617352853928, 1.0668105024738395, 0.03646879682204495, -0.15565418931819383, 0.17944566056749772, 1.8907510307884106, 64567.25091631662]\n",
    "# 1250 take 10mn\n",
    "#  29.89872140748479, 1.1529714369768411, 1.7862356661866714, 0.03927727761515986, -0.15656505052873793, 0.1320573870050866, 4.130349222670235, 15749.646109280728"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d702e3da",
   "metadata": {},
   "source": [
    "# try just fitting one hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cfc780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading data for all days...\n",
      "Data loaded for 31 days.\n",
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 1250.0, smooth: 0.5\n",
      "mm_cond_number: 20,\n",
      "initial parameters: \n",
      " [tensor([28.7500], dtype=torch.float64, requires_grad=True), tensor([0.9800], dtype=torch.float64, requires_grad=True), tensor([1.0600], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([1.8900], dtype=torch.float64, requires_grad=True)]\n",
      "Converged at epoch 701\n",
      "Epoch 702,  \n",
      " vecc Parameters: [ 2.81337847e+01  9.07524208e-01  1.81091766e+00 -4.03066803e-06\n",
      "  5.84709994e-06  0.00000000e+00  1.51297605e+00]\n",
      "FINAL STATE: Epoch 702, Loss: 2087.696500414076, \n",
      " vecc Parameters: [28.133784739040472, 0.9075242075841814, 1.8109176606611543, -4.030668029920314e-06, 5.847099939599732e-06, 0.0, 1.512976047183501]\n",
      "Day 1 optimization finished in 155.04s over 702 epochs.\n",
      "Day 1 final results: [28.133784739040472, 0.9075242075841814, 1.8109176606611543, -4.030668029920314e-06, 5.847099939599732e-06, 0.0, 1.512976047183501, 2087.696500414076]\n",
      "\n",
      "--- All Days Processed ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ------------------------------------\n",
    "# Main Optimization Loop (Updated)\n",
    "# ------------------------------------\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "num_days_to_load = 31 # From reference code\n",
    "\n",
    "for i in range(num_days_to_load):\n",
    "    idx_for_datamap = [i*8, i*8+1]\n",
    "    # Using the new load function from the reference code\n",
    "    cur_map, cur_df = data_load_instance.load_working_data(\n",
    "        df_map, \n",
    "        idx_for_datamap, \n",
    "        ord_mm=None,\n",
    "        # Using float64 to ensure compatibility with kernels\n",
    "        dtype=torch.float64 \n",
    "    )\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "print(f\"Data loaded for {len(df_day_map_list)} days.\")\n",
    "# print(df_day_aggregated_list[0].shape) # From reference code\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "\n",
    "# This list is now just for iterating\n",
    "#days_list = range(len(df_day_map_list)) \n",
    "days_list = [0]\n",
    "for day in days_list:  \n",
    "    \n",
    "    # Get the pre-loaded data for this day\n",
    "    analysis_data_map = df_day_map_list[day]\n",
    "    aggregated_data = df_day_aggregated_list[day]\n",
    "\n",
    "    \n",
    "    #a = [36.75, 0.98, 1.06, 0, 0, 0, 1.890]\n",
    "    #params = torch.tensor(a, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "    a = [22, 0.98, 1.06, 0, 0, 0, 1.890]\n",
    "    a = [28.75, 0.98, 1.06, 0.036, -0.155, 0.179, 1.890]\n",
    "    a = [28.75, 0.98, 1.06, 0, 0, 0, 1.890]\n",
    "    #a = [21, 1, 1.5, 0,0,0,4.769]\n",
    "    # NEW: Define params as a list of 1-element tensors\n",
    "    # This is required to group them for the optimizer.\n",
    "    params_list = [\n",
    "        torch.tensor([val], dtype=torch.float64, requires_grad=True) for val in a\n",
    "    ]\n",
    "\n",
    "    \n",
    "\n",
    "    # NEW: Define learning rates and groups exactly as in your example\n",
    "    lr_slow, lr_fast = 0.02, 0.02\n",
    "    slow_indices = [ 1, 2, 3,4,5,6]\n",
    "    fast_indices = [0]\n",
    "\n",
    "    param_groups = [\n",
    "        {'params': [params_list[idx] for idx in slow_indices], 'lr': lr_slow, 'name': 'slow_group'},\n",
    "        {'params': [params_list[idx] for idx in fast_indices], 'lr': lr_fast, 'name': 'fast_group'}\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Calculate resolution for printing\n",
    "    res_calc = (200 / lat_lon_resolution[0]) * (100 / lat_lon_resolution[0])\n",
    "    print(f'\\n--- Starting Day {day+1} (2024-07-{day+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params_list}')\n",
    "            \n",
    "    # --- Data loading is now done *before* the loop ---\n",
    "\n",
    "    # We need to define the device (though we aren't passing it anymore)\n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model_instance = kernels.model_fitting(\n",
    "            smooth = v,\n",
    "            input_map = analysis_data_map,\n",
    "            aggregated_data = aggregated_data,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "            # device = device_str  <--- REMOVED: This was causing the TypeError\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Adjusted optimizer call based on expected return values (step size changed to step)\n",
    "    optimizer, scheduler = model_instance.optimizer_fun10_23(\n",
    "            param_groups,     # <--- Pass the new param_groups list\n",
    "            lr=lr,            # <--- This is now just a default\n",
    "            betas=(0.9, 0.8), \n",
    "            eps=1e-8, \n",
    "            step_size=step, \n",
    "            gamma=gamma_par\n",
    "        )\n",
    "\n",
    "    # --- CRITICAL CORRECTION ---\n",
    "    # 1. We no longer need to create a separate 'instance_map'.\n",
    "    #    'model_instance' is already the correct instance.\n",
    "    # 2. We do NOT pre-calculate 'cov_map'. The optimized training loop\n",
    "    #    'run_vecc_may9' does this internally on each epoch\n",
    "    #    to ensure the gradients are correct.\n",
    "    # 3. We call 'run_vecc_may9' (the optimized loop) instead of 'run_vecc_grp9'.\n",
    "    #    This version does not take 'cov_map' as an argument.\n",
    "    \n",
    "    # Calling the optimized 'run_vecc_may9'\n",
    "    out, epoch_ran = model_instance.run_vecc_oct23(\n",
    "            params_list,     # <--- Pass the new params_list\n",
    "            optimizer,\n",
    "            scheduler, \n",
    "            model_instance.matern_cov_anisotropy_v05, \n",
    "            epochs=epochs\n",
    "        )\n",
    "    # --- End Correction ---\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"Day {day+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day+1} final results: {out}\")\n",
    "\n",
    "print(\"\\n--- All Days Processed ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8050808c",
   "metadata": {},
   "source": [
    "# fit one hour old version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "186f78f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading data for all days...\n",
      "Data loaded for 31 days.\n",
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 1250.0, smooth: 0.5\n",
      "mm_cond_number: 20,\n",
      "initial parameters: \n",
      " [21.303  1.307  1.563  0.     0.     0.     4.769]\n",
      "Converged at epoch 230\n",
      "Epoch 231,  \n",
      " vecc Parameters: [ 2.20562660e+01  2.10108999e+00  3.29873099e+00 -2.36777772e-06\n",
      " -2.32200519e-07  0.00000000e+00  4.00362099e+00]\n",
      "FINAL STATE: Epoch 231, Loss: 2036.6417848718395, \n",
      " vecc Parameters: [22.056265955285376, 2.101089989181403, 3.298730986510232, -2.3677777249201222e-06, -2.3220051907190966e-07, 0.0, 4.003620989283949]\n",
      "Day 1 optimization finished in 52.15s over 231 epochs.\n",
      "Day 1 final results: [22.056265955285376, 2.101089989181403, 3.298730986510232, -2.3677777249201222e-06, -2.3220051907190966e-07, 0.0, 4.003620989283949, 2036.6417848718395]\n",
      "\n",
      "--- All Days Processed ---\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------\n",
    "# Main Optimization Loop (Updated)\n",
    "# ------------------------------------\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "num_days_to_load = 31 # From reference code\n",
    "\n",
    "for i in range(num_days_to_load):\n",
    "    idx_for_datamap = [i*8, i*8+1]\n",
    "    # Using the new load function from the reference code\n",
    "    cur_map, cur_df = data_load_instance.load_working_data(\n",
    "        df_map, \n",
    "        idx_for_datamap, \n",
    "        ord_mm=None,\n",
    "        # Using float64 to ensure compatibility with kernels\n",
    "        dtype=torch.float64 \n",
    "    )\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "print(f\"Data loaded for {len(df_day_map_list)} days.\")\n",
    "# print(df_day_aggregated_list[0].shape) # From reference code\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "\n",
    "# This list is now just for iterating\n",
    "#days_list = range(len(df_day_map_list)) \n",
    "days_list = [0]\n",
    "for day in days_list:  \n",
    "    \n",
    "    # Get the pre-loaded data for this day\n",
    "    analysis_data_map = df_day_map_list[day]\n",
    "    aggregated_data = df_day_aggregated_list[day]\n",
    "\n",
    "    a = [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]\n",
    "    a = [21.303, 1.307, 1.563, 0, 0, 0, 4.769]\n",
    "    params = torch.tensor(a, dtype=torch.float64, requires_grad=True)\n",
    "    \n",
    "    # Calculate resolution for printing\n",
    "    res_calc = (200 / lat_lon_resolution[0]) * (100 / lat_lon_resolution[0])\n",
    "    print(f'\\n--- Starting Day {day+1} (2024-07-{day+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params.detach().numpy()}')\n",
    "            \n",
    "    # --- Data loading is now done *before* the loop ---\n",
    "\n",
    "    # We need to define the device (though we aren't passing it anymore)\n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model_instance = kernels.model_fitting(\n",
    "            smooth = v,\n",
    "            input_map = analysis_data_map,\n",
    "            aggregated_data = aggregated_data,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "            # device = device_str  <--- REMOVED: This was causing the TypeError\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Adjusted optimizer call based on expected return values (step size changed to step)\n",
    "    optimizer, scheduler = model_instance.optimizer_fun(\n",
    "        params, \n",
    "        lr=lr, \n",
    "        betas=(0.9, 0.8), \n",
    "        eps=1e-8, \n",
    "        step_size=step, # Using the 'step' variable here\n",
    "        gamma=gamma_par  # Using gamma_par\n",
    "    ) \n",
    "\n",
    "    # --- CRITICAL CORRECTION ---\n",
    "    # 1. We no longer need to create a separate 'instance_map'.\n",
    "    #    'model_instance' is already the correct instance.\n",
    "    # 2. We do NOT pre-calculate 'cov_map'. The optimized training loop\n",
    "    #    'run_vecc_may9' does this internally on each epoch\n",
    "    #    to ensure the gradients are correct.\n",
    "    # 3. We call 'run_vecc_may9' (the optimized loop) instead of 'run_vecc_grp9'.\n",
    "    #    This version does not take 'cov_map' as an argument.\n",
    "    \n",
    "    # Calling the optimized 'run_vecc_may9'\n",
    "    out, epoch_ran = model_instance.run_vecc_oct22(\n",
    "        params, \n",
    "        optimizer,\n",
    "        scheduler, \n",
    "        model_instance.matern_cov_anisotropy_v05, \n",
    "        epochs=epochs\n",
    "    )\n",
    "    # --- End Correction ---\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"Day {day+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day+1} final results: {out}\")\n",
    "\n",
    "print(\"\\n--- All Days Processed ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166ed834",
   "metadata": {},
   "source": [
    "## dont use below above is much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6da9381c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 1250.0, smooth: 0.5\n",
      "mm_cond_number: 20\n",
      "Using device: cpu\n",
      "\n",
      "--- RUN: Using 'run_vecc_advanced' with STABLE StepLR ---\n",
      "Initial parameters (natural scale): \n",
      " [21.303  1.307  1.563  0.022 -0.144  0.198  4.769]\n",
      "FINAL STATE: Epoch 500, Loss: 16717.15700508694, \n",
      " vecc Parameters: [18.592316201664097, 1.4125517238033354, 1.2647947634869554, -0.10675469109734659, -0.13885701275772608, 0.2199784453104275, 1.3479833641217873]\n",
      "--- 'run_vecc_oct22' (Stable Setup) finished in 899.35s over 500 epochs ---\n",
      "Final results: [18.592316201664097, 1.4125517238033354, 1.2647947634869554, -0.10675469109734659, -0.13885701275772608, 0.2199784453104275, 1.3479833641217873, 16717.15700508694]\n",
      "\n",
      "--- All Days Processed ---\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Run optimization loop over pre-loaded data (IMPROVED) ---\n",
    "days_list = [0] # Just run for the first day\n",
    "for day in days_list:  \n",
    "    \n",
    "    # Get the pre-loaded data for this day\n",
    "    analysis_data_map = df_day_map_list[day]\n",
    "    aggregated_data = df_day_aggregated_list[day]\n",
    "\n",
    "    # --- Initial parameters in NATURAL scale (STABLE) ---\n",
    "    a_natural = [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]\n",
    "    # NOTE: The log_indices are still useful for *printing* in the advanced loop,\n",
    "    # but we will no longer use the log-transformation for initialization.\n",
    "    log_indices = [0, 1, 2, 6] \n",
    "    \n",
    "    # Calculate resolution for printing\n",
    "    res_calc = (200 / lat_lon_resolution[0]) * (100 / lat_lon_resolution[0])\n",
    "    print(f'\\n--- Starting Day {day+1} (2024-07-{day+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number}')\n",
    "            \n",
    "    # --- Determine device ---\n",
    "    # Moved device logic up for clarity, but keep the check\n",
    "    device_str = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            torch.tensor([1.0]).to('cuda')\n",
    "            device_str = 'cuda'\n",
    "            print(\"Using device: cuda\")\n",
    "        except Exception as e:\n",
    "            # Added import check for warnings\n",
    "            import warnings\n",
    "            warnings.warn(f\"CUDA available but failed to initialize. {e}. Defaulting to CPU.\")\n",
    "            device_str = 'cpu'\n",
    "            print(\"Using device: cpu\")\n",
    "    else:\n",
    "        print(\"Using device: cpu\")\n",
    "    \n",
    "    \n",
    "    # ====================================================================\n",
    "    # --- RUN: Advanced Loop with Stable Setup ---\n",
    "    # ====================================================================\n",
    "    print(f\"\\n--- RUN: Using 'run_vecc_advanced' with STABLE StepLR ---\")\n",
    "    \n",
    "    # Initialize params using NATURAL SCALE directly\n",
    "    # This avoids the numerical instability from taking the log of near-zero values.\n",
    "    # The advanced loop's printing function will convert back to natural scale.\n",
    "    params_stable = torch.tensor(a_natural, dtype=torch.float64, requires_grad=True).to(device_str)\n",
    "    print(f'Initial parameters (natural scale): \\n {params_stable.detach().cpu().numpy().round(4)}')\n",
    "    \n",
    "    # --- Model Initialization ---\n",
    "    # (Identical to previous code)\n",
    "    model_instance = kernels.model_fitting(\n",
    "            smooth = v,\n",
    "            input_map = analysis_data_map,\n",
    "            aggregated_data = aggregated_data,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads,\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- OPTIMIZER: Use StepLR for stability ---\n",
    "    optimizer, scheduler = model_instance.optimizer_fun(\n",
    "        params_stable, \n",
    "        scheduler_type='step', # Explicitly use the 'step' scheduler\n",
    "        lr=lr, \n",
    "        betas=(0.9, 0.8), \n",
    "        eps=1e-8,\n",
    "        step_size=step, # Use step=80 from your configuration\n",
    "        gamma=gamma_par # Use gamma_par=0.1 from your configuration\n",
    "    ) \n",
    "    \n",
    "    # Calling the advanced 'run_vecc_advanced' loop\n",
    "    # NOTE: The printing in this function needs to be checked. If parameters are\n",
    "    # NOT log-transformed, the printing function (get_printable_params) will\n",
    "    # apply 'torch.exp' to the log_indices, which is now incorrect.\n",
    "    #\n",
    "    # Assuming 'run_vecc_advanced' is modified to correctly handle natural-scale\n",
    "    # parameters when log_param_indices is passed but the input is not log-space.\n",
    "    # --- CRITICAL ASSUMPTION ---\n",
    "    # Since you want to use natural scale, we should NOT pass log_param_indices\n",
    "    # unless you explicitly want to apply constraints/transforms inside the loop.\n",
    "    # Since the simpler approach worked, let's revert to the minimal stable setup.\n",
    "    #\n",
    "    \n",
    "    # *** REVISED: Calling the STABLE 'run_vecc_oct22' loop ***\n",
    "    # This is the approach that worked, but with your desired high epoch count\n",
    "    out, epoch_ran = model_instance.run_vecc_oct22(\n",
    "        params_stable, # Use natural-scale params\n",
    "        optimizer,\n",
    "        scheduler, \n",
    "        model_instance.matern_cov_anisotropy_v05, \n",
    "        epochs=epochs # Use epochs=300\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"--- 'run_vecc_oct22' (Stable Setup) finished in {epoch_time:.2f}s over {epoch_ran+1} epochs ---\")\n",
    "    print(f\"Final results: {out}\")\n",
    "\n",
    "print(\"\\n--- All Days Processed ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
