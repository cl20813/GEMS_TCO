{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b8b1cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy                    # clone tensor\n",
    "import time\n",
    "\n",
    "# Custom imports\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels\n",
    "from GEMS_TCO import data_preprocess \n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "import typer\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO.data_loader import load_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03c9da5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting data to lat: [0.0, 5.0], lon: [123.0, 133.0]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "# Assuming 'config' and 'load_data' class are defined and imported elsewhere\n",
    "\n",
    "# --- Parameters derived from your framework ---\n",
    "v: float = 0.5\n",
    "space: List[str] = ['4', '4']\n",
    "days: List[str] = ['0', '31']\n",
    "mm_cond_number: int = 20\n",
    "# --- End of framework parameters ---\n",
    "\n",
    "lat_lon_resolution = [int(s) for s in space]\n",
    "days_s_e = [int(d) for d in days]\n",
    "days_list = list(range(days_s_e[0], days_s_e[1]))\n",
    "\n",
    "# These values were not in the framework, so they remain as set in your snippet\n",
    "years = ['2024']\n",
    "month_range = [7] \n",
    "\n",
    "# Assuming 'config' is available in your environment\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "\n",
    "## load ozone data from amarel\n",
    "data_load_instance = load_data2(config.mac_data_load_path)\n",
    "\n",
    "# Call the function using the variables from the framework\n",
    "df_map, ord_mm, nns_map = data_load_instance.load_maxmin_ordered_data_bymonthyear(\n",
    "lat_lon_resolution=lat_lon_resolution, \n",
    "mm_cond_number=mm_cond_number,\n",
    "years_=years, \n",
    "months_=month_range,\n",
    "lat_range=[0.0, 5.0],      # <-- Add this\n",
    "lon_range=[123.0, 133.0]   # <-- Add this\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e09bdf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8960, 4])\n"
     ]
    }
   ],
   "source": [
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "for i in range(31):\n",
    "    idx_for_datamap = [i*8, (i+1)*8]\n",
    "    cur_map, cur_df = analysis_map_no_mm, agg_data_no_mm = data_load_instance.load_working_data(\n",
    "    df_map, \n",
    "    idx_for_datamap, \n",
    "    ord_mm=None,  # or just omit it\n",
    "    dtype=torch.float # or just omit it\n",
    ")\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "print(df_day_aggregated_list[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c8a0ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20832.3250, dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance1 = kernels.vecchia_experiment(0.5, df_day_map_list[0], df_day_aggregated_list[0], nns_map, mm_cond_number, nheads=10)\n",
    "\n",
    "a = [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]\n",
    "early_stop_vecc_5000= [2.8770e+01, 9.6000e-01, 1.0800e+00, 6.6789e-07, 1.1368e-06, 0.0000e+00,\n",
    "        1.9100e+00]\n",
    "scheduler_vecc_5000= [30.823365978489328, 1.325000558988864, 2.300043515398019, 5.191119207004892e-06, -9.752316964224805e-07, 0.0, 3.1756405728254657]\n",
    "\n",
    "a = early_stop_vecc_5000 #  2174.9120\n",
    "a = [2.9527e+01, 1.3223e+00, 2.2772e+00, 1.3499e-05, 5.9777e-06, 0.0000e+00,\n",
    "        3.2343e+00] # 2108.4802\n",
    "a = [30.527, 1.3223e+00, 2.2772e+00, 0, 0, 0,\n",
    "        3.2343e+00] # 29.527: 2108.4802. 30.527: 2109.0693   -0.5891 difference\n",
    "\n",
    "#a = scheduler_vecc_5000 # 2108.8091\n",
    "#a = [30.2594, 0.665, 1.8981, 0.0, 0.1317, -0.0, 1.9785]\n",
    "#a = [45.1402, 0.6299, 0.7308, -0.0003, -0.0151, 0.0, 7.8922]\n",
    "#a = [21.7335, 1.2817, 1.5946, 0.042, -0.1241, 0.218, 4.8654]\n",
    "#a = [20.453542336448137, 1.4506118600616982, 2.43096923637867, -0.03476556019978718, -0.1559262606484541, 0.1254833595232136, 3.938183829354925]\n",
    "params = torch.tensor(a, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "cov_map = instance1.cov_structure_saver(params, instance1.matern_cov_anisotropy_v05)  \n",
    "instance1.vecchia_oct22( params, instance1.matern_cov_anisotropy_v05, cov_map )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7499a13",
   "metadata": {},
   "source": [
    "v = 0.5 # smooth\n",
    "mm_cond_number = 20\n",
    "nheads = 20\n",
    "lr = 0.01\n",
    "step = 80\n",
    "gamma_par = 0.9\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c05cf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0.5 # smooth\n",
    "mm_cond_number = 20\n",
    "nheads = 300\n",
    "lr = 0.02\n",
    "step = 100\n",
    "gamma_par = 0.5\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afd1495",
   "metadata": {},
   "source": [
    "# fit 8 hours of data idx_for_datamap = [i*8, (i+1)*8]\n",
    "\n",
    "# try just fitting one hour idx_for_datamap = [i*8, (i*8+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "239fc35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading data for all days...\n",
      "Pre-loading data for all days...\n",
      "Data loaded for 31 days.\n",
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 1250.0, smooth: 0.5\n",
      "mm_cond_number: 20,\n",
      "initial parameters: \n",
      " [tensor([28.7500], dtype=torch.float64, requires_grad=True), tensor([0.9800], dtype=torch.float64, requires_grad=True), tensor([1.0600], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([1.8900], dtype=torch.float64, requires_grad=True)]\n",
      "--- Epoch 1 / Loss: 2125.707079 ---\n",
      "  Param 0: Value=28.7500, Grad=0.42832305446676067\n",
      "  Param 1: Value=0.9800, Grad=3.7607030582832763\n",
      "  Param 2: Value=1.0600, Grad=-22.898722379837636\n",
      "  Param 3: Value=0.0000, Grad=-8.526512829121202e-14\n",
      "  Param 4: Value=0.0000, Grad=2.4158453015843406e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=1.8900, Grad=2.638213385652388\n",
      "------------------------------\n",
      "--- Epoch 11 / Loss: 2135.978520 ---\n",
      "  Param 0: Value=28.5498, Grad=0.43845135892244036\n",
      "  Param 1: Value=0.7912, Grad=-0.18778810166159587\n",
      "  Param 2: Value=1.2577, Grad=-16.111381478232488\n",
      "  Param 3: Value=0.0000, Grad=-3.979039320256561e-13\n",
      "  Param 4: Value=-0.0000, Grad=-2.4158453015843406e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=1.6882, Grad=3.7152603464679346\n",
      "------------------------------\n",
      "--- Epoch 21 / Loss: 2131.855174 ---\n",
      "  Param 0: Value=28.3520, Grad=0.3050125649582385\n",
      "  Param 1: Value=0.7631, Grad=-1.2233301652574227\n",
      "  Param 2: Value=1.4410, Grad=-10.489973674924272\n",
      "  Param 3: Value=-0.0000, Grad=5.115907697472721e-13\n",
      "  Param 4: Value=0.0000, Grad=5.684341886080802e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=1.4820, Grad=3.3744882926813737\n",
      "------------------------------\n",
      "--- Epoch 31 / Loss: 2120.896633 ---\n",
      "  Param 0: Value=28.1830, Grad=0.10021538909474526\n",
      "  Param 1: Value=0.8086, Grad=1.0314411519694602\n",
      "  Param 2: Value=1.5984, Grad=-6.63363465383884\n",
      "  Param 3: Value=-0.0000, Grad=5.115907697472721e-13\n",
      "  Param 4: Value=0.0000, Grad=0.0\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=1.2856, Grad=2.3299952156548294\n",
      "------------------------------\n",
      "--- Epoch 41 / Loss: 2125.979955 ---\n",
      "  Param 0: Value=28.0781, Grad=-0.027463574712022654\n",
      "  Param 1: Value=0.7780, Grad=-0.5832466738843465\n",
      "  Param 2: Value=1.7225, Grad=-3.2219844669258464\n",
      "  Param 3: Value=-0.0000, Grad=1.5916157281026244e-12\n",
      "  Param 4: Value=0.0000, Grad=-6.252776074688882e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=1.1164, Grad=1.4884671450046962\n",
      "------------------------------\n",
      "--- Epoch 51 / Loss: 2124.685462 ---\n",
      "  Param 0: Value=28.0566, Grad=-0.16652412145082063\n",
      "  Param 1: Value=0.7948, Grad=0.25470986398642026\n",
      "  Param 2: Value=1.8066, Grad=-1.1747895687403798\n",
      "  Param 3: Value=-0.0000, Grad=3.410605131648481e-13\n",
      "  Param 4: Value=0.0000, Grad=5.115907697472721e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.9865, Grad=0.46050338339338603\n",
      "------------------------------\n",
      "--- Epoch 61 / Loss: 2126.677873 ---\n",
      "  Param 0: Value=28.1333, Grad=-0.22996146063162692\n",
      "  Param 1: Value=0.7896, Grad=-0.1671889008644598\n",
      "  Param 2: Value=1.8535, Grad=0.062236388684141275\n",
      "  Param 3: Value=-0.0000, Grad=2.8421709430404007e-13\n",
      "  Param 4: Value=0.0000, Grad=-4.547473508864641e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.9130, Grad=-0.1050653145173972\n",
      "------------------------------\n",
      "--- Epoch 71 / Loss: 2125.655029 ---\n",
      "  Param 0: Value=28.2793, Grad=-0.24884209437307558\n",
      "  Param 1: Value=0.7973, Grad=0.075751107822569\n",
      "  Param 2: Value=1.8712, Grad=0.32019556584987185\n",
      "  Param 3: Value=-0.0000, Grad=-4.547473508864641e-13\n",
      "  Param 4: Value=0.0000, Grad=-3.410605131648481e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8908, Grad=-0.28646346151134194\n",
      "------------------------------\n",
      "--- Epoch 81 / Loss: 2125.082406 ---\n",
      "  Param 0: Value=28.4573, Grad=-0.22885592190539938\n",
      "  Param 1: Value=0.7983, Grad=-0.09249924907294371\n",
      "  Param 2: Value=1.8734, Grad=0.15805385572684827\n",
      "  Param 3: Value=-0.0000, Grad=5.115907697472721e-13\n",
      "  Param 4: Value=0.0000, Grad=-4.263256414560601e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8991, Grad=-0.158790171728024\n",
      "------------------------------\n",
      "--- Epoch 91 / Loss: 2123.576551 ---\n",
      "  Param 0: Value=28.6397, Grad=-0.20829114562513207\n",
      "  Param 1: Value=0.8046, Grad=0.038567223519507365\n",
      "  Param 2: Value=1.8722, Grad=-0.1414115251649264\n",
      "  Param 3: Value=-0.0000, Grad=-5.684341886080802e-14\n",
      "  Param 4: Value=0.0000, Grad=9.663381206337363e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.9127, Grad=-0.009668445797677894\n",
      "------------------------------\n",
      "--- Epoch 101 / Loss: 2122.901330 ---\n",
      "  Param 0: Value=28.8160, Grad=-0.19235055846722737\n",
      "  Param 1: Value=0.8068, Grad=-0.04554581920990586\n",
      "  Param 2: Value=1.8745, Grad=-0.2853347918888858\n",
      "  Param 3: Value=-0.0000, Grad=1.1368683772161603e-12\n",
      "  Param 4: Value=0.0000, Grad=2.842170943040401e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.9179, Grad=0.08911638941643041\n",
      "------------------------------\n",
      "--- Epoch 111 / Loss: 2122.408907 ---\n",
      "  Param 0: Value=28.9010, Grad=-0.19052761274039354\n",
      "  Param 1: Value=0.8095, Grad=0.005837702862294236\n",
      "  Param 2: Value=1.8778, Grad=-0.3019930404009301\n",
      "  Param 3: Value=-0.0000, Grad=8.526512829121202e-13\n",
      "  Param 4: Value=0.0000, Grad=-5.968558980384842e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.9154, Grad=0.09110216189655063\n",
      "------------------------------\n",
      "--- Epoch 121 / Loss: 2122.186047 ---\n",
      "  Param 0: Value=28.9853, Grad=-0.1897722099295418\n",
      "  Param 1: Value=0.8107, Grad=-0.01882617497264505\n",
      "  Param 2: Value=1.8822, Grad=-0.2702503923004542\n",
      "  Param 3: Value=-0.0000, Grad=5.684341886080801e-13\n",
      "  Param 4: Value=0.0000, Grad=5.684341886080801e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.9104, Grad=0.0799119092860936\n",
      "------------------------------\n",
      "--- Epoch 131 / Loss: 2121.875013 ---\n",
      "  Param 0: Value=29.0703, Grad=-0.19017087076859585\n",
      "  Param 1: Value=0.8125, Grad=-0.0083493087047799\n",
      "  Param 2: Value=1.8871, Grad=-0.23519867258016802\n",
      "  Param 3: Value=-0.0000, Grad=-1.2505552149377763e-12\n",
      "  Param 4: Value=0.0000, Grad=-3.126388037344441e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.9048, Grad=0.060527846929576246\n",
      "------------------------------\n",
      "--- Epoch 141 / Loss: 2121.592849 ---\n",
      "  Param 0: Value=29.1566, Grad=-0.1899834391180858\n",
      "  Param 1: Value=0.8142, Grad=-0.011320757877683718\n",
      "  Param 2: Value=1.8918, Grad=-0.2035603428342938\n",
      "  Param 3: Value=-0.0000, Grad=-9.663381206337363e-13\n",
      "  Param 4: Value=0.0000, Grad=-1.0231815394945443e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8996, Grad=0.04539704504738884\n",
      "------------------------------\n",
      "--- Epoch 151 / Loss: 2121.283209 ---\n",
      "  Param 0: Value=29.2441, Grad=-0.18921572260112063\n",
      "  Param 1: Value=0.8160, Grad=-0.010045413835399586\n",
      "  Param 2: Value=1.8963, Grad=-0.18272603205866744\n",
      "  Param 3: Value=0.0000, Grad=-6.252776074688882e-13\n",
      "  Param 4: Value=-0.0000, Grad=7.105427357601002e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8951, Grad=0.03551757708513703\n",
      "------------------------------\n",
      "--- Epoch 161 / Loss: 2120.963821 ---\n",
      "  Param 0: Value=29.3326, Grad=-0.18788019743314321\n",
      "  Param 1: Value=0.8178, Grad=-0.009784844988530494\n",
      "  Param 2: Value=1.9005, Grad=-0.170376926399058\n",
      "  Param 3: Value=0.0000, Grad=-7.958078640513122e-13\n",
      "  Param 4: Value=-0.0000, Grad=-1.9895196601282805e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8914, Grad=0.03059059072978032\n",
      "------------------------------\n",
      "--- Epoch 171 / Loss: 2120.636734 ---\n",
      "  Param 0: Value=29.4218, Grad=-0.18624390098199983\n",
      "  Param 1: Value=0.8197, Grad=-0.009129820200001859\n",
      "  Param 2: Value=1.9047, Grad=-0.16301699205325804\n",
      "  Param 3: Value=0.0000, Grad=-5.684341886080802e-14\n",
      "  Param 4: Value=-0.0000, Grad=-6.821210263296962e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8881, Grad=0.028424144085612113\n",
      "------------------------------\n",
      "--- Epoch 181 / Loss: 2120.309780 ---\n",
      "  Param 0: Value=29.5114, Grad=-0.1845116600733211\n",
      "  Param 1: Value=0.8216, Grad=-0.008941028635220505\n",
      "  Param 2: Value=1.9089, Grad=-0.1571448715318411\n",
      "  Param 3: Value=0.0000, Grad=-5.684341886080802e-14\n",
      "  Param 4: Value=0.0000, Grad=2.8421709430404007e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8849, Grad=0.027208474616636558\n",
      "------------------------------\n",
      "--- Epoch 191 / Loss: 2119.983088 ---\n",
      "  Param 0: Value=29.6014, Grad=-0.18282323140988344\n",
      "  Param 1: Value=0.8235, Grad=-0.008161232102210647\n",
      "  Param 2: Value=1.9132, Grad=-0.15105813873776697\n",
      "  Param 3: Value=0.0000, Grad=1.0800249583553523e-12\n",
      "  Param 4: Value=-0.0000, Grad=3.694822225952521e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8819, Grad=0.025785920690256425\n",
      "------------------------------\n",
      "--- Epoch 201 / Loss: 2119.661283 ---\n",
      "  Param 0: Value=29.6917, Grad=-0.18118612061338535\n",
      "  Param 1: Value=0.8254, Grad=-0.008092517917077657\n",
      "  Param 2: Value=1.9175, Grad=-0.14411670451443115\n",
      "  Param 3: Value=0.0000, Grad=1.1368683772161603e-12\n",
      "  Param 4: Value=-0.0000, Grad=-1.4210854715202004e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8788, Grad=0.024022661793471123\n",
      "------------------------------\n",
      "--- Epoch 211 / Loss: 2119.499892 ---\n",
      "  Param 0: Value=29.7369, Grad=-0.1804001799000755\n",
      "  Param 1: Value=0.8264, Grad=-0.007607154143002504\n",
      "  Param 2: Value=1.9197, Grad=-0.14039523335134163\n",
      "  Param 3: Value=0.0000, Grad=3.410605131648481e-13\n",
      "  Param 4: Value=-0.0000, Grad=-1.1368683772161603e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8772, Grad=0.02293176215328785\n",
      "------------------------------\n",
      "--- Epoch 221 / Loss: 2119.339012 ---\n",
      "  Param 0: Value=29.7824, Grad=-0.17964618893440276\n",
      "  Param 1: Value=0.8273, Grad=-0.007277096546232542\n",
      "  Param 2: Value=1.9219, Grad=-0.13609298043547824\n",
      "  Param 3: Value=0.0000, Grad=9.094947017729282e-13\n",
      "  Param 4: Value=-0.0000, Grad=0.0\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8757, Grad=0.02157353187009825\n",
      "------------------------------\n",
      "--- Epoch 231 / Loss: 2119.178230 ---\n",
      "  Param 0: Value=29.8281, Grad=-0.17892266040955118\n",
      "  Param 1: Value=0.8283, Grad=-0.00704199807684347\n",
      "  Param 2: Value=1.9242, Grad=-0.13124600166759137\n",
      "  Param 3: Value=0.0000, Grad=2.8421709430404007e-13\n",
      "  Param 4: Value=-0.0000, Grad=3.694822225952521e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8742, Grad=0.019962521764603114\n",
      "------------------------------\n",
      "--- Epoch 241 / Loss: 2119.016661 ---\n",
      "  Param 0: Value=29.8740, Grad=-0.17821915655652398\n",
      "  Param 1: Value=0.8293, Grad=-0.006699495984577197\n",
      "  Param 2: Value=1.9266, Grad=-0.12610571084383082\n",
      "  Param 3: Value=0.0000, Grad=-4.547473508864641e-13\n",
      "  Param 4: Value=-0.0000, Grad=3.979039320256561e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8727, Grad=0.018205849411855546\n",
      "------------------------------\n",
      "--- Epoch 251 / Loss: 2118.854647 ---\n",
      "  Param 0: Value=29.9202, Grad=-0.1775171860967668\n",
      "  Param 1: Value=0.8304, Grad=-0.00646763673394446\n",
      "  Param 2: Value=1.9290, Grad=-0.12087819722057702\n",
      "  Param 3: Value=0.0000, Grad=-2.2737367544323206e-13\n",
      "  Param 4: Value=-0.0000, Grad=3.126388037344441e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8712, Grad=0.016460639930768295\n",
      "------------------------------\n",
      "--- Epoch 261 / Loss: 2118.691564 ---\n",
      "  Param 0: Value=29.9665, Grad=-0.17680869307091684\n",
      "  Param 1: Value=0.8314, Grad=-0.006180445749365049\n",
      "  Param 2: Value=1.9314, Grad=-0.11576923330585487\n",
      "  Param 3: Value=0.0000, Grad=1.7053025658242404e-13\n",
      "  Param 4: Value=-0.0000, Grad=4.263256414560601e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8698, Grad=0.014813171339812747\n",
      "------------------------------\n",
      "--- Epoch 271 / Loss: 2118.527673 ---\n",
      "  Param 0: Value=30.0130, Grad=-0.17608660868223758\n",
      "  Param 1: Value=0.8324, Grad=-0.005940163306291879\n",
      "  Param 2: Value=1.9339, Grad=-0.11085640956912357\n",
      "  Param 3: Value=0.0000, Grad=1.1368683772161603e-13\n",
      "  Param 4: Value=-0.0000, Grad=1.4210854715202004e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8685, Grad=0.013325490476108648\n",
      "------------------------------\n",
      "--- Epoch 281 / Loss: 2118.362882 ---\n",
      "  Param 0: Value=30.0597, Grad=-0.1753510826299448\n",
      "  Param 1: Value=0.8335, Grad=-0.005685456739442429\n",
      "  Param 2: Value=1.9363, Grad=-0.10617258014519848\n",
      "  Param 3: Value=0.0000, Grad=-1.3642420526593924e-12\n",
      "  Param 4: Value=-0.0000, Grad=-6.536993168992922e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8672, Grad=0.012003685582347323\n",
      "------------------------------\n",
      "--- Epoch 291 / Loss: 2118.197427 ---\n",
      "  Param 0: Value=30.1065, Grad=-0.17460334268735056\n",
      "  Param 1: Value=0.8346, Grad=-0.0054508470539644804\n",
      "  Param 2: Value=1.9388, Grad=-0.10169272760231252\n",
      "  Param 3: Value=0.0000, Grad=-8.526512829121202e-13\n",
      "  Param 4: Value=-0.0000, Grad=-5.684341886080802e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8660, Grad=0.010837893518409913\n",
      "------------------------------\n",
      "--- Epoch 301 / Loss: 2118.031414 ---\n",
      "  Param 0: Value=30.1534, Grad=-0.17384642802741979\n",
      "  Param 1: Value=0.8357, Grad=-0.00521598882640717\n",
      "  Param 2: Value=1.9414, Grad=-0.09738589018070343\n",
      "  Param 3: Value=0.0000, Grad=1.1368683772161603e-13\n",
      "  Param 4: Value=-0.0000, Grad=-4.547473508864641e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8648, Grad=0.00980428527599031\n",
      "------------------------------\n",
      "--- Epoch 311 / Loss: 2117.947968 ---\n",
      "  Param 0: Value=30.1770, Grad=-0.17346869146923616\n",
      "  Param 1: Value=0.8362, Grad=-0.00505670511429912\n",
      "  Param 2: Value=1.9426, Grad=-0.09523929723673064\n",
      "  Param 3: Value=0.0000, Grad=-6.821210263296962e-13\n",
      "  Param 4: Value=-0.0000, Grad=-1.7053025658242404e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8643, Grad=0.009303370591553506\n",
      "------------------------------\n",
      "--- Epoch 321 / Loss: 2117.864006 ---\n",
      "  Param 0: Value=30.2006, Grad=-0.17310609644903796\n",
      "  Param 1: Value=0.8368, Grad=-0.004832788864504778\n",
      "  Param 2: Value=1.9440, Grad=-0.0928575186743501\n",
      "  Param 3: Value=0.0000, Grad=0.0\n",
      "  Param 4: Value=-0.0000, Grad=1.9895196601282805e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8638, Grad=0.008684063506330997\n",
      "------------------------------\n",
      "--- Epoch 331 / Loss: 2117.779866 ---\n",
      "  Param 0: Value=30.2242, Grad=-0.1727605493152672\n",
      "  Param 1: Value=0.8373, Grad=-0.004667455537600063\n",
      "  Param 2: Value=1.9453, Grad=-0.0901539206599864\n",
      "  Param 3: Value=0.0000, Grad=3.979039320256561e-13\n",
      "  Param 4: Value=-0.0000, Grad=2.5579538487363607e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8632, Grad=0.007922882220510186\n",
      "------------------------------\n",
      "--- Epoch 341 / Loss: 2117.695236 ---\n",
      "  Param 0: Value=30.2480, Grad=-0.17242884244802092\n",
      "  Param 1: Value=0.8379, Grad=-0.004492347676531949\n",
      "  Param 2: Value=1.9467, Grad=-0.08720798187403034\n",
      "  Param 3: Value=0.0000, Grad=7.958078640513122e-13\n",
      "  Param 4: Value=-0.0000, Grad=-3.979039320256561e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8627, Grad=0.007054249614513619\n",
      "------------------------------\n",
      "--- Epoch 351 / Loss: 2117.610012 ---\n",
      "  Param 0: Value=30.2718, Grad=-0.17210380899702227\n",
      "  Param 1: Value=0.8385, Grad=-0.0043225731682809965\n",
      "  Param 2: Value=1.9481, Grad=-0.08412803899102528\n",
      "  Param 3: Value=0.0000, Grad=-5.684341886080802e-14\n",
      "  Param 4: Value=-0.0000, Grad=3.694822225952521e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8622, Grad=0.006143805754123655\n",
      "------------------------------\n",
      "--- Epoch 361 / Loss: 2117.524091 ---\n",
      "  Param 0: Value=30.2957, Grad=-0.17177897174732726\n",
      "  Param 1: Value=0.8391, Grad=-0.004165269719550224\n",
      "  Param 2: Value=1.9495, Grad=-0.08101561573880645\n",
      "  Param 3: Value=0.0000, Grad=-5.684341886080802e-14\n",
      "  Param 4: Value=-0.0000, Grad=2.5579538487363607e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8618, Grad=0.005251433322576826\n",
      "------------------------------\n",
      "--- Epoch 371 / Loss: 2117.437361 ---\n",
      "  Param 0: Value=30.3197, Grad=-0.17145011253434483\n",
      "  Param 1: Value=0.8397, Grad=-0.004005982906221561\n",
      "  Param 2: Value=1.9509, Grad=-0.0779460571262689\n",
      "  Param 3: Value=0.0000, Grad=1.0800249583553523e-12\n",
      "  Param 4: Value=-0.0000, Grad=-2.2737367544323206e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8614, Grad=0.004417664547102795\n",
      "------------------------------\n",
      "--- Epoch 381 / Loss: 2117.349830 ---\n",
      "  Param 0: Value=30.3437, Grad=-0.1711147099955781\n",
      "  Param 1: Value=0.8403, Grad=-0.003855005503211828\n",
      "  Param 2: Value=1.9524, Grad=-0.07495765097201001\n",
      "  Param 3: Value=0.0000, Grad=-2.8421709430404007e-13\n",
      "  Param 4: Value=-0.0000, Grad=-3.979039320256561e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8610, Grad=0.0036659656002639984\n",
      "------------------------------\n",
      "--- Epoch 391 / Loss: 2117.261502 ---\n",
      "  Param 0: Value=30.3678, Grad=-0.17077222803778125\n",
      "  Param 1: Value=0.8409, Grad=-0.0037053219497522605\n",
      "  Param 2: Value=1.9539, Grad=-0.07206476213718815\n",
      "  Param 3: Value=0.0000, Grad=3.979039320256561e-13\n",
      "  Param 4: Value=-0.0000, Grad=-4.263256414560601e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8607, Grad=0.0030024921877473965\n",
      "------------------------------\n",
      "--- Epoch 401 / Loss: 2117.172436 ---\n",
      "  Param 0: Value=30.3919, Grad=-0.1704230258593089\n",
      "  Param 1: Value=0.8415, Grad=-0.0035603488316695575\n",
      "  Param 2: Value=1.9554, Grad=-0.06926324977689546\n",
      "  Param 3: Value=0.0000, Grad=7.958078640513122e-13\n",
      "  Param 4: Value=-0.0000, Grad=-1.7053025658242404e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8604, Grad=0.0024243433956079308\n",
      "------------------------------\n",
      "--- Epoch 411 / Loss: 2117.127465 ---\n",
      "  Param 0: Value=30.4040, Grad=-0.17024805518958275\n",
      "  Param 1: Value=0.8418, Grad=-0.0034629988866470285\n",
      "  Param 2: Value=1.9561, Grad=-0.0678686683165779\n",
      "  Param 3: Value=0.0000, Grad=-1.7053025658242404e-12\n",
      "  Param 4: Value=-0.0000, Grad=-7.389644451905042e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8603, Grad=0.002151413724268192\n",
      "------------------------------\n",
      "--- Epoch 421 / Loss: 2117.082038 ---\n",
      "  Param 0: Value=30.4161, Grad=-0.1700809923199883\n",
      "  Param 1: Value=0.8421, Grad=-0.003321078907291053\n",
      "  Param 2: Value=1.9569, Grad=-0.06634986313827085\n",
      "  Param 3: Value=0.0000, Grad=-8.526512829121202e-13\n",
      "  Param 4: Value=-0.0000, Grad=4.547473508864641e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8602, Grad=0.0018204228662725974\n",
      "------------------------------\n",
      "--- Epoch 431 / Loss: 2117.036368 ---\n",
      "  Param 0: Value=30.4282, Grad=-0.1699232071203629\n",
      "  Param 1: Value=0.8424, Grad=-0.0032103530019611526\n",
      "  Param 2: Value=1.9577, Grad=-0.06465107234942025\n",
      "  Param 3: Value=0.0000, Grad=4.547473508864641e-13\n",
      "  Param 4: Value=-0.0000, Grad=2.842170943040401e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8601, Grad=0.0014156049092726164\n",
      "------------------------------\n",
      "--- Epoch 441 / Loss: 2116.990325 ---\n",
      "  Param 0: Value=30.4403, Grad=-0.1697734846418233\n",
      "  Param 1: Value=0.8428, Grad=-0.003104629062704589\n",
      "  Param 2: Value=1.9585, Grad=-0.06280215744209272\n",
      "  Param 3: Value=0.0000, Grad=4.547473508864641e-13\n",
      "  Param 4: Value=-0.0000, Grad=4.547473508864641e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8600, Grad=0.000950492468932751\n",
      "------------------------------\n",
      "--- Epoch 451 / Loss: 2116.943802 ---\n",
      "  Param 0: Value=30.4525, Grad=-0.16962882033612314\n",
      "  Param 1: Value=0.8431, Grad=-0.0029931358459691637\n",
      "  Param 2: Value=1.9594, Grad=-0.060855354523141614\n",
      "  Param 3: Value=0.0000, Grad=-3.979039320256561e-13\n",
      "  Param 4: Value=-0.0000, Grad=-2.842170943040401e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8599, Grad=0.0004539265194454245\n",
      "------------------------------\n",
      "--- Epoch 461 / Loss: 2116.896773 ---\n",
      "  Param 0: Value=30.4647, Grad=-0.16948569030342864\n",
      "  Param 1: Value=0.8434, Grad=-0.0028923867545529447\n",
      "  Param 2: Value=1.9602, Grad=-0.05885984431051838\n",
      "  Param 3: Value=0.0000, Grad=9.663381206337363e-13\n",
      "  Param 4: Value=-0.0000, Grad=6.536993168992922e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8599, Grad=-4.220282321476532e-05\n",
      "------------------------------\n",
      "--- Epoch 471 / Loss: 2116.849157 ---\n",
      "  Param 0: Value=30.4769, Grad=-0.16934164694642717\n",
      "  Param 1: Value=0.8438, Grad=-0.0027917352110478078\n",
      "  Param 2: Value=1.9611, Grad=-0.05685946727612645\n",
      "  Param 3: Value=0.0000, Grad=1.7053025658242404e-13\n",
      "  Param 4: Value=-0.0000, Grad=4.831690603168681e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8599, Grad=-0.0005143397738600619\n",
      "------------------------------\n",
      "--- Epoch 481 / Loss: 2116.800927 ---\n",
      "  Param 0: Value=30.4891, Grad=-0.16919488354432144\n",
      "  Param 1: Value=0.8441, Grad=-0.0026932803324655197\n",
      "  Param 2: Value=1.9620, Grad=-0.05488267470718711\n",
      "  Param 3: Value=0.0000, Grad=-5.684341886080802e-14\n",
      "  Param 4: Value=-0.0000, Grad=8.526512829121202e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8599, Grad=-0.0009456611411633453\n",
      "------------------------------\n",
      "--- Epoch 491 / Loss: 2116.752079 ---\n",
      "  Param 0: Value=30.5014, Grad=-0.16904442173715883\n",
      "  Param 1: Value=0.8445, Grad=-0.0025980788226576124\n",
      "  Param 2: Value=1.9629, Grad=-0.052945381017025284\n",
      "  Param 3: Value=0.0000, Grad=-1.7053025658242404e-13\n",
      "  Param 4: Value=-0.0000, Grad=4.831690603168681e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=0.8599, Grad=-0.0013269962813038383\n",
      "------------------------------\n",
      "FINAL STATE: Epoch 500, Loss: 2116.7075860017403, \n",
      " vecc Parameters: [30.512432567587712, 0.8448107519195627, 1.9637104684223585, 4.709318887197267e-06, -1.4682903302486788e-06, 0.0, 0.8600113249150504]\n",
      "Day 1 optimization finished in 124.54s over 500 epochs.\n",
      "Day 1 final results: [30.512432567587712, 0.8448107519195627, 1.9637104684223585, 4.709318887197267e-06, -1.4682903302486788e-06, 0.0, 0.8600113249150504, 2116.7075860017403]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------\n",
    "# Main Optimization Loop (Updated and Corrected)\n",
    "# ------------------------------------\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "num_days_to_load = 31 # From reference code\n",
    "\n",
    "for i in range(num_days_to_load):\n",
    "    idx_for_datamap = [i*8, i*8+1]\n",
    "    #idx_for_datamap = [i*8, (i+1)*8]    \n",
    "    # Using the new load function from the reference code\n",
    "    cur_map, cur_df = data_load_instance.load_working_data(\n",
    "        df_map, \n",
    "        idx_for_datamap, \n",
    "        ord_mm=None,\n",
    "        # Using float64 to ensure compatibility with kernels\n",
    "        dtype=torch.float64 \n",
    "    )\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "# ... (Data loading section unchanged and correct) ...\n",
    "print(f\"Data loaded for {len(df_day_map_list)} days.\")\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "days_list = [0]\n",
    "for day in days_list:  \n",
    "    \n",
    "    # Get the pre-loaded data for this day\n",
    "    analysis_data_map = df_day_map_list[day]\n",
    "    aggregated_data = df_day_aggregated_list[day]\n",
    "\n",
    "\n",
    "    # Initial parameters (full vector 'a' is for reference)\n",
    "    a = [28.75, 0.98, 1.06, 0, 0, 0, 1.890]\n",
    "    #a = [21.75, 0.98, 1.56, 0, 0, 0, 1.890]\n",
    "    #a = [28.303, 1.307, 1.563, 0, 0, 0, 4.769]\n",
    "    \n",
    "    # NEW: Define params as a list of 1-element tensors (one per parameter)\n",
    "    params_list = [\n",
    "        torch.tensor([val], dtype=torch.float64, requires_grad=True) for val in a\n",
    "    ]\n",
    "\n",
    "    # NEW: Define learning rates and groups\n",
    "    lr_slow, lr_fast = 0.02, 0.02 # Assuming these are defined elsewhere\n",
    "    slow_indices = [ 1, 2, 3, 4, 5, 6] # e.g., ranges, advection, beta, nugget\n",
    "    fast_indices = [0] # e.g., sigmasq\n",
    "    \n",
    "    # Define Parameter Groups for the optimizer\n",
    "    param_groups = [\n",
    "        {'params': [params_list[idx] for idx in slow_indices], 'lr': lr_slow, 'name': 'slow_group'},\n",
    "        {'params': [params_list[idx] for idx in fast_indices], 'lr': lr_fast, 'name': 'fast_group'}\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Calculate resolution for printing\n",
    "    res_calc = (200 / lat_lon_resolution[0]) * (100 / lat_lon_resolution[0])\n",
    "    print(f'\\n--- Starting Day {day+1} (2024-07-{day+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params_list}')\n",
    "            \n",
    "    # --- Data loading is now done *before* the loop ---\n",
    "\n",
    "    # Define device\n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model_instance = kernels.model_fitting(\n",
    "            smooth = v,\n",
    "            input_map = analysis_data_map,\n",
    "            aggregated_data = aggregated_data,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Adjusted optimizer call: Passes the list of parameter groups\n",
    "    optimizer, scheduler = model_instance.optimizer_fun(\n",
    "            param_groups,     # <--- Pass the list of groups\n",
    "            lr=lr,            # <--- Default LR (will be overridden by groups, but needed for function sig)\n",
    "            betas=(0.9, 0.99), \n",
    "            eps=1e-8, \n",
    "            step_size=step, \n",
    "            gamma=gamma_par\n",
    "        )\n",
    "\n",
    "    # Calling the robust training loop\n",
    "    out, epoch_ran = model_instance.run_vecc_scheduler_oct23(\n",
    "            params_list,     # <--- Pass the list of parameter tensors\n",
    "            optimizer,\n",
    "            scheduler, \n",
    "            model_instance.matern_cov_anisotropy_v05, \n",
    "            epochs=epochs\n",
    "        )\n",
    "    # --- End Correction ---\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"Day {day+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day+1} final results: {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b64c20",
   "metadata": {},
   "source": [
    "# tmp testing just one block below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "defe4f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading data for all days...\n",
      "Pre-loading data for all days...\n",
      "Data loaded for 31 days.\n",
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 1250.0, smooth: 0.5\n",
      "mm_cond_number: 20,\n",
      "initial parameters: \n",
      " [tensor([28.3030], dtype=torch.float64, requires_grad=True), tensor([1.3070], dtype=torch.float64, requires_grad=True), tensor([1.5630], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([4.7690], dtype=torch.float64, requires_grad=True)]\n",
      "--- Epoch 1 / Loss: 2072.542406 ---\n",
      "  Param 0: Value=28.3030, Grad=0.7997467182134974\n",
      "  Param 1: Value=1.3070, Grad=-6.035422239558298\n",
      "  Param 2: Value=1.5630, Grad=-12.561598310167653\n",
      "  Param 3: Value=0.0000, Grad=3.659295089164516e-13\n",
      "  Param 4: Value=0.0000, Grad=-1.1368683772161603e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=4.7690, Grad=5.266075690826141\n",
      "------------------------------\n",
      "--- Epoch 11 / Loss: 2051.618898 ---\n",
      "  Param 0: Value=28.1042, Grad=0.65830742718585\n",
      "  Param 1: Value=1.5036, Grad=-3.765240267536992\n",
      "  Param 2: Value=1.7613, Grad=-9.77000901723693\n",
      "  Param 3: Value=0.0000, Grad=-2.0605739337042905e-13\n",
      "  Param 4: Value=0.0000, Grad=1.1368683772161603e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=4.5697, Grad=4.688089740372215\n",
      "------------------------------\n",
      "--- Epoch 21 / Loss: 2037.263424 ---\n",
      "  Param 0: Value=27.9124, Grad=0.5319766273177589\n",
      "  Param 1: Value=1.6819, Grad=-2.2329948768132137\n",
      "  Param 2: Value=1.9511, Grad=-7.741906917821154\n",
      "  Param 3: Value=0.0000, Grad=-1.3145040611561853e-13\n",
      "  Param 4: Value=0.0000, Grad=1.4210854715202004e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=4.3748, Grad=4.011390501895402\n",
      "------------------------------\n",
      "--- Epoch 31 / Loss: 2028.193307 ---\n",
      "  Param 0: Value=27.7335, Grad=0.42009590100242633\n",
      "  Param 1: Value=1.8279, Grad=-1.1798260856755274\n",
      "  Param 2: Value=2.1271, Grad=-6.2377119392123745\n",
      "  Param 3: Value=0.0000, Grad=2.6290081223123707e-13\n",
      "  Param 4: Value=0.0000, Grad=4.263256414560601e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=4.1895, Grad=3.2828500157010243\n",
      "------------------------------\n",
      "--- Epoch 41 / Loss: 2023.156244 ---\n",
      "  Param 0: Value=27.5722, Grad=0.32147020752726085\n",
      "  Param 1: Value=1.9333, Grad=-0.44149153461167057\n",
      "  Param 2: Value=2.2874, Grad=-5.0856206542573545\n",
      "  Param 3: Value=0.0000, Grad=-3.836930773104541e-13\n",
      "  Param 4: Value=0.0000, Grad=-5.115907697472721e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=4.0194, Grad=2.5544783787910146\n",
      "------------------------------\n",
      "--- Epoch 51 / Loss: 2021.104263 ---\n",
      "  Param 0: Value=27.4318, Grad=0.23549260253841342\n",
      "  Param 1: Value=1.9947, Grad=0.07454647829425376\n",
      "  Param 2: Value=2.4319, Grad=-4.170953335687216\n",
      "  Param 3: Value=0.0000, Grad=3.268496584496461e-13\n",
      "  Param 4: Value=0.0000, Grad=-1.4210854715202004e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.8696, Grad=1.8824565873783938\n",
      "------------------------------\n",
      "--- Epoch 61 / Loss: 2021.220209 ---\n",
      "  Param 0: Value=27.3147, Grad=0.16233801070713727\n",
      "  Param 1: Value=2.0134, Grad=0.41416440159356505\n",
      "  Param 2: Value=2.5614, Grad=-3.420916377644346\n",
      "  Param 3: Value=0.0000, Grad=-1.7053025658242404e-13\n",
      "  Param 4: Value=0.0000, Grad=3.126388037344441e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.7435, Grad=1.3205545495638151\n",
      "------------------------------\n",
      "--- Epoch 71 / Loss: 2022.885917 ---\n",
      "  Param 0: Value=27.2219, Grad=0.1023685462974715\n",
      "  Param 1: Value=1.9946, Grad=0.5981908251153962\n",
      "  Param 2: Value=2.6770, Grad=-2.7906259240972418\n",
      "  Param 3: Value=-0.0000, Grad=-1.5631940186722204e-13\n",
      "  Param 4: Value=0.0000, Grad=-1.4210854715202004e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6424, Grad=0.9055637942202921\n",
      "------------------------------\n",
      "--- Epoch 81 / Loss: 2025.630644 ---\n",
      "  Param 0: Value=27.1536, Grad=0.05522405291846322\n",
      "  Param 1: Value=1.9475, Grad=0.6404771841108676\n",
      "  Param 2: Value=2.7794, Grad=-2.252539202060518\n",
      "  Param 3: Value=0.0000, Grad=5.400124791776761e-13\n",
      "  Param 4: Value=0.0000, Grad=-5.400124791776761e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.5638, Grad=0.6418533827113171\n",
      "------------------------------\n",
      "--- Epoch 91 / Loss: 2029.053657 ---\n",
      "  Param 0: Value=27.1085, Grad=0.019246280564431784\n",
      "  Param 1: Value=1.8842, Grad=0.562593013463613\n",
      "  Param 2: Value=2.8690, Grad=-1.7898857783992526\n",
      "  Param 3: Value=0.0000, Grad=-5.044853423896711e-13\n",
      "  Param 4: Value=0.0000, Grad=-1.7053025658242404e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.5028, Grad=0.4975266934021758\n",
      "------------------------------\n",
      "--- Epoch 101 / Loss: 2032.731869 ---\n",
      "  Param 0: Value=27.0847, Grad=-0.008344992247364957\n",
      "  Param 1: Value=1.8178, Grad=0.4039244939240225\n",
      "  Param 2: Value=2.9465, Grad=-1.393883583548984\n",
      "  Param 3: Value=0.0000, Grad=6.252776074688882e-13\n",
      "  Param 4: Value=0.0000, Grad=-1.9895196601282805e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.4535, Grad=0.4177666389009279\n",
      "------------------------------\n",
      "--- Epoch 111 / Loss: 2033.769856 ---\n",
      "  Param 0: Value=27.0828, Grad=-0.015146087603664204\n",
      "  Param 1: Value=1.7999, Grad=0.35119256442419733\n",
      "  Param 2: Value=2.9666, Grad=-1.291993300011427\n",
      "  Param 3: Value=0.0000, Grad=1.4566126083082054e-13\n",
      "  Param 4: Value=0.0000, Grad=1.1368683772161603e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.4407, Grad=0.3988127469738427\n",
      "------------------------------\n",
      "--- Epoch 121 / Loss: 2034.733629 ---\n",
      "  Param 0: Value=27.0843, Grad=-0.021411941019661263\n",
      "  Param 1: Value=1.7838, Grad=0.30139338095710766\n",
      "  Param 2: Value=2.9852, Grad=-1.1988729522354333\n",
      "  Param 3: Value=0.0000, Grad=1.8474111129762605e-13\n",
      "  Param 4: Value=0.0000, Grad=-8.526512829121202e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.4285, Grad=0.3789630032594402\n",
      "------------------------------\n",
      "--- Epoch 131 / Loss: 2035.647811 ---\n",
      "  Param 0: Value=27.0881, Grad=-0.027535457336567815\n",
      "  Param 1: Value=1.7692, Grad=0.25481055468256564\n",
      "  Param 2: Value=3.0030, Grad=-1.1104379044634811\n",
      "  Param 3: Value=0.0000, Grad=-8.171241461241152e-13\n",
      "  Param 4: Value=0.0000, Grad=-1.2221335055073723e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.4165, Grad=0.35587736435131134\n",
      "------------------------------\n",
      "--- Epoch 141 / Loss: 2036.515902 ---\n",
      "  Param 0: Value=27.0941, Grad=-0.03362839970680609\n",
      "  Param 1: Value=1.7559, Grad=0.2118467633299277\n",
      "  Param 2: Value=3.0205, Grad=-1.0255468486463428\n",
      "  Param 3: Value=0.0000, Grad=8.881784197001252e-14\n",
      "  Param 4: Value=0.0000, Grad=-9.379164112033322e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.4045, Grad=0.3289672730638358\n",
      "------------------------------\n",
      "--- Epoch 151 / Loss: 2037.333625 ---\n",
      "  Param 0: Value=27.1021, Grad=-0.03968114708172443\n",
      "  Param 1: Value=1.7441, Grad=0.172817951506806\n",
      "  Param 2: Value=3.0376, Grad=-0.9441990977614978\n",
      "  Param 3: Value=0.0000, Grad=5.613287612504791e-13\n",
      "  Param 4: Value=0.0000, Grad=-2.2737367544323206e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3927, Grad=0.29853380583551714\n",
      "------------------------------\n",
      "--- Epoch 161 / Loss: 2038.094560 ---\n",
      "  Param 0: Value=27.1123, Grad=-0.04563111861439939\n",
      "  Param 1: Value=1.7337, Grad=0.1378744910358698\n",
      "  Param 2: Value=3.0543, Grad=-0.8668001472479432\n",
      "  Param 3: Value=0.0000, Grad=5.151434834260726e-13\n",
      "  Param 4: Value=0.0000, Grad=2.8421709430404007e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3812, Grad=0.26535376132240596\n",
      "------------------------------\n",
      "--- Epoch 171 / Loss: 2038.792492 ---\n",
      "  Param 0: Value=27.1250, Grad=-0.05139428885949626\n",
      "  Param 1: Value=1.7248, Grad=0.10698835323729128\n",
      "  Param 2: Value=3.0706, Grad=-0.7938553814051943\n",
      "  Param 3: Value=0.0000, Grad=7.318590178329032e-13\n",
      "  Param 4: Value=0.0000, Grad=1.4210854715202004e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3702, Grad=0.23045351071635478\n",
      "------------------------------\n",
      "--- Epoch 181 / Loss: 2039.422348 ---\n",
      "  Param 0: Value=27.1402, Grad=-0.05688183743660391\n",
      "  Param 1: Value=1.7174, Grad=0.07997800539333966\n",
      "  Param 2: Value=3.0864, Grad=-0.7258371839134838\n",
      "  Param 3: Value=0.0000, Grad=-2.7000623958883807e-13\n",
      "  Param 4: Value=0.0000, Grad=1.4210854715202004e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3600, Grad=0.19496031600372676\n",
      "------------------------------\n",
      "--- Epoch 191 / Loss: 2039.980510 ---\n",
      "  Param 0: Value=27.1581, Grad=-0.06201057682888533\n",
      "  Param 1: Value=1.7113, Grad=0.05655468557199761\n",
      "  Param 2: Value=3.1018, Grad=-0.6631241865455308\n",
      "  Param 3: Value=0.0000, Grad=4.369837824924616e-13\n",
      "  Param 4: Value=0.0000, Grad=-5.684341886080802e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3506, Grad=0.15998849946736549\n",
      "------------------------------\n",
      "--- Epoch 201 / Loss: 2040.464849 ---\n",
      "  Param 0: Value=27.1788, Grad=-0.06671024969616168\n",
      "  Param 1: Value=1.7066, Grad=0.03637655347467916\n",
      "  Param 2: Value=3.1166, Grad=-0.605972484038034\n",
      "  Param 3: Value=0.0000, Grad=3.588240815588506e-13\n",
      "  Param 4: Value=0.0000, Grad=-2.2737367544323206e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3424, Grad=0.126548018715674\n",
      "------------------------------\n",
      "--- Epoch 211 / Loss: 2040.590036 ---\n",
      "  Param 0: Value=27.1858, Grad=-0.06799736518685384\n",
      "  Param 1: Value=1.7056, Grad=0.031143914836157194\n",
      "  Param 2: Value=3.1209, Grad=-0.5902759498879391\n",
      "  Param 3: Value=0.0000, Grad=6.039613253960852e-14\n",
      "  Param 4: Value=0.0000, Grad=-3.410605131648481e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3402, Grad=0.11712149692930707\n",
      "------------------------------\n",
      "--- Epoch 221 / Loss: 2040.704521 ---\n",
      "  Param 0: Value=27.1935, Grad=-0.0692272174900348\n",
      "  Param 1: Value=1.7047, Grad=0.02632644764988612\n",
      "  Param 2: Value=3.1252, Grad=-0.575202168110934\n",
      "  Param 3: Value=0.0000, Grad=2.7000623958883807e-13\n",
      "  Param 4: Value=0.0000, Grad=0.0\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3383, Grad=0.10797876888960212\n",
      "------------------------------\n",
      "--- Epoch 231 / Loss: 2040.813551 ---\n",
      "  Param 0: Value=27.2017, Grad=-0.07043412625982437\n",
      "  Param 1: Value=1.7040, Grad=0.021676968819777542\n",
      "  Param 2: Value=3.1297, Grad=-0.56033215608295\n",
      "  Param 3: Value=0.0000, Grad=-4.973799150320701e-14\n",
      "  Param 4: Value=0.0000, Grad=0.0\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3364, Grad=0.09891850747596198\n",
      "------------------------------\n",
      "--- Epoch 241 / Loss: 2040.918777 ---\n",
      "  Param 0: Value=27.2104, Grad=-0.07162832450291956\n",
      "  Param 1: Value=1.7033, Grad=0.017108533117981217\n",
      "  Param 2: Value=3.1342, Grad=-0.5455337341481368\n",
      "  Param 3: Value=0.0000, Grad=4.334310688136611e-13\n",
      "  Param 4: Value=0.0000, Grad=-5.684341886080801e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3345, Grad=0.08988679143085943\n",
      "------------------------------\n",
      "--- Epoch 251 / Loss: 2041.020423 ---\n",
      "  Param 0: Value=27.2197, Grad=-0.07281090039640936\n",
      "  Param 1: Value=1.7028, Grad=0.012600649737447789\n",
      "  Param 2: Value=3.1388, Grad=-0.5307860219684244\n",
      "  Param 3: Value=0.0000, Grad=1.2434497875801753e-13\n",
      "  Param 4: Value=0.0000, Grad=-8.526512829121202e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3328, Grad=0.08088499361371637\n",
      "------------------------------\n",
      "--- Epoch 261 / Loss: 2041.118153 ---\n",
      "  Param 0: Value=27.2295, Grad=-0.07397950188043456\n",
      "  Param 1: Value=1.7024, Grad=0.008160553251602032\n",
      "  Param 2: Value=3.1436, Grad=-0.5161111878445581\n",
      "  Param 3: Value=0.0000, Grad=6.821210263296962e-13\n",
      "  Param 4: Value=0.0000, Grad=-1.9895196601282805e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3311, Grad=0.07193533871236824\n",
      "------------------------------\n",
      "--- Epoch 271 / Loss: 2041.211409 ---\n",
      "  Param 0: Value=27.2398, Grad=-0.07513052187458291\n",
      "  Param 1: Value=1.7021, Grad=0.003806870089975156\n",
      "  Param 2: Value=3.1484, Grad=-0.5015476544938338\n",
      "  Param 3: Value=0.0000, Grad=8.171241461241152e-13\n",
      "  Param 4: Value=0.0000, Grad=-1.7053025658242404e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3295, Grad=0.0630681188722626\n",
      "------------------------------\n",
      "--- Epoch 281 / Loss: 2041.299552 ---\n",
      "  Param 0: Value=27.2507, Grad=-0.07625994589673424\n",
      "  Param 1: Value=1.7019, Grad=-0.00043754634334458586\n",
      "  Param 2: Value=3.1534, Grad=-0.4871393512037798\n",
      "  Param 3: Value=0.0000, Grad=2.8421709430404007e-13\n",
      "  Param 4: Value=0.0000, Grad=-1.7053025658242404e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3281, Grad=0.05431695700180428\n",
      "------------------------------\n",
      "--- Epoch 291 / Loss: 2041.381925 ---\n",
      "  Param 0: Value=27.2621, Grad=-0.0773636935210007\n",
      "  Param 1: Value=1.7018, Grad=-0.004548978672300308\n",
      "  Param 2: Value=3.1584, Grad=-0.4729311898893229\n",
      "  Param 3: Value=0.0000, Grad=3.197442310920451e-14\n",
      "  Param 4: Value=0.0000, Grad=5.684341886080802e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3267, Grad=0.045717001551659475\n",
      "------------------------------\n",
      "--- Epoch 301 / Loss: 2041.457883 ---\n",
      "  Param 0: Value=27.2740, Grad=-0.07843776995220919\n",
      "  Param 1: Value=1.7019, Grad=-0.00850458958179967\n",
      "  Param 2: Value=3.1636, Grad=-0.4589669655389912\n",
      "  Param 3: Value=0.0000, Grad=1.4210854715202004e-14\n",
      "  Param 4: Value=0.0000, Grad=-2.842170943040401e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3255, Grad=0.03730414039332941\n",
      "------------------------------\n",
      "--- Epoch 311 / Loss: 2041.478857 ---\n",
      "  Param 0: Value=27.2777, Grad=-0.07875296220475925\n",
      "  Param 1: Value=1.7020, Grad=-0.009646192115947194\n",
      "  Param 2: Value=3.1651, Grad=-0.45482973352409317\n",
      "  Param 3: Value=0.0000, Grad=-3.623767952376511e-13\n",
      "  Param 4: Value=0.0000, Grad=-1.1368683772161603e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3252, Grad=0.03482691629964885\n",
      "------------------------------\n",
      "--- Epoch 321 / Loss: 2041.499001 ---\n",
      "  Param 0: Value=27.2816, Grad=-0.07907059406631045\n",
      "  Param 1: Value=1.7021, Grad=-0.010768169852769738\n",
      "  Param 2: Value=3.1668, Grad=-0.45064805586969214\n",
      "  Param 3: Value=0.0000, Grad=5.400124791776761e-13\n",
      "  Param 4: Value=0.0000, Grad=-1.4210854715202004e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3249, Grad=0.03232640075008142\n",
      "------------------------------\n",
      "--- Epoch 331 / Loss: 2041.518840 ---\n",
      "  Param 0: Value=27.2855, Grad=-0.07939573786207565\n",
      "  Param 1: Value=1.7022, Grad=-0.011883781015994188\n",
      "  Param 2: Value=3.1685, Grad=-0.44637279989086665\n",
      "  Param 3: Value=0.0000, Grad=-2.1316282072803006e-14\n",
      "  Param 4: Value=0.0000, Grad=-3.694822225952521e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3246, Grad=0.029763208027527188\n",
      "------------------------------\n",
      "--- Epoch 341 / Loss: 2041.538523 ---\n",
      "  Param 0: Value=27.2896, Grad=-0.0797301709952889\n",
      "  Param 1: Value=1.7024, Grad=-0.012996280509515401\n",
      "  Param 2: Value=3.1702, Grad=-0.4419874218199311\n",
      "  Param 3: Value=0.0000, Grad=-1.3500311979441904e-13\n",
      "  Param 4: Value=0.0000, Grad=2.8421709430404007e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3243, Grad=0.027123897803471797\n",
      "------------------------------\n",
      "--- Epoch 351 / Loss: 2041.558052 ---\n",
      "  Param 0: Value=27.2938, Grad=-0.080074380866478\n",
      "  Param 1: Value=1.7026, Grad=-0.014105223958349455\n",
      "  Param 2: Value=3.1720, Grad=-0.43748804669485253\n",
      "  Param 3: Value=0.0000, Grad=-9.983125437429408e-13\n",
      "  Param 4: Value=0.0000, Grad=1.7053025658242404e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3241, Grad=0.024405345785250443\n",
      "------------------------------\n",
      "--- Epoch 361 / Loss: 2041.577369 ---\n",
      "  Param 0: Value=27.2981, Grad=-0.08042831376414683\n",
      "  Param 1: Value=1.7028, Grad=-0.015208832618883505\n",
      "  Param 2: Value=3.1739, Grad=-0.43287602499589184\n",
      "  Param 3: Value=0.0000, Grad=-5.044853423896711e-13\n",
      "  Param 4: Value=0.0000, Grad=8.526512829121202e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3238, Grad=0.02160889520908449\n",
      "------------------------------\n",
      "--- Epoch 371 / Loss: 2041.596390 ---\n",
      "  Param 0: Value=27.3025, Grad=-0.08079165225702162\n",
      "  Param 1: Value=1.7030, Grad=-0.016304888595086275\n",
      "  Param 2: Value=3.1759, Grad=-0.42815516461502057\n",
      "  Param 3: Value=0.0000, Grad=1.4210854715202004e-14\n",
      "  Param 4: Value=0.0000, Grad=2.2737367544323206e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3236, Grad=0.018738191575569108\n",
      "------------------------------\n",
      "--- Epoch 381 / Loss: 2041.615014 ---\n",
      "  Param 0: Value=27.3070, Grad=-0.08116391586912486\n",
      "  Param 1: Value=1.7032, Grad=-0.01739108024050129\n",
      "  Param 2: Value=3.1779, Grad=-0.4233307128789079\n",
      "  Param 3: Value=0.0000, Grad=2.842170943040401e-14\n",
      "  Param 4: Value=0.0000, Grad=-3.126388037344441e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3233, Grad=0.015798401419977237\n",
      "------------------------------\n",
      "--- Epoch 391 / Loss: 2041.633129 ---\n",
      "  Param 0: Value=27.3115, Grad=-0.08154449600694724\n",
      "  Param 1: Value=1.7035, Grad=-0.01846513960125229\n",
      "  Param 2: Value=3.1800, Grad=-0.41840899046809454\n",
      "  Param 3: Value=0.0000, Grad=-6.679101716144942e-13\n",
      "  Param 4: Value=0.0000, Grad=-5.684341886080802e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3231, Grad=0.01279594607862622\n",
      "------------------------------\n",
      "--- Epoch 401 / Loss: 2041.650614 ---\n",
      "  Param 0: Value=27.3162, Grad=-0.08193266706663749\n",
      "  Param 1: Value=1.7039, Grad=-0.019524898352906384\n",
      "  Param 2: Value=3.1821, Grad=-0.4133972640237289\n",
      "  Param 3: Value=0.0000, Grad=-2.7000623958883807e-13\n",
      "  Param 4: Value=0.0000, Grad=3.979039320256561e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3230, Grad=0.009738420402819692\n",
      "------------------------------\n",
      "--- Epoch 411 / Loss: 2041.655677 ---\n",
      "  Param 0: Value=27.3176, Grad=-0.0820517236780291\n",
      "  Param 1: Value=1.7040, Grad=-0.019838343421924698\n",
      "  Param 2: Value=3.1828, Grad=-0.4118634261567422\n",
      "  Param 3: Value=0.0000, Grad=4.050093593832571e-13\n",
      "  Param 4: Value=0.0000, Grad=-1.9895196601282805e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3229, Grad=0.008802636949887699\n",
      "------------------------------\n",
      "--- Epoch 421 / Loss: 2041.660698 ---\n",
      "  Param 0: Value=27.3190, Grad=-0.08217515851633939\n",
      "  Param 1: Value=1.7041, Grad=-0.02014801438126046\n",
      "  Param 2: Value=3.1835, Grad=-0.4102833468915037\n",
      "  Param 3: Value=0.0000, Grad=1.2967404927621828e-12\n",
      "  Param 4: Value=0.0000, Grad=7.673861546209082e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3229, Grad=0.007833533904103795\n",
      "------------------------------\n",
      "--- Epoch 431 / Loss: 2041.665748 ---\n",
      "  Param 0: Value=27.3205, Grad=-0.08230393954919257\n",
      "  Param 1: Value=1.7042, Grad=-0.020453799288379315\n",
      "  Param 2: Value=3.1842, Grad=-0.4086489278899421\n",
      "  Param 3: Value=0.0000, Grad=-1.6697754290362354e-13\n",
      "  Param 4: Value=0.0000, Grad=0.0\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3228, Grad=0.006822774706912504\n",
      "------------------------------\n",
      "--- Epoch 441 / Loss: 2041.670840 ---\n",
      "  Param 0: Value=27.3220, Grad=-0.08243841950013381\n",
      "  Param 1: Value=1.7043, Grad=-0.020755095159428194\n",
      "  Param 2: Value=3.1849, Grad=-0.4069574714169448\n",
      "  Param 3: Value=0.0000, Grad=-1.2185807918285718e-12\n",
      "  Param 4: Value=0.0000, Grad=5.968558980384842e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3228, Grad=0.005767386584102807\n",
      "------------------------------\n",
      "--- Epoch 451 / Loss: 2041.675968 ---\n",
      "  Param 0: Value=27.3234, Grad=-0.08257869948429053\n",
      "  Param 1: Value=1.7045, Grad=-0.021051143268110195\n",
      "  Param 2: Value=3.1857, Grad=-0.40520847280453154\n",
      "  Param 3: Value=0.0000, Grad=-1.4210854715202004e-14\n",
      "  Param 4: Value=0.0000, Grad=1.7053025658242404e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3228, Grad=0.004666602395299924\n",
      "------------------------------\n",
      "--- Epoch 461 / Loss: 2041.681112 ---\n",
      "  Param 0: Value=27.3249, Grad=-0.0827247622410075\n",
      "  Param 1: Value=1.7046, Grad=-0.021341147796727356\n",
      "  Param 2: Value=3.1865, Grad=-0.40340244777286216\n",
      "  Param 3: Value=0.0000, Grad=2.0250467969162855e-13\n",
      "  Param 4: Value=0.0000, Grad=3.694822225952521e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3228, Grad=0.0035207000840964398\n",
      "------------------------------\n",
      "--- Epoch 471 / Loss: 2041.686249 ---\n",
      "  Param 0: Value=27.3264, Grad=-0.08287652011220548\n",
      "  Param 1: Value=1.7048, Grad=-0.021624321134480873\n",
      "  Param 2: Value=3.1873, Grad=-0.4015405091120101\n",
      "  Param 3: Value=0.0000, Grad=9.237055564881302e-13\n",
      "  Param 4: Value=0.0000, Grad=0.0\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3227, Grad=0.002330584950002823\n",
      "------------------------------\n",
      "--- Epoch 481 / Loss: 2041.691350 ---\n",
      "  Param 0: Value=27.3280, Grad=-0.08303383158640765\n",
      "  Param 1: Value=1.7050, Grad=-0.021899905039349576\n",
      "  Param 2: Value=3.1882, Grad=-0.3996242179393903\n",
      "  Param 3: Value=0.0000, Grad=2.6290081223123707e-13\n",
      "  Param 4: Value=0.0000, Grad=2.842170943040401e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3227, Grad=0.0010976474476120934\n",
      "------------------------------\n",
      "--- Epoch 491 / Loss: 2041.696384 ---\n",
      "  Param 0: Value=27.3295, Grad=-0.0831965063966143\n",
      "  Param 1: Value=1.7052, Grad=-0.02216718425627473\n",
      "  Param 2: Value=3.1891, Grad=-0.3976555341610606\n",
      "  Param 3: Value=0.0000, Grad=-6.785683126508957e-13\n",
      "  Param 4: Value=0.0000, Grad=4.263256414560601e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3227, Grad=-0.00017627785375973204\n",
      "------------------------------\n",
      "--- Epoch 501 / Loss: 2041.701317 ---\n",
      "  Param 0: Value=27.3311, Grad=-0.0833643066214722\n",
      "  Param 1: Value=1.7054, Grad=-0.022425497962619723\n",
      "  Param 2: Value=3.1900, Grad=-0.3956368010896085\n",
      "  Param 3: Value=0.0000, Grad=8.171241461241152e-13\n",
      "  Param 4: Value=0.0000, Grad=-5.684341886080802e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3227, Grad=-0.0014889175374377395\n",
      "------------------------------\n",
      "--- Epoch 511 / Loss: 2041.702763 ---\n",
      "  Param 0: Value=27.3315, Grad=-0.0834162228531839\n",
      "  Param 1: Value=1.7054, Grad=-0.02249986212098065\n",
      "  Param 2: Value=3.1903, Grad=-0.39501594894011305\n",
      "  Param 3: Value=0.0000, Grad=-3.659295089164516e-13\n",
      "  Param 4: Value=0.0000, Grad=0.0\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3227, Grad=-0.0018946560869537166\n",
      "------------------------------\n",
      "--- Epoch 521 / Loss: 2041.704189 ---\n",
      "  Param 0: Value=27.3320, Grad=-0.0834700599244228\n",
      "  Param 1: Value=1.7055, Grad=-0.02256985557684743\n",
      "  Param 2: Value=3.1905, Grad=-0.39437747696428005\n",
      "  Param 3: Value=0.0000, Grad=-1.2327916465437738e-12\n",
      "  Param 4: Value=0.0000, Grad=-9.663381206337363e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3227, Grad=-0.002315744579252099\n",
      "------------------------------\n",
      "--- Epoch 531 / Loss: 2041.705601 ---\n",
      "  Param 0: Value=27.3325, Grad=-0.08352598734983313\n",
      "  Param 1: Value=1.7056, Grad=-0.02263446607153785\n",
      "  Param 2: Value=3.1908, Grad=-0.3937204770027165\n",
      "  Param 3: Value=0.0000, Grad=-1.3855583347321954e-13\n",
      "  Param 4: Value=0.0000, Grad=2.5579538487363607e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3227, Grad=-0.0027540067542217717\n",
      "------------------------------\n",
      "--- Epoch 541 / Loss: 2041.706993 ---\n",
      "  Param 0: Value=27.3329, Grad=-0.08358403952088544\n",
      "  Param 1: Value=1.7056, Grad=-0.02269291357576808\n",
      "  Param 2: Value=3.1912, Grad=-0.39304506398701733\n",
      "  Param 3: Value=0.0000, Grad=-1.1013412404281553e-12\n",
      "  Param 4: Value=0.0000, Grad=-1.7053025658242404e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3227, Grad=-0.0032099704877717183\n",
      "------------------------------\n",
      "--- Epoch 551 / Loss: 2041.708358 ---\n",
      "  Param 0: Value=27.3334, Grad=-0.08364419356328107\n",
      "  Param 1: Value=1.7057, Grad=-0.022744504392521847\n",
      "  Param 2: Value=3.1915, Grad=-0.39235179607652526\n",
      "  Param 3: Value=0.0000, Grad=2.4158453015843406e-13\n",
      "  Param 4: Value=0.0000, Grad=-3.410605131648481e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3227, Grad=-0.0036836224386651573\n",
      "------------------------------\n",
      "--- Epoch 561 / Loss: 2041.709683 ---\n",
      "  Param 0: Value=27.3339, Grad=-0.08370639789481948\n",
      "  Param 1: Value=1.7058, Grad=-0.02278857946773183\n",
      "  Param 2: Value=3.1918, Grad=-0.39164146093122554\n",
      "  Param 3: Value=0.0000, Grad=3.375077994860476e-13\n",
      "  Param 4: Value=0.0000, Grad=-1.6484591469634324e-12\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3227, Grad=-0.004174684238730686\n",
      "------------------------------\n",
      "--- Epoch 571 / Loss: 2041.710958 ---\n",
      "  Param 0: Value=27.3344, Grad=-0.08377058270730364\n",
      "  Param 1: Value=1.7059, Grad=-0.022824499652654318\n",
      "  Param 2: Value=3.1921, Grad=-0.39091499484203496\n",
      "  Param 3: Value=0.0000, Grad=-5.684341886080801e-13\n",
      "  Param 4: Value=0.0000, Grad=2.842170943040401e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3227, Grad=-0.004682712139726153\n",
      "------------------------------\n",
      "--- Epoch 581 / Loss: 2041.712170 ---\n",
      "  Param 0: Value=27.3348, Grad=-0.08383666385221633\n",
      "  Param 1: Value=1.7060, Grad=-0.02285164580591381\n",
      "  Param 2: Value=3.1925, Grad=-0.39017344963423284\n",
      "  Param 3: Value=0.0000, Grad=3.375077994860476e-13\n",
      "  Param 4: Value=0.0000, Grad=2.2737367544323206e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3228, Grad=-0.00520713163643971\n",
      "------------------------------\n",
      "--- Epoch 591 / Loss: 2041.713305 ---\n",
      "  Param 0: Value=27.3353, Grad=-0.08390454434875594\n",
      "  Param 1: Value=1.7061, Grad=-0.022869425114055986\n",
      "  Param 2: Value=3.1928, Grad=-0.38941797621481555\n",
      "  Param 3: Value=0.0000, Grad=-3.232969447708456e-13\n",
      "  Param 4: Value=0.0000, Grad=-6.536993168992922e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3228, Grad=-0.005747248262802707\n",
      "------------------------------\n",
      "--- Epoch 601 / Loss: 2041.714349 ---\n",
      "  Param 0: Value=27.3358, Grad=-0.0839741150615752\n",
      "  Param 1: Value=1.7062, Grad=-0.022877280192757254\n",
      "  Param 2: Value=3.1932, Grad=-0.38864981362041817\n",
      "  Param 3: Value=0.0000, Grad=-4.085620730620576e-13\n",
      "  Param 4: Value=0.0000, Grad=5.684341886080802e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.3228, Grad=-0.0063022498070006705\n",
      "------------------------------\n",
      "Converged at epoch 600\n",
      "Epoch 601,  \n",
      " vecc Parameters: [2.73357963e+01 1.70617355e+00 3.19316762e+00 1.80270996e-06\n",
      " 8.33864108e-06 0.00000000e+00 3.32279279e+00]\n",
      "FINAL STATE: Epoch 601, Loss: 2041.7143492605223, \n",
      " vecc Parameters: [27.335796332861143, 1.7061735519183214, 3.1931676157831097, 1.8027099625144449e-06, 8.338641084585175e-06, 0.0, 3.322792788077663]\n",
      "Day 1 optimization finished in 134.16s over 601 epochs.\n",
      "Day 1 final results: [27.335796332861143, 1.7061735519183214, 3.1931676157831097, 1.8027099625144449e-06, 8.338641084585175e-06, 0.0, 3.322792788077663, 2041.7143492605223]\n"
     ]
    }
   ],
   "source": [
    "v = 0.5 # smooth\n",
    "mm_cond_number = 20\n",
    "nheads = 300\n",
    "lr = 0.02\n",
    "step = 100\n",
    "gamma_par = 0.3\n",
    "epochs = 700\n",
    "\n",
    "# ------------------------------------\n",
    "# Main Optimization Loop (Updated and Corrected)\n",
    "# ------------------------------------\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "num_days_to_load = 31 # From reference code\n",
    "\n",
    "for i in range(num_days_to_load):\n",
    "    idx_for_datamap = [i*8, i*8+1]\n",
    "    #idx_for_datamap = [i*8, (i+1)*8]    \n",
    "    # Using the new load function from the reference code\n",
    "    cur_map, cur_df = data_load_instance.load_working_data(\n",
    "        df_map, \n",
    "        idx_for_datamap, \n",
    "        ord_mm=None,\n",
    "        # Using float64 to ensure compatibility with kernels\n",
    "        dtype=torch.float64 \n",
    "    )\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "# ... (Data loading section unchanged and correct) ...\n",
    "print(f\"Data loaded for {len(df_day_map_list)} days.\")\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "days_list = [0]\n",
    "for day in days_list:  \n",
    "    \n",
    "    # Get the pre-loaded data for this day\n",
    "    analysis_data_map = df_day_map_list[day]\n",
    "    aggregated_data = df_day_aggregated_list[day]\n",
    "\n",
    "\n",
    "    # Initial parameters (full vector 'a' is for reference)\n",
    "    a = [28.75, 0.98, 1.56, 0, 0, 0, 1.890]\n",
    "    a = [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]\n",
    "    a = [21.303, 1.307, 1.563, 0, 0, 0, 4.769]\n",
    "\n",
    "    a = [28.303, 1.307, 1.563, 0, 0, 0, 4.769]\n",
    "    # NEW: Define params as a list of 1-element tensors (one per parameter)\n",
    "    params_list = [\n",
    "        torch.tensor([val], dtype=torch.float64, requires_grad=True) for val in a\n",
    "    ]\n",
    "\n",
    "    # NEW: Define learning rates and groups\n",
    "    lr_slow, lr_fast = 0.02, 0.02 # Assuming these are defined elsewhere\n",
    "    slow_indices = [ 1, 2, 3, 4, 5, 6] # e.g., ranges, advection, beta, nugget\n",
    "    fast_indices = [0] # e.g., sigmasq\n",
    "    \n",
    "    # Define Parameter Groups for the optimizer\n",
    "    param_groups = [\n",
    "        {'params': [params_list[idx] for idx in slow_indices], 'lr': lr_slow, 'name': 'slow_group'},\n",
    "        {'params': [params_list[idx] for idx in fast_indices], 'lr': lr_fast, 'name': 'fast_group'}\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Calculate resolution for printing\n",
    "    res_calc = (200 / lat_lon_resolution[0]) * (100 / lat_lon_resolution[0])\n",
    "    print(f'\\n--- Starting Day {day+1} (2024-07-{day+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params_list}')\n",
    "            \n",
    "    # --- Data loading is now done *before* the loop ---\n",
    "\n",
    "    # Define device\n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model_instance = kernels.model_fitting(\n",
    "            smooth = v,\n",
    "            input_map = analysis_data_map,\n",
    "            aggregated_data = aggregated_data,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Adjusted optimizer call: Passes the list of parameter groups\n",
    "    optimizer, scheduler = model_instance.optimizer_fun(\n",
    "            param_groups,     # <--- Pass the list of groups\n",
    "            lr=lr,            # <--- Default LR (will be overridden by groups, but needed for function sig)\n",
    "            betas=(0.9, 0.99), \n",
    "            eps=1e-8, \n",
    "            step_size=step, \n",
    "            gamma=gamma_par\n",
    "        )\n",
    "\n",
    "    # Calling the robust training loop\n",
    "    out, epoch_ran = model_instance.run_vecc_scheduler_oct23(\n",
    "            params_list,     # <--- Pass the list of parameter tensors\n",
    "            optimizer,\n",
    "            scheduler, \n",
    "            model_instance.matern_cov_anisotropy_v05, \n",
    "            epochs=epochs\n",
    "        )\n",
    "    # --- End Correction ---\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"Day {day+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day+1} final results: {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbdc4d9",
   "metadata": {},
   "source": [
    "# antoerh test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "743c530f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading data for all days...\n",
      "Pre-loading data for all days...\n",
      "Data loaded for 31 days.\n",
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 1250.0, smooth: 0.5\n",
      "mm_cond_number: 20,\n",
      "initial parameters: \n",
      " [tensor([28.3030], dtype=torch.float64, requires_grad=True), tensor([1.3070], dtype=torch.float64, requires_grad=True), tensor([1.5630], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([4.7690], dtype=torch.float64, requires_grad=True)]\n",
      "--- Epoch 1 / Loss: 2072.542406 ---\n",
      "  Param 0: Value=28.3030, Grad=0.7997467182134974\n",
      "  Param 1: Value=1.3070, Grad=-6.035422239558298\n",
      "  Param 2: Value=1.5630, Grad=-12.561598310167653\n",
      "  Param 3: Value=0.0000, Grad=3.659295089164516e-13\n",
      "  Param 4: Value=0.0000, Grad=-1.1368683772161603e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=4.7690, Grad=5.266075690826141\n",
      "------------------------------\n",
      "--- Epoch 11 / Loss: 2051.182166 ---\n",
      "  Param 0: Value=28.1018, Grad=0.655708645140205\n",
      "  Param 1: Value=1.5097, Grad=-3.7187432182762716\n",
      "  Param 2: Value=1.7646, Grad=-9.729823819424155\n",
      "  Param 3: Value=-0.0000, Grad=2.1316282072803006e-14\n",
      "  Param 4: Value=-0.0000, Grad=2.1316282072803006e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=4.5683, Grad=4.673769316838718\n",
      "------------------------------\n",
      "--- Epoch 21 / Loss: 2034.909593 ---\n",
      "  Param 0: Value=27.8946, Grad=0.5153955475635343\n",
      "  Param 1: Value=1.7262, Grad=-1.9964288130483911\n",
      "  Param 2: Value=1.9732, Grad=-7.539581102585487\n",
      "  Param 3: Value=-0.0000, Grad=-3.019806626980426e-13\n",
      "  Param 4: Value=-0.0000, Grad=4.405364961712621e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=4.3634, Grad=3.890984728568271\n",
      "------------------------------\n",
      "--- Epoch 31 / Loss: 2022.760378 ---\n",
      "  Param 0: Value=27.6780, Grad=0.3733227530137575\n",
      "  Param 1: Value=1.9688, Grad=-0.633716829655671\n",
      "  Param 2: Value=2.1903, Grad=-5.7967263635751465\n",
      "  Param 3: Value=-0.0000, Grad=3.943512183468556e-13\n",
      "  Param 4: Value=-0.0000, Grad=8.242295734817162e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=4.1505, Grad=2.8458707347270207\n",
      "------------------------------\n",
      "--- Epoch 41 / Loss: 2014.834379 ---\n",
      "  Param 0: Value=27.4487, Grad=0.22572838208534662\n",
      "  Param 1: Value=2.2384, Grad=0.4771637281051766\n",
      "  Param 2: Value=2.4145, Grad=-4.391307302873713\n",
      "  Param 3: Value=-0.0000, Grad=-3.339550858072471e-13\n",
      "  Param 4: Value=-0.0000, Grad=-1.7053025658242404e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.9223, Grad=1.4476452688930521\n",
      "------------------------------\n",
      "--- Epoch 51 / Loss: 2015.846074 ---\n",
      "  Param 0: Value=27.1975, Grad=0.08517966731784399\n",
      "  Param 1: Value=2.2760, Grad=1.1388977115431622\n",
      "  Param 2: Value=2.6447, Grad=-3.099163588367162\n",
      "  Param 3: Value=-0.0000, Grad=-3.197442310920451e-14\n",
      "  Param 4: Value=-0.0000, Grad=1.9895196601282805e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6605, Grad=0.18433793115446728\n",
      "------------------------------\n",
      "--- Epoch 61 / Loss: 2022.541184 ---\n",
      "  Param 0: Value=26.9155, Grad=-0.045668615602820095\n",
      "  Param 1: Value=2.1530, Grad=1.4681582405421887\n",
      "  Param 2: Value=2.8846, Grad=-1.8387233701174068\n",
      "  Param 3: Value=-0.0000, Grad=-4.263256414560601e-14\n",
      "  Param 4: Value=-0.0000, Grad=-5.684341886080801e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.4406, Grad=-0.7245923083526202\n",
      "------------------------------\n",
      "--- Epoch 71 / Loss: 2030.353575 ---\n",
      "  Param 0: Value=26.8376, Grad=-0.07632394024057193\n",
      "  Param 1: Value=1.9796, Grad=0.7204539577459759\n",
      "  Param 2: Value=3.1426, Grad=-0.8916896313994016\n",
      "  Param 3: Value=-0.0000, Grad=8.135714324453147e-13\n",
      "  Param 4: Value=-0.0000, Grad=3.694822225952521e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.4855, Grad=-0.23414565726096814\n",
      "------------------------------\n",
      "--- Epoch 81 / Loss: 2042.012887 ---\n",
      "  Param 0: Value=26.9468, Grad=-0.08174103381660747\n",
      "  Param 1: Value=1.7724, Grad=-0.5790831072411695\n",
      "  Param 2: Value=3.4310, Grad=-0.07326120090502286\n",
      "  Param 3: Value=-0.0000, Grad=2.3092638912203256e-13\n",
      "  Param 4: Value=-0.0000, Grad=8.526512829121202e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.5613, Grad=0.4163382598683447\n",
      "------------------------------\n",
      "--- Epoch 91 / Loss: 2046.547038 ---\n",
      "  Param 0: Value=27.1023, Grad=-0.15125163345515913\n",
      "  Param 1: Value=1.7850, Grad=-0.5470991671566274\n",
      "  Param 2: Value=3.6631, Grad=0.46338806401956845\n",
      "  Param 3: Value=-0.0000, Grad=-8.348877145181177e-13\n",
      "  Param 4: Value=-0.0000, Grad=-5.968558980384842e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.4765, Grad=-0.22760141691934765\n",
      "------------------------------\n",
      "--- Epoch 101 / Loss: 2039.241820 ---\n",
      "  Param 0: Value=27.2702, Grad=-0.1363648879545013\n",
      "  Param 1: Value=1.9264, Grad=-0.037312649338734794\n",
      "  Param 2: Value=3.6218, Grad=0.15811303300012014\n",
      "  Param 3: Value=-0.0000, Grad=-2.4158453015843406e-13\n",
      "  Param 4: Value=-0.0000, Grad=-1.4210854715202004e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.5252, Grad=-0.2667116248004262\n",
      "------------------------------\n",
      "--- Epoch 111 / Loss: 2036.044581 ---\n",
      "  Param 0: Value=27.3282, Grad=-0.11577083308802966\n",
      "  Param 1: Value=1.9786, Grad=0.05456867215905348\n",
      "  Param 2: Value=3.5802, Grad=-0.03070829728929425\n",
      "  Param 3: Value=-0.0000, Grad=-6.394884621840902e-14\n",
      "  Param 4: Value=-0.0000, Grad=1.1368683772161603e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.5719, Grad=-0.12266694835411807\n",
      "------------------------------\n",
      "--- Epoch 121 / Loss: 2034.336100 ---\n",
      "  Param 0: Value=27.3910, Grad=-0.09392901480573501\n",
      "  Param 1: Value=2.0019, Grad=-0.012500752725593145\n",
      "  Param 2: Value=3.5662, Grad=-0.1488553053926296\n",
      "  Param 3: Value=-0.0000, Grad=-2.4868995751603507e-14\n",
      "  Param 4: Value=-0.0000, Grad=8.526512829121202e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6295, Grad=0.11065785254175764\n",
      "------------------------------\n",
      "--- Epoch 131 / Loss: 2034.233439 ---\n",
      "  Param 0: Value=27.4549, Grad=-0.10158238696275729\n",
      "  Param 1: Value=2.0219, Grad=0.029364721227529422\n",
      "  Param 2: Value=3.6020, Grad=-0.10040359550563949\n",
      "  Param 3: Value=-0.0000, Grad=-2.5934809855243657e-13\n",
      "  Param 4: Value=-0.0000, Grad=1.1368683772161603e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6245, Grad=0.022874058644258843\n",
      "------------------------------\n",
      "--- Epoch 141 / Loss: 2035.845720 ---\n",
      "  Param 0: Value=27.5149, Grad=-0.11242073582347856\n",
      "  Param 1: Value=2.0042, Grad=-0.0391529041341947\n",
      "  Param 2: Value=3.6580, Grad=0.02475833921474191\n",
      "  Param 3: Value=-0.0000, Grad=-1.6342482922482304e-13\n",
      "  Param 4: Value=-0.0000, Grad=6.252776074688882e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6096, Grad=-0.036882497482547905\n",
      "------------------------------\n",
      "--- Epoch 151 / Loss: 2035.048184 ---\n",
      "  Param 0: Value=27.5737, Grad=-0.10813360870363486\n",
      "  Param 1: Value=2.0295, Grad=-0.047844638380412086\n",
      "  Param 2: Value=3.6811, Grad=0.010474968310489885\n",
      "  Param 3: Value=-0.0000, Grad=1.9539925233402755e-13\n",
      "  Param 4: Value=-0.0000, Grad=-3.126388037344441e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6362, Grad=0.014776201194165228\n",
      "------------------------------\n",
      "--- Epoch 161 / Loss: 2033.653537 ---\n",
      "  Param 0: Value=27.6338, Grad=-0.10679914209624769\n",
      "  Param 1: Value=2.0701, Grad=0.03022230550750926\n",
      "  Param 2: Value=3.6892, Grad=-0.028465924794573993\n",
      "  Param 3: Value=-0.0000, Grad=-8.526512829121202e-14\n",
      "  Param 4: Value=-0.0000, Grad=-1.7053025658242404e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6494, Grad=-0.014114051296785446\n",
      "------------------------------\n",
      "--- Epoch 171 / Loss: 2033.987638 ---\n",
      "  Param 0: Value=27.6941, Grad=-0.10965153823100388\n",
      "  Param 1: Value=2.0753, Grad=-0.0029713933123183267\n",
      "  Param 2: Value=3.7275, Grad=0.019982607530380392\n",
      "  Param 3: Value=-0.0000, Grad=-7.602807272633072e-13\n",
      "  Param 4: Value=-0.0000, Grad=2.2737367544323206e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6557, Grad=-0.016968621036288156\n",
      "------------------------------\n",
      "--- Epoch 181 / Loss: 2033.325683 ---\n",
      "  Param 0: Value=27.7540, Grad=-0.10531104003224567\n",
      "  Param 1: Value=2.0895, Grad=0.009539502479665174\n",
      "  Param 2: Value=3.7268, Grad=-0.01130271783293324\n",
      "  Param 3: Value=-0.0000, Grad=1.3784529073745944e-12\n",
      "  Param 4: Value=-0.0000, Grad=-8.526512829121202e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6653, Grad=0.011438827041978739\n",
      "------------------------------\n",
      "--- Epoch 191 / Loss: 2033.756863 ---\n",
      "  Param 0: Value=27.8142, Grad=-0.10618609951587266\n",
      "  Param 1: Value=2.0772, Grad=-0.0009537695228338805\n",
      "  Param 2: Value=3.7289, Grad=0.0032617207209568733\n",
      "  Param 3: Value=-0.0000, Grad=-5.933031843596837e-13\n",
      "  Param 4: Value=-0.0000, Grad=-8.526512829121202e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6521, Grad=-0.0015035519212390769\n",
      "------------------------------\n",
      "--- Epoch 201 / Loss: 2033.946640 ---\n",
      "  Param 0: Value=27.8744, Grad=-0.10543833764303018\n",
      "  Param 1: Value=2.0694, Grad=-0.007045018555196236\n",
      "  Param 2: Value=3.7273, Grad=0.004007506122661653\n",
      "  Param 3: Value=-0.0000, Grad=1.6342482922482304e-13\n",
      "  Param 4: Value=-0.0000, Grad=4.547473508864641e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6431, Grad=-0.001899387953869347\n",
      "------------------------------\n",
      "--- Epoch 211 / Loss: 2033.694095 ---\n",
      "  Param 0: Value=27.8925, Grad=-0.10449555514521175\n",
      "  Param 1: Value=2.0758, Grad=0.0026589471673306164\n",
      "  Param 2: Value=3.7280, Grad=-0.005576948313660068\n",
      "  Param 3: Value=-0.0000, Grad=-8.562039965909207e-13\n",
      "  Param 4: Value=-0.0000, Grad=1.7053025658242404e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6460, Grad=0.0006395859661810777\n",
      "------------------------------\n",
      "--- Epoch 221 / Loss: 2033.710377 ---\n",
      "  Param 0: Value=27.9105, Grad=-0.10490459359418791\n",
      "  Param 1: Value=2.0782, Grad=-0.0014271387116178147\n",
      "  Param 2: Value=3.7359, Grad=0.002179754733901973\n",
      "  Param 3: Value=-0.0000, Grad=9.734435479913373e-13\n",
      "  Param 4: Value=-0.0000, Grad=8.526512829121202e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6475, Grad=-0.0004036325720784273\n",
      "------------------------------\n",
      "--- Epoch 231 / Loss: 2033.677284 ---\n",
      "  Param 0: Value=27.9285, Grad=-0.10469901475241905\n",
      "  Param 1: Value=2.0796, Grad=-0.001236838376296845\n",
      "  Param 2: Value=3.7384, Grad=0.0023984743804419395\n",
      "  Param 3: Value=-0.0000, Grad=-1.6342482922482304e-13\n",
      "  Param 4: Value=-0.0000, Grad=-5.968558980384842e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6476, Grad=7.777360593319571e-07\n",
      "------------------------------\n",
      "--- Epoch 241 / Loss: 2033.714044 ---\n",
      "  Param 0: Value=27.9466, Grad=-0.10430481244172007\n",
      "  Param 1: Value=2.0772, Grad=-0.0018232049868913691\n",
      "  Param 2: Value=3.7367, Grad=0.0005723546777005595\n",
      "  Param 3: Value=-0.0000, Grad=-8.881784197001252e-14\n",
      "  Param 4: Value=-0.0000, Grad=3.126388037344441e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6448, Grad=0.0004274468617151417\n",
      "------------------------------\n",
      "--- Epoch 251 / Loss: 2033.669657 ---\n",
      "  Param 0: Value=27.9646, Grad=-0.10413269646668688\n",
      "  Param 1: Value=2.0791, Grad=-0.000988101962930088\n",
      "  Param 2: Value=3.7395, Grad=0.0007922783884737328\n",
      "  Param 3: Value=-0.0000, Grad=1.2079226507921703e-13\n",
      "  Param 4: Value=-0.0000, Grad=-1.1368683772161603e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6452, Grad=0.00035007345088278186\n",
      "------------------------------\n",
      "--- Epoch 261 / Loss: 2033.636575 ---\n",
      "  Param 0: Value=27.9826, Grad=-0.10381204638361818\n",
      "  Param 1: Value=2.0797, Grad=0.00023853361332459855\n",
      "  Param 2: Value=3.7404, Grad=-0.0006901198851210211\n",
      "  Param 3: Value=-0.0000, Grad=4.547473508864641e-13\n",
      "  Param 4: Value=-0.0000, Grad=5.684341886080802e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6445, Grad=0.0005741374005965305\n",
      "------------------------------\n",
      "--- Epoch 271 / Loss: 2033.638166 ---\n",
      "  Param 0: Value=28.0006, Grad=-0.10370550949256907\n",
      "  Param 1: Value=2.0800, Grad=-7.389708574834231e-06\n",
      "  Param 2: Value=3.7423, Grad=0.00020163046602306167\n",
      "  Param 3: Value=-0.0000, Grad=7.069900220812997e-13\n",
      "  Param 4: Value=-0.0000, Grad=-8.526512829121202e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6436, Grad=-0.00019537860329732482\n",
      "------------------------------\n",
      "--- Epoch 281 / Loss: 2033.635266 ---\n",
      "  Param 0: Value=28.0186, Grad=-0.10343494681697085\n",
      "  Param 1: Value=2.0799, Grad=-0.0006566067142870224\n",
      "  Param 2: Value=3.7435, Grad=-3.871664944199438e-08\n",
      "  Param 3: Value=-0.0000, Grad=5.115907697472721e-13\n",
      "  Param 4: Value=-0.0000, Grad=1.9895196601282805e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6427, Grad=0.0005796581839667692\n",
      "------------------------------\n",
      "--- Epoch 291 / Loss: 2033.612342 ---\n",
      "  Param 0: Value=28.0367, Grad=-0.10324929005848382\n",
      "  Param 1: Value=2.0808, Grad=-0.00013011607843438355\n",
      "  Param 2: Value=3.7454, Grad=7.864779718147474e-06\n",
      "  Param 3: Value=-0.0000, Grad=-4.618527782440651e-14\n",
      "  Param 4: Value=-0.0000, Grad=-7.673861546209082e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6423, Grad=0.00029805387971748587\n",
      "------------------------------\n",
      "--- Epoch 301 / Loss: 2033.601922 ---\n",
      "  Param 0: Value=28.0547, Grad=-0.10304818022793205\n",
      "  Param 1: Value=2.0810, Grad=0.0005214137139804009\n",
      "  Param 2: Value=3.7464, Grad=-0.00037023878672926003\n",
      "  Param 3: Value=-0.0000, Grad=-3.552713678800501e-14\n",
      "  Param 4: Value=-0.0000, Grad=5.400124791776761e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6412, Grad=-0.0003346635528145292\n",
      "------------------------------\n",
      "--- Epoch 311 / Loss: 2033.597509 ---\n",
      "  Param 0: Value=28.0601, Grad=-0.10297696620007535\n",
      "  Param 1: Value=2.0812, Grad=0.00017621284435165308\n",
      "  Param 2: Value=3.7472, Grad=-0.00020048889260237956\n",
      "  Param 3: Value=-0.0000, Grad=-1.900701818158268e-12\n",
      "  Param 4: Value=-0.0000, Grad=-8.526512829121202e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6412, Grad=6.810824791148029e-05\n",
      "------------------------------\n",
      "--- Epoch 321 / Loss: 2033.603299 ---\n",
      "  Param 0: Value=28.0655, Grad=-0.1029566647068505\n",
      "  Param 1: Value=2.0811, Grad=-9.694684934746789e-05\n",
      "  Param 2: Value=3.7477, Grad=0.00022650967611070882\n",
      "  Param 3: Value=-0.0000, Grad=5.258016244624741e-13\n",
      "  Param 4: Value=-0.0000, Grad=2.842170943040401e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6408, Grad=-0.00020169551623105964\n",
      "------------------------------\n",
      "--- Epoch 331 / Loss: 2033.593985 ---\n",
      "  Param 0: Value=28.0709, Grad=-0.10285684206775914\n",
      "  Param 1: Value=2.0814, Grad=2.3733598258246502e-05\n",
      "  Param 2: Value=3.7481, Grad=-9.863890323913438e-05\n",
      "  Param 3: Value=-0.0000, Grad=9.947598300641403e-14\n",
      "  Param 4: Value=-0.0000, Grad=-6.821210263296962e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6407, Grad=0.0001016252988959021\n",
      "------------------------------\n",
      "Converged at epoch 335\n",
      "Epoch 336,  \n",
      " vecc Parameters: [ 2.80735826e+01  2.08143716e+00  3.74831400e+00 -1.75082373e-05\n",
      " -3.88104054e-06  0.00000000e+00  3.64062205e+00]\n",
      "FINAL STATE: Epoch 336, Loss: 2033.59141857436, \n",
      " vecc Parameters: [28.07358259766055, 2.0814371586888822, 3.7483140035232414, -1.750823729127743e-05, -3.881040539593337e-06, 0.0, 3.6406220522441455]\n",
      "Day 1 optimization finished in 75.41s over 336 epochs.\n",
      "Day 1 final results: [28.07358259766055, 2.0814371586888822, 3.7483140035232414, -1.750823729127743e-05, -3.881040539593337e-06, 0.0, 3.6406220522441455, 2033.59141857436]\n"
     ]
    }
   ],
   "source": [
    "v = 0.5 # smooth\n",
    "mm_cond_number = 20\n",
    "nheads = 300\n",
    "lr = 0.02\n",
    "step = 100\n",
    "gamma_par = 0.3\n",
    "epochs = 500\n",
    "\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "num_days_to_load = 31 # From reference code\n",
    "\n",
    "for i in range(num_days_to_load):\n",
    "    idx_for_datamap = [i*8, i*8+1]\n",
    "    #idx_for_datamap = [i*8, (i+1)*8]    \n",
    "    # Using the new load function from the reference code\n",
    "    cur_map, cur_df = data_load_instance.load_working_data(\n",
    "        df_map, \n",
    "        idx_for_datamap, \n",
    "        ord_mm=None,\n",
    "        # Using float64 to ensure compatibility with kernels\n",
    "        dtype=torch.float64 \n",
    "    )\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "# ... (Data loading section unchanged and correct) ...\n",
    "print(f\"Data loaded for {len(df_day_map_list)} days.\")\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "days_list = [0]\n",
    "for day in days_list:  \n",
    "    \n",
    "    # Get the pre-loaded data for this day\n",
    "    analysis_data_map = df_day_map_list[day]\n",
    "    aggregated_data = df_day_aggregated_list[day]\n",
    "\n",
    "\n",
    "    # Initial parameters (full vector 'a' is for reference)\n",
    "    a = [28.75, 0.98, 1.56, 0, 0, 0, 1.890]\n",
    "    a = [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]\n",
    "    a = [28.303, 1.307, 1.563, 0, 0, 0, 4.769]\n",
    "    # NEW: Define params as a list of 1-element tensors (one per parameter)\n",
    "    params_list = [\n",
    "        torch.tensor([val], dtype=torch.float64, requires_grad=True) for val in a\n",
    "    ]\n",
    "\n",
    "    # NEW: Define learning rates and groups\n",
    "    lr_slow, lr_fast = 0.02, 0.02 # Assuming these are defined elsewhere\n",
    "    slow_indices = [ 1, 2, 3, 4, 5, 6] # e.g., ranges, advection, beta, nugget\n",
    "    fast_indices = [0] # e.g., sigmasq\n",
    "    \n",
    "    # Define Parameter Groups for the optimizer\n",
    "    param_groups = [\n",
    "        {'params': [params_list[idx] for idx in slow_indices], 'lr': lr_slow, 'name': 'slow_group'},\n",
    "        {'params': [params_list[idx] for idx in fast_indices], 'lr': lr_fast, 'name': 'fast_group'}\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Calculate resolution for printing\n",
    "    res_calc = (200 / lat_lon_resolution[0]) * (100 / lat_lon_resolution[0])\n",
    "    print(f'\\n--- Starting Day {day+1} (2024-07-{day+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params_list}')\n",
    "            \n",
    "    # --- Data loading is now done *before* the loop ---\n",
    "\n",
    "    # Define device\n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model_instance = kernels.model_fitting(\n",
    "            smooth = v,\n",
    "            input_map = analysis_data_map,\n",
    "            aggregated_data = aggregated_data,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Adjusted optimizer call: Passes the list of parameter groups\n",
    "    optimizer, scheduler = model_instance.optimizer_fun(\n",
    "            param_groups,     # <--- Pass the list of groups\n",
    "            lr=lr,            # <--- Default LR (will be overridden by groups, but needed for function sig)\n",
    "            betas=(0.9, 0.8), \n",
    "            eps=1e-8, \n",
    "            step_size=step, \n",
    "            gamma=gamma_par\n",
    "        )\n",
    "\n",
    "    # Calling the robust training loop\n",
    "    out, epoch_ran = model_instance.run_vecc_scheduler_oct23(\n",
    "            params_list,     # <--- Pass the list of parameter tensors\n",
    "            optimizer,\n",
    "            scheduler, \n",
    "            model_instance.matern_cov_anisotropy_v05, \n",
    "            epochs=epochs\n",
    "        )\n",
    "    # --- End Correction ---\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"Day {day+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day+1} final results: {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af67e88",
   "metadata": {},
   "source": [
    "# thirs test step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a72bf37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading data for all days...\n",
      "Pre-loading data for all days...\n",
      "Data loaded for 31 days.\n",
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 1250.0, smooth: 0.5\n",
      "mm_cond_number: 20,\n",
      "initial parameters: \n",
      " [tensor([28.3030], dtype=torch.float64, requires_grad=True), tensor([1.3070], dtype=torch.float64, requires_grad=True), tensor([1.5630], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([0.], dtype=torch.float64, requires_grad=True), tensor([4.7690], dtype=torch.float64, requires_grad=True)]\n",
      "--- Epoch 1 / Loss: 2072.542406 ---\n",
      "  Param 0: Value=28.3030, Grad=0.7997467182134974\n",
      "  Param 1: Value=1.3070, Grad=-6.035422239558298\n",
      "  Param 2: Value=1.5630, Grad=-12.561598310167653\n",
      "  Param 3: Value=0.0000, Grad=3.659295089164516e-13\n",
      "  Param 4: Value=0.0000, Grad=-1.1368683772161603e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=4.7690, Grad=5.266075690826141\n",
      "------------------------------\n",
      "--- Epoch 11 / Loss: 2051.182166 ---\n",
      "  Param 0: Value=28.1018, Grad=0.655708645140205\n",
      "  Param 1: Value=1.5097, Grad=-3.7187432182762716\n",
      "  Param 2: Value=1.7646, Grad=-9.729823819424155\n",
      "  Param 3: Value=-0.0000, Grad=2.1316282072803006e-14\n",
      "  Param 4: Value=-0.0000, Grad=2.1316282072803006e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=4.5683, Grad=4.673769316838718\n",
      "------------------------------\n",
      "--- Epoch 21 / Loss: 2034.909593 ---\n",
      "  Param 0: Value=27.8946, Grad=0.5153955475635343\n",
      "  Param 1: Value=1.7262, Grad=-1.9964288130483911\n",
      "  Param 2: Value=1.9732, Grad=-7.539581102585487\n",
      "  Param 3: Value=-0.0000, Grad=-3.019806626980426e-13\n",
      "  Param 4: Value=-0.0000, Grad=4.405364961712621e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=4.3634, Grad=3.890984728568271\n",
      "------------------------------\n",
      "--- Epoch 31 / Loss: 2022.760378 ---\n",
      "  Param 0: Value=27.6780, Grad=0.3733227530137575\n",
      "  Param 1: Value=1.9688, Grad=-0.633716829655671\n",
      "  Param 2: Value=2.1903, Grad=-5.7967263635751465\n",
      "  Param 3: Value=-0.0000, Grad=3.943512183468556e-13\n",
      "  Param 4: Value=-0.0000, Grad=8.242295734817162e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=4.1505, Grad=2.8458707347270207\n",
      "------------------------------\n",
      "--- Epoch 41 / Loss: 2014.834379 ---\n",
      "  Param 0: Value=27.4487, Grad=0.22572838208534662\n",
      "  Param 1: Value=2.2384, Grad=0.4771637281051766\n",
      "  Param 2: Value=2.4145, Grad=-4.391307302873713\n",
      "  Param 3: Value=-0.0000, Grad=-3.339550858072471e-13\n",
      "  Param 4: Value=-0.0000, Grad=-1.7053025658242404e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.9223, Grad=1.4476452688930521\n",
      "------------------------------\n",
      "--- Epoch 51 / Loss: 2015.846074 ---\n",
      "  Param 0: Value=27.1975, Grad=0.08517966731784399\n",
      "  Param 1: Value=2.2760, Grad=1.1388977115431622\n",
      "  Param 2: Value=2.6447, Grad=-3.099163588367162\n",
      "  Param 3: Value=-0.0000, Grad=-3.197442310920451e-14\n",
      "  Param 4: Value=-0.0000, Grad=1.9895196601282805e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6605, Grad=0.18433793115446728\n",
      "------------------------------\n",
      "--- Epoch 61 / Loss: 2022.541184 ---\n",
      "  Param 0: Value=26.9155, Grad=-0.045668615602820095\n",
      "  Param 1: Value=2.1530, Grad=1.4681582405421887\n",
      "  Param 2: Value=2.8846, Grad=-1.8387233701174068\n",
      "  Param 3: Value=-0.0000, Grad=-4.263256414560601e-14\n",
      "  Param 4: Value=-0.0000, Grad=-5.684341886080801e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.4406, Grad=-0.7245923083526202\n",
      "------------------------------\n",
      "--- Epoch 71 / Loss: 2030.353575 ---\n",
      "  Param 0: Value=26.8376, Grad=-0.07632394024057193\n",
      "  Param 1: Value=1.9796, Grad=0.7204539577459759\n",
      "  Param 2: Value=3.1426, Grad=-0.8916896313994016\n",
      "  Param 3: Value=-0.0000, Grad=8.135714324453147e-13\n",
      "  Param 4: Value=-0.0000, Grad=3.694822225952521e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.4855, Grad=-0.23414565726096814\n",
      "------------------------------\n",
      "--- Epoch 81 / Loss: 2033.406163 ---\n",
      "  Param 0: Value=26.8704, Grad=-0.07128798834096539\n",
      "  Param 1: Value=1.9130, Grad=0.30718584723446085\n",
      "  Param 2: Value=3.2255, Grad=-0.6545751719119153\n",
      "  Param 3: Value=-0.0000, Grad=5.897504706808832e-13\n",
      "  Param 4: Value=-0.0000, Grad=-8.526512829121202e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.5269, Grad=0.07773956172607344\n",
      "------------------------------\n",
      "--- Epoch 91 / Loss: 2037.562935 ---\n",
      "  Param 0: Value=26.9213, Grad=-0.07412930751145175\n",
      "  Param 1: Value=1.8328, Grad=-0.10442268005246014\n",
      "  Param 2: Value=3.3073, Grad=-0.38866536119218154\n",
      "  Param 3: Value=-0.0000, Grad=5.861977570020827e-13\n",
      "  Param 4: Value=-0.0000, Grad=3.979039320256561e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.5343, Grad=0.26799911008587407\n",
      "------------------------------\n",
      "--- Epoch 101 / Loss: 2040.552631 ---\n",
      "  Param 0: Value=26.9764, Grad=-0.09844612352832516\n",
      "  Param 1: Value=1.8018, Grad=-0.22400990530833376\n",
      "  Param 2: Value=3.3921, Grad=-0.12036606233306202\n",
      "  Param 3: Value=-0.0000, Grad=4.263256414560601e-13\n",
      "  Param 4: Value=-0.0000, Grad=2.842170943040401e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.4959, Grad=0.10762188490332258\n",
      "------------------------------\n",
      "--- Epoch 111 / Loss: 2041.355322 ---\n",
      "  Param 0: Value=27.0298, Grad=-0.13191947981220342\n",
      "  Param 1: Value=1.8315, Grad=-0.0842043497234588\n",
      "  Param 2: Value=3.4840, Grad=0.09052849223451176\n",
      "  Param 3: Value=-0.0000, Grad=3.446132268436486e-13\n",
      "  Param 4: Value=-0.0000, Grad=7.673861546209082e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.4590, Grad=-0.23847039963175476\n",
      "------------------------------\n",
      "--- Epoch 121 / Loss: 2039.437073 ---\n",
      "  Param 0: Value=27.0842, Grad=-0.13146516256806362\n",
      "  Param 1: Value=1.8841, Grad=0.028997759512389365\n",
      "  Param 2: Value=3.5093, Grad=0.049469731836893516\n",
      "  Param 3: Value=-0.0000, Grad=-2.7711166694643907e-13\n",
      "  Param 4: Value=-0.0000, Grad=2.842170943040401e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.4865, Grad=-0.2681552547324142\n",
      "------------------------------\n",
      "--- Epoch 131 / Loss: 2037.867256 ---\n",
      "  Param 0: Value=27.1426, Grad=-0.11228705843896503\n",
      "  Param 1: Value=1.9058, Grad=-0.02503522740662323\n",
      "  Param 2: Value=3.5014, Grad=-0.056811485841734566\n",
      "  Param 3: Value=-0.0000, Grad=2.948752353404416e-13\n",
      "  Param 4: Value=-0.0000, Grad=-5.115907697472721e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.5370, Grad=-0.06470305961467826\n",
      "------------------------------\n",
      "--- Epoch 141 / Loss: 2036.797075 ---\n",
      "  Param 0: Value=27.2047, Grad=-0.10110806673027206\n",
      "  Param 1: Value=1.9335, Grad=-0.09408843686030455\n",
      "  Param 2: Value=3.5315, Grad=-0.08714318791936648\n",
      "  Param 3: Value=-0.0000, Grad=-1.7763568394002505e-13\n",
      "  Param 4: Value=-0.0000, Grad=-1.7053025658242404e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.5893, Grad=0.08419470112182226\n",
      "------------------------------\n",
      "--- Epoch 151 / Loss: 2036.545846 ---\n",
      "  Param 0: Value=27.2234, Grad=-0.10553849433280416\n",
      "  Param 1: Value=1.9478, Grad=-0.050919800344935595\n",
      "  Param 2: Value=3.5461, Grad=-0.07002232312672163\n",
      "  Param 3: Value=-0.0000, Grad=7.815970093361102e-13\n",
      "  Param 4: Value=-0.0000, Grad=0.0\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.5868, Grad=0.026666550215424856\n",
      "------------------------------\n",
      "--- Epoch 161 / Loss: 2036.232977 ---\n",
      "  Param 0: Value=27.2414, Grad=-0.11215223178332334\n",
      "  Param 1: Value=1.9668, Grad=0.012880937403151549\n",
      "  Param 2: Value=3.5640, Grad=-0.04689008066361566\n",
      "  Param 3: Value=-0.0000, Grad=1.5987211554602254e-13\n",
      "  Param 4: Value=-0.0000, Grad=2.8421709430404007e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.5812, Grad=-0.06096337086311476\n",
      "------------------------------\n",
      "--- Epoch 171 / Loss: 2036.156184 ---\n",
      "  Param 0: Value=27.2592, Grad=-0.11314743116887538\n",
      "  Param 1: Value=1.9760, Grad=0.0016954408619573869\n",
      "  Param 2: Value=3.5842, Grad=-0.027301859133814332\n",
      "  Param 3: Value=-0.0000, Grad=-6.750155989720952e-14\n",
      "  Param 4: Value=-0.0000, Grad=2.2737367544323206e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.5906, Grad=-0.05817957490216541\n",
      "------------------------------\n",
      "--- Epoch 181 / Loss: 2036.163503 ---\n",
      "  Param 0: Value=27.2770, Grad=-0.11216935776180759\n",
      "  Param 1: Value=1.9825, Grad=-0.03789711942770069\n",
      "  Param 2: Value=3.6071, Grad=-0.007410624587635084\n",
      "  Param 3: Value=-0.0000, Grad=6.288303211476887e-13\n",
      "  Param 4: Value=-0.0000, Grad=-2.842170943040401e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6065, Grad=-0.02112697953179521\n",
      "------------------------------\n",
      "--- Epoch 191 / Loss: 2035.981312 ---\n",
      "  Param 0: Value=27.2950, Grad=-0.11094401227328365\n",
      "  Param 1: Value=1.9945, Grad=-0.06885958745143483\n",
      "  Param 2: Value=3.6320, Grad=0.0073425924256582675\n",
      "  Param 3: Value=-0.0000, Grad=-4.760636329592671e-13\n",
      "  Param 4: Value=-0.0000, Grad=7.389644451905042e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6260, Grad=0.01615763769832923\n",
      "------------------------------\n",
      "--- Epoch 201 / Loss: 2035.505043 ---\n",
      "  Param 0: Value=27.3131, Grad=-0.11166997188975014\n",
      "  Param 1: Value=2.0095, Grad=-0.033562077161789716\n",
      "  Param 2: Value=3.6371, Grad=-0.0010694449608479317\n",
      "  Param 3: Value=-0.0000, Grad=-3.197442310920451e-13\n",
      "  Param 4: Value=-0.0000, Grad=-2.2737367544323206e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6290, Grad=-0.007180025499000431\n",
      "------------------------------\n",
      "--- Epoch 211 / Loss: 2034.859740 ---\n",
      "  Param 0: Value=27.3310, Grad=-0.11173792697077561\n",
      "  Param 1: Value=2.0292, Grad=0.004962823400146199\n",
      "  Param 2: Value=3.6430, Grad=-0.015325677054988773\n",
      "  Param 3: Value=-0.0000, Grad=-7.212008767965017e-13\n",
      "  Param 4: Value=-0.0000, Grad=0.0\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6359, Grad=-0.025572062240721394\n",
      "------------------------------\n",
      "--- Epoch 221 / Loss: 2034.730668 ---\n",
      "  Param 0: Value=27.3364, Grad=-0.11134790896209912\n",
      "  Param 1: Value=2.0338, Grad=0.00538491735328428\n",
      "  Param 2: Value=3.6468, Grad=-0.016622371621679122\n",
      "  Param 3: Value=-0.0000, Grad=-1.5631940186722204e-13\n",
      "  Param 4: Value=-0.0000, Grad=5.684341886080801e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6400, Grad=-0.020663065212338627\n",
      "------------------------------\n",
      "--- Epoch 231 / Loss: 2034.699508 ---\n",
      "  Param 0: Value=27.3419, Grad=-0.11042959904128646\n",
      "  Param 1: Value=2.0354, Grad=-0.006982174756798187\n",
      "  Param 2: Value=3.6515, Grad=-0.015239665821781045\n",
      "  Param 3: Value=-0.0000, Grad=1.8829382497642655e-13\n",
      "  Param 4: Value=-0.0000, Grad=5.684341886080802e-14\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6453, Grad=-0.0037216669119271018\n",
      "------------------------------\n",
      "--- Epoch 241 / Loss: 2034.627747 ---\n",
      "  Param 0: Value=27.3473, Grad=-0.10990768561782926\n",
      "  Param 1: Value=2.0387, Grad=-0.01351410017046284\n",
      "  Param 2: Value=3.6568, Grad=-0.01376452636253589\n",
      "  Param 3: Value=-0.0000, Grad=-9.947598300641403e-14\n",
      "  Param 4: Value=-0.0000, Grad=-3.694822225952521e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6503, Grad=0.006680074920700552\n",
      "------------------------------\n",
      "--- Epoch 251 / Loss: 2034.568902 ---\n",
      "  Param 0: Value=27.3527, Grad=-0.11106618765865361\n",
      "  Param 1: Value=2.0433, Grad=-0.005247185911681385\n",
      "  Param 2: Value=3.6625, Grad=-0.007868735611879174\n",
      "  Param 3: Value=-0.0000, Grad=1.7763568394002505e-13\n",
      "  Param 4: Value=-0.0000, Grad=-3.126388037344441e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6506, Grad=-0.006959017226652375\n",
      "------------------------------\n",
      "--- Epoch 261 / Loss: 2034.450224 ---\n",
      "  Param 0: Value=27.3581, Grad=-0.1116864557957461\n",
      "  Param 1: Value=2.0494, Grad=0.000643323190170797\n",
      "  Param 2: Value=3.6690, Grad=-0.004235327527709387\n",
      "  Param 3: Value=-0.0000, Grad=1.6697754290362354e-13\n",
      "  Param 4: Value=-0.0000, Grad=-3.979039320256561e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6535, Grad=-0.013576983170180967\n",
      "------------------------------\n",
      "--- Epoch 271 / Loss: 2034.376817 ---\n",
      "  Param 0: Value=27.3635, Grad=-0.1119053746352423\n",
      "  Param 1: Value=2.0541, Grad=-0.0013672246276650313\n",
      "  Param 2: Value=3.6763, Grad=0.0006149525392231681\n",
      "  Param 3: Value=-0.0000, Grad=-5.151434834260726e-13\n",
      "  Param 4: Value=-0.0000, Grad=1.4210854715202004e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6577, Grad=-0.011815213097505217\n",
      "------------------------------\n",
      "--- Epoch 281 / Loss: 2034.271560 ---\n",
      "  Param 0: Value=27.3688, Grad=-0.11112553816633652\n",
      "  Param 1: Value=2.0579, Grad=-0.007005371608118871\n",
      "  Param 2: Value=3.6807, Grad=-0.0002961878812186569\n",
      "  Param 3: Value=-0.0000, Grad=1.4210854715202004e-14\n",
      "  Param 4: Value=-0.0000, Grad=-6.252776074688882e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6631, Grad=0.0003924297252160258\n",
      "------------------------------\n",
      "--- Epoch 291 / Loss: 2034.233025 ---\n",
      "  Param 0: Value=27.3705, Grad=-0.11091257086281936\n",
      "  Param 1: Value=2.0591, Grad=-0.00750822569283649\n",
      "  Param 2: Value=3.6816, Grad=-0.001096266308650229\n",
      "  Param 3: Value=-0.0000, Grad=4.440892098500626e-13\n",
      "  Param 4: Value=-0.0000, Grad=-1.7053025658242404e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6645, Grad=0.003041571247136443\n",
      "------------------------------\n",
      "--- Epoch 301 / Loss: 2034.208774 ---\n",
      "  Param 0: Value=27.3721, Grad=-0.11126233460090296\n",
      "  Param 1: Value=2.0606, Grad=-0.004212162253952378\n",
      "  Param 2: Value=3.6831, Grad=0.00019891134076033268\n",
      "  Param 3: Value=-0.0000, Grad=-6.536993168992922e-13\n",
      "  Param 4: Value=-0.0000, Grad=1.9895196601282805e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6644, Grad=-0.0016629706374016173\n",
      "------------------------------\n",
      "--- Epoch 311 / Loss: 2034.156635 ---\n",
      "  Param 0: Value=27.3737, Grad=-0.11132446224448944\n",
      "  Param 1: Value=2.0625, Grad=-0.0011587685791809577\n",
      "  Param 2: Value=3.6839, Grad=-0.0004476346179558277\n",
      "  Param 3: Value=-0.0000, Grad=2.4513724383723456e-13\n",
      "  Param 4: Value=-0.0000, Grad=3.410605131648481e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6651, Grad=-0.0036090163201201175\n",
      "------------------------------\n",
      "--- Epoch 321 / Loss: 2034.093063 ---\n",
      "  Param 0: Value=27.3753, Grad=-0.11122566263083777\n",
      "  Param 1: Value=2.0645, Grad=0.0010075313210418102\n",
      "  Param 2: Value=3.6849, Grad=-0.0017283115486463885\n",
      "  Param 3: Value=-0.0000, Grad=-6.643574579356937e-13\n",
      "  Param 4: Value=-0.0000, Grad=-8.242295734817162e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6663, Grad=-0.003449720577021087\n",
      "------------------------------\n",
      "--- Epoch 331 / Loss: 2034.068060 ---\n",
      "  Param 0: Value=27.3770, Grad=-0.11095338302872382\n",
      "  Param 1: Value=2.0654, Grad=-0.0013076664017006578\n",
      "  Param 2: Value=3.6861, Grad=-0.001998973994204789\n",
      "  Param 3: Value=-0.0000, Grad=-1.0089706847793423e-12\n",
      "  Param 4: Value=-0.0000, Grad=-8.242295734817162e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6679, Grad=0.0008540015481394159\n",
      "------------------------------\n",
      "--- Epoch 341 / Loss: 2034.050883 ---\n",
      "  Param 0: Value=27.3786, Grad=-0.11107104166850706\n",
      "  Param 1: Value=2.0665, Grad=-0.000723793363674119\n",
      "  Param 2: Value=3.6875, Grad=-0.0009750550321996343\n",
      "  Param 3: Value=-0.0000, Grad=-6.927791673660977e-13\n",
      "  Param 4: Value=-0.0000, Grad=2.8421709430404007e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6684, Grad=-0.00013248945836763504\n",
      "------------------------------\n",
      "--- Epoch 351 / Loss: 2034.036395 ---\n",
      "  Param 0: Value=27.3802, Grad=-0.11127910254334106\n",
      "  Param 1: Value=2.0677, Grad=0.00016402176270258195\n",
      "  Param 2: Value=3.6893, Grad=0.0005857295127782436\n",
      "  Param 3: Value=-0.0000, Grad=-5.400124791776761e-13\n",
      "  Param 4: Value=-0.0000, Grad=-3.694822225952521e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6689, Grad=-0.002037955807388636\n",
      "------------------------------\n",
      "--- Epoch 361 / Loss: 2034.026676 ---\n",
      "  Param 0: Value=27.3807, Grad=-0.11117716094487451\n",
      "  Param 1: Value=2.0679, Grad=-0.00023635118356057916\n",
      "  Param 2: Value=3.6894, Grad=0.00015984749003905563\n",
      "  Param 3: Value=-0.0000, Grad=-3.623767952376511e-13\n",
      "  Param 4: Value=-0.0000, Grad=2.2737367544323206e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6693, Grad=-0.0008248442749917428\n",
      "------------------------------\n",
      "--- Epoch 371 / Loss: 2034.010166 ---\n",
      "  Param 0: Value=27.3812, Grad=-0.11103992897556625\n",
      "  Param 1: Value=2.0682, Grad=-0.00042640639116431345\n",
      "  Param 2: Value=3.6894, Grad=-0.0006201850936804476\n",
      "  Param 3: Value=-0.0000, Grad=-1.9895196601282805e-13\n",
      "  Param 4: Value=-0.0000, Grad=-3.410605131648481e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6698, Grad=0.000596812676092906\n",
      "------------------------------\n",
      "--- Epoch 381 / Loss: 2034.001631 ---\n",
      "  Param 0: Value=27.3816, Grad=-0.11109457384081645\n",
      "  Param 1: Value=2.0686, Grad=0.0003184869871848406\n",
      "  Param 2: Value=3.6897, Grad=-0.0004872648280525027\n",
      "  Param 3: Value=-0.0000, Grad=2.984279490192421e-13\n",
      "  Param 4: Value=-0.0000, Grad=-5.684341886080801e-13\n",
      "  Param 5: Value=0.0000, Grad=0.0\n",
      "  Param 6: Value=3.6698, Grad=-0.00025049408307009635\n",
      "------------------------------\n",
      "Converged at epoch 381\n",
      "Epoch 382,  \n",
      " vecc Parameters: [ 2.73816983e+01  2.06864843e+00  3.68976029e+00 -8.89726343e-06\n",
      " -9.77431690e-06  0.00000000e+00  3.66984849e+00]\n",
      "FINAL STATE: Epoch 382, Loss: 2034.0016651968785, \n",
      " vecc Parameters: [27.381698284163683, 2.0686484309400095, 3.689760286439995, -8.897263427279982e-06, -9.77431690317228e-06, 0.0, 3.669848493534735]\n",
      "Day 1 optimization finished in 85.23s over 382 epochs.\n",
      "Day 1 final results: [27.381698284163683, 2.0686484309400095, 3.689760286439995, -8.897263427279982e-06, -9.77431690317228e-06, 0.0, 3.669848493534735, 2034.0016651968785]\n"
     ]
    }
   ],
   "source": [
    "v = 0.5 # smooth\n",
    "mm_cond_number = 20\n",
    "nheads = 300\n",
    "lr = 0.02\n",
    "step = 70\n",
    "gamma_par = 0.3\n",
    "epochs = 700\n",
    "\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "num_days_to_load = 31 # From reference code\n",
    "\n",
    "for i in range(num_days_to_load):\n",
    "    idx_for_datamap = [i*8, i*8+1]\n",
    "    #idx_for_datamap = [i*8, (i+1)*8]    \n",
    "    # Using the new load function from the reference code\n",
    "    cur_map, cur_df = data_load_instance.load_working_data(\n",
    "        df_map, \n",
    "        idx_for_datamap, \n",
    "        ord_mm=None,\n",
    "        # Using float64 to ensure compatibility with kernels\n",
    "        dtype=torch.float64 \n",
    "    )\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "# ... (Data loading section unchanged and correct) ...\n",
    "print(f\"Data loaded for {len(df_day_map_list)} days.\")\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "days_list = [0]\n",
    "for day in days_list:  \n",
    "    \n",
    "    # Get the pre-loaded data for this day\n",
    "    analysis_data_map = df_day_map_list[day]\n",
    "    aggregated_data = df_day_aggregated_list[day]\n",
    "\n",
    "\n",
    "    # Initial parameters (full vector 'a' is for reference)\n",
    "    a = [28.75, 0.98, 1.56, 0, 0, 0, 1.890]\n",
    "    a = [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]\n",
    "    a = [28.303, 1.307, 1.563, 0, 0, 0, 4.769]\n",
    "    # NEW: Define params as a list of 1-element tensors (one per parameter)\n",
    "    params_list = [\n",
    "        torch.tensor([val], dtype=torch.float64, requires_grad=True) for val in a\n",
    "    ]\n",
    "\n",
    "    # NEW: Define learning rates and groups\n",
    "    lr_slow, lr_fast = 0.02, 0.02 # Assuming these are defined elsewhere\n",
    "    slow_indices = [ 1, 2, 3, 4, 5, 6] # e.g., ranges, advection, beta, nugget\n",
    "    fast_indices = [0] # e.g., sigmasq\n",
    "    \n",
    "    # Define Parameter Groups for the optimizer\n",
    "    param_groups = [\n",
    "        {'params': [params_list[idx] for idx in slow_indices], 'lr': lr_slow, 'name': 'slow_group'},\n",
    "        {'params': [params_list[idx] for idx in fast_indices], 'lr': lr_fast, 'name': 'fast_group'}\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Calculate resolution for printing\n",
    "    res_calc = (200 / lat_lon_resolution[0]) * (100 / lat_lon_resolution[0])\n",
    "    print(f'\\n--- Starting Day {day+1} (2024-07-{day+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params_list}')\n",
    "            \n",
    "    # --- Data loading is now done *before* the loop ---\n",
    "\n",
    "    # Define device\n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model_instance = kernels.model_fitting(\n",
    "            smooth = v,\n",
    "            input_map = analysis_data_map,\n",
    "            aggregated_data = aggregated_data,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Adjusted optimizer call: Passes the list of parameter groups\n",
    "    optimizer, scheduler = model_instance.optimizer_fun(\n",
    "            param_groups,     # <--- Pass the list of groups\n",
    "            lr=lr,            # <--- Default LR (will be overridden by groups, but needed for function sig)\n",
    "            betas=(0.9, 0.8), \n",
    "            eps=1e-8, \n",
    "            step_size=step, \n",
    "            gamma=gamma_par\n",
    "        )\n",
    "\n",
    "    # Calling the robust training loop\n",
    "    out, epoch_ran = model_instance.run_vecc_scheduler_oct23(\n",
    "            params_list,     # <--- Pass the list of parameter tensors\n",
    "            optimizer,\n",
    "            scheduler, \n",
    "            model_instance.matern_cov_anisotropy_v05, \n",
    "            epochs=epochs\n",
    "        )\n",
    "    # --- End Correction ---\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"Day {day+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day+1} final results: {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0798b4",
   "metadata": {},
   "source": [
    "# old veresion single learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a92b486d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading data for all days...\n",
      "Data loaded for 31 days.\n",
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 1250.0, smooth: 0.5\n",
      "mm_cond_number: 20,\n",
      "initial parameters: \n",
      " [28.75  0.98  1.06  0.    0.    0.    1.89]\n",
      "Epoch 1, Gradients: [ 6.76930898e-01  2.84719128e+00 -2.83749606e+01 -3.55271368e-13\n",
      " -6.53699317e-13  0.00000000e+00  2.00032705e+00]\n",
      " Loss: 2105.305294117777, Parameters: [28.75  0.98  1.06  0.    0.    0.    1.89]\n",
      "Epoch 11, Gradients: [ 6.14380725e-01 -1.75437339e+00 -1.88931280e+01  8.10018719e-13\n",
      "  2.27373675e-13  0.00000000e+00  2.92466141e+00]\n",
      " Loss: 2108.104279627454, Parameters: [2.85498405e+01 8.29343906e-01 1.26224386e+00 1.10189496e-06\n",
      " 5.16845217e-06 0.00000000e+00 1.69434411e+00]\n",
      "Epoch 21, Gradients: [ 2.77650151e-01  1.03782795e+00 -1.07808718e+01  3.26849658e-13\n",
      " -4.26325641e-13  0.00000000e+00  1.05741661e+00]\n",
      " Loss: 2094.7642846757913, Parameters: [ 2.83390166e+01  8.66321677e-01  1.47686706e+00 -2.06307303e-06\n",
      "  6.78750835e-06  0.00000000e+00  1.48996304e+00]\n",
      "Epoch 31, Gradients: [-8.13475556e-02 -3.51067116e-01 -2.48603373e+00 -6.96331881e-13\n",
      " -1.70530257e-13  0.00000000e+00 -1.53679023e+00]\n",
      " Loss: 2095.766957210188, Parameters: [ 2.80968413e+01  8.47680334e-01  1.71747580e+00 -5.51855082e-06\n",
      "  7.85664648e-06  0.00000000e+00  1.27140029e+00]\n",
      "Epoch 41, Gradients: [-3.43914254e-01 -2.86432425e-01  1.85604975e+00 -5.54223334e-13\n",
      " -1.13686838e-12  0.00000000e+00 -3.89200978e+00]\n",
      " Loss: 2092.2147247798166, Parameters: [ 2.80277470e+01  9.02445209e-01  1.98326477e+00 -5.51129958e-06\n",
      "  5.91121577e-06  0.00000000e+00  1.31979290e+00]\n",
      "Epoch 51, Gradients: [-3.08762064e-01 -1.41420755e-01  1.42810862e+00  1.63424829e-12\n",
      "  1.70530257e-13  0.00000000e+00 -3.51830851e+00]\n",
      " Loss: 2087.340999790398, Parameters: [ 2.81476260e+01  9.44324352e-01  2.06568094e+00 -4.82583827e-06\n",
      "  7.87370759e-06  0.00000000e+00  1.46489253e+00]\n",
      "Epoch 61, Gradients: [-1.29902903e-01  5.50664380e-02 -1.04711942e+00 -1.13686838e-13\n",
      "  4.54747351e-13  0.00000000e+00 -1.75840986e+00]\n",
      " Loss: 2081.328080685316, Parameters: [ 2.83301083e+01  9.73281089e-01  2.00954228e+00 -3.89240058e-06\n",
      "  1.16609947e-05  0.00000000e+00  1.65579719e+00]\n",
      "Epoch 71, Gradients: [-4.93456631e-02 -4.80454513e-03 -1.87339444e+00 -1.56319402e-13\n",
      "  1.70530257e-13  0.00000000e+00 -9.30415912e-01]\n",
      " Loss: 2074.407829869785, Parameters: [2.85663920e+01 1.03449444e+00 2.08298255e+00 1.87481781e-06\n",
      " 1.43862059e-05 0.00000000e+00 1.89031569e+00]\n",
      "Epoch 81, Gradients: [-1.34469954e-02 -7.31970207e-02 -1.93179503e+00 -1.53477231e-12\n",
      "  2.84217094e-13  0.00000000e+00 -4.54888713e-01]\n",
      " Loss: 2066.5608262279848, Parameters: [2.88514321e+01 1.12441918e+00 2.23454730e+00 7.88649787e-06\n",
      " 1.63254309e-05 0.00000000e+00 2.15564127e+00]\n",
      "Epoch 91, Gradients: [ 1.68058337e-02 -7.42122917e-02 -1.92596469e+00 -2.77111667e-13\n",
      " -3.97903932e-13  0.00000000e+00  3.29855029e-03]\n",
      " Loss: 2058.0447531662685, Parameters: [2.91668547e+01 1.24254365e+00 2.41567744e+00 1.41338634e-05\n",
      " 1.70519470e-05 0.00000000e+00 2.45154807e+00]\n",
      "Epoch 101, Gradients: [ 8.36588101e-03 -3.28412163e-02 -1.56899203e+00 -1.70530257e-13\n",
      "  8.52651283e-13  0.00000000e+00  1.16580795e-01]\n",
      " Loss: 2051.9061643061236, Parameters: [2.92154826e+01 1.35700977e+00 2.61055564e+00 1.42909470e-05\n",
      " 1.57015681e-05 0.00000000e+00 2.68552606e+00]\n",
      "Epoch 111, Gradients: [-1.26764914e-02 -1.36084007e-02 -1.28992172e+00  4.19220214e-13\n",
      "  1.98951966e-13  0.00000000e+00 -4.05028461e-02]\n",
      " Loss: 2051.595088626843, Parameters: [2.91992208e+01 1.37593863e+00 2.67358643e+00 1.40904140e-05\n",
      " 1.54867352e-05 0.00000000e+00 2.70164174e+00]\n",
      "Epoch 121, Gradients: [-2.69341400e-02  7.68364431e-04 -1.08448988e+00  6.32383035e-13\n",
      " -2.84217094e-14  0.00000000e+00 -1.35798639e-01]\n",
      " Loss: 2050.8467888827486, Parameters: [2.92269330e+01 1.40176308e+00 2.73915643e+00 1.43734452e-05\n",
      " 1.54651915e-05 0.00000000e+00 2.73292197e+00]\n",
      "Epoch 131, Gradients: [-3.41414375e-02 -3.76273102e-02 -9.33137969e-01  2.48689958e-13\n",
      "  7.10542736e-13  0.00000000e+00 -1.56911587e-01]\n",
      " Loss: 2049.9859121852955, Parameters: [2.92714870e+01 1.42895923e+00 2.80494384e+00 1.50913065e-05\n",
      " 1.49765399e-05 0.00000000e+00 2.77730443e+00]\n",
      "Epoch 141, Gradients: [-3.96526427e-02  3.39235203e-03 -8.41495220e-01  2.62900812e-13\n",
      " -6.53699317e-13  0.00000000e+00 -1.70009334e-01]\n",
      " Loss: 2048.525366865766, Parameters: [2.93230580e+01 1.46678623e+00 2.87013268e+00 1.50887011e-05\n",
      " 1.44942533e-05 0.00000000e+00 2.82980630e+00]\n",
      "Epoch 151, Gradients: [-4.04426592e-02 -2.97904856e-02 -7.66593029e-01 -2.48689958e-13\n",
      " -4.83169060e-13  0.00000000e+00 -1.27409618e-01]\n",
      " Loss: 2047.2797947849774, Parameters: [2.93783809e+01 1.50102505e+00 2.93439186e+00 1.34950510e-05\n",
      " 1.40414991e-05 0.00000000e+00 2.88787594e+00]\n",
      "Epoch 161, Gradients: [-4.01868412e-02 -9.88533351e-03 -7.29676886e-01  1.06581410e-12\n",
      "  2.84217094e-14  0.00000000e+00 -8.11124381e-02]\n",
      " Loss: 2045.5969963382213, Parameters: [2.94361188e+01 1.54399660e+00 2.99760741e+00 1.15661840e-05\n",
      " 1.41724024e-05 0.00000000e+00 2.95152045e+00]\n",
      "Epoch 171, Gradients: [-3.42547024e-02 -8.92765133e-02 -7.01507713e-01  9.52127266e-13\n",
      "  4.26325641e-13  0.00000000e+00  3.88404457e-02]\n",
      " Loss: 2044.2379725671744, Parameters: [2.94961349e+01 1.58056416e+00 3.05985802e+00 1.02855292e-05\n",
      " 1.36649318e-05 0.00000000e+00 3.02329252e+00]\n",
      "Epoch 181, Gradients: [-4.46848313e-02  3.24440852e-02 -6.23470430e-01  6.67910172e-13\n",
      " -3.41060513e-13  0.00000000e+00 -5.34625062e-02]\n",
      " Loss: 2042.861500082288, Parameters: [2.95570454e+01 1.62485340e+00 3.12185251e+00 9.11311206e-06\n",
      " 1.31002482e-05 0.00000000e+00 3.05467933e+00]\n",
      "Epoch 191, Gradients: [-4.67535321e-02 -6.09126014e-02 -5.24386205e-01 -1.20081722e-12\n",
      " -3.12638804e-13  0.00000000e+00 -2.02690456e-02]\n",
      " Loss: 2042.575352744371, Parameters: [2.96139011e+01 1.64603614e+00 3.18507501e+00 8.13708792e-06\n",
      " 1.29696177e-05 0.00000000e+00 3.09427132e+00]\n",
      "Epoch 201, Gradients: [-4.89971275e-02 -7.31442205e-02 -4.63325402e-01  9.52127266e-13\n",
      "  1.02318154e-12  0.00000000e+00 -1.30971639e-03]\n",
      " Loss: 2041.6045466865512, Parameters: [2.96720192e+01 1.68226316e+00 3.24941094e+00 8.30287256e-06\n",
      " 1.29942175e-05 0.00000000e+00 3.14114267e+00]\n",
      "Epoch 211, Gradients: [-5.10842064e-02 -3.88643629e-02 -4.51080136e-01 -4.97379915e-14\n",
      "  3.97903932e-13  0.00000000e+00 -1.86971979e-02]\n",
      " Loss: 2041.111962934043, Parameters: [2.96895207e+01 1.69822538e+00 3.26855938e+00 8.47281048e-06\n",
      " 1.28024250e-05 0.00000000e+00 3.15358884e+00]\n",
      "Epoch 221, Gradients: [-5.33634175e-02  1.68562478e-02 -4.47793106e-01 -7.10542736e-14\n",
      " -3.12638804e-13  0.00000000e+00 -4.33974292e-02]\n",
      " Loss: 2040.4327749015524, Parameters: [2.97070860e+01 1.71844106e+00 3.28719482e+00 8.58248590e-06\n",
      " 1.26728754e-05 0.00000000e+00 3.16712712e+00]\n",
      "Epoch 231, Gradients: [-5.26994207e-02 -1.31500886e-02 -4.30097767e-01  1.42108547e-12\n",
      " -4.83169060e-13  0.00000000e+00 -1.89607378e-02]\n",
      " Loss: 2040.289632822778, Parameters: [2.97247477e+01 1.72583237e+00 3.30552412e+00 8.78360499e-06\n",
      " 1.25893715e-05 0.00000000e+00 3.18181746e+00]\n",
      "Epoch 241, Gradients: [-5.14026991e-02 -4.13502750e-02 -4.18558988e-01  2.13162821e-13\n",
      "  3.69482223e-13  0.00000000e+00  1.24167645e-02]\n",
      " Loss: 2040.0623798965844, Parameters: [2.97427175e+01 1.73480091e+00 3.32384683e+00 8.93810311e-06\n",
      " 1.29218548e-05 0.00000000e+00 3.19902102e+00]\n",
      "Epoch 251, Gradients: [-5.46192653e-02 -3.36939062e-03 -4.00469549e-01 -1.24344979e-13\n",
      " -6.53699317e-13  0.00000000e+00 -2.08603789e-02]\n",
      " Loss: 2039.6808592888121, Parameters: [2.97606981e+01 1.74947815e+00 3.34216688e+00 8.83572536e-06\n",
      " 1.32561100e-05 0.00000000e+00 3.20622176e+00]\n",
      "Epoch 261, Gradients: [-5.56636787e-02 -3.76078282e-04 -3.84477760e-01 -2.23820962e-13\n",
      "  4.54747351e-13  0.00000000e+00 -2.25610988e-02]\n",
      " Loss: 2039.4100928014993, Parameters: [2.97784315e+01 1.76079469e+00 3.36050753e+00 8.94156001e-06\n",
      " 1.35568561e-05 0.00000000e+00 3.21775737e+00]\n",
      "Epoch 271, Gradients: [-5.44852684e-02 -3.10025714e-02 -3.71280327e-01 -1.32160949e-12\n",
      "  7.10542736e-13  0.00000000e+00  8.18608618e-03]\n",
      " Loss: 2039.2362386649727, Parameters: [2.97962918e+01 1.76889072e+00 3.37888302e+00 9.20078536e-06\n",
      " 1.36730273e-05 0.00000000e+00 3.23396355e+00]\n",
      "Epoch 281, Gradients: [-5.68998258e-02 -5.95316000e-03 -3.54741529e-01 -4.15667500e-13\n",
      "  6.53699317e-13  0.00000000e+00 -1.41072647e-02]\n",
      " Loss: 2038.9006241301051, Parameters: [2.98142516e+01 1.78261462e+00 3.39724370e+00 9.41186758e-06\n",
      " 1.37088900e-05 0.00000000e+00 3.24247091e+00]\n",
      "Epoch 291, Gradients: [-5.80267895e-02  4.83572722e-05 -3.40147558e-01 -1.10844667e-12\n",
      " -3.12638804e-13  0.00000000e+00 -1.77688869e-02]\n",
      " Loss: 2038.6170200337413, Parameters: [2.98320340e+01 1.79463059e+00 3.41561457e+00 9.48847666e-06\n",
      " 1.35912156e-05 0.00000000e+00 3.25372121e+00]\n",
      "Epoch 301, Gradients: [-5.71940418e-02 -2.52944719e-02 -3.26999453e-01  1.06581410e-14\n",
      " -1.13686838e-13  0.00000000e+00  8.06932379e-03]\n",
      " Loss: 2038.4430091910456, Parameters: [2.98499066e+01 1.80314121e+00 3.43401124e+00 9.54568319e-06\n",
      " 1.34236854e-05 0.00000000e+00 3.26889403e+00]\n",
      "Epoch 311, Gradients: [-5.83606028e-02 -1.41214895e-02 -3.20423988e-01 -4.97379915e-14\n",
      " -8.81072992e-13  0.00000000e+00 -4.73229074e-03]\n",
      " Loss: 2038.357798324537, Parameters: [2.98552967e+01 1.80725804e+00 3.43951428e+00 9.58738591e-06\n",
      " 1.33961325e-05 0.00000000e+00 3.27002278e+00]\n",
      "Epoch 321, Gradients: [-5.92649775e-02  5.51332760e-04 -3.17599687e-01 -2.55795385e-13\n",
      " -1.13686838e-13  0.00000000e+00 -1.54256868e-02]\n",
      " Loss: 2038.2015983457852, Parameters: [2.98606641e+01 1.81290716e+00 3.44498771e+00 9.45624989e-06\n",
      " 1.34516731e-05 0.00000000e+00 3.27259734e+00]\n",
      "Epoch 331, Gradients: [-5.93666069e-02  9.45296562e-04 -3.14628589e-01  1.13686838e-13\n",
      "  4.83169060e-13  0.00000000e+00 -1.36377728e-02]\n",
      " Loss: 2038.1064626157943, Parameters: [2.98660301e+01 1.81668143e+00 3.45042978e+00 9.36303904e-06\n",
      " 1.34884969e-05 0.00000000e+00 3.27660574e+00]\n",
      "Converged at epoch 332\n",
      "Epoch 333,  \n",
      " vecc Parameters: [2.98676438e+01 1.81704459e+00 3.45206005e+00 9.34680812e-06\n",
      " 1.34802033e-05 0.00000000e+00 3.27804523e+00]\n",
      "FINAL STATE: Epoch 333, Loss: 2038.106906954074, \n",
      " vecc Parameters: [29.867643794098317, 1.8170445928338301, 3.452060053856289, 9.346808116428221e-06, 1.3480203304274802e-05, 0.0, 3.278045234831132]\n",
      "Day 1 optimization finished in 74.56s over 333 epochs.\n",
      "Day 1 final results: [29.867643794098317, 1.8170445928338301, 3.452060053856289, 9.346808116428221e-06, 1.3480203304274802e-05, 0.0, 3.278045234831132, 2038.106906954074]\n",
      "\n",
      "--- All Days Processed ---\n"
     ]
    }
   ],
   "source": [
    "v = 0.5 # smooth\n",
    "mm_cond_number = 20\n",
    "nheads = 300\n",
    "lr = 0.02\n",
    "step = 100\n",
    "gamma_par = 0.3\n",
    "epochs = 500\n",
    "\n",
    "# ------------------------------------\n",
    "# Main Optimization Loop (Updated)\n",
    "# ------------------------------------\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "num_days_to_load = 31 # From reference code\n",
    "\n",
    "for i in range(num_days_to_load):\n",
    "    idx_for_datamap = [i*8, i*8+1]\n",
    "    # Using the new load function from the reference code\n",
    "    cur_map, cur_df = data_load_instance.load_working_data(\n",
    "        df_map, \n",
    "        idx_for_datamap, \n",
    "        ord_mm=None,\n",
    "        # Using float64 to ensure compatibility with kernels\n",
    "        dtype=torch.float64 \n",
    "    )\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "print(f\"Data loaded for {len(df_day_map_list)} days.\")\n",
    "# print(df_day_aggregated_list[0].shape) # From reference code\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "\n",
    "# This list is now just for iterating\n",
    "#days_list = range(len(df_day_map_list)) \n",
    "days_list = [0]\n",
    "for day in days_list:  \n",
    "    \n",
    "    # Get the pre-loaded data for this day\n",
    "    analysis_data_map = df_day_map_list[day]\n",
    "    aggregated_data = df_day_aggregated_list[day]\n",
    "\n",
    "\n",
    "    a = [28.75, 0.98, 1.06, 0, 0, 0, 1.890]\n",
    "    params = torch.tensor(a, dtype=torch.float64, requires_grad=True)\n",
    "    \n",
    "    # Calculate resolution for printing\n",
    "    res_calc = (200 / lat_lon_resolution[0]) * (100 / lat_lon_resolution[0])\n",
    "    print(f'\\n--- Starting Day {day+1} (2024-07-{day+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params.detach().numpy()}')\n",
    "            \n",
    "    # --- Data loading is now done *before* the loop ---\n",
    "\n",
    "    # We need to define the device (though we aren't passing it anymore)\n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model_instance = kernels.model_fitting(\n",
    "            smooth = v,\n",
    "            input_map = analysis_data_map,\n",
    "            aggregated_data = aggregated_data,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "            # device = device_str  <--- REMOVED: This was causing the TypeError\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Adjusted optimizer call based on expected return values (step size changed to step)\n",
    "    optimizer, scheduler = model_instance.optimizer_fun_scheduler_same_lr(\n",
    "        params, \n",
    "        lr=lr, \n",
    "        betas=(0.9, 0.8), \n",
    "        eps=1e-8, \n",
    "        step_size=step, # Using the 'step' variable here\n",
    "        gamma=gamma_par  # Using gamma_par\n",
    "    ) \n",
    "\n",
    "    # --- CRITICAL CORRECTION ---\n",
    "    # 1. We no longer need to create a separate 'instance_map'.\n",
    "    #    'model_instance' is already the correct instance.\n",
    "    # 2. We do NOT pre-calculate 'cov_map'. The optimized training loop\n",
    "    #    'run_vecc_may9' does this internally on each epoch\n",
    "    #    to ensure the gradients are correct.\n",
    "    # 3. We call 'run_vecc_may9' (the optimized loop) instead of 'run_vecc_grp9'.\n",
    "    #    This version does not take 'cov_map' as an argument.\n",
    "    \n",
    "    # Calling the optimized 'run_vecc_may9'\n",
    "    out, epoch_ran = model_instance.run_vecc_scheduler_same_lr(\n",
    "        params, \n",
    "        optimizer,\n",
    "        scheduler, \n",
    "        model_instance.matern_cov_anisotropy_v05, \n",
    "        epochs=epochs\n",
    "    )\n",
    "    # --- End Correction ---\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"Day {day+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day+1} final results: {out}\")\n",
    "\n",
    "print(\"\\n--- All Days Processed ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa577f1",
   "metadata": {},
   "source": [
    "fit one hour but fix time relevant parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6cc1afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading data for all days...\n",
      "Data loaded for 31 days.\n",
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 5000.0, smooth: 0.5\n",
      "mm_cond_number: 20,\n",
      "initial parameters: \n",
      " [29.75  0.98  1.06  0.    0.    0.    1.89]\n",
      "Epoch 1, Gradients (Full): [ -0.02896164  10.53635213 -13.62827347   0.           0.\n",
      "   0.          -2.77344428]\n",
      " Loss: 8570.70743603638, Parameters (Full): [29.75  0.98  1.06  0.    0.    0.    1.89]\n",
      "Epoch 51, Gradients (Full): [-0.07348001 -0.39344868 -0.09763837  0.          0.          0.\n",
      "  0.83158837]\n",
      " Loss: 8315.909261744184, Parameters (Full): [29.83286556  1.31416734  2.30829604  0.          0.          0.\n",
      "  3.3709474 ]\n",
      "Epoch 101, Gradients (Full): [-0.08185817 -0.02543247 -0.01331501  0.          0.          0.\n",
      " -0.08306768]\n",
      " Loss: 8314.451279317555, Parameters (Full): [31.35687947  1.32904135  2.31022858  0.          0.          0.\n",
      "  3.1419399 ]\n",
      "Epoch 151, Gradients (Full): [-0.07862879 -0.0057642  -0.00551532  0.          0.          0.\n",
      " -0.01512818]\n",
      " Loss: 8306.998993282872, Parameters (Full): [31.81120533  1.36151861  2.36134084  0.          0.          0.\n",
      "  3.16570644]\n",
      "Epoch 201, Gradients (Full): [-0.07543935  0.00286917  0.00232835  0.          0.          0.\n",
      " -0.00412687]\n",
      " Loss: 8302.473691396288, Parameters (Full): [32.26362054  1.38459573  2.39912546  0.          0.          0.\n",
      "  3.17070669]\n",
      "Epoch 251, Gradients (Full): [-0.07401028 -0.00073788  0.00080021  0.          0.          0.\n",
      " -0.00073742]\n",
      " Loss: 8301.648258752262, Parameters (Full): [32.39896961  1.38973796  2.40800013  0.          0.          0.\n",
      "  3.17053696]\n",
      "Epoch 301, Gradients (Full): [-0.07279156  0.00137997 -0.00147572  0.          0.          0.\n",
      "  0.00128842]\n",
      " Loss: 8300.640251681454, Parameters (Full): [32.53418235  1.39563478  2.41725927  0.          0.          0.\n",
      "  3.17060846]\n",
      "Epoch 351, Gradients (Full): [-0.0724622  -0.00044228 -0.00027337  0.          0.          0.\n",
      "  0.00049904]\n",
      " Loss: 8300.429217435363, Parameters (Full): [32.57471312  1.39705742  2.42003357  0.          0.          0.\n",
      "  3.17025848]\n",
      "Epoch 401, Gradients (Full): [-7.21883412e-02  4.46175600e-04  7.56418107e-05  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -4.26302804e-04]\n",
      " Loss: 8300.127970073137, Parameters (Full): [32.61523211  1.39883124  2.4229159   0.          0.          0.\n",
      "  3.17007093]\n",
      "Converged at epoch 439\n",
      "Epoch 440,  \n",
      " vecc Parameters: [32.62495453  1.39919311  2.42357956  0.          0.          0.\n",
      "  3.17010189]\n",
      "FINAL STATE: Epoch 440, Loss: 8300.073187, \n",
      " vecc Parameters: [32.62495453438428, 1.3991931103706319, 2.4235795624478227, 0.0, 0.0, 0.0, 3.1701018909601357]\n",
      "Day 1 optimization finished in 749.29s over 440 epochs.\n",
      "Day 1 final results: [32.62495453438428, 1.3991931103706319, 2.4235795624478227, 0.0, 0.0, 0.0, 3.1701018909601357, 8300.073187093943]\n",
      "\n",
      "--- All Days Processed ---\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------\n",
    "# Main Optimization Loop (Updated)\n",
    "# ------------------------------------\n",
    "\n",
    "# --- 1. Pre-load all data (based on reference code) ---\n",
    "print(\"Pre-loading data for all days...\")\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "num_days_to_load = 31 # From reference code\n",
    "\n",
    "for i in range(num_days_to_load):\n",
    "    idx_for_datamap = [i*8, i*8+1]\n",
    "    # Using the new load function from the reference code\n",
    "    cur_map, cur_df = data_load_instance.load_working_data(\n",
    "        df_map, \n",
    "        idx_for_datamap, \n",
    "        ord_mm=None,\n",
    "        # Using float64 to ensure compatibility with kernels\n",
    "        dtype=torch.float64 \n",
    "    )\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )\n",
    "\n",
    "print(f\"Data loaded for {len(df_day_map_list)} days.\")\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "\n",
    "# This list is now just for iterating\n",
    "#days_list = range(len(df_day_map_list)) \n",
    "days_list = [0]\n",
    "for day in days_list:  \n",
    "    \n",
    "    # Get the pre-loaded data for this day\n",
    "    analysis_data_map = df_day_map_list[day]\n",
    "    aggregated_data = df_day_aggregated_list[day]\n",
    "\n",
    "    # **CHANGE**: Use a single initialization vector 'a' with fixed indices (3, 4, 5) set to 0.\n",
    "    # a = [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769] # Original\n",
    "    a = [29.75, 0.98, 1.06, 0, 0, 0, 1.890]\n",
    "    params = torch.tensor(a, dtype=torch.float64, requires_grad=True)\n",
    "    \n",
    "    # Calculate resolution for printing\n",
    "    res_calc = (200 / lat_lon_resolution[0]) * (100 / lat_lon_resolution[0])\n",
    "    print(f'\\n--- Starting Day {day+1} (2024-07-{day+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params.detach().numpy()}')\n",
    "            \n",
    "    # --- Data loading is now done *before* the loop ---\n",
    "\n",
    "    # We need to define the device (though we aren't passing it anymore)\n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model_instance = kernels.model_fitting(\n",
    "            smooth = v,\n",
    "            input_map = analysis_data_map,\n",
    "            aggregated_data = aggregated_data,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "            # device = device_str  <--- REMOVED: This was causing the TypeError\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Adjusted optimizer call based on expected return values (step size changed to step)\n",
    "    lr=0.03\n",
    "    optimizer, scheduler = model_instance.optimizer_fun_scheduler_same_lr(\n",
    "        params, \n",
    "        lr=lr, \n",
    "        betas=(0.9, 0.8), \n",
    "        eps=1e-8, \n",
    "        step_size=step, # Using the 'step' variable here\n",
    "        gamma=gamma_par  # Using gamma_par\n",
    "    ) \n",
    "\n",
    "    # --- CRITICAL CORRECTION: Call the new masked function ---\n",
    "    out, epoch_ran = model_instance.run_vecc_scheduler_same_lr_fit_hour( # <-- Function name changed\n",
    "        params, \n",
    "        optimizer,\n",
    "        scheduler, \n",
    "        model_instance.matern_cov_anisotropy_v05, \n",
    "        epochs=epochs\n",
    "    )\n",
    "    # --- End Correction ---\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f\"Day {day+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day+1} final results: {out}\")\n",
    "\n",
    "print(\"\\n--- All Days Processed ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
