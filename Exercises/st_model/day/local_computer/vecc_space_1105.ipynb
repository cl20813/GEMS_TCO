{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b892ee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from GEMS_TCO import kernels\n",
    "from GEMS_TCO import data_preprocess \n",
    "from GEMS_TCO import kernels_new, kernels_reparametrization_space as kernels_repar_space\n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO.data_loader import load_data2\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2470c27e",
   "metadata": {},
   "source": [
    "Load monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77b35dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting data to lat: [0.0, 5.0], lon: [123.0, 133.0]\n"
     ]
    }
   ],
   "source": [
    "space: List[str] = ['2', '2']\n",
    "lat_lon_resolution = [int(s) for s in space]\n",
    "mm_cond_number: int = 20\n",
    "years = ['2024']\n",
    "month_range = [7] \n",
    "\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "data_load_instance = load_data2(config.mac_data_load_path)\n",
    "\n",
    "\n",
    "df_map, ord_mm, nns_map = data_load_instance.load_maxmin_ordered_data_bymonthyear(\n",
    "lat_lon_resolution=lat_lon_resolution, \n",
    "mm_cond_number=mm_cond_number,\n",
    "years_=years, \n",
    "months_=month_range,\n",
    "lat_range=[0.0, 5.0],      \n",
    "lon_range=[123.0, 133.0] \n",
    ")\n",
    "\n",
    "#days: List[str] = ['0', '31']\n",
    "#days_s_e = [int(d) for d in days]\n",
    "#days_list = list(range(days_s_e[0], days_s_e[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beebcc11",
   "metadata": {},
   "source": [
    "Load daily data applying max-min ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afe39312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4560, 4])\n"
     ]
    }
   ],
   "source": [
    "daily_aggregated_tensors = [] \n",
    "daily_hourly_maps = []        \n",
    "\n",
    "for day_index in range(31):\n",
    "  \n",
    "    hour_start_index = day_index * 8\n",
    "    #hour_end_index = (day_index + 1) * 8\n",
    "    hour_end_index = day_index*8 + 1\n",
    "    hour_indices = [hour_start_index, hour_end_index]\n",
    "    \n",
    "    # Load the data for the current day\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "        df_map, \n",
    "        hour_indices, \n",
    "        ord_mm=ord_mm,  \n",
    "        dtype=torch.float \n",
    "    )\n",
    "    # Append the day's data to their respective lists\n",
    "    daily_aggregated_tensors.append(day_aggregated_tensor)\n",
    "    daily_hourly_maps.append(day_hourly_map) \n",
    "\n",
    "print(daily_aggregated_tensors[0].shape)\n",
    "#print(daily_hourly_maps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c47c20",
   "metadata": {},
   "source": [
    "Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0172ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0.5 # smooth\n",
    "mm_cond_number = 8\n",
    "nheads = 300\n",
    "#nheads = 1230\n",
    "lr = 0.01\n",
    "step = 80\n",
    "gamma_par = 0.5\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bdac7f",
   "metadata": {},
   "source": [
    "Adams Optimization Full batch + plateau scheduler (5)\n",
    "\n",
    "lr 0.01 use plateau scheduler for 1120 epochs 400, pateince 5 factor 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a84a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 4424, smooth: 0.5\n",
      "mm_cond_number: 8,\n",
      "initial parameters: \n",
      " [tensor([2.6882], dtype=torch.float64, requires_grad=True), tensor([-0.5306], dtype=torch.float64, requires_grad=True), tensor([0.0476], dtype=torch.float64, requires_grad=True), tensor([0.4055], dtype=torch.float64, requires_grad=True)]\n",
      "--- Epoch 1 / Loss: 1.529651 ---\n",
      "  Param 0: Value=2.6882, Grad=-0.17775174156687523\n",
      "  Param 1: Value=-0.5306, Grad=-0.0009060805636219584\n",
      "  Param 2: Value=0.0476, Grad=-0.05973198127290758\n",
      "  Param 3: Value=0.4055, Grad=-0.198994974622458\n",
      "  Max Abs Grad: 1.989950e-01\n",
      "------------------------------\n",
      "--- Epoch 11 / Loss: 1.442262 ---\n",
      "  Param 0: Value=3.3867, Grad=0.05416855368273368\n",
      "  Param 1: Value=0.4591, Grad=-0.005728516201638002\n",
      "  Param 2: Value=0.7246, Grad=0.016886499998323867\n",
      "  Param 3: Value=1.1296, Grad=0.04449027147519396\n",
      "  Max Abs Grad: 5.416855e-02\n",
      "------------------------------\n",
      "--- Epoch 21 / Loss: 1.434111 ---\n",
      "  Param 0: Value=3.2891, Grad=0.023738451514675983\n",
      "  Param 1: Value=1.0136, Grad=0.0011382805812107358\n",
      "  Param 2: Value=0.6253, Grad=0.008717383377788464\n",
      "  Param 3: Value=1.0937, Grad=0.024344464335323796\n",
      "  Max Abs Grad: 2.434446e-02\n",
      "------------------------------\n",
      "--- Epoch 31 / Loss: 1.431819 ---\n",
      "  Param 0: Value=3.2223, Grad=0.006124277931748279\n",
      "  Param 1: Value=1.0481, Grad=0.00536360720840366\n",
      "  Param 2: Value=0.5527, Grad=0.0043994309346667955\n",
      "  Param 3: Value=1.0368, Grad=0.007839277002439873\n",
      "  Max Abs Grad: 7.839277e-03\n",
      "------------------------------\n",
      "--- Epoch 41 / Loss: 1.431295 ---\n",
      "  Param 0: Value=3.2071, Grad=0.003372720848890021\n",
      "  Param 1: Value=0.9978, Grad=0.005299408128406254\n",
      "  Param 2: Value=0.5305, Grad=0.003515398796023442\n",
      "  Param 3: Value=1.0217, Grad=0.004754239286145334\n",
      "  Max Abs Grad: 5.299408e-03\n",
      "------------------------------\n",
      "--- Epoch 51 / Loss: 1.430849 ---\n",
      "  Param 0: Value=3.1977, Grad=0.002245785403036644\n",
      "  Param 1: Value=0.9338, Grad=0.00457047345342066\n",
      "  Param 2: Value=0.5112, Grad=0.0029106714811661548\n",
      "  Param 3: Value=1.0112, Grad=0.003093494376056954\n",
      "  Max Abs Grad: 4.570473e-03\n",
      "------------------------------\n",
      "--- Epoch 61 / Loss: 1.430512 ---\n",
      "  Param 0: Value=3.1915, Grad=0.0017670281671957001\n",
      "  Param 1: Value=0.8710, Grad=0.003684480084895905\n",
      "  Param 2: Value=0.4940, Grad=0.002425910931287602\n",
      "  Param 3: Value=1.0039, Grad=0.002040655425447318\n",
      "  Max Abs Grad: 3.684480e-03\n",
      "------------------------------\n",
      "--- Epoch 71 / Loss: 1.430276 ---\n",
      "  Param 0: Value=3.1869, Grad=0.0015010883368946023\n",
      "  Param 1: Value=0.8146, Grad=0.0028697285829069347\n",
      "  Param 2: Value=0.4783, Grad=0.00200233241625117\n",
      "  Param 3: Value=0.9986, Grad=0.00124198765689644\n",
      "  Max Abs Grad: 2.869729e-03\n",
      "------------------------------\n",
      "--- Epoch 81 / Loss: 1.430179 ---\n",
      "  Param 0: Value=3.1846, Grad=0.0013830928518599919\n",
      "  Param 1: Value=0.7862, Grad=0.002470246073841956\n",
      "  Param 2: Value=0.4701, Grad=0.001782795481828177\n",
      "  Param 3: Value=0.9964, Grad=0.000855909161660463\n",
      "  Max Abs Grad: 2.470246e-03\n",
      "------------------------------\n",
      "--- Epoch 91 / Loss: 1.430154 ---\n",
      "  Param 0: Value=3.1839, Grad=0.0013519522485294453\n",
      "  Param 1: Value=0.7779, Grad=0.0023563502328369733\n",
      "  Param 2: Value=0.4676, Grad=0.0017174452171496136\n",
      "  Param 3: Value=0.9959, Grad=0.0007483065865257721\n",
      "  Max Abs Grad: 2.356350e-03\n",
      "------------------------------\n",
      "--- Epoch 101 / Loss: 1.430146 ---\n",
      "  Param 0: Value=3.1837, Grad=0.0013424370204300146\n",
      "  Param 1: Value=0.7754, Grad=0.002322265545824168\n",
      "  Param 2: Value=0.4668, Grad=0.0016971539879442545\n",
      "  Param 3: Value=0.9958, Grad=0.0007160364255375233\n",
      "  Max Abs Grad: 2.322266e-03\n",
      "------------------------------\n",
      "--- Epoch 111 / Loss: 1.430144 ---\n",
      "  Param 0: Value=3.1837, Grad=0.0013389509430028579\n",
      "  Param 1: Value=0.7746, Grad=0.0023111562342341776\n",
      "  Param 2: Value=0.4665, Grad=0.0016902588372565302\n",
      "  Param 3: Value=0.9958, Grad=0.0007051410639172746\n",
      "  Max Abs Grad: 2.311156e-03\n",
      "------------------------------\n",
      "--- Epoch 121 / Loss: 1.430143 ---\n",
      "  Param 0: Value=3.1836, Grad=0.0013377064329754699\n",
      "  Param 1: Value=0.7743, Grad=0.002307778568366797\n",
      "  Param 2: Value=0.4664, Grad=0.0016880628447999266\n",
      "  Param 3: Value=0.9958, Grad=0.0007016355362340067\n",
      "  Max Abs Grad: 2.307779e-03\n",
      "------------------------------\n",
      "--- Epoch 131 / Loss: 1.430143 ---\n",
      "  Param 0: Value=3.1836, Grad=0.0013372343427963946\n",
      "  Param 1: Value=0.7742, Grad=0.002306687395272526\n",
      "  Param 2: Value=0.4664, Grad=0.0016873191023220658\n",
      "  Param 3: Value=0.9957, Grad=0.0007004284584057959\n",
      "  Max Abs Grad: 2.306687e-03\n",
      "------------------------------\n",
      "--- Epoch 141 / Loss: 1.430143 ---\n",
      "  Param 0: Value=3.1836, Grad=0.0013370486580536686\n",
      "  Param 1: Value=0.7742, Grad=0.0023063208861587043\n",
      "  Param 2: Value=0.4664, Grad=0.0016870564652083796\n",
      "  Param 3: Value=0.9957, Grad=0.0006999939413660494\n",
      "  Max Abs Grad: 2.306321e-03\n",
      "------------------------------\n",
      "--- Epoch 151 / Loss: 1.430143 ---\n",
      "  Param 0: Value=3.1836, Grad=0.0013369825799786287\n",
      "  Param 1: Value=0.7742, Grad=0.002306208814367221\n",
      "  Param 2: Value=0.4664, Grad=0.0016869718237419505\n",
      "  Param 3: Value=0.9957, Grad=0.0006998510756989191\n",
      "  Max Abs Grad: 2.306209e-03\n",
      "------------------------------\n",
      "--- Epoch 161 / Loss: 1.430143 ---\n",
      "  Param 0: Value=3.1836, Grad=0.0013369581917206123\n",
      "  Param 1: Value=0.7742, Grad=0.002306172772015156\n",
      "  Param 2: Value=0.4664, Grad=0.0016869431528274553\n",
      "  Param 3: Value=0.9957, Grad=0.0006998017529582372\n",
      "  Max Abs Grad: 2.306173e-03\n",
      "------------------------------\n",
      "--- Epoch 171 / Loss: 1.430143 ---\n",
      "  Param 0: Value=3.1836, Grad=0.0013369489106329778\n",
      "  Param 1: Value=0.7742, Grad=0.0023061607651022526\n",
      "  Param 2: Value=0.4664, Grad=0.0016869330698390205\n",
      "  Param 3: Value=0.9957, Grad=0.0006997840765401297\n",
      "  Max Abs Grad: 2.306161e-03\n",
      "------------------------------\n",
      "--- Epoch 181 / Loss: 1.430143 ---\n",
      "  Param 0: Value=3.1836, Grad=0.001336945712452534\n",
      "  Param 1: Value=0.7742, Grad=0.002306157127839535\n",
      "  Param 2: Value=0.4664, Grad=0.0016869298384182883\n",
      "  Param 3: Value=0.9957, Grad=0.0006997783054075358\n",
      "  Max Abs Grad: 2.306157e-03\n",
      "------------------------------\n",
      "--- Epoch 191 / Loss: 1.430143 ---\n",
      "  Param 0: Value=3.1836, Grad=0.0013369443730897406\n",
      "  Param 1: Value=0.7742, Grad=0.0023061557852169156\n",
      "  Param 2: Value=0.4664, Grad=0.001686928573277118\n",
      "  Param 3: Value=0.9957, Grad=0.0006997760039477709\n",
      "  Max Abs Grad: 2.306156e-03\n",
      "------------------------------\n",
      "FINAL STATE: Epoch 200, Loss: 1.4301425248738802, \n",
      " vecc Parameters: [3.1836255554119988, 0.774207850774099, 0.46637751999071425, 0.9957470677878054]\n",
      "Day 1 optimization finished in 448.63s over 200 epochs.\n",
      "Day 1 final results: [3.1836255554119988, 0.774207850774099, 0.46637751999071425, 0.9957470677878054, 1.4301425248738802]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- End Placeholders ---\n",
    "\n",
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "day_indices = [0] \n",
    "for day_idx in day_indices:  \n",
    "\n",
    "    daily_hourly_map = daily_hourly_maps[day_idx]\n",
    "    daily_aggregated_tensor = daily_aggregated_tensors[day_idx]\n",
    "\n",
    "    # --- Correct Parameter Initialization (SPATIAL-ONLY) ---\n",
    "    init_sigmasq   = 15.0\n",
    "    init_range_lat = 0.66 \n",
    "    init_range_lon = 0.7 \n",
    "    init_nugget    = 1.5\n",
    "    \n",
    "    # Map model parameters to the 'phi' reparameterization\n",
    "    init_phi2 = 1.0 / init_range_lon                \n",
    "    init_phi1 = init_sigmasq * init_phi2            \n",
    "    init_phi3 = (init_range_lon / init_range_lat)**2  \n",
    "    \n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # 4-parameter spatial-only list\n",
    "    params_list = [\n",
    "        torch.tensor([np.log(init_phi1)],      requires_grad=True, dtype=torch.float64, device=device_str ),\n",
    "        torch.tensor([np.log(init_phi2)],      requires_grad=True, dtype=torch.float64, device=device_str ),\n",
    "        torch.tensor([np.log(init_phi3)],      requires_grad=True, dtype=torch.float64, device=device_str ),\n",
    "        torch.tensor([np.log(init_nugget)],    requires_grad=True, dtype=torch.float64, device=device_str )\n",
    "    ]\n",
    "\n",
    "    # --- Define learning rates and parameter groups ---\n",
    "    # ðŸ’¥ REVISED: Set lr_fast from the global lr\n",
    "    lr_fast = lr \n",
    "    fast_indices = [0, 1, 2, 3] # All parameters\n",
    "    \n",
    "    param_groups = [\n",
    "        {'params': [params_list[idx] for idx in fast_indices], 'lr': lr_fast, 'name': 'fast_group'}\n",
    "    ]\n",
    "\n",
    "    # --- Print Job Info ---\n",
    "    # Assuming lat_lon_resolution, v, mm_cond_number are defined globally\n",
    "    res_calc = (113 // lat_lon_resolution[0]) * (158 // lat_lon_resolution[0]) \n",
    "    print(f'\\n--- Starting Day {day_idx+1} (2024-07-{day_idx+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params_list}')\n",
    "            \n",
    "    # --- Instantiate the Correct Class ---\n",
    "    model_instance = kernels_repar_space.fit_vecchia_adams_fullbatch(\n",
    "            smooth = v,\n",
    "            input_map = daily_hourly_map,\n",
    "            aggregated_data = daily_aggregated_tensor,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- ðŸ’¥ REVISED: Call the Optimizer Method with Plateau settings ---\n",
    "    optimizer, scheduler = model_instance.set_optimizer(\n",
    "            param_groups,     \n",
    "            lr=lr,            \n",
    "            betas=(0.9, 0.99), \n",
    "            eps=1e-5, \n",
    "            scheduler_type='plateau', # Explicitly set to plateau\n",
    "            patience=patience,        # Pass patience\n",
    "            factor=factor             # Pass factor\n",
    "        )\n",
    "\n",
    "    # --- Call the Correct Fit Method ---\n",
    "    out, epoch_ran = model_instance.fit_vecc_scheduler_fullbatch(\n",
    "            params_list,\n",
    "            optimizer,\n",
    "            scheduler, \n",
    "            model_instance.matern_cov_SPATIAL_log_reparam, \n",
    "            epochs=epochs\n",
    "        )\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Day {day_idx+1} optimization finished in {epoch_time:.2f}s over {epoch_ran+1} epochs.\")\n",
    "    print(f\"Day {day_idx+1} final results: {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045aefcd",
   "metadata": {},
   "source": [
    "L-BFGS Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22bb62c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Day 1 (2024-07-1) ---\n",
      "Data size per day: 4424, smooth: 0.5\n",
      "mm_cond_number: 8,\n",
      "initial parameters: \n",
      " [tensor([2.6882], dtype=torch.float64, requires_grad=True), tensor([-0.5306], dtype=torch.float64, requires_grad=True), tensor([0.0476], dtype=torch.float64, requires_grad=True), tensor([0.4055], dtype=torch.float64, requires_grad=True)]\n",
      "--- Starting L-BFGS Optimization ---\n",
      "--- Step 1/50 / Loss: 1.529651 ---\n",
      "  Param 0: Value=3.0082, Grad=3.5383358435151673e-07\n",
      "  Param 1: Value=0.4645, Grad=-8.407410577923204e-08\n",
      "  Param 2: Value=0.3914, Grad=5.322419736374343e-07\n",
      "  Param 3: Value=1.1232, Grad=2.039513370494456e-07\n",
      "  Max Abs Grad: 5.322420e-07\n",
      "------------------------------\n",
      "\n",
      "Converged on gradient norm (max|grad| < 1e-05) at step 1\n",
      "FINAL STATE: Step 1, Loss: 1.5296512488160736\n",
      "  Raw (vecc) Parameters: [3.0081699944388, 0.4644597377673711, 0.3913610046862658, 1.1231926500859029]\n",
      "  Interpretable Parameters:\n",
      "    sigma_sq  : 12.726803\n",
      "    range_lon : 0.628475\n",
      "    range_lat : 0.516779\n",
      "    nugget    : 3.074655\n",
      "Day 1 optimization finished in 42.09s over 1 steps.\n",
      "Day 1 final results: [3.0081699944388, 0.4644597377673711, 0.3913610046862658, 1.1231926500859029, 1.5296512488160736]\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Run optimization loop over pre-loaded data ---\n",
    "day_indices = [0] \n",
    "for day_idx in day_indices:  \n",
    "\n",
    "    daily_hourly_map = daily_hourly_maps[day_idx]\n",
    "    daily_aggregated_tensor = daily_aggregated_tensors[day_idx]\n",
    "\n",
    "    # --- Correct Parameter Initialization (SPATIAL-ONLY) ---\n",
    "    \n",
    "    # Define initial *model* parameters\n",
    "    init_sigmasq   = 25.0 #15\n",
    "    init_range_lat = 1.66 \n",
    "    init_range_lon = 1.7 \n",
    "    init_nugget    = 1.5\n",
    "    \n",
    "    # Map model parameters to the 'phi' reparameterization\n",
    "    init_phi2 = 1.0 / init_range_lon                # [1] phi2 = 1 / range_lon\n",
    "    init_phi1 = init_sigmasq * init_phi2            # [0] phi1 = sigmasq * phi2 = sigmasq / range_lon\n",
    "    init_phi3 = (init_range_lon / init_range_lat)**2  # [2] phi3 = (range_lon / range_lat)^2\n",
    "    \n",
    "    device_str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # 4-parameter spatial-only list\n",
    "    params_list = [\n",
    "        torch.tensor([np.log(init_phi1)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [0] log(phi1)\n",
    "        torch.tensor([np.log(init_phi2)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [1] log(phi2)\n",
    "        torch.tensor([np.log(init_phi3)],      requires_grad=True, dtype=torch.float64, device=device_str ), # [2] log(phi3)\n",
    "        torch.tensor([np.log(init_nugget)],    requires_grad=True, dtype=torch.float64, device=device_str )  # [3] log(nugget)\n",
    "    ]\n",
    "\n",
    "    # --- Print Job Info ---\n",
    "    res_calc = (113 // lat_lon_resolution[0]) * (158 // lat_lon_resolution[0]) \n",
    "    print(f'\\n--- Starting Day {day_idx+1} (2024-07-{day_idx+1}) ---')\n",
    "    print(f'Data size per day: { res_calc }, smooth: {v}')\n",
    "    print(f'mm_cond_number: {mm_cond_number},\\ninitial parameters: \\n {params_list}')\n",
    "    \n",
    "    # --- ðŸ’¥ REVISED: Instantiate the LBFGS Class ---\n",
    "    model_instance = kernels_repar_space.fit_vecchia_lbfgs(\n",
    "            smooth = v,\n",
    "            input_map = daily_hourly_map,\n",
    "            aggregated_data = daily_aggregated_tensor,\n",
    "            nns_map = nns_map,\n",
    "            mm_cond_number = mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- ðŸ’¥ REVISED: Call the LBFGS Optimizer Method ---\n",
    "    # L-BFGS uses a different set of parameters and doesn't return a scheduler.\n",
    "    # We pass the list of parameters directly, which set_optimizer will wrap.\n",
    "    optimizer = model_instance.set_optimizer(\n",
    "            params_list,     \n",
    "            lr=1.0,            # L-BFGS often uses lr=1.0 for its line search\n",
    "            max_iter=20        # Max iterations *per step*\n",
    "        )\n",
    "\n",
    "    # --- ðŸ’¥ REVISED: Call the LBFGS Fit Method ---\n",
    "    # This method takes 'max_steps' (epochs) and no scheduler.\n",
    "    out, steps_ran = model_instance.fit_vecc_lbfgs(\n",
    "            params_list,\n",
    "            optimizer,\n",
    "            model_instance.matern_cov_SPATIAL_log_reparam, # Pass the spatial-only method\n",
    "            max_steps=50 # Total number of L-BFGS steps\n",
    "        )\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Day {day_idx+1} optimization finished in {epoch_time:.2f}s over {steps_ran+1} steps.\")\n",
    "    print(f\"Day {day_idx+1} final results: {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2de27da",
   "metadata": {},
   "source": [
    "Day 1 final results: [3.008192996038009, 0.46428191135000707, 0.39131753527431623, 1.1231542142224333, 1.4700110749410846]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c45877c",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "925fb149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Negative Log-Likelihood: (1602.6438361691046, 1.4301507065573866)\n",
      "1000000 448400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cal(a):\n",
    "    day_indices = [0] \n",
    "    for day_idx in day_indices:  \n",
    "\n",
    "        daily_hourly_map = daily_hourly_maps[day_idx]\n",
    "        daily_aggregated_tensor = daily_aggregated_tensors[day_idx]\n",
    "\n",
    "        # Convert initial parameters to a list of 1-element tensors\n",
    "        params_list = [\n",
    "            torch.tensor([val], dtype=torch.float64, requires_grad=True, device=device_str) for val in a\n",
    "        ]\n",
    "        \n",
    "        # ðŸ’¡ CRITICAL: Concatenate the list into a single tensor\n",
    "        # The full_likelihood function expects a single tensor, not a list\n",
    "        params_tensor = torch.cat(params_list)\n",
    "\n",
    "        # Assuming 'kernels_repar_space' has your 'fit_vecchia_adams' class\n",
    "        model_instance = kernels_repar_space.fit_vecchia_adams_fullbatch(\n",
    "                smooth = v,\n",
    "                input_map = daily_hourly_map,\n",
    "                aggregated_data = daily_aggregated_tensor,\n",
    "                nns_map = nns_map,\n",
    "                mm_cond_number = mm_cond_number,\n",
    "                nheads = nheads\n",
    "            )\n",
    "        \n",
    "        # ðŸ’¡ Pass the single 'params_tensor' and the correct 4-parameter spatial covariance function\n",
    "        bb = model_instance.full_likelihood_avg(\n",
    "            params = params_tensor, \n",
    "            input_data = daily_aggregated_tensor, \n",
    "            y = daily_aggregated_tensor[:,2], \n",
    "            covariance_function = model_instance.matern_cov_SPATIAL_log_reparam\n",
    "        )\n",
    "     \n",
    "        cov_map = model_instance.cov_structure_saver(params_tensor, model_instance.matern_cov_SPATIAL_log_reparam)\n",
    "        vecchia_nll = model_instance.vecchia_space_fullbatch( # Change this to your chosen Vecchia implementation\n",
    "            params = params_tensor, \n",
    "            covariance_function = model_instance.matern_cov_SPATIAL_log_reparam, \n",
    "            cov_map = cov_map # Assuming cov_map is precomputed or computed internally\n",
    "        )\n",
    " \n",
    "    return bb, vecchia_nll\n",
    "   \n",
    "# adams and lfgs\n",
    "a= [3.2691297610712176, 0.6684659526552683, 0.4027674047362919, 0.8989261051578574, 6521.4909959990455]\n",
    "\n",
    "bb, nll_value = cal(a)\n",
    "print(f\"Full Negative Log-Likelihood: {bb.item()*1120, nll_value.item()}\")\n",
    "\n",
    "def cc(n,nheads,mm):\n",
    "    print(n**2 , nheads**2 + (n-nheads)*mm**3)\n",
    "    return n**2 >nheads**2 + (n-nheads)*mm**3\n",
    "\n",
    "cc(1000,300,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba3130e",
   "metadata": {},
   "source": [
    "# 4560\n",
    "\n",
    "\n",
    "full adams plateau / full adams scheduler 80 / l bfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43529318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Negative Log-Likelihood: (6525.265703352871, 6521.895711794799)\n",
      "Full Negative Log-Likelihood: (6530.741398205831, 6527.769810700424)\n",
      "Full Negative Log-Likelihood: (6526.764426877938, 6522.974295124629)\n",
      "Full Negative Log-Likelihood: (6555.803170980535, 6552.803221680461)\n",
      "Full Negative Log-Likelihood: (6545.497437273466, 6542.951497200811)\n",
      "Full Negative Log-Likelihood: (6574.435587568947, 6570.946772134326)\n",
      "Full Negative Log-Likelihood: (6523.731296642555, 6519.89079612747)\n",
      "Full Negative Log-Likelihood: (6524.706108680448, 6521.245086255674)\n",
      "Full Negative Log-Likelihood: (6527.921681011694, 6523.681908978209)\n"
     ]
    }
   ],
   "source": [
    "#mplateau 5 + lr 0.1 adams full batch 4560\n",
    "\n",
    "\n",
    "a = [3.2813904798406806, 0.6529141404208029, 0.39520496555136536, 0.9020642550783797]\n",
    " # estimates\n",
    "bb, nll_value = cal(a)\n",
    "print(f\"Full Negative Log-Likelihood: {bb.item()*4560, nll_value.item()*4560}\")\n",
    "\n",
    "a = [3.3813904798406806, 0.6529141404208029, 0.39520496555136536, 0.9020642550783797]\n",
    " \n",
    "bb, nll_value = cal(a)\n",
    "\n",
    "print(f\"Full Negative Log-Likelihood: {bb.item()*4560, nll_value.item()*4560}\")\n",
    "\n",
    "a = [3.1813904798406806, 0.6529141404208029, 0.39520496555136536, 0.9020642550783797]\n",
    "\n",
    "bb, nll_value = cal(a)\n",
    "\n",
    "print(f\"Full Negative Log-Likelihood: {bb.item()*4560, nll_value.item()*4560}\")\n",
    "\n",
    "\n",
    "\n",
    "# l bfgs\n",
    "a = [3.008192996038009, 0.46428191135000707, 0.39131753527431623, 1.1231542142224333]\n",
    " # estimates\n",
    "bb, nll_value = cal(a)\n",
    "print(f\"Full Negative Log-Likelihood: {bb.item()*4560, nll_value.item()*4560}\")\n",
    "\n",
    "a = [3.108192996038009, 0.46428191135000707, 0.39131753527431623, 1.1231542142224333]\n",
    "bb, nll_value = cal(a)\n",
    "\n",
    "print(f\"Full Negative Log-Likelihood: {bb.item()*4560, nll_value.item()*4560}\")\n",
    "\n",
    "a = [2.908192996038009, 0.46428191135000707, 0.39131753527431623, 1.1231542142224333]\n",
    "bb, nll_value = cal(a)\n",
    "\n",
    "print(f\"Full Negative Log-Likelihood: {bb.item()*4560, nll_value.item()*4560}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d94430",
   "metadata": {},
   "source": [
    "# 1120\n",
    "\n",
    "epochs 200 lr 0.1 stepsize 80 nheads 300 gamma 0.5 pleateau 5 mm 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e94e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Negative Log-Likelihood: (1770.830952651946, 1768.8074920088832)\n",
      "Full Negative Log-Likelihood: (1771.3446302559594, 1769.488217146782)\n",
      "Full Negative Log-Likelihood: (1771.6574766693973, 1769.4531362117486)\n",
      "Full Negative Log-Likelihood: (1770.80325694022, 1768.8027446852573)\n",
      "Full Negative Log-Likelihood: (1771.3090202134513, 1769.472988550741)\n",
      "Full Negative Log-Likelihood: (1771.6300464789101, 1769.4515538182861)\n"
     ]
    }
   ],
   "source": [
    "# full batch adams plateau 5, 1120 mm 8 nheads 300 gamma 0.5 step 80 lr 0.1\n",
    "\n",
    "a = [2.586668163372335, 0.04786782666397918, 0.7218958281570125, 1.4053391894954903]\n",
    "bb, nll_value = cal(a)\n",
    "print(f\"Full Negative Log-Likelihood: {bb.item()*1120, nll_value.item()*1120}\")\n",
    "\n",
    "a = [2.686668163372335, 0.04786782666397918, 0.7218958281570125, 1.4053391894954903] \n",
    "bb, nll_value = cal(a)\n",
    "\n",
    "print(f\"Full Negative Log-Likelihood: {bb.item()*1120, nll_value.item()*1120}\")\n",
    "\n",
    "a = [2.486668163372335, 0.04786782666397918, 0.7218958281570125, 1.4053391894954903]\n",
    "bb, nll_value = cal(a)\n",
    "\n",
    "print(f\"Full Negative Log-Likelihood: {bb.item()*1120, nll_value.item()*1120}\")\n",
    "\n",
    "\n",
    "# LBFGS \n",
    "\n",
    "a = [2.5769925485448995, 0.0307829717973246, 0.7400768379343448, 1.4074311122243917]\n",
    " # estimates\n",
    "bb, nll_value = cal(a)\n",
    "print(f\"Full Negative Log-Likelihood: {bb.item()*1120, nll_value.item()*1120}\")\n",
    "\n",
    "a = [2.6769925485448995, 0.0307829717973246, 0.7400768379343448, 1.4074311122243917]\n",
    " \n",
    "bb, nll_value = cal(a)\n",
    "\n",
    "print(f\"Full Negative Log-Likelihood: {bb.item()*1120, nll_value.item()*1120}\")\n",
    "\n",
    "a = [2.4769925485448995, 0.0307829717973246, 0.7400768379343448, 1.4074311122243917]\n",
    "bb, nll_value = cal(a)\n",
    "\n",
    "print(f\"Full Negative Log-Likelihood: {bb.item()*1120, nll_value.item()*1120}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
