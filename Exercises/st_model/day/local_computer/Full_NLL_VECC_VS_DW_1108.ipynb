{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5872717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from GEMS_TCO import kernels\n",
    "from GEMS_TCO import data_preprocess \n",
    "from GEMS_TCO import kernels_new, kernels_reparametrization_space as kernels_repar_space\n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO.data_loader import load_data2\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d14b8b",
   "metadata": {},
   "source": [
    "Load monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29373ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting data to lat: [0.0, 5.0], lon: [123.0, 133.0]\n"
     ]
    }
   ],
   "source": [
    "space: List[str] = ['2', '1']\n",
    "lat_lon_resolution = [int(s) for s in space]\n",
    "mm_cond_number: int = 20\n",
    "years = ['2024']\n",
    "month_range = [7] \n",
    "\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "data_load_instance = load_data2(config.mac_data_load_path)\n",
    "\n",
    "\n",
    "df_map, ord_mm, nns_map = data_load_instance.load_maxmin_ordered_data_bymonthyear(\n",
    "lat_lon_resolution=lat_lon_resolution, \n",
    "mm_cond_number=mm_cond_number,\n",
    "years_=years, \n",
    "months_=month_range,\n",
    "lat_range=[0.0, 5.0],      \n",
    "lon_range=[123.0, 133.0] \n",
    ")\n",
    "\n",
    "#days: List[str] = ['0', '31']\n",
    "#days_s_e = [int(d) for d in days]\n",
    "#days_list = list(range(days_s_e[0], days_s_e[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c06c8d",
   "metadata": {},
   "source": [
    "Load daily data applying max-min ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7677058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18126, 4])\n"
     ]
    }
   ],
   "source": [
    "daily_aggregated_tensors = [] \n",
    "daily_hourly_maps = []        \n",
    "\n",
    "analysis_hour =2\n",
    "for day_index in range(31):\n",
    "  \n",
    "    hour_start_index = day_index * 8\n",
    "    #hour_end_index = (day_index + 1) * 8\n",
    "    #hour_end_index = day_index*8 + 1\n",
    "    hour_end_index = day_index*8 + analysis_hour\n",
    "\n",
    "    hour_indices = [hour_start_index, hour_end_index]\n",
    "    \n",
    "    # Load the data for the current day\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "        df_map, \n",
    "        hour_indices, \n",
    "        ord_mm=None,  \n",
    "        dtype=torch.float \n",
    "    )\n",
    "    # Append the day's data to their respective lists\n",
    "    daily_aggregated_tensors.append(day_aggregated_tensor)\n",
    "    daily_hourly_maps.append(day_hourly_map) \n",
    "\n",
    "print(daily_aggregated_tensors[0].shape)\n",
    "#print(daily_hourly_maps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46f2f21",
   "metadata": {},
   "source": [
    "Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8c81076",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0.5 # smooth\n",
    "mm_cond_number = 8\n",
    "nheads = 300\n",
    "#nheads = 1230\n",
    "#lr = 0.01\n",
    "#step = 80\n",
    "#gamma_par = 0.5\n",
    "\n",
    "# --- Placeholder Global Variables ---\n",
    "# ðŸ’¥ REVISED: Added lr, patience, factor. Removed step, gamma_par\n",
    "lr=0.1\n",
    "patience = 5       # Scheduler: Epochs to wait for improvement\n",
    "factor = 0.5         # Scheduler: Factor to reduce LR by (e.g., 0.5 = 50% cut)\n",
    "epochs=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3baceb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Callable\n",
    "\n",
    "# =========================================================================\n",
    "# 1. Distance and Covariance Functions (Optimized)\n",
    "# =========================================================================\n",
    "\n",
    "def precompute_coords_anisotropy(params: torch.Tensor, y: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Pre-computes the anisotropic coordinates and squared distance\n",
    "    using broadcasting to be more memory-efficient.\n",
    "    \"\"\"\n",
    "    sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "\n",
    "    if y is None or x is None:\n",
    "        raise ValueError(\"Both y and x must be provided.\")\n",
    "        \n",
    "    # x is (N, 4), y is (M, 4)\n",
    "    x1, y1, t1 = x[:, 0], x[:, 1], x[:, 3]\n",
    "    x2, y2, t2 = y[:, 0], y[:, 1], y[:, 3]\n",
    "    \n",
    "    # Pre-calculate advected coordinates\n",
    "    # x_adv = (x - v_lat*t)\n",
    "    # y_adv = (y - v_lon*t)\n",
    "    x1_adv = x1 - advec_lat * t1\n",
    "    y1_adv = y1 - advec_lon * t1\n",
    "    \n",
    "    x2_adv = x2 - advec_lat * t2\n",
    "    y2_adv = y2 - advec_lon * t2\n",
    "\n",
    "    # Use broadcasting to compute pairwise differences (N, M)\n",
    "    # (N, 1) - (1, M) -> (N, M)\n",
    "    delta_x_adv = x1_adv.unsqueeze(1) - x2_adv.unsqueeze(0)\n",
    "    delta_y_adv = y1_adv.unsqueeze(1) - y2_adv.unsqueeze(0)\n",
    "    delta_t     = t1.unsqueeze(1)     - t2.unsqueeze(0)\n",
    "\n",
    "    # Calculate squared distance terms\n",
    "    # d^2 = (delta_x_adv / r_lat)^2 + (delta_y_adv / r_lon)^2 + (delta_t * beta)^2\n",
    "    term1_sq = (delta_x_adv / range_lat).pow(2)\n",
    "    term2_sq = (delta_y_adv / range_lon).pow(2)\n",
    "    term3_sq = (delta_t * beta).pow(2)\n",
    "    \n",
    "    distance_sq = term1_sq + term2_sq + term3_sq\n",
    "    return distance_sq\n",
    "\n",
    "def matern_cov_anisotropy_v05(params: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the Matern 0.5 (Exponential) covariance.\n",
    "    (Optimized to remove boolean masking)\n",
    "    \"\"\"\n",
    "    sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "    \n",
    "    # Call the optimized precompute function\n",
    "    distance_sq = precompute_coords_anisotropy(params, x, y)\n",
    "\n",
    "    # Add a small epsilon for numerical stability at d=0\n",
    "    # This prevents NaN gradients when distance is exactly zero\n",
    "    epsilon = 1e-12\n",
    "    sqrt_distance = torch.sqrt(distance_sq + epsilon)\n",
    "    \n",
    "    # Compute covariance directly.\n",
    "    # torch.exp(-0.0) is 1.0, which correctly handles the zero-lag case.\n",
    "    out = sigmasq * torch.exp(-sqrt_distance)\n",
    "\n",
    "    # Add nugget to the diagonal\n",
    "    if x.shape[0] == y.shape[0]:\n",
    "        out.diagonal().add_(nugget) \n",
    "    \n",
    "    return out\n",
    "\n",
    "# =========================================================================\n",
    "# 2. Full Likelihood Function (Corrected)\n",
    "# =========================================================================\n",
    "\n",
    "def full_likelihood(params: torch.Tensor, input_data: torch.Tensor, y: torch.Tensor, covariance_function: Callable) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Optimized likelihood function using Cholesky decomposition.\n",
    "    (Corrected to include an intercept in the mean trend)\n",
    "    \"\"\"\n",
    "    input_data = input_data.to(torch.float64)\n",
    "    y = y.to(torch.float64)\n",
    "            \n",
    "    cov_matrix = covariance_function(params=params, y=input_data, x=input_data)\n",
    "    \n",
    "    try:\n",
    "        # Add a small jitter for stability\n",
    "        jitter = torch.eye(cov_matrix.shape[0], device=cov_matrix.device, dtype=torch.float64) * 1e-6\n",
    "        L = torch.linalg.cholesky(cov_matrix + jitter)\n",
    "    except torch.linalg.LinAlgError:\n",
    "        print(\"Warning: Cholesky decomposition failed. Matrix may not be positive definite.\")\n",
    "        return torch.tensor(torch.inf, device=params.device, dtype=params.dtype)\n",
    "\n",
    "    log_det = 2 * torch.sum(torch.log(torch.diag(L)))\n",
    "    \n",
    "    # --- ðŸ’¥ START FIX: Add intercept to the trend model ðŸ’¥ ---\n",
    "    locs_original = input_data[:, :2].to(torch.float64) # [lat, lon]\n",
    "    intercept = torch.ones(locs_original.shape[0], 1, \n",
    "                           device=locs_original.device, \n",
    "                           dtype=torch.float64)\n",
    "    # X matrix is now [1, lat, lon]\n",
    "    locs = torch.cat((intercept, locs_original), dim=1) \n",
    "    # --- END FIX ---\n",
    "    \n",
    "    if y.dim() == 1:\n",
    "        y_col = y.unsqueeze(-1).to(torch.float64)\n",
    "    else:\n",
    "        y_col = y.to(torch.float64)\n",
    "\n",
    "    # Solve for C_inv_X and C_inv_y\n",
    "    C_inv_X = torch.cholesky_solve(locs, L, upper=False)\n",
    "    C_inv_y = torch.cholesky_solve(y_col, L, upper=False)\n",
    "\n",
    "    # Compute beta\n",
    "    tmp1 = torch.matmul(locs.T, C_inv_X) # (3, N) @ (N, 3) = (3, 3)\n",
    "    tmp2 = torch.matmul(locs.T, C_inv_y) # (3, N) @ (N, 1) = (3, 1)\n",
    "    \n",
    "    try:\n",
    "        # Add jitter to the small system as well\n",
    "        jitter_beta = torch.eye(tmp1.shape[0], device=tmp1.device, dtype=torch.float64) * 1e-8\n",
    "        beta = torch.linalg.solve(tmp1 + jitter_beta, tmp2) # Solves (3,3) system\n",
    "    except torch.linalg.LinAlgError:\n",
    "        print(\"Warning: Could not solve for beta. X^T C_inv X may be singular.\")\n",
    "        return torch.tensor(torch.inf, device=locs.device, dtype=locs.dtype)\n",
    "\n",
    "    # Compute the mean\n",
    "    mu = torch.matmul(locs, beta) # (N, 3) @ (3, 1) = (N, 1)\n",
    "    y_mu = y_col - mu\n",
    "\n",
    "    # Compute the quadratic form\n",
    "    C_inv_y_mu = torch.cholesky_solve(y_mu, L, upper=False)\n",
    "    quad_form = torch.matmul(y_mu.T, C_inv_y_mu)\n",
    "\n",
    "    # Compute the negative log likelihood\n",
    "    neg_log_lik = 0.5 * (log_det + quad_form.squeeze())\n",
    "    return neg_log_lik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b0fab",
   "metadata": {},
   "source": [
    "dw adams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da0feb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Negative Log-Likelihood: 24807.323750727002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 1. Create dummy parameters (sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget)\n",
    "params_tensor = torch.tensor([\n",
    "    11.7989,  # sigmasq\n",
    "    0.1104,   # range_lat\n",
    "    0.1643,   # range_lon\n",
    "    0.0223,   # advec_lat\n",
    "    -0.1672,  # advec_lon\n",
    "    0.1864,   # beta\n",
    "    0.0000   # nugget\n",
    "], dtype=torch.float64, device=device)\n",
    "\n",
    "\n",
    "# 4. Call the standalone likelihood function\n",
    "loss = full_likelihood(\n",
    "    params=params_tensor,\n",
    "    input_data= day_aggregated_tensor,\n",
    "    y=day_aggregated_tensor[:, 2],\n",
    "    covariance_function=matern_cov_anisotropy_v05\n",
    ")\n",
    "\n",
    "print(f\"Full Negative Log-Likelihood: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39988a22",
   "metadata": {},
   "source": [
    "vecchia adams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cd3f873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Negative Log-Likelihood: 23293.543004957515\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 1. Create dummy parameters (sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget)\n",
    "params_tensor = torch.tensor([\n",
    "  12.826226,  # sigma_sq\n",
    "  0.353326,   # range_lat\n",
    "  0.442368,   # range_lon\n",
    "  0.035456,   # advec_lat\n",
    "  -0.214677,  # advec_lon\n",
    "  0.180337,   # beta\n",
    "  2.856746   # nugget\n",
    "], dtype=torch.float64, device=device)\n",
    "\n",
    "# 4. Call the standalone likelihood function\n",
    "loss = full_likelihood(\n",
    "    params=params_tensor,\n",
    "    input_data= day_aggregated_tensor,\n",
    "    y=day_aggregated_tensor[:, 2],\n",
    "    covariance_function=matern_cov_anisotropy_v05\n",
    ")\n",
    "\n",
    "print(f\"Full Negative Log-Likelihood: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4e6c10",
   "metadata": {},
   "source": [
    "vecchia l bfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de04387e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Negative Log-Likelihood: 23797.00324943286\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 1. Create dummy parameters (sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget)\n",
    "params_tensor = torch.tensor([\n",
    "    12.635289,  # sigmasq\n",
    "    0.562862,   # range_lon\n",
    "    0.412029,   # range_lat\n",
    "    0.204350,   # beta\n",
    "    0.038643,   # advec_lat\n",
    "    -0.214493,  # advec_lon\n",
    "    3.488041    # nugget\n",
    "], dtype=torch.float64, device=device)\n",
    "\n",
    "# 4. Call the standalone likelihood function\n",
    "loss = full_likelihood(\n",
    "    params=params_tensor,\n",
    "    input_data= day_aggregated_tensor,\n",
    "    y=day_aggregated_tensor[:, 2],\n",
    "    covariance_function=matern_cov_anisotropy_v05\n",
    ")\n",
    "\n",
    "print(f\"Full Negative Log-Likelihood: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
