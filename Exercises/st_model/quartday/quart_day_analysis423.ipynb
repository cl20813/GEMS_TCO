{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7756a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "import csv\n",
    "\n",
    "\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "from GEMS_TCO import orderings as _orderings\n",
    "\n",
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "sys.path.append(\"/cache/home/jl2815/tco\")\n",
    "import os\n",
    "\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy                    # clone tensor\n",
    "import time\n",
    "\n",
    "# Custom imports\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels\n",
    "from GEMS_TCO import orbitmap \n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data_amarel\n",
    "\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910e0268",
   "metadata": {},
   "source": [
    "load estimates for initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9ff4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"C:\\\\Users\\\\joonw\\\\tco\\\\GEMS_TCO-2\\\\Exercises\\\\st_model\\\\estimates\\\\\"\n",
    "input_filename = \"full_estimation_1250_july24.pkl\"\n",
    "input_filepath = os.path.join(input_path, input_filename)\n",
    "# Load pickle\n",
    "with open(input_filepath, 'rb') as pickle_file:\n",
    "    amarel_map1250= pickle.load(pickle_file)\n",
    "\n",
    "df_1250 = pd.DataFrame()\n",
    "for key in amarel_map1250:\n",
    "    tmp = pd.DataFrame(amarel_map1250[key][0].reshape(1, -1), columns=['sigmasq', 'range_lat', 'range_lon', 'advec_lat', 'advec_lon', 'beta', 'nugget'])\n",
    "    tmp['loss'] = amarel_map1250[key][1]\n",
    "    df_1250 = pd.concat((df_1250, tmp), axis=0)\n",
    "\n",
    "date_range = pd.date_range(start='07-01-24', end='07-31-24')\n",
    "\n",
    "# Ensure the number of dates matches the number of rows in df_1250\n",
    "if len(date_range) == len(df_1250):\n",
    "    df_1250.index = date_range\n",
    "else:\n",
    "    print(\"The number of dates does not match the number of rows in the DataFrame.\")\n",
    "\n",
    "df = df_1250.copy( deep=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ecc28df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class load_data:\n",
    "    def __init__(self, datapath):\n",
    "        self.datapath = datapath\n",
    "    \n",
    "    def load_mm20k_data_bymonthyear(self, lat_lon_resolution= [10,10], mm_cond_number=10, years_=['2024'], months_=[7,8]):\n",
    "\n",
    "        ## Load the one dictionary to set spaital coordinates\n",
    "        # filepath = \"C:/Users/joonw/TCO/GEMS_data/data_2023/sparse_cen_map23_01.pkl\"\n",
    "        filepath =  Path(self.datapath) / \"pickle_2023\\\\coarse_cen_map23_01.pkl\"\n",
    "        \n",
    "        with open(filepath, 'rb') as pickle_file:\n",
    "            coarse_dict_24_1 = pickle.load(pickle_file)\n",
    "\n",
    "        sample_df = coarse_dict_24_1['y23m01day01_hm02:12']\n",
    "        sample_key = coarse_dict_24_1.get('y23m01day01_hm02:12')\n",
    "        if sample_key is None:\n",
    "            print(\"Key 'y23m01day01_hm02:12' not found in the dictionary.\")\n",
    "\n",
    "        rho_lat = lat_lon_resolution[0]          \n",
    "        rho_lon = lat_lon_resolution[1]\n",
    "        lat_n = sample_df['Latitude'].unique()[::rho_lat]\n",
    "        lon_n = sample_df['Longitude'].unique()[::rho_lon]\n",
    "\n",
    "        # Set spatial coordinates for each dataset\n",
    "        coarse_dicts = {}\n",
    "        years = years_\n",
    "        for year in years:\n",
    "            for month in range(months_[0], months_[1]):  # Iterate over all months\n",
    "                # filepath = f\"C:/Users/joonw/TCO/GEMS_data/data_{year}/sparse_cen_map{year[2:]}_{month:02d}.pkl\"\n",
    "                filepath = Path(self.datapath) / f\"pickle_{year}/coarse_cen_map{year[2:]}_{month:02d}.pkl\"\n",
    "                with open(filepath, 'rb') as pickle_file:\n",
    "                    loaded_map = pickle.load(pickle_file)\n",
    "                    for key in loaded_map:\n",
    "                        tmp_df = loaded_map[key]\n",
    "                        coarse_filter = (tmp_df['Latitude'].isin(lat_n)) & (tmp_df['Longitude'].isin(lon_n))\n",
    "                        coarse_dicts[f\"{year}_{month:02d}_{key}\"] = tmp_df[coarse_filter].reset_index(drop=True)\n",
    "\n",
    "        key_idx = sorted(coarse_dicts)\n",
    "        if not key_idx:\n",
    "            raise ValueError(\"coarse_dicts is empty\")\n",
    "\n",
    "        # extract first hour data because all data shares the same spatial grid\n",
    "        data_for_coord = coarse_dicts[key_idx[0]]\n",
    "        x1 = data_for_coord['Longitude'].values\n",
    "        y1 = data_for_coord['Latitude'].values \n",
    "        coords1 = np.stack((x1, y1), axis=-1)\n",
    "\n",
    "        # instance = orbitmap.MakeOrbitdata(data_for_coord, lat_s=5, lat_e=10, lon_s=110, lon_e=120)\n",
    "        # s_dist = cdist(coords1, coords1, 'euclidean')\n",
    "        # ord_mm, _ = instance.maxmin_naive(s_dist, 0)\n",
    "\n",
    "        ord_mm = _orderings.maxmin_cpp(coords1)\n",
    "        data_for_coord = data_for_coord.iloc[ord_mm].reset_index(drop=True)\n",
    "        coords1_reordered = np.stack((data_for_coord['Longitude'].values, data_for_coord['Latitude'].values), axis=-1)\n",
    "        # nns_map = instance.find_nns_naive(locs=coords1_reordered, dist_fun='euclidean', max_nn=mm_cond_number)\n",
    "        nns_map=_orderings.find_nns_l2(locs= coords1_reordered  ,max_nn = mm_cond_number)\n",
    "        return coarse_dicts, ord_mm, nns_map\n",
    "\n",
    "    def load_working_data_byday(self, coarse_dicts,  ord_mm, nns_map, idx_for_datamap=[0,8]):\n",
    "        key_idx = sorted(coarse_dicts)\n",
    "        if not key_idx:\n",
    "            raise ValueError(\"coarse_dicts is empty\")\n",
    "        analysis_data_map = {}\n",
    "        for i in range(idx_for_datamap[0],idx_for_datamap[1]):\n",
    "            tmp = coarse_dicts[key_idx[i]].copy()\n",
    "            tmp['Hours_elapsed'] = np.round(tmp['Hours_elapsed']-477700)\n",
    "            tmp = tmp.iloc[ord_mm, :4].to_numpy()  # reorder the data\n",
    "            tmp = torch.from_numpy(tmp).double()\n",
    "            analysis_data_map[key_idx[i]] = tmp\n",
    "\n",
    "        aggregated_data = pd.DataFrame()\n",
    "        for i in range(idx_for_datamap[0],idx_for_datamap[1]):\n",
    "            tmp = coarse_dicts[key_idx[i]].copy()\n",
    "            tmp['Hours_elapsed'] = np.round(tmp['Hours_elapsed']-477700)\n",
    "            tmp = tmp.iloc[ord_mm].reset_index(drop=True)  \n",
    "            aggregated_data = pd.concat((aggregated_data, tmp), axis=0)\n",
    "\n",
    "        aggregated_data = aggregated_data.iloc[:, :4].to_numpy()\n",
    "        #aggregated_data = torch.from_numpy(aggregated_data).float() \n",
    "        aggregated_data = torch.from_numpy(aggregated_data).double()\n",
    "\n",
    "        return analysis_data_map, aggregated_data\n",
    "\n",
    "\n",
    "    def reorder_data(self, analysis_data_map, aggregated_data, key_order):\n",
    "        # key_order = [0, 1, 2, 4, 3, 5, 7, 6]\n",
    "        keys = list(analysis_data_map.keys())\n",
    "        reordered_dict = {keys[key]: analysis_data_map[keys[key]] for key in key_order}\n",
    "        reorder_keys = list(reordered_dict.keys())\n",
    "        data_frames = []\n",
    "\n",
    "        for key in reorder_keys:\n",
    "            tensor_data = reordered_dict[key]\n",
    "            if isinstance(tensor_data, torch.Tensor):\n",
    "                tensor_data = tensor_data.numpy()  # Convert tensor to NumPy array\n",
    "                tensor_df = pd.DataFrame(tensor_data)  # Convert NumPy array to DataFrame\n",
    "            else:\n",
    "                tensor_df = tensor_data  # If it's already a DataFrame\n",
    "            data_frames.append(tensor_df)\n",
    "\n",
    "        reordered_df = pd.concat(data_frames, axis=0)\n",
    "        reordered_df = reordered_df.to_numpy()\n",
    "        reordered_df = torch.from_numpy(reordered_df).double()\n",
    "\n",
    "        return reordered_dict, reordered_df\n",
    "\n",
    "\n",
    "\n",
    "    def load_working_data_by_quarterday(self, coarse_dicts, ord_mm, nns_map, which_group, qrt_idx, avg_days):\n",
    "        keys = sorted(coarse_dicts)\n",
    "        if not keys:\n",
    "            raise ValueError(\"coarse_dicts is empty\")\n",
    "        \n",
    "        avg_idx = 8 * avg_days\n",
    "        analysis_data_map = {}\n",
    "        entire_data = []\n",
    "\n",
    "        # qrt_idx takes 1, 2, 3, 4 for 4 quarters\n",
    "        for i in range(which_group-1, which_group ):\n",
    "            idx_quarter = [[avg_idx * i + 8 * j + 2 * (qrt_idx - 1), avg_idx * i + 8 * j + (2 * qrt_idx - 1)] for j in range(avg_days)]\n",
    "            idx_quarter = [item for sublist in idx_quarter for item in sublist]\n",
    "\n",
    "            aggregated_data = []\n",
    "            for key_idx in idx_quarter:\n",
    "                tmp = coarse_dicts[keys[key_idx]].copy()\n",
    "                tmp['Hours_elapsed'] = np.round(tmp['Hours_elapsed'] - 477700)\n",
    "                tmp['new_key'] = key_idx % 8\n",
    "                aggregated_data.append(tmp)\n",
    "                \n",
    "                tmp = tmp.iloc[ord_mm, [0,1,2,3,5]  ].to_numpy()\n",
    "                tmp = torch.from_numpy(tmp).double()\n",
    "\n",
    "                analysis_data_map[f'unit_{i}_quarter_{key_idx % 8}'] = tmp\n",
    "\n",
    "            aggregated_data = pd.concat(aggregated_data, axis=0)\n",
    "            aggregated_data = aggregated_data[['Latitude', 'Longitude', 'ColumnAmountO3', 'new_key']].groupby(['Latitude', 'Longitude', 'new_key']).mean().reset_index()\n",
    "            aggregated_data['quarter'] = qrt_idx\n",
    "        \n",
    "            aggregated_data = aggregated_data.iloc[:, :5 ].to_numpy()\n",
    "            aggregated_data = torch.from_numpy(aggregated_data).double()\n",
    "            entire_data.append(aggregated_data)\n",
    "\n",
    "        entire_data = torch.cat(entire_data, dim=0)\n",
    "        entire_data = entire_data[:, [0, 1, 3, 2]]\n",
    "        return analysis_data_map, entire_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f580771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Group 1 data size per day: 50.0 \n",
      "\n",
      "\n",
      " Group 2 data size per day: 50.0 \n",
      "\n",
      "\n",
      " Group 3 data size per day: 50.0 \n",
      "\n",
      "\n",
      " Group 4 data size per day: 50.0 \n",
      "\n",
      "\n",
      " Group 5 data size per day: 50.0 \n",
      "\n",
      "\n",
      " Group 6 data size per day: 50.0 \n",
      "\n",
      "\n",
      " Group 7 data size per day: 50.0 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  5.0250, 110.0250, 273.2014,   0.0000],\n",
       "        [  5.0250, 110.0250, 270.8236,   1.0000],\n",
       "        [  5.0250, 111.0250, 272.7497,   0.0000],\n",
       "        [  5.0250, 111.0250, 271.7945,   1.0000],\n",
       "        [  5.0250, 112.0250, 272.6109,   0.0000],\n",
       "        [  5.0250, 112.0250, 271.2740,   1.0000],\n",
       "        [  5.0250, 113.0250, 274.1084,   0.0000],\n",
       "        [  5.0250, 113.0250, 271.1791,   1.0000],\n",
       "        [  5.0250, 114.0250, 272.0381,   0.0000],\n",
       "        [  5.0250, 114.0250, 270.5241,   1.0000],\n",
       "        [  5.0250, 115.0250, 271.2672,   0.0000],\n",
       "        [  5.0250, 115.0250, 270.2809,   1.0000],\n",
       "        [  5.0250, 116.0250, 270.5139,   0.0000],\n",
       "        [  5.0250, 116.0250, 269.6774,   1.0000],\n",
       "        [  5.0250, 117.0250, 271.3630,   0.0000],\n",
       "        [  5.0250, 117.0250, 271.5377,   1.0000],\n",
       "        [  5.0250, 118.0250, 270.5465,   0.0000],\n",
       "        [  5.0250, 118.0250, 277.6002,   1.0000],\n",
       "        [  5.0250, 119.0250, 269.5345,   0.0000],\n",
       "        [  5.0250, 119.0250, 268.9179,   1.0000],\n",
       "        [  6.0250, 110.0250, 273.4041,   0.0000],\n",
       "        [  6.0250, 110.0250, 271.6668,   1.0000],\n",
       "        [  6.0250, 111.0250, 273.3806,   0.0000],\n",
       "        [  6.0250, 111.0250, 272.2725,   1.0000],\n",
       "        [  6.0250, 112.0250, 273.8598,   0.0000],\n",
       "        [  6.0250, 112.0250, 272.5477,   1.0000],\n",
       "        [  6.0250, 113.0250, 273.1793,   0.0000],\n",
       "        [  6.0250, 113.0250, 271.8386,   1.0000],\n",
       "        [  6.0250, 114.0250, 273.0315,   0.0000],\n",
       "        [  6.0250, 114.0250, 272.6455,   1.0000],\n",
       "        [  6.0250, 115.0250, 272.7065,   0.0000],\n",
       "        [  6.0250, 115.0250, 272.0320,   1.0000],\n",
       "        [  6.0250, 116.0250, 271.9352,   0.0000],\n",
       "        [  6.0250, 116.0250, 272.1370,   1.0000],\n",
       "        [  6.0250, 117.0250, 272.2896,   0.0000],\n",
       "        [  6.0250, 117.0250, 271.5415,   1.0000],\n",
       "        [  6.0250, 118.0250, 271.5263,   0.0000],\n",
       "        [  6.0250, 118.0250, 269.1265,   1.0000],\n",
       "        [  6.0250, 119.0250, 271.7211,   0.0000],\n",
       "        [  6.0250, 119.0250, 271.9900,   1.0000],\n",
       "        [  7.0250, 110.0250, 273.7879,   0.0000],\n",
       "        [  7.0250, 110.0250, 274.0050,   1.0000],\n",
       "        [  7.0250, 111.0250, 273.8611,   0.0000],\n",
       "        [  7.0250, 111.0250, 273.1495,   1.0000],\n",
       "        [  7.0250, 112.0250, 275.0285,   0.0000],\n",
       "        [  7.0250, 112.0250, 273.7270,   1.0000],\n",
       "        [  7.0250, 113.0250, 274.5652,   0.0000],\n",
       "        [  7.0250, 113.0250, 273.1567,   1.0000],\n",
       "        [  7.0250, 114.0250, 274.8799,   0.0000],\n",
       "        [  7.0250, 114.0250, 272.0238,   1.0000],\n",
       "        [  7.0250, 115.0250, 275.8239,   0.0000],\n",
       "        [  7.0250, 115.0250, 271.9019,   1.0000],\n",
       "        [  7.0250, 116.0250, 275.1974,   0.0000],\n",
       "        [  7.0250, 116.0250, 272.2851,   1.0000],\n",
       "        [  7.0250, 117.0250, 275.2671,   0.0000],\n",
       "        [  7.0250, 117.0250, 272.9561,   1.0000],\n",
       "        [  7.0250, 118.0250, 275.0319,   0.0000],\n",
       "        [  7.0250, 118.0250, 273.0406,   1.0000],\n",
       "        [  7.0250, 119.0250, 275.9119,   0.0000],\n",
       "        [  7.0250, 119.0250, 272.8962,   1.0000],\n",
       "        [  8.0250, 110.0250, 274.2068,   0.0000],\n",
       "        [  8.0250, 110.0250, 273.8699,   1.0000],\n",
       "        [  8.0250, 111.0250, 274.2479,   0.0000],\n",
       "        [  8.0250, 111.0250, 274.0265,   1.0000],\n",
       "        [  8.0250, 112.0250, 272.9609,   0.0000],\n",
       "        [  8.0250, 112.0250, 271.9447,   1.0000],\n",
       "        [  8.0250, 113.0250, 273.1921,   0.0000],\n",
       "        [  8.0250, 113.0250, 271.5027,   1.0000],\n",
       "        [  8.0250, 114.0250, 272.7684,   0.0000],\n",
       "        [  8.0250, 114.0250, 271.6064,   1.0000],\n",
       "        [  8.0250, 115.0250, 272.9433,   0.0000],\n",
       "        [  8.0250, 115.0250, 271.8292,   1.0000],\n",
       "        [  8.0250, 116.0250, 273.8168,   0.0000],\n",
       "        [  8.0250, 116.0250, 273.5328,   1.0000],\n",
       "        [  8.0250, 117.0250, 272.8879,   0.0000],\n",
       "        [  8.0250, 117.0250, 273.4268,   1.0000],\n",
       "        [  8.0250, 118.0250, 272.5906,   0.0000],\n",
       "        [  8.0250, 118.0250, 272.5639,   1.0000],\n",
       "        [  8.0250, 119.0250, 273.8080,   0.0000],\n",
       "        [  8.0250, 119.0250, 271.4877,   1.0000],\n",
       "        [  9.0250, 110.0250, 274.3029,   0.0000],\n",
       "        [  9.0250, 110.0250, 274.2142,   1.0000],\n",
       "        [  9.0250, 111.0250, 275.4347,   0.0000],\n",
       "        [  9.0250, 111.0250, 274.0978,   1.0000],\n",
       "        [  9.0250, 112.0250, 274.1561,   0.0000],\n",
       "        [  9.0250, 112.0250, 274.1691,   1.0000],\n",
       "        [  9.0250, 113.0250, 273.6036,   0.0000],\n",
       "        [  9.0250, 113.0250, 273.9522,   1.0000],\n",
       "        [  9.0250, 114.0250, 271.6334,   0.0000],\n",
       "        [  9.0250, 114.0250, 271.5018,   1.0000],\n",
       "        [  9.0250, 115.0250, 272.3587,   0.0000],\n",
       "        [  9.0250, 115.0250, 271.6414,   1.0000],\n",
       "        [  9.0250, 116.0250, 271.0756,   0.0000],\n",
       "        [  9.0250, 116.0250, 273.0857,   1.0000],\n",
       "        [  9.0250, 117.0250, 271.4948,   0.0000],\n",
       "        [  9.0250, 117.0250, 270.0117,   1.0000],\n",
       "        [  9.0250, 118.0250, 271.2439,   0.0000],\n",
       "        [  9.0250, 118.0250, 271.4867,   1.0000],\n",
       "        [  9.0250, 119.0250, 273.4887,   0.0000],\n",
       "        [  9.0250, 119.0250, 272.3910,   1.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_lon_resolution = [20,20]\n",
    "years = ['2024']\n",
    "month_range =[7,8]\n",
    "v= 0.5\n",
    "lr = 0.02\n",
    "days=10\n",
    "mm_cond_number=10\n",
    "epochs= 100\n",
    "nheads = 10\n",
    "\n",
    "result_q1 = {}\n",
    "result_q2 = {}\n",
    "result_q3 = {}\n",
    "result_q4 = {}\n",
    "\n",
    "\n",
    "avg_days = 4\n",
    "\n",
    "datapath = \"C:\\\\Users\\\\joonw\\\\tco\\\\Extracted_data\\\\\"\n",
    "instance = load_data(datapath)\n",
    "mm_cond_number = 10\n",
    "coarse_dicts, ord_mm, nns_map = instance.load_mm20k_data_bymonthyear( lat_lon_resolution= lat_lon_resolution, mm_cond_number=mm_cond_number,years_=years, months_=month_range)\n",
    "\n",
    "# (31//avg_days)+1\n",
    "\n",
    "for group_idx in range(1, 8):\n",
    "    print(f'\\n Group {group_idx} data size per day: { (200/lat_lon_resolution[0])*(100/lat_lon_resolution[0])  } \\n')\n",
    "\n",
    "    # parameters\n",
    "\n",
    "    params = list(df.iloc[ 4*(group_idx-1)][:-1])\n",
    "    params = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "    analysis_data_map_q1, entire_data_q1 = instance.load_working_data_by_quarterday( coarse_dicts, ord_mm, nns_map, which_group=group_idx, qrt_idx=1, avg_days= avg_days)\n",
    "\n",
    "\n",
    "entire_data_q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8a55aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Group 1 data size per day: 50.0 \n",
      "\n",
      "group_idx 2, data size per hour: 100, smooth: 0.5\n",
      "[20, 20] 10 tensor([ 2.4793e+01,  1.5845e+00,  1.7182e+00,  9.0885e-03, -1.0730e-01,\n",
      "         1.3104e-01,  2.7172e+00], dtype=torch.float64, requires_grad=True) 0.5 0.02\n",
      "FINAL STATE: Epoch 100, Loss: 115.17850850988697, \n",
      " vecc Parameters: [ 2.35885897e+01  2.83081303e+00  2.77694861e+00 -2.18183046e-02\n",
      " -7.11644834e-02  8.20515711e-03  6.78789892e-01]\n",
      "FINAL STATE: Epoch 100, Loss: 114.93692153521754, \n",
      " vecc Parameters: [ 2.31086570e+01  3.32602781e+00  3.26188336e+00  1.11338617e-01\n",
      " -2.08904947e-01 -7.96766784e-04  2.15580617e-01]\n",
      "FINAL STATE: Epoch 100, Loss: 123.95686058163915, \n",
      " vecc Parameters: [ 2.28259748e+01  3.61269298e+00  3.51942594e+00  3.77234410e-02\n",
      "  9.68388859e-03 -9.81099548e-02  9.53452767e-02]\n",
      "FINAL STATE: Epoch 100, Loss: 128.28839378969286, \n",
      " vecc Parameters: [22.75633693  3.79319402  3.70338232  0.17804059  0.19207337 -0.06625913\n",
      " -0.08508345]\n",
      "group_idx 2 took 1.11 for each morning and noon\n",
      "\n",
      " Group 2 data size per day: 50.0 \n",
      "\n",
      "group_idx 3, data size per hour: 100, smooth: 0.5\n",
      "[20, 20] 10 tensor([ 2.2599e+01,  2.9012e+00,  3.7223e+00, -1.1729e-02, -1.5207e-01,\n",
      "         7.2866e-02,  2.3972e+00], dtype=torch.float64, requires_grad=True) 0.5 0.02\n",
      "FINAL STATE: Epoch 100, Loss: 139.11385258831257, \n",
      " vecc Parameters: [ 2.51660086e+01  2.05892534e+00  2.40519631e+00 -6.62948570e-03\n",
      " -4.05700993e-02  1.79706551e-03  2.07209393e+00]\n",
      "FINAL STATE: Epoch 100, Loss: 141.49635507316458, \n",
      " vecc Parameters: [ 2.46821954e+01  2.55060324e+00  2.89697235e+00  6.40807924e-02\n",
      " -1.78737028e-01 -6.56503499e-04  1.58761597e+00]\n",
      "FINAL STATE: Epoch 100, Loss: 136.18500573611897, \n",
      " vecc Parameters: [ 2.43921238e+01  2.84101767e+00  3.19306963e+00  6.60311064e-02\n",
      " -1.29137034e-01  6.68354050e-05  1.29897223e+00]\n",
      "FINAL STATE: Epoch 100, Loss: 147.85354235702354, \n",
      " vecc Parameters: [24.4252355   3.0083012   3.25178988  0.25513039 -0.07121067  0.16230931\n",
      "  1.36363054]\n",
      "group_idx 3 took 1.01 for each morning and noon\n",
      "\n",
      " Group 3 data size per day: 50.0 \n",
      "\n",
      "group_idx 4, data size per hour: 100, smooth: 0.5\n",
      "[20, 20] 10 tensor([ 2.4052e+01,  1.3778e+00,  2.3577e+00,  2.1439e-02, -2.2032e-01,\n",
      "         1.4285e-01,  1.6755e+00], dtype=torch.float64, requires_grad=True) 0.5 0.02\n",
      "FINAL STATE: Epoch 100, Loss: 123.68993520229992, \n",
      " vecc Parameters: [2.38601412e+01 2.45476339e+00 2.67697988e+00 5.38119219e-04\n",
      " 5.68185815e-04 2.01760187e-04 2.83799605e+00]\n",
      "FINAL STATE: Epoch 100, Loss: 127.11965407820404, \n",
      " vecc Parameters: [ 2.33687081e+01  2.94940296e+00  3.17207282e+00  7.93661919e-04\n",
      "  9.22029113e-05 -3.87996309e-04  2.34391825e+00]\n",
      "FINAL STATE: Epoch 100, Loss: 123.82007729120394, \n",
      " vecc Parameters: [ 2.30772426e+01  3.23721005e+00  3.46346491e+00 -2.40732906e-04\n",
      " -1.53319082e-04  2.88958142e-04  2.06080511e+00]\n",
      "FINAL STATE: Epoch 100, Loss: 132.01787992677544, \n",
      " vecc Parameters: [ 2.29140884e+01  3.40298260e+00  3.62889117e+00 -1.87493742e-02\n",
      "  1.66526003e-01  6.63487953e-02  1.92531108e+00]\n",
      "group_idx 4 took 1.10 for each morning and noon\n",
      "\n",
      " Group 4 data size per day: 50.0 \n",
      "\n",
      "group_idx 5, data size per hour: 100, smooth: 0.5\n",
      "[20, 20] 10 tensor([22.7910,  2.0725,  3.6167, -0.1302,  0.0769,  0.1356,  1.4419],\n",
      "       dtype=torch.float64, requires_grad=True) 0.5 0.02\n",
      "FINAL STATE: Epoch 100, Loss: 107.76534208133128, \n",
      " vecc Parameters: [ 2.17507001e+01  3.73077668e+00  4.56926854e+00 -1.20861200e-03\n",
      " -1.15087889e-03  1.66750742e-04  1.59099667e+00]\n",
      "FINAL STATE: Epoch 100, Loss: 113.64779444108369, \n",
      " vecc Parameters: [ 2.18937829e+01  4.21581214e+00  4.99783054e+00 -2.68408610e-04\n",
      " -3.35667461e-03  3.45455581e-04  1.08183603e+00]\n",
      "FINAL STATE: Epoch 100, Loss: 121.32838874218868, \n",
      " vecc Parameters: [21.65162287  4.50469938  5.26118351 -0.06871928 -0.10933622  0.038645\n",
      "  0.89434271]\n",
      "FINAL STATE: Epoch 100, Loss: 104.47544166697858, \n",
      " vecc Parameters: [ 2.14924690e+01  4.67318488e+00  5.42124079e+00 -6.60842278e-03\n",
      "  4.74716907e-02  1.52843491e-02  7.29295025e-01]\n",
      "group_idx 5 took 1.08 for each morning and noon\n",
      "\n",
      " Group 5 data size per day: 50.0 \n",
      "\n",
      "group_idx 6, data size per hour: 100, smooth: 0.5\n",
      "[20, 20] 10 tensor([24.9783,  1.3712,  2.2366, -0.0689, -0.1266,  0.1374,  3.0443],\n",
      "       dtype=torch.float64, requires_grad=True) 0.5 0.02\n",
      "FINAL STATE: Epoch 100, Loss: 138.22068609002983, \n",
      " vecc Parameters: [ 2.47523842e+01  2.53753224e+00  3.10722428e+00 -1.12946907e-03\n",
      " -3.28947028e-04 -9.53475843e-05  3.03660976e+00]\n",
      "FINAL STATE: Epoch 100, Loss: 127.5577518338174, \n",
      " vecc Parameters: [ 2.42737638e+01  3.02913343e+00  3.59703057e+00 -1.17030877e-04\n",
      " -9.90083824e-04 -3.35263862e-04  2.56064157e+00]\n",
      "FINAL STATE: Epoch 100, Loss: 132.3082757223237, \n",
      " vecc Parameters: [ 2.39750424e+01  3.31928914e+00  3.89527973e+00 -8.50074134e-04\n",
      " -1.58420377e-03  9.93926387e-05  2.26812370e+00]\n",
      "FINAL STATE: Epoch 100, Loss: 131.3624269661216, \n",
      " vecc Parameters: [ 2.38130860e+01  3.48724111e+00  4.05834506e+00  3.07274242e-02\n",
      "  1.01941976e-01 -1.72092289e-06  2.09877825e+00]\n",
      "group_idx 6 took 1.10 for each morning and noon\n",
      "\n",
      " Group 6 data size per day: 50.0 \n",
      "\n",
      "group_idx 7, data size per hour: 100, smooth: 0.5\n",
      "[20, 20] 10 tensor([ 2.3972e+01,  2.3290e+00,  3.3506e+00, -2.1695e-03, -7.0489e-02,\n",
      "         1.0945e-01,  2.8070e+00], dtype=torch.float64, requires_grad=True) 0.5 0.02\n",
      "FINAL STATE: Epoch 100, Loss: 133.24843285109245, \n",
      " vecc Parameters: [ 2.51848160e+01  2.10184533e+00  3.69104567e+00 -7.81016040e-04\n",
      " -1.12472786e-03  3.47640806e-04  3.79354319e+00]\n",
      "FINAL STATE: Epoch 100, Loss: 137.4007138492964, \n",
      " vecc Parameters: [ 2.46954381e+01  2.59702543e+00  4.19230704e+00  3.52392440e-04\n",
      " -6.56925888e-04  1.40366751e-04  3.29901190e+00]\n",
      "FINAL STATE: Epoch 100, Loss: 130.3021822249036, \n",
      " vecc Parameters: [ 2.44134124e+01  2.88855735e+00  4.47111903e+00 -8.20804334e-05\n",
      "  1.87196844e-05 -1.60949109e-04  3.01672389e+00]\n",
      "FINAL STATE: Epoch 100, Loss: 130.87306382241218, \n",
      " vecc Parameters: [ 2.42492630e+01  3.05421468e+00  4.63845702e+00  5.96270459e-03\n",
      "  1.60868120e-01 -4.16432605e-02  2.84719130e+00]\n",
      "group_idx 7 took 1.26 for each morning and noon\n",
      "\n",
      " Group 7 data size per day: 50.0 \n",
      "\n",
      "group_idx 8, data size per hour: 100, smooth: 0.5\n",
      "[20, 20] 10 tensor([ 2.2398e+01,  3.9685e+00,  3.9453e+00,  6.2299e-03, -1.3954e-02,\n",
      "         4.7208e-02,  1.3455e+00], dtype=torch.float64, requires_grad=True) 0.5 0.02\n",
      "FINAL STATE: Epoch 100, Loss: 133.11592342124547, \n",
      " vecc Parameters: [ 2.51988358e+01  1.84118737e+00  2.49620022e+00  7.09588180e-06\n",
      " -7.41879429e-03  9.74983442e-04  1.93648552e+00]\n",
      "FINAL STATE: Epoch 100, Loss: 115.47061760875623, \n",
      " vecc Parameters: [ 2.47165155e+01  2.33112294e+00  2.98919532e+00 -8.83462029e-04\n",
      " -1.24566168e-03 -3.83768398e-04  1.47257934e+00]\n",
      "FINAL STATE: Epoch 100, Loss: 106.328275429444, \n",
      " vecc Parameters: [ 2.44313628e+01  2.61797415e+00  3.27990745e+00  2.71985540e-04\n",
      " -6.75001999e-04 -6.03681121e-05  1.19177065e+00]\n",
      "FINAL STATE: Epoch 100, Loss: 117.95146387075987, \n",
      " vecc Parameters: [ 2.42623351e+01  2.78801505e+00  3.44812868e+00  1.31789940e-02\n",
      "  7.12205959e-02 -8.23362704e-04  1.02229051e+00]\n",
      "group_idx 8 took 1.09 for each morning and noon\n"
     ]
    }
   ],
   "source": [
    "lat_lon_resolution = [20,20]\n",
    "years = ['2024']\n",
    "month_range =[7,8]\n",
    "v= 0.5\n",
    "lr = 0.02\n",
    "days=10\n",
    "mm_cond_number=10\n",
    "epochs= 100\n",
    "nheads = 10\n",
    "\n",
    "result_q1 = {}\n",
    "result_q2 = {}\n",
    "result_q3 = {}\n",
    "result_q4 = {}\n",
    "\n",
    "\n",
    "avg_days = 4\n",
    "\n",
    "datapath = \"C:\\\\Users\\\\joonw\\\\tco\\\\Extracted_data\\\\\"\n",
    "instance = load_data(datapath)\n",
    "mm_cond_number = 10\n",
    "coarse_dicts, ord_mm, nns_map = instance.load_mm20k_data_bymonthyear( lat_lon_resolution= lat_lon_resolution, mm_cond_number=mm_cond_number,years_=years, months_=month_range)\n",
    "\n",
    "# (31//avg_days)+1\n",
    "\n",
    "for group_idx in range(1, 8):\n",
    "    print(f'\\n Group {group_idx} data size per day: { (200/lat_lon_resolution[0])*(100/lat_lon_resolution[0])  } \\n')\n",
    "\n",
    "    # parameters\n",
    "\n",
    "    params = list(df.iloc[ 4*(group_idx-1)][:-1])\n",
    "    params = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "    analysis_data_map_q1, entire_data_q1 = instance.load_working_data_by_quarterday( coarse_dicts, ord_mm, nns_map, which_group=group_idx, qrt_idx=1, avg_days= avg_days)\n",
    "    \n",
    "    params = list(df.iloc[ 4*(group_idx-1)][:-1])\n",
    "    params = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "    analysis_data_map_q2, entire_data_q2 = instance.load_working_data_by_quarterday( coarse_dicts, ord_mm, nns_map, which_group=group_idx, qrt_idx=2, avg_days= avg_days)\n",
    "    \n",
    "    params = list(df.iloc[ 4*(group_idx-1)][:-1])\n",
    "    params = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "    analysis_data_map_q3, entire_data_q3 = instance.load_working_data_by_quarterday( coarse_dicts, ord_mm, nns_map, which_group=group_idx, qrt_idx=3, avg_days= avg_days)\n",
    "    \n",
    "    params = list(df.iloc[ 4*(group_idx-1)][:-1])\n",
    "    params = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "    analysis_data_map_q4, entire_data_q4 = instance.load_working_data_by_quarterday( coarse_dicts, ord_mm, nns_map, which_group=group_idx, qrt_idx=4, avg_days= avg_days)\n",
    "    \n",
    "    print(f'group_idx {group_idx+1}, data size per hour: {entire_data_q1.shape[0]}, smooth: {v}')\n",
    "    print(lat_lon_resolution, mm_cond_number , params, v,lr)\n",
    "    \n",
    "    params = list(df_1250.iloc[group_idx][:-1])\n",
    "    params = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "    model_q1 = kernels.model_fitting(\n",
    "            smooth=v,\n",
    "            input_map= analysis_data_map_q1,\n",
    "            aggregated_data= entire_data_q1,\n",
    "            nns_map=nns_map,\n",
    "            mm_cond_number=mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    model_q2 = kernels.model_fitting(\n",
    "            smooth=v,\n",
    "            input_map=analysis_data_map_q2,\n",
    "            aggregated_data=entire_data_q2,\n",
    "            nns_map=nns_map,\n",
    "            mm_cond_number=mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    model_q3 = kernels.model_fitting(\n",
    "            smooth=v,\n",
    "            input_map=analysis_data_map_q3,\n",
    "            aggregated_data=entire_data_q3,\n",
    "            nns_map=nns_map,\n",
    "            mm_cond_number=mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "\n",
    "    model_q4 = kernels.model_fitting(\n",
    "            smooth=v,\n",
    "            input_map=analysis_data_map_q4,\n",
    "            aggregated_data=entire_data_q4,\n",
    "            nns_map=nns_map,\n",
    "            mm_cond_number=mm_cond_number,\n",
    "            nheads = nheads\n",
    "        )\n",
    "    \n",
    "\n",
    "    start_time = time.time()\n",
    "    # optimizer = optim.Adam([params], lr=0.01)  # For Adam\n",
    "    optimizer, scheduler = model_q1.optimizer_fun(params, lr=0.01, betas=(0.9, 0.8), eps=1e-8, step_size=20, gamma=0.9)    \n",
    "    \n",
    "    out_q1, epoch = model_q1.run_full(params, optimizer,scheduler, model_q1.matern_cov_anisotropy_v05, epochs=epochs)\n",
    "    out_q2 = model_q2.run_full(params, optimizer,scheduler, model_q2.matern_cov_anisotropy_v05, epochs=epochs)\n",
    "    out_q3 = model_q3.run_full(params, optimizer,scheduler, model_q3.matern_cov_anisotropy_v05, epochs=epochs)\n",
    "    out_q4 = model_q4.run_full(params, optimizer,scheduler, model_q4.matern_cov_anisotropy_v05, epochs=epochs)\n",
    "    result_q1[group_idx+1] = out_q1\n",
    "    result_q2[group_idx+1] = out_q2\n",
    "    result_q3[group_idx+1] = out_q3\n",
    "    result_q4[group_idx+1] = out_q4\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    print(f'group_idx {group_idx + 1} took {epoch_time/2:.2f} for each morning and noon')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
