{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path in sys.path:\n",
    "#   print(path)\n",
    "\n",
    "import sys\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import concurrent\n",
    "from concurrent.futures import ThreadPoolExecutor  # Importing specific executor for clarity\n",
    "import time\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Nearest neighbor search\n",
    "import sklearn\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# Special functions and optimizations\n",
    "from scipy.special import gamma, kv  # Bessel function and gamma function\n",
    "from scipy.stats import multivariate_normal  # Simulation\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import cdist  # For space and time distance\n",
    "from scipy.spatial import distance  # Find closest spatial point\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "# Plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Type hints\n",
    "from typing import Callable, Union, Tuple\n",
    "\n",
    "# Add your custom path\n",
    "# sys.path.append(\"/cache/home/jl2815/tco\")\n",
    "\n",
    "# Custom imports\n",
    "\n",
    "from GEMS_TCO import orbitmap \n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import evaluate\n",
    "from GEMS_TCO import orderings as _orderings\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy                    # clone tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_resolution = [4,4]\n",
    "mm_cond_number = 10\n",
    "params= [20, 8.25, 5.25, 0.2, 0.5, 5]\n",
    "idx_for_datamap= [0,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the one dictionary to set spaital coordinates\n",
    "# filepath = \"C:/Users/joonw/TCO/GEMS_data/data_2023/sparse_cen_map23_01.pkl\"\n",
    "filepath = \"/Users/joonwonlee/Documents/GEMS_DATA/pickle_2023/coarse_cen_map23_01.pkl\"\n",
    "with open(filepath, 'rb') as pickle_file:\n",
    "    coarse_dict_24_1 = pickle.load(pickle_file)\n",
    "\n",
    "sample_df = coarse_dict_24_1['y23m01day01_hm02:12']\n",
    "\n",
    "sample_key = coarse_dict_24_1.get('y23m01day01_hm02:12')\n",
    "if sample_key is None:\n",
    "    print(\"Key 'y23m01day01_hm02:12' not found in the dictionary.\")\n",
    "\n",
    "# { (20,20):(5,1), (5,5):(20,40) }\n",
    "rho_lat = lat_lon_resolution[0]          \n",
    "rho_lon = lat_lon_resolution[1]\n",
    "lat_n = sample_df['Latitude'].unique()[::rho_lat]\n",
    "lon_n = sample_df['Longitude'].unique()[::rho_lon]\n",
    "\n",
    "lat_number = len(lat_n)\n",
    "lon_number = len(lon_n)\n",
    "\n",
    "# Set spatial coordinates for each dataset\n",
    "coarse_dicts = {}\n",
    "\n",
    "years = ['2024']\n",
    "for year in years:\n",
    "    for month in range(7, 8):  # Iterate over all months\n",
    "        # filepath = f\"C:/Users/joonw/TCO/GEMS_data/data_{year}/sparse_cen_map{year[2:]}_{month:02d}.pkl\"\n",
    "        filepath = f\"/Users/joonwonlee/Documents/GEMS_DATA/pickle_{year}/coarse_cen_map{year[2:]}_{month:02d}.pkl\"\n",
    "        with open(filepath, 'rb') as pickle_file:\n",
    "            loaded_map = pickle.load(pickle_file)\n",
    "            for key in loaded_map:\n",
    "                tmp_df = loaded_map[key]\n",
    "                coarse_filter = (tmp_df['Latitude'].isin(lat_n)) & (tmp_df['Longitude'].isin(lon_n))\n",
    "                coarse_dicts[f\"{year}_{month:02d}_{key}\"] = tmp_df[coarse_filter].reset_index(drop=True)\n",
    "\n",
    "\n",
    "key_idx = sorted(coarse_dicts)\n",
    "if not key_idx:\n",
    "    raise ValueError(\"coarse_dicts is empty\")\n",
    "\n",
    "# extract first hour data because all data shares the same spatial grid\n",
    "data_for_coord = coarse_dicts[key_idx[0]]\n",
    "x1 = data_for_coord['Longitude'].values\n",
    "y1 = data_for_coord['Latitude'].values \n",
    "coords1 = np.stack((x1, y1), axis=-1)\n",
    "\n",
    "\n",
    "# instance = orbitmap.MakeOrbitdata(data_for_coord, lat_s=5, lat_e=10, lon_s=110, lon_e=120)\n",
    "# s_dist = cdist(coords1, coords1, 'euclidean')\n",
    "# ord_mm, _ = instance.maxmin_naive(s_dist, 0)\n",
    "\n",
    "ord_mm = _orderings.maxmin_cpp(coords1)\n",
    "data_for_coord = data_for_coord.iloc[ord_mm].reset_index(drop=True)\n",
    "coords1_reordered = np.stack((data_for_coord['Longitude'].values, data_for_coord['Latitude'].values), axis=-1)\n",
    "# nns_map = instance.find_nns_naive(locs=coords1_reordered, dist_fun='euclidean', max_nn=mm_cond_number)\n",
    "nns_map=_orderings.find_nns_l2(locs= coords1_reordered  ,max_nn = mm_cond_number)\n",
    "\n",
    "\n",
    "analysis_data_map = {}\n",
    "for i in range(idx_for_datamap[0],idx_for_datamap[1]):\n",
    "    tmp = coarse_dicts[key_idx[i]].copy()\n",
    "    tmp['Hours_elapsed'] = np.round(tmp['Hours_elapsed']-477700)\n",
    "\n",
    "    tmp = tmp.iloc[ord_mm, :4].to_numpy()\n",
    "    tmp = torch.from_numpy(tmp).float()  # Convert NumPy to Tensor\n",
    "    # tmp = tmp.clone().detach().requires_grad_(True)  # Enable gradients\n",
    "    \n",
    "    analysis_data_map[key_idx[i]] = tmp\n",
    "\n",
    "aggregated_data = pd.DataFrame()\n",
    "for i in range(idx_for_datamap[0],idx_for_datamap[1]):\n",
    "    tmp = coarse_dicts[key_idx[i]].copy()\n",
    "    tmp['Hours_elapsed'] = np.round(tmp['Hours_elapsed']-477700)\n",
    "    tmp = tmp.iloc[ord_mm].reset_index(drop=True)  \n",
    "    aggregated_data = pd.concat((aggregated_data, tmp), axis=0)\n",
    "\n",
    "aggregated_data = aggregated_data.iloc[:, :4].to_numpy()\n",
    "\n",
    "aggregated_data = torch.from_numpy(aggregated_data).float()  # Convert NumPy to Tensor\n",
    "# aggregated_np = aggregated_np.clone().detach().requires_grad_(True)  # Enable gradients\n",
    "\n",
    "\n",
    "instance = kernels.likelihood_function(smooth=0.5, input_map=analysis_data_map, aggregated_data=aggregated_data,nns_map=nns_map, mm_cond_number=mm_cond_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2542.7729, grad_fn=<MulBackward0>)\n",
      "tensor(2640.7676, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define your initial parameters\n",
    "params = [21.8, 1.09, 1.17, 0.2, .2, 0.5, 1]\n",
    "params = [52.627, 4, 5.685, 6.77e-2, -4.19e-3, 0.0585, 3.143]  # 50x8 lr=0.01  24.42 1.92, 1.92, 0.001, -0.045, -.237, 3.34\n",
    "params = [51.79, 3.894, 4.135, -2.08e-2, -7.71e-2, 0.061, 3.5]\n",
    "params = torch.tensor(params, requires_grad=True)\n",
    "\n",
    "torch_smooth = torch.tensor(0.5, dtype=torch.float32)\n",
    "\n",
    "instance = kernels.likelihood_function(smooth=torch_smooth , input_map=analysis_data_map,aggregated_data=aggregated_data, nns_map=nns_map, mm_cond_number=mm_cond_number)\n",
    "\n",
    "out0 = instance.full_likelihood(params, aggregated_data[:,:4],aggregated_data[:,2], instance.matern_cov_anisotropy_v05)\n",
    "print(out0)\n",
    "\n",
    "out0 = instance.vecchia_like_local_computer(params, instance.matern_cov_anisotropy_v05)\n",
    "print(out0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization full likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 200 x 8\n",
    "\n",
    "lr 0.001 without scheduler  same as lr, step_size, gamma  0.01 40 0.5  (9.8s)\n",
    "\n",
    " Loss: 2549.066650390625, full Parameters: [ 2.48777485e+01  2.05998826e+00  2.16013098e+00  2.20775465e-03\n",
    " -7.89414570e-02  1.05411254e-01  3.75236106e+00]\n",
    "\n",
    " lr 0.01  step size 40  betas 0.9 , 0.8 gamma 0.9  30 s\n",
    "\n",
    "  Loss: 2547.1728515625, full Parameters: [ 2.7377291e+01  2.2077193e+00  2.3204505e+00  1.0307773e-03\n",
    " -8.0311157e-02  9.8579854e-02  3.6677265e+00]\n",
    "\n",
    " lr 0.01  step size 10 betas 0.9 , 0.8 gamma 0.9  30 s\n",
    "  Loss: 2548.87841796875, full Parameters: [ 2.5092268e+01  2.0689390e+00  2.1694989e+00  2.0285936e-03\n",
    " -7.9028614e-02  1.0501490e-01  3.7373385e+00]\n",
    "Training full likelihood complete.   11.8 sc\n",
    "\n",
    " lr 0.01  step size 20 betas 0.9 , 0.8 gamma 0.9  30 s\n",
    " Loss: 2548.15283203125, full Parameters: [ 2.59814014e+01  2.12175608e+00  2.22699022e+00  1.73025124e-03\n",
    " -7.93599486e-02  1.02427535e-01  3.70715070e+00]\n",
    "\n",
    "\n",
    "\n",
    "lr 0.01  step size 20 beta 0.9 0.99 gamma 0.9\n",
    " Loss: 2548.18603515625, full Parameters: [ 2.5938652e+01  2.1110108e+00  2.2155209e+00  1.5893303e-03\n",
    " -7.9482891e-02  1.0297947e-01  3.6958976e+00]\n",
    " 21.6\n",
    "\n",
    "lr 0.01  step size 20 beta 0.9 0.8 gamma 0.9\n",
    " Loss: 2548.15283203125, full Parameters: [ 2.59814014e+01  2.12175608e+00  2.22699022e+00  1.73025124e-03\n",
    " -7.93599486e-02  1.02427535e-01  3.70715070e+00]\n",
    " 22.9 s\n",
    "\n",
    "lr 0.01  step size 10 beta 0.9 0.99 gamma 0.9\n",
    "Loss: 2548.95361328125, full Parameters: [ 2.5118145e+01  1.9827319e+00  2.0768294e+00  1.0898338e-03\n",
    " -8.0070712e-02  1.1034889e-01  3.5647078e+00]\n",
    "\n",
    "\n",
    "## 1250 x 8\n",
    "\n",
    "1250* 8 55m using constant learning rate 0.0001 \n",
    "Loss: 14068.798828125, full Parameters: [ 2.46198387e+01  1.61719894e+00  1.76454413e+00  8.55297223e-03\n",
    " -1.08275235e-01  1.28809512e-01  2.80795789e+00]\n",
    "\n",
    "1250* 8 10m 32s\n",
    "lr 0.01  step size 40 beta 0.9 0.8 gamma 0.9\n",
    "  Loss: 14068.1953125, full Parameters: [ 2.5030930e+01  1.6107724e+00  1.7573007e+00  8.8407323e-03\n",
    " -1.0820019e-01  1.2936097e-01  2.7430327e+00]\n",
    "Training full likelihood complete.\n",
    "\n",
    "9m 33s\n",
    "lr 0.01  step size 20 beta 0.9 0.8 gamma 0.9\n",
    " Loss: 14068.29296875, full Parameters: \n",
    " [ 2.4933689e+01  1.6009743e+00  1.7502663e+00  9.2404895e-03 -1.0737537e-01  1.2953614e-01 \n",
    "  2.7420275e+00]\n",
    "Training full likelihood complete.\n",
    "\n",
    "#### high resolution data might benefits from larger step size high resolution data often provides \n",
    "#### more stable gradients, so larger step size less likely to cause significant fluctuations\n",
    "14n 41.8s\n",
    "lr 0.01  step size 10 beta 0.9 0.99 gamma 0.9\n",
    "\n",
    "FINAL STATE: Epoch 199, \n",
    " Loss: 14068.8828125, full Parameters: \n",
    " [ 2.4707581e+01  1.6489888e+00  1.7993137e+00  8.4043797e-03 -1.0836436e-01  1.2655504e-01  \n",
    " 2.8416286e+00]\n",
    "\n",
    "#### beta 0.9 0.99 might be too conservative for high resolution data\n",
    "13m 44.8s\n",
    "lr 0.01  step size 20 beta 0.9 0.99 gamma 0.9\n",
    "\n",
    " Loss: 14068.318359375, full Parameters: [ 2.4938175e+01  1.6203119e+00  1.7678342e+00  8.6686825e-03\n",
    " -1.0813228e-01  1.2845081e-01  2.7731323e+00]\n",
    "\n",
    "\n",
    "18m\n",
    "lr 0.01  step size 40 beta 0.9 0.99 gamma 0.9\n",
    "\n",
    " Loss: 14067.970703125, full Parameters: [ 2.5205673e+01  1.6159834e+00  1.7630767e+00  8.7957922e-03\n",
    " -1.0802399e-01  1.2862283e-01  2.7390635e+00]\n",
    "\n",
    "9m 52s\n",
    "lr 0.01  step size 20 beta 0.9 0.8 gamma 0.9\n",
    "\n",
    "Loss: 14068.29296875, full Parameters: [ 2.4933689e+01  1.6009743e+00  1.7502663e+00  9.2404895e-03\n",
    " -1.0737537e-01  1.2953614e-01  2.7420275e+00]\n",
    "Training full likelihood complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Gradients: [   5.967201   64.56529    25.027222  -61.038574  203.27737  1465.2336\n",
      "   56.982613]\n",
      " Loss: 14257.3193359375, Parameters: [ 2.442e+01  1.920e+00  1.920e+00  1.000e-03 -4.500e-02  2.370e-01\n",
      "  3.340e+00]\n",
      "Epoch 101, Gradients: [ -0.7736554   -0.61901855  -0.24597168  -5.3624268  -12.07843\n",
      "  71.16577      2.4958801 ]\n",
      " Loss: 14068.568359375, Parameters: [ 2.4761600e+01  1.5730878e+00  1.7161615e+00  8.2207927e-03\n",
      " -1.0916295e-01  1.3317481e-01  2.7271605e+00]\n",
      "Converged at epoch 129\n",
      "Epoch 130, Gradients: [ -1.2897807   -1.5251465    0.8886719   -0.49572754   1.3125\n",
      " -29.372559    -0.845871  ]\n",
      " Loss: 14068.29296875, full Parameters: [ 2.4933689e+01  1.6009743e+00  1.7502663e+00  9.2404895e-03\n",
      " -1.0737537e-01  1.2953614e-01  2.7420275e+00]\n",
      "FINAL STATE: Epoch 130, Gradients: [ -1.2897807   -1.5251465    0.8886719   -0.49572754   1.3125\n",
      " -29.372559    -0.845871  ]\n",
      " Loss: 14068.29296875, full Parameters: [ 2.4933689e+01  1.6009743e+00  1.7502663e+00  9.2404895e-03\n",
      " -1.0737537e-01  1.2953614e-01  2.7420275e+00]\n",
      "Training full likelihood complete.\n"
     ]
    }
   ],
   "source": [
    "params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "params = torch.tensor(params, requires_grad=True)\n",
    "\n",
    "instance = kernels.model_fitting(\n",
    "    smooth=0.5,\n",
    "    input_map=analysis_data_map,\n",
    "    aggregated_data=aggregated_data,\n",
    "    nns_map=nns_map,\n",
    "    mm_cond_number=mm_cond_number\n",
    ")\n",
    "\n",
    "# optimizer = optim.Adam([params], lr=0.01)  # For Adam\n",
    "optimizer, scheduler = instance.optimizer_fun2( params, lr=0.01, betas=(0.9, 0.8), eps=1e-8, step_size=20, gamma=0.9)    \n",
    "instance.run_full2(params, optimizer,scheduler, epochs=3000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Gradients: [  0.5812483  19.981602   11.618286    2.1514587  18.508026  447.89014\n",
      "   5.896081 ]\n",
      " Loss: 2588.835693359375, Parameters: [ 2.442e+01  1.920e+00  1.920e+00  1.000e-03 -4.500e-02  2.370e-01\n",
      "  3.340e+00]\n",
      "Epoch 101, Gradients: [-4.4301248e-01  5.5735703e+00  2.6599121e-01  9.8266602e-03\n",
      "  8.8864136e-01  1.9039636e+02  1.7605927e+00]\n",
      " Loss: 2552.7041015625, Parameters: [ 2.4379133e+01  1.8349186e+00  1.8445122e+00 -1.1280666e-03\n",
      " -7.4676104e-02  1.4577185e-01  3.2494757e+00]\n",
      "Epoch 201, Gradients: [-8.6255789e-01 -8.7886429e-01 -1.7917480e+00 -1.6784668e-04\n",
      " -2.0629883e-02  3.6184082e+00 -1.7865100e+00]\n",
      " Loss: 2550.16357421875, Parameters: [ 2.45150394e+01  1.82015252e+00  1.87993288e+00 -4.56443377e-04\n",
      " -8.19397792e-02  1.22905836e-01  3.27331400e+00]\n",
      "Epoch 301, Gradients: [-8.4441233e-01 -1.4291420e+00 -1.0267334e+00 -1.8310547e-03\n",
      " -7.8430176e-03  3.1687012e+00 -1.4634323e+00]\n",
      " Loss: 2549.85693359375, Parameters: [ 2.4625595e+01  1.8435745e+00  1.9267030e+00 -4.1488409e-05\n",
      " -8.1376150e-02  1.2013363e-01  3.3412797e+00]\n",
      "Epoch 401, Gradients: [-0.8504753  -1.1661701  -0.7297363   0.00292969 -0.00363159  1.9820557\n",
      " -1.2275319 ]\n",
      " Loss: 2549.567626953125, Parameters: [ 2.4728565e+01  1.8846155e+00  1.9728407e+00  3.6250908e-04\n",
      " -8.0913275e-02  1.1697658e-01  3.4154367e+00]\n",
      "Epoch 501, Gradients: [-0.86108583 -0.7798214  -0.49420166  0.02737427 -0.01132202  1.144043\n",
      " -0.9900532 ]\n",
      " Loss: 2549.315673828125, Parameters: [ 2.4830246e+01  1.9307530e+00  2.0213842e+00  7.8439940e-04\n",
      " -8.0444857e-02  1.1379821e-01  3.4926479e+00]\n",
      "Epoch 601, Gradients: [-0.8653918  -0.47185326 -0.3057251  -0.02084351  0.01010132  0.60180664\n",
      " -0.71925116]\n",
      " Loss: 2549.114990234375, Parameters: [ 2.4931366e+01  1.9752837e+00  2.0685945e+00  1.1428846e-03\n",
      " -7.9990953e-02  1.1085911e-01  3.5676155e+00]\n",
      "Converged at epoch 696\n",
      "Epoch 697, Gradients: [-0.8628311  -0.26616287 -0.17138672 -0.04431152  0.02255249  0.2998047\n",
      " -0.45212746]\n",
      " Loss: 2548.9697265625, full Parameters: [ 2.5028803e+01  2.0138140e+00  2.1099041e+00  1.5061441e-03\n",
      " -7.9614460e-02  1.0839696e-01  3.6325567e+00]\n",
      "FINAL STATE: Epoch 697, Gradients: [-0.8628311  -0.26616287 -0.17138672 -0.04431152  0.02255249  0.2998047\n",
      " -0.45212746]\n",
      " Loss: 2548.9697265625, full Parameters: [ 2.5028803e+01  2.0138140e+00  2.1099041e+00  1.5061441e-03\n",
      " -7.9614460e-02  1.0839696e-01  3.6325567e+00]\n",
      "Training full likelihood complete.\n"
     ]
    }
   ],
   "source": [
    "params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "params = torch.tensor(params, requires_grad=True)\n",
    "\n",
    "instance = kernels.model_fitting(\n",
    "    smooth=0.5,\n",
    "    input_map=analysis_data_map,\n",
    "    aggregated_data=aggregated_data,\n",
    "    nns_map=nns_map,\n",
    "    mm_cond_number=mm_cond_number\n",
    ")\n",
    "\n",
    "# optimizer = optim.Adam([params], lr=0.01)  # For Adam\n",
    "optimizer = instance.optimizer_fun( params, lr=0.001, betas=(0.9, 0.99), eps=1e-8)    \n",
    "instance.run_full(params, optimizer, epochs=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization vecchia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Gradients: [ -1.6359289  -2.443027   -1.6558454  -4.108972  -14.154725  320.9304\n",
      "   5.662271 ]\n",
      " Loss: 2711.34814453125, Parameters: [ 2.442e+01  1.920e+00  1.920e+00  1.000e-03 -4.500e-02  2.370e-01\n",
      "  3.340e+00]\n",
      "Epoch 101, Gradients: [-3.3657918  -0.401838   -0.08449596  0.57027197  0.6423551   4.046658\n",
      " -5.713063  ]\n",
      " Loss: 2667.8994140625, Parameters: [25.402588    2.8637505   2.692839    0.10317959 -0.02825812  0.07111356\n",
      "  4.0243974 ]\n",
      "Epoch 201, Gradients: [-2.5897667   0.04656863  0.18996137 -0.7142588  -0.55480295 -0.16142726\n",
      "  0.3011552 ]\n",
      " Loss: 2661.966796875, Parameters: [ 2.6416969e+01  3.6419423e+00  3.2790620e+00  1.6556214e-01\n",
      " -2.6265675e-02 -5.8514145e-03  5.1216049e+00]\n",
      "Epoch 301, Gradients: [-2.4217558  -0.01603395  0.03121904  0.59986     0.29007858  0.2992233\n",
      "  0.04264249]\n",
      " Loss: 2659.416748046875, Parameters: [ 2.7421131e+01  3.5703635e+00  3.2777872e+00  1.7882232e-01\n",
      " -2.6097119e-02  2.6434304e-03  4.9496832e+00]\n",
      "Epoch 401, Gradients: [-2.2370813e+00  1.9012690e-03 -1.5897214e-02 -7.3483640e-01\n",
      "  1.1398555e+00 -3.9103076e-02 -3.2070801e-02]\n",
      " Loss: 2657.078369140625, Parameters: [ 2.8425072e+01  3.5813193e+00  3.3257673e+00  1.8328580e-01\n",
      " -2.5916990e-02 -3.4732622e-04  4.8616967e+00]\n",
      "Epoch 501, Gradients: [-2.0723207  -0.02020568  0.02552626  0.910083   -0.44359332 -0.2022377\n",
      "  0.05108117]\n",
      " Loss: 2654.9169921875, Parameters: [ 2.9428978e+01  3.6002333e+00  3.3813336e+00  1.8837000e-01\n",
      " -2.7221492e-02 -6.0052704e-04  4.7845006e+00]\n",
      "Epoch 601, Gradients: [-1.9175332   0.03418767 -0.03290242 -1.4604852   0.11076576  0.14412238\n",
      " -0.07143649]\n",
      " Loss: 2652.91796875, Parameters: [ 3.0432850e+01  3.6260834e+00  3.4352937e+00  1.9044372e-01\n",
      " -2.7322166e-02  4.6426331e-04  4.7115598e+00]\n",
      "Epoch 701, Gradients: [-1.7774423   0.03225672 -0.03537774 -0.9485266  -0.6540529  -0.26165727\n",
      " -0.04528993]\n",
      " Loss: 2651.063720703125, Parameters: [ 3.1436686e+01  3.6537328e+00  3.4922788e+00  1.9370507e-01\n",
      " -2.8117776e-02 -5.2978925e-04  4.6453395e+00]\n",
      "Epoch 801, Gradients: [-1.6496968  -0.02044457  0.01430237  1.1612439   0.53334635 -0.33799833\n",
      "  0.05061819]\n",
      " Loss: 2649.343017578125, Parameters: [ 3.2440414e+01  3.6840506e+00  3.5541883e+00  1.9753094e-01\n",
      " -2.7222510e-02 -4.2467014e-04  4.5857358e+00]\n",
      "Epoch 901, Gradients: [-1.5318928  -0.02701139  0.01646692  0.9255717   0.55946165 -0.21726921\n",
      "  0.04576959]\n",
      " Loss: 2647.744873046875, Parameters: [ 3.3444065e+01  3.7182360e+00  3.6157470e+00  1.9955915e-01\n",
      " -2.7450146e-02 -2.2462622e-04  4.5293012e+00]\n",
      "Epoch 1001, Gradients: [-1.4246099  -0.01590598  0.00885177  0.63399357  0.3591131   0.21735308\n",
      "  0.03037983]\n",
      " Loss: 2646.260986328125, Parameters: [ 3.4447712e+01  3.7564855e+00  3.6783395e+00  2.0129128e-01\n",
      " -2.7855657e-02  1.8917985e-04  4.4765825e+00]\n",
      "Epoch 1101, Gradients: [-1.3261977   0.00898385 -0.01315153 -0.18276475  0.13360871  0.6410347\n",
      " -0.00651208]\n",
      " Loss: 2644.882080078125, Parameters: [ 3.5451359e+01  3.7971413e+00  3.7412250e+00  2.0255652e-01\n",
      " -2.8323438e-02  4.8919546e-04  4.4267931e+00]\n",
      "Epoch 1201, Gradients: [-1.2356203   0.00811177 -0.01496232 -0.28679958 -0.60562056  0.49673384\n",
      " -0.002323  ]\n",
      " Loss: 2643.600830078125, Parameters: [ 3.6454956e+01  3.8391259e+00  3.8063018e+00  2.0392151e-01\n",
      " -2.9109996e-02  3.2248738e-04  4.3811259e+00]\n",
      "Epoch 1301, Gradients: [-1.1531898e+00 -4.9338937e-03  9.2172623e-04  3.6367276e-01\n",
      " -5.9121042e-01  7.5302404e-01  1.6942620e-02]\n",
      " Loss: 2642.39697265625, Parameters: [ 3.7458504e+01  3.8827853e+00  3.8733366e+00  2.0554663e-01\n",
      " -2.9216964e-02  4.1060185e-04  4.3379607e+00]\n",
      "Epoch 1401, Gradients: [-1.0768769e+00 -5.8364868e-04 -4.5836568e-03 -1.9859204e-01\n",
      "  7.1623468e-01  9.6778625e-01 -5.1905513e-03]\n",
      " Loss: 2641.282470703125, Parameters: [ 3.8461910e+01  3.9282663e+00  3.9409201e+00  2.0656936e-01\n",
      " -2.8402220e-02  4.7076325e-04  4.2974954e+00]\n",
      "Epoch 1501, Gradients: [-1.0072668  -0.01220322  0.01154655  0.72392255 -1.2794945  -1.2940633\n",
      "  0.02960363]\n",
      " Loss: 2640.2353515625, Parameters: [ 3.9465206e+01  3.9758494e+00  4.0094957e+00  2.0785546e-01\n",
      " -3.0080607e-02 -5.3686870e-04  4.2596583e+00]\n",
      "Epoch 1601, Gradients: [-0.94308794  0.01397103 -0.01062614 -0.27614084  0.5154649   1.1732042\n",
      " -0.01895475]\n",
      " Loss: 2639.25634765625, Parameters: [ 4.0468475e+01  4.0253334e+00  4.0780230e+00  2.0854901e-01\n",
      " -2.8890504e-02  4.4707980e-04  4.2233253e+00]\n",
      "Epoch 1701, Gradients: [-0.8833996  -0.00431424  0.0043391   0.59295464  0.21958517 -0.709503\n",
      "  0.01209968]\n",
      " Loss: 2638.3408203125, Parameters: [ 4.1471741e+01  4.0750837e+00  4.1486354e+00  2.0973676e-01\n",
      " -2.9196659e-02 -2.3628690e-04  4.1899643e+00]\n",
      "Epoch 1801, Gradients: [-8.2762331e-01 -2.7596951e-04 -7.7885389e-03 -2.2108498e-01\n",
      "  1.2484300e+00 -3.1338805e-01 -3.6878288e-03]\n",
      " Loss: 2637.484375, Parameters: [ 4.2474987e+01  4.1260719e+00  4.2193761e+00  2.1022917e-01\n",
      " -2.8571039e-02 -9.5896074e-05  4.1588721e+00]\n",
      "Epoch 1901, Gradients: [-0.7769958  -0.02211541  0.01327252  1.2120421  -0.04098698 -1.1320223\n",
      "  0.04056701]\n",
      " Loss: 2636.677490234375, Parameters: [ 4.3478195e+01  4.1784444e+00  4.2913556e+00  2.1139944e-01\n",
      " -2.9624259e-02 -3.0374405e-04  4.1288810e+00]\n",
      "Epoch 2001, Gradients: [-0.7306371   0.01594287 -0.01450491 -1.1388044  -0.21289183 -0.18216212\n",
      " -0.02938974]\n",
      " Loss: 2635.91943359375, Parameters: [ 4.4481377e+01  4.2322502e+00  4.3627653e+00  2.1109112e-01\n",
      " -3.0096088e-02 -4.6631787e-05  4.1000385e+00]\n",
      "Epoch 2101, Gradients: [-0.68648785 -0.01292717  0.00431019  0.7324958   0.12986988  1.640889\n",
      "  0.02676603]\n",
      " Loss: 2635.2080078125, Parameters: [ 4.5484467e+01  4.2871075e+00  4.4365144e+00  2.1241805e-01\n",
      " -2.9770976e-02  3.7085480e-04  4.0740590e+00]\n",
      "Epoch 2201, Gradients: [-0.64635754 -0.03160983  0.02375585  1.4646233  -0.20483708  2.698588\n",
      "  0.04705754]\n",
      " Loss: 2634.5419921875, Parameters: [ 4.6487488e+01  4.3418369e+00  4.5111918e+00  2.1317786e-01\n",
      " -3.0092113e-02  5.5296783e-04  4.0485458e+00]\n",
      "Epoch 2301, Gradients: [-6.0891831e-01 -8.4196925e-03  1.9914806e-03  3.7778729e-01\n",
      "  5.2041394e-01  2.9119554e+00  1.0749429e-02]\n",
      " Loss: 2633.91552734375, Parameters: [ 4.7490437e+01  4.3984418e+00  4.5840464e+00  2.1332185e-01\n",
      " -2.9701898e-02  5.6110078e-04  4.0241160e+00]\n",
      "Epoch 2401, Gradients: [-0.5740036  -0.02693945  0.01683471  1.4037604   0.4655776   2.494625\n",
      "  0.03971145]\n",
      " Loss: 2633.318115234375, Parameters: [ 4.8493385e+01  4.4554911e+00  4.6596937e+00  2.1413435e-01\n",
      " -2.9750437e-02  4.3764678e-04  4.0016479e+00]\n",
      "Epoch 2501, Gradients: [-0.5421301  -0.01552129  0.01135811  0.85707307 -0.6208777   2.3682916\n",
      "  0.02591074]\n",
      " Loss: 2632.760009765625, Parameters: [ 4.9496281e+01  4.5138612e+00  4.7346082e+00  2.1425766e-01\n",
      " -3.0791942e-02  3.8833535e-04  3.9797988e+00]\n",
      "Epoch 2601, Gradients: [-5.1219463e-01  1.6380548e-03 -1.3046265e-03 -1.7531230e-01\n",
      " -6.6206795e-01  3.1697085e+00 -1.2410581e-03]\n",
      " Loss: 2632.22802734375, Parameters: [ 5.0499176e+01  4.5729117e+00  4.8099127e+00  2.1425256e-01\n",
      " -3.1002667e-02  4.9078115e-04  3.9592595e+00]\n",
      "Epoch 2701, Gradients: [-0.48506117  0.03139126 -0.01863441 -1.701085   -0.23775148  1.4683398\n",
      " -0.0530816 ]\n",
      " Loss: 2631.72900390625, Parameters: [ 5.1502033e+01  4.6328692e+00  4.8856192e+00  2.1412124e-01\n",
      " -3.0872172e-02  2.1657790e-04  3.9391193e+00]\n",
      "Epoch 2801, Gradients: [-0.4569082  -0.02167499  0.00817856  1.187526    0.6978995  -3.1176927\n",
      "  0.03628412]\n",
      " Loss: 2631.2587890625, Parameters: [ 5.2504852e+01  4.6919332e+00  4.9633684e+00  2.1550162e-01\n",
      " -2.9931633e-02 -4.1442126e-04  3.9219549e+00]\n",
      "Epoch 2901, Gradients: [-0.4332917  -0.01322961  0.00826362  0.70204985 -0.55760604 -2.9987102\n",
      "  0.02124843]\n",
      " Loss: 2630.810302734375, Parameters: [ 5.3507614e+01  4.7525878e+00  5.0405192e+00  2.1553428e-01\n",
      " -3.1094380e-02 -3.7545621e-04  3.9036882e+00]\n",
      "Converged at epoch 2951\n",
      "Epoch 2952, Gradients: [-0.42093587 -0.01584715  0.00616997  0.7758982  -0.06065058 -3.7790027\n",
      "  0.0275405 ]\n",
      " Loss: 2630.593994140625, vecc Parameters: [ 5.4029030e+01  4.7844071e+00  5.0809073e+00  2.1581475e-01\n",
      " -3.0050706e-02  4.1509076e-04  3.8952506e+00]\n",
      "FINAL STATE: Epoch 2952, Gradients: [-0.42093587 -0.01584715  0.00616997  0.7758982  -0.06065058 -3.7790027\n",
      "  0.0275405 ]\n",
      " Loss: 2630.593994140625, vecc Parameters: [ 5.4029030e+01  4.7844071e+00  5.0809073e+00  2.1581475e-01\n",
      " -3.0050706e-02  4.1509076e-04  3.8952506e+00]\n",
      "Training vecchia likelihood complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "params = torch.tensor(params, requires_grad=True)\n",
    "\n",
    "instance = kernels.model_fitting(\n",
    "    smooth=0.5,\n",
    "    input_map=analysis_data_map,\n",
    "    aggregated_data=aggregated_data,\n",
    "    nns_map=nns_map,\n",
    "    mm_cond_number=mm_cond_number\n",
    ")\n",
    "# optimizer = optim.Adam([params], lr=0.01)  # For Adam\n",
    "optimizer = instance.optimizer_fun( params, lr=0.01, betas=(0.9, 0.8), eps=1e-8)    \n",
    "instance.run_vecc_local(params, optimizer, epochs=3000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
