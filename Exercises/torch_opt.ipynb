{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path in sys.path:\n",
    "#   print(path)\n",
    "\n",
    "import sys\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import concurrent\n",
    "from concurrent.futures import ThreadPoolExecutor  # Importing specific executor for clarity\n",
    "import time\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Nearest neighbor search\n",
    "import sklearn\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# Special functions and optimizations\n",
    "from scipy.special import gamma, kv  # Bessel function and gamma function\n",
    "from scipy.stats import multivariate_normal  # Simulation\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial.distance import cdist  # For space and time distance\n",
    "from scipy.spatial import distance  # Find closest spatial point\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "# Plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Type hints\n",
    "from typing import Callable, Union, Tuple\n",
    "\n",
    "# Add your custom path\n",
    "# sys.path.append(\"/cache/home/jl2815/tco\")\n",
    "\n",
    "# Custom imports\n",
    "\n",
    "from GEMS_TCO import orbitmap \n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import evaluate\n",
    "from GEMS_TCO import orderings as _orderings\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy                    # clone tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_resolution = [2,2]\n",
    "mm_cond_number = 10\n",
    "params= [20, 8.25, 5.25, 0.2, 0.5, 5]\n",
    "idx_for_datamap= [0,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the one dictionary to set spaital coordinates\n",
    "# filepath = \"C:/Users/joonw/TCO/GEMS_data/data_2023/sparse_cen_map23_01.pkl\"\n",
    "filepath = \"/Users/joonwonlee/Documents/GEMS_DATA/pickle_2023/coarse_cen_map23_01.pkl\"\n",
    "with open(filepath, 'rb') as pickle_file:\n",
    "    coarse_dict_24_1 = pickle.load(pickle_file)\n",
    "\n",
    "sample_df = coarse_dict_24_1['y23m01day01_hm02:12']\n",
    "\n",
    "sample_key = coarse_dict_24_1.get('y23m01day01_hm02:12')\n",
    "if sample_key is None:\n",
    "    print(\"Key 'y23m01day01_hm02:12' not found in the dictionary.\")\n",
    "\n",
    "# { (20,20):(5,1), (5,5):(20,40) }\n",
    "rho_lat = lat_lon_resolution[0]          \n",
    "rho_lon = lat_lon_resolution[1]\n",
    "lat_n = sample_df['Latitude'].unique()[::rho_lat]\n",
    "lon_n = sample_df['Longitude'].unique()[::rho_lon]\n",
    "\n",
    "lat_number = len(lat_n)\n",
    "lon_number = len(lon_n)\n",
    "\n",
    "# Set spatial coordinates for each dataset\n",
    "coarse_dicts = {}\n",
    "\n",
    "years = ['2024']\n",
    "for year in years:\n",
    "    for month in range(7, 8):  # Iterate over all months\n",
    "        # filepath = f\"C:/Users/joonw/TCO/GEMS_data/data_{year}/sparse_cen_map{year[2:]}_{month:02d}.pkl\"\n",
    "        filepath = f\"/Users/joonwonlee/Documents/GEMS_DATA/pickle_{year}/coarse_cen_map{year[2:]}_{month:02d}.pkl\"\n",
    "        with open(filepath, 'rb') as pickle_file:\n",
    "            loaded_map = pickle.load(pickle_file)\n",
    "            for key in loaded_map:\n",
    "                tmp_df = loaded_map[key]\n",
    "                coarse_filter = (tmp_df['Latitude'].isin(lat_n)) & (tmp_df['Longitude'].isin(lon_n))\n",
    "                coarse_dicts[f\"{year}_{month:02d}_{key}\"] = tmp_df[coarse_filter].reset_index(drop=True)\n",
    "\n",
    "\n",
    "key_idx = sorted(coarse_dicts)\n",
    "if not key_idx:\n",
    "    raise ValueError(\"coarse_dicts is empty\")\n",
    "\n",
    "# extract first hour data because all data shares the same spatial grid\n",
    "data_for_coord = coarse_dicts[key_idx[0]]\n",
    "x1 = data_for_coord['Longitude'].values\n",
    "y1 = data_for_coord['Latitude'].values \n",
    "coords1 = np.stack((x1, y1), axis=-1)\n",
    "\n",
    "\n",
    "# instance = orbitmap.MakeOrbitdata(data_for_coord, lat_s=5, lat_e=10, lon_s=110, lon_e=120)\n",
    "# s_dist = cdist(coords1, coords1, 'euclidean')\n",
    "# ord_mm, _ = instance.maxmin_naive(s_dist, 0)\n",
    "\n",
    "ord_mm = _orderings.maxmin_cpp(coords1)\n",
    "data_for_coord = data_for_coord.iloc[ord_mm].reset_index(drop=True)\n",
    "coords1_reordered = np.stack((data_for_coord['Longitude'].values, data_for_coord['Latitude'].values), axis=-1)\n",
    "# nns_map = instance.find_nns_naive(locs=coords1_reordered, dist_fun='euclidean', max_nn=mm_cond_number)\n",
    "nns_map=_orderings.find_nns_l2(locs= coords1_reordered  ,max_nn = mm_cond_number)\n",
    "\n",
    "\n",
    "analysis_data_map = {}\n",
    "for i in range(idx_for_datamap[0],idx_for_datamap[1]):\n",
    "    tmp = coarse_dicts[key_idx[i]].copy()\n",
    "    tmp['Hours_elapsed'] = np.round(tmp['Hours_elapsed']-477700)\n",
    "\n",
    "    tmp = tmp.iloc[ord_mm, :4].to_numpy()\n",
    "    tmp = torch.from_numpy(tmp).float()  # Convert NumPy to Tensor\n",
    "    # tmp = tmp.clone().detach().requires_grad_(True)  # Enable gradients\n",
    "    \n",
    "    analysis_data_map[key_idx[i]] = tmp\n",
    "\n",
    "aggregated_data = pd.DataFrame()\n",
    "for i in range(idx_for_datamap[0],idx_for_datamap[1]):\n",
    "    tmp = coarse_dicts[key_idx[i]].copy()\n",
    "    tmp['Hours_elapsed'] = np.round(tmp['Hours_elapsed']-477700)\n",
    "    tmp = tmp.iloc[ord_mm].reset_index(drop=True)  \n",
    "    aggregated_data = pd.concat((aggregated_data, tmp), axis=0)\n",
    "\n",
    "aggregated_data = aggregated_data.iloc[:, :4].to_numpy()\n",
    "\n",
    "aggregated_data = torch.from_numpy(aggregated_data).float()  # Convert NumPy to Tensor\n",
    "# aggregated_np = aggregated_np.clone().detach().requires_grad_(True)  # Enable gradients\n",
    "\n",
    "\n",
    "instance = kernels.likelihood_function(smooth=0.5, input_map=analysis_data_map, aggregated_data=aggregated_data,nns_map=nns_map, mm_cond_number=mm_cond_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization full likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40000, 4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Gradients: [   3.9227905  221.56133    -16.73999   -134.6278     387.07025\n",
      " 2210.11       755.78564  ]\n",
      " Loss: 51193.6875, Parameters: [ 2.442e+01  1.920e+00  1.920e+00  1.000e-03 -4.500e-02  2.370e-01\n",
      "  3.340e+00]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# optimizer = optim.Adam([params], lr=0.01)  # For Adam\u001b[39;00m\n\u001b[32m     13\u001b[39m optimizer = instance.optimizer_fun( params, lr=\u001b[32m0.01\u001b[39m, betas=(\u001b[32m0.9\u001b[39m, \u001b[32m0.8\u001b[39m), eps=\u001b[32m1e-8\u001b[39m)    \n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43minstance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/kernels.py:442\u001b[39m, in \u001b[36mmodel_fitting.run_full\u001b[39m\u001b[34m(self, params, optimizer, epochs)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):  \u001b[38;5;66;03m# Number of epochs\u001b[39;00m\n\u001b[32m    440\u001b[39m     optimizer.zero_grad()  \u001b[38;5;66;03m# Zero the gradients \u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_full_nll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m     loss.backward()  \u001b[38;5;66;03m# Backpropagate the loss\u001b[39;00m\n\u001b[32m    445\u001b[39m     \u001b[38;5;66;03m# Print gradients and parameters every 10th epoch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/kernels.py:428\u001b[39m, in \u001b[36mmodel_fitting.compute_full_nll\u001b[39m\u001b[34m(self, params)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_full_nll\u001b[39m(\u001b[38;5;28mself\u001b[39m,params):\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m     full_nll = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfull_likelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_np\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maggregated_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maggregated_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcovariance_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmatern_cov_anisotropy_v05\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m full_nll\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/kernels.py:144\u001b[39m, in \u001b[36mlikelihood_function.full_likelihood\u001b[39m\u001b[34m(self, params, input_np, y, covariance_function)\u001b[39m\n\u001b[32m    141\u001b[39m y_mu = y_arr - mu\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# Compute the quadratic form\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m quad_form = torch.matmul(y_mu, \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcov_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_mu\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    146\u001b[39m \u001b[38;5;66;03m# Compute the negative log likelihood\u001b[39;00m\n\u001b[32m    147\u001b[39m neg_log_lik = \u001b[32m0.5\u001b[39m * (log_det + quad_form)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "params = torch.tensor(params, requires_grad=True)\n",
    "\n",
    "instance = kernels.model_fitting(\n",
    "    smooth=0.5,\n",
    "    input_map=analysis_data_map,\n",
    "    aggregated_data=aggregated_data,\n",
    "    nns_map=nns_map,\n",
    "    mm_cond_number=mm_cond_number\n",
    ")\n",
    "\n",
    "# optimizer = optim.Adam([params], lr=0.01)  # For Adam\n",
    "optimizer = instance.optimizer_fun( params, lr=0.01, betas=(0.9, 0.8), eps=1e-8)    \n",
    "instance.run_full(params, optimizer, epochs=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization vecchia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Gradients: [ -1.6359289  -2.443027   -1.6558454  -4.108972  -14.154725  320.9304\n",
      "   5.662271 ]\n",
      " Loss: 2711.34814453125, Parameters: [ 2.442e+01  1.920e+00  1.920e+00  1.000e-03 -4.500e-02  2.370e-01\n",
      "  3.340e+00]\n",
      "Epoch 101, Gradients: [-3.3657918  -0.401838   -0.08449596  0.57027197  0.6423551   4.046658\n",
      " -5.713063  ]\n",
      " Loss: 2667.8994140625, Parameters: [25.402588    2.8637505   2.692839    0.10317959 -0.02825812  0.07111356\n",
      "  4.0243974 ]\n",
      "Epoch 201, Gradients: [-2.5897667   0.04656863  0.18996137 -0.7142588  -0.55480295 -0.16142726\n",
      "  0.3011552 ]\n",
      " Loss: 2661.966796875, Parameters: [ 2.6416969e+01  3.6419423e+00  3.2790620e+00  1.6556214e-01\n",
      " -2.6265675e-02 -5.8514145e-03  5.1216049e+00]\n",
      "Epoch 301, Gradients: [-2.4217558  -0.01603395  0.03121904  0.59986     0.29007858  0.2992233\n",
      "  0.04264249]\n",
      " Loss: 2659.416748046875, Parameters: [ 2.7421131e+01  3.5703635e+00  3.2777872e+00  1.7882232e-01\n",
      " -2.6097119e-02  2.6434304e-03  4.9496832e+00]\n",
      "Epoch 401, Gradients: [-2.2370813e+00  1.9012690e-03 -1.5897214e-02 -7.3483640e-01\n",
      "  1.1398555e+00 -3.9103076e-02 -3.2070801e-02]\n",
      " Loss: 2657.078369140625, Parameters: [ 2.8425072e+01  3.5813193e+00  3.3257673e+00  1.8328580e-01\n",
      " -2.5916990e-02 -3.4732622e-04  4.8616967e+00]\n",
      "Epoch 501, Gradients: [-2.0723207  -0.02020568  0.02552626  0.910083   -0.44359332 -0.2022377\n",
      "  0.05108117]\n",
      " Loss: 2654.9169921875, Parameters: [ 2.9428978e+01  3.6002333e+00  3.3813336e+00  1.8837000e-01\n",
      " -2.7221492e-02 -6.0052704e-04  4.7845006e+00]\n",
      "Epoch 601, Gradients: [-1.9175332   0.03418767 -0.03290242 -1.4604852   0.11076576  0.14412238\n",
      " -0.07143649]\n",
      " Loss: 2652.91796875, Parameters: [ 3.0432850e+01  3.6260834e+00  3.4352937e+00  1.9044372e-01\n",
      " -2.7322166e-02  4.6426331e-04  4.7115598e+00]\n",
      "Epoch 701, Gradients: [-1.7774423   0.03225672 -0.03537774 -0.9485266  -0.6540529  -0.26165727\n",
      " -0.04528993]\n",
      " Loss: 2651.063720703125, Parameters: [ 3.1436686e+01  3.6537328e+00  3.4922788e+00  1.9370507e-01\n",
      " -2.8117776e-02 -5.2978925e-04  4.6453395e+00]\n",
      "Epoch 801, Gradients: [-1.6496968  -0.02044457  0.01430237  1.1612439   0.53334635 -0.33799833\n",
      "  0.05061819]\n",
      " Loss: 2649.343017578125, Parameters: [ 3.2440414e+01  3.6840506e+00  3.5541883e+00  1.9753094e-01\n",
      " -2.7222510e-02 -4.2467014e-04  4.5857358e+00]\n",
      "Epoch 901, Gradients: [-1.5318928  -0.02701139  0.01646692  0.9255717   0.55946165 -0.21726921\n",
      "  0.04576959]\n",
      " Loss: 2647.744873046875, Parameters: [ 3.3444065e+01  3.7182360e+00  3.6157470e+00  1.9955915e-01\n",
      " -2.7450146e-02 -2.2462622e-04  4.5293012e+00]\n",
      "Epoch 1001, Gradients: [-1.4246099  -0.01590598  0.00885177  0.63399357  0.3591131   0.21735308\n",
      "  0.03037983]\n",
      " Loss: 2646.260986328125, Parameters: [ 3.4447712e+01  3.7564855e+00  3.6783395e+00  2.0129128e-01\n",
      " -2.7855657e-02  1.8917985e-04  4.4765825e+00]\n",
      "Epoch 1101, Gradients: [-1.3261977   0.00898385 -0.01315153 -0.18276475  0.13360871  0.6410347\n",
      " -0.00651208]\n",
      " Loss: 2644.882080078125, Parameters: [ 3.5451359e+01  3.7971413e+00  3.7412250e+00  2.0255652e-01\n",
      " -2.8323438e-02  4.8919546e-04  4.4267931e+00]\n",
      "Epoch 1201, Gradients: [-1.2356203   0.00811177 -0.01496232 -0.28679958 -0.60562056  0.49673384\n",
      " -0.002323  ]\n",
      " Loss: 2643.600830078125, Parameters: [ 3.6454956e+01  3.8391259e+00  3.8063018e+00  2.0392151e-01\n",
      " -2.9109996e-02  3.2248738e-04  4.3811259e+00]\n",
      "Epoch 1301, Gradients: [-1.1531898e+00 -4.9338937e-03  9.2172623e-04  3.6367276e-01\n",
      " -5.9121042e-01  7.5302404e-01  1.6942620e-02]\n",
      " Loss: 2642.39697265625, Parameters: [ 3.7458504e+01  3.8827853e+00  3.8733366e+00  2.0554663e-01\n",
      " -2.9216964e-02  4.1060185e-04  4.3379607e+00]\n",
      "Epoch 1401, Gradients: [-1.0768769e+00 -5.8364868e-04 -4.5836568e-03 -1.9859204e-01\n",
      "  7.1623468e-01  9.6778625e-01 -5.1905513e-03]\n",
      " Loss: 2641.282470703125, Parameters: [ 3.8461910e+01  3.9282663e+00  3.9409201e+00  2.0656936e-01\n",
      " -2.8402220e-02  4.7076325e-04  4.2974954e+00]\n",
      "Epoch 1501, Gradients: [-1.0072668  -0.01220322  0.01154655  0.72392255 -1.2794945  -1.2940633\n",
      "  0.02960363]\n",
      " Loss: 2640.2353515625, Parameters: [ 3.9465206e+01  3.9758494e+00  4.0094957e+00  2.0785546e-01\n",
      " -3.0080607e-02 -5.3686870e-04  4.2596583e+00]\n",
      "Epoch 1601, Gradients: [-0.94308794  0.01397103 -0.01062614 -0.27614084  0.5154649   1.1732042\n",
      " -0.01895475]\n",
      " Loss: 2639.25634765625, Parameters: [ 4.0468475e+01  4.0253334e+00  4.0780230e+00  2.0854901e-01\n",
      " -2.8890504e-02  4.4707980e-04  4.2233253e+00]\n",
      "Epoch 1701, Gradients: [-0.8833996  -0.00431424  0.0043391   0.59295464  0.21958517 -0.709503\n",
      "  0.01209968]\n",
      " Loss: 2638.3408203125, Parameters: [ 4.1471741e+01  4.0750837e+00  4.1486354e+00  2.0973676e-01\n",
      " -2.9196659e-02 -2.3628690e-04  4.1899643e+00]\n",
      "Epoch 1801, Gradients: [-8.2762331e-01 -2.7596951e-04 -7.7885389e-03 -2.2108498e-01\n",
      "  1.2484300e+00 -3.1338805e-01 -3.6878288e-03]\n",
      " Loss: 2637.484375, Parameters: [ 4.2474987e+01  4.1260719e+00  4.2193761e+00  2.1022917e-01\n",
      " -2.8571039e-02 -9.5896074e-05  4.1588721e+00]\n",
      "Epoch 1901, Gradients: [-0.7769958  -0.02211541  0.01327252  1.2120421  -0.04098698 -1.1320223\n",
      "  0.04056701]\n",
      " Loss: 2636.677490234375, Parameters: [ 4.3478195e+01  4.1784444e+00  4.2913556e+00  2.1139944e-01\n",
      " -2.9624259e-02 -3.0374405e-04  4.1288810e+00]\n",
      "Epoch 2001, Gradients: [-0.7306371   0.01594287 -0.01450491 -1.1388044  -0.21289183 -0.18216212\n",
      " -0.02938974]\n",
      " Loss: 2635.91943359375, Parameters: [ 4.4481377e+01  4.2322502e+00  4.3627653e+00  2.1109112e-01\n",
      " -3.0096088e-02 -4.6631787e-05  4.1000385e+00]\n",
      "Epoch 2101, Gradients: [-0.68648785 -0.01292717  0.00431019  0.7324958   0.12986988  1.640889\n",
      "  0.02676603]\n",
      " Loss: 2635.2080078125, Parameters: [ 4.5484467e+01  4.2871075e+00  4.4365144e+00  2.1241805e-01\n",
      " -2.9770976e-02  3.7085480e-04  4.0740590e+00]\n",
      "Epoch 2201, Gradients: [-0.64635754 -0.03160983  0.02375585  1.4646233  -0.20483708  2.698588\n",
      "  0.04705754]\n",
      " Loss: 2634.5419921875, Parameters: [ 4.6487488e+01  4.3418369e+00  4.5111918e+00  2.1317786e-01\n",
      " -3.0092113e-02  5.5296783e-04  4.0485458e+00]\n",
      "Epoch 2301, Gradients: [-6.0891831e-01 -8.4196925e-03  1.9914806e-03  3.7778729e-01\n",
      "  5.2041394e-01  2.9119554e+00  1.0749429e-02]\n",
      " Loss: 2633.91552734375, Parameters: [ 4.7490437e+01  4.3984418e+00  4.5840464e+00  2.1332185e-01\n",
      " -2.9701898e-02  5.6110078e-04  4.0241160e+00]\n",
      "Epoch 2401, Gradients: [-0.5740036  -0.02693945  0.01683471  1.4037604   0.4655776   2.494625\n",
      "  0.03971145]\n",
      " Loss: 2633.318115234375, Parameters: [ 4.8493385e+01  4.4554911e+00  4.6596937e+00  2.1413435e-01\n",
      " -2.9750437e-02  4.3764678e-04  4.0016479e+00]\n",
      "Epoch 2501, Gradients: [-0.5421301  -0.01552129  0.01135811  0.85707307 -0.6208777   2.3682916\n",
      "  0.02591074]\n",
      " Loss: 2632.760009765625, Parameters: [ 4.9496281e+01  4.5138612e+00  4.7346082e+00  2.1425766e-01\n",
      " -3.0791942e-02  3.8833535e-04  3.9797988e+00]\n",
      "Epoch 2601, Gradients: [-5.1219463e-01  1.6380548e-03 -1.3046265e-03 -1.7531230e-01\n",
      " -6.6206795e-01  3.1697085e+00 -1.2410581e-03]\n",
      " Loss: 2632.22802734375, Parameters: [ 5.0499176e+01  4.5729117e+00  4.8099127e+00  2.1425256e-01\n",
      " -3.1002667e-02  4.9078115e-04  3.9592595e+00]\n",
      "Epoch 2701, Gradients: [-0.48506117  0.03139126 -0.01863441 -1.701085   -0.23775148  1.4683398\n",
      " -0.0530816 ]\n",
      " Loss: 2631.72900390625, Parameters: [ 5.1502033e+01  4.6328692e+00  4.8856192e+00  2.1412124e-01\n",
      " -3.0872172e-02  2.1657790e-04  3.9391193e+00]\n",
      "Epoch 2801, Gradients: [-0.4569082  -0.02167499  0.00817856  1.187526    0.6978995  -3.1176927\n",
      "  0.03628412]\n",
      " Loss: 2631.2587890625, Parameters: [ 5.2504852e+01  4.6919332e+00  4.9633684e+00  2.1550162e-01\n",
      " -2.9931633e-02 -4.1442126e-04  3.9219549e+00]\n",
      "Epoch 2901, Gradients: [-0.4332917  -0.01322961  0.00826362  0.70204985 -0.55760604 -2.9987102\n",
      "  0.02124843]\n",
      " Loss: 2630.810302734375, Parameters: [ 5.3507614e+01  4.7525878e+00  5.0405192e+00  2.1553428e-01\n",
      " -3.1094380e-02 -3.7545621e-04  3.9036882e+00]\n",
      "Converged at epoch 2951\n",
      "Epoch 2952, Gradients: [-0.42093587 -0.01584715  0.00616997  0.7758982  -0.06065058 -3.7790027\n",
      "  0.0275405 ]\n",
      " Loss: 2630.593994140625, vecc Parameters: [ 5.4029030e+01  4.7844071e+00  5.0809073e+00  2.1581475e-01\n",
      " -3.0050706e-02  4.1509076e-04  3.8952506e+00]\n",
      "FINAL STATE: Epoch 2952, Gradients: [-0.42093587 -0.01584715  0.00616997  0.7758982  -0.06065058 -3.7790027\n",
      "  0.0275405 ]\n",
      " Loss: 2630.593994140625, vecc Parameters: [ 5.4029030e+01  4.7844071e+00  5.0809073e+00  2.1581475e-01\n",
      " -3.0050706e-02  4.1509076e-04  3.8952506e+00]\n",
      "Training vecchia likelihood complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "params = torch.tensor(params, requires_grad=True)\n",
    "\n",
    "instance = kernels.model_fitting(\n",
    "    smooth=0.5,\n",
    "    input_map=analysis_data_map,\n",
    "    aggregated_data=aggregated_data,\n",
    "    nns_map=nns_map,\n",
    "    mm_cond_number=mm_cond_number\n",
    ")\n",
    "# optimizer = optim.Adam([params], lr=0.01)  # For Adam\n",
    "optimizer = instance.optimizer_fun( params, lr=0.01, betas=(0.9, 0.8), eps=1e-8)    \n",
    "instance.run_vecc_local(params, optimizer, epochs=3000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
