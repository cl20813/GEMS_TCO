{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, install graphviz above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "# sys.path.append(gems_tco_path)\n",
    "\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings\n",
    "from GEMS_TCO import load_data_local_computer\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import copy                    # clone tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_resolution = [10,10]\n",
    "mm_cond_number = 20\n",
    "years = ['2024']\n",
    "month_range =[7,8]\n",
    "idx_for_datamap= [0,8]\n",
    "\n",
    "params= [20, 8.25, 5.25, 0.2, 0.5, 5]\n",
    "\n",
    "instance = load_data_local_computer()\n",
    "map, ord_mm, nns_map= instance.load_mm20k_data_bymonthyear( lat_lon_resolution= lat_lon_resolution, mm_cond_number=mm_cond_number,years_=years, months_=month_range)\n",
    "\n",
    "analysis_data_map, aggregated_data = instance.load_working_data_byday( map, ord_mm, nns_map, idx_for_datamap=[0,8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2675.2982, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2664.4289, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define your initial parameters\n",
    "params = [21.8, 1.09, 1.17, 0.2, .2, 0.5, 1]\n",
    "params = torch.tensor(params, requires_grad=True)\n",
    "\n",
    "torch_smooth = torch.tensor(0.5, dtype=torch.float32)\n",
    "\n",
    "instance = kernels.likelihood_function(smooth=torch_smooth , input_map=analysis_data_map,aggregated_data=aggregated_data, nns_map=nns_map, mm_cond_number=mm_cond_number)\n",
    "\n",
    "out0 = instance.full_likelihood(params, aggregated_data[:,:4],aggregated_data[:,2], instance.matern_cov_anisotropy_v05)\n",
    "print(out0)\n",
    "\n",
    "out0 = instance.vecchia_extrapolate(params, instance.matern_cov_anisotropy_v05)\n",
    "print(out0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma log_det:52.751060485839844\n",
      "sigma quad_form:-279.7126770019531\n",
      "sigma quad_form:-113.48080444335938\n",
      "\n",
      "range log_det:-126.85018920898438\n",
      "range quad_form:672.7844848632812\n",
      "range quad_form:272.9671630859375\n",
      "\n",
      "nugget log_det:1089.956298828125\n",
      "nugget quad_form:-6223.43359375\n",
      "nugget quad_form:-2566.73876953125\n",
      "\n",
      "beta log_det:599.5950927734375\n",
      "beta quad_form:-1744.1077880859375\n",
      "beta quad_form:-572.25634765625\n",
      "\n",
      "advec log_det:61.72618865966797\n",
      "advec quad_form:1965.310302734375\n",
      "advec quad_form:1013.5182495117188\n"
     ]
    }
   ],
   "source": [
    "def matern_cov_yx( params: torch.Tensor, y: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "    sigmasq, range_, range_lon, advec, beta, nugget = params\n",
    "    if y is None or x is None:\n",
    "        raise ValueError(\"Both y and x_df must be provided.\")\n",
    "\n",
    "    x1 = x[:, 0]\n",
    "    y1 = x[:, 1]\n",
    "    t1 = x[:, 3]\n",
    "\n",
    "    x2 = y[:, 0]\n",
    "    y2 = y[:, 1]\n",
    "    t2 = y[:, 3]\n",
    "\n",
    "    spat_coord1 = torch.stack((x1 - advec * t1, y1 - advec * t1), dim=-1)\n",
    "    spat_coord2 = torch.stack((x2 - advec * t2, y2 - advec * t2), dim=-1)\n",
    "\n",
    "    coords1 = torch.cat((spat_coord1, (beta * t1).reshape(-1, 1)), dim=1)\n",
    "    coords2 = torch.cat((spat_coord2, (beta * t2).reshape(-1, 1)), dim=1)\n",
    "\n",
    "  \n",
    "    def custom_distance_matrix(U, V):\n",
    "        diff = U[:, :2].unsqueeze(1) - V[:, :2].unsqueeze(0)\n",
    "        spatial_diff = torch.norm(diff, dim=2)\n",
    "        temporal_diff = torch.abs(U[:, 2].unsqueeze(1) - V[:, 2].unsqueeze(0))\n",
    "        return torch.sqrt(spatial_diff**2 + temporal_diff**2)\n",
    "\n",
    "    distance = custom_distance_matrix(coords1, coords2)\n",
    "    out = torch.zeros_like(distance)\n",
    "\n",
    "    non_zero_indices = distance != 0\n",
    "    if torch.any(non_zero_indices):\n",
    "        out[non_zero_indices] = sigmasq * torch.exp(-distance[non_zero_indices]/range_)\n",
    "    out[~non_zero_indices] = sigmasq\n",
    "\n",
    "    # Add a small jitter term to the diagonal for numerical stability\n",
    "    out += torch.eye(out.shape[0]) * nugget\n",
    "    return out\n",
    "\n",
    "# to track gradient, torch sqrt in distance is moved \n",
    "def matern_cov_yx2( params: torch.Tensor, y: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "    sigmasq, range_, range_lon, advec, beta, nugget = params\n",
    "    if y is None or x is None:\n",
    "        raise ValueError(\"Both y and x_df must be provided.\")\n",
    "\n",
    "    x1 = x[:, 0]\n",
    "    y1 = x[:, 1]\n",
    "    t1 = x[:, 3]\n",
    "\n",
    "    x2 = y[:, 0]\n",
    "    y2 = y[:, 1]\n",
    "    t2 = y[:, 3]\n",
    "\n",
    "    spat_coord1 = torch.stack((x1 - advec * t1, y1 - advec * t1), dim=-1)\n",
    "    spat_coord2 = torch.stack((x2 - advec * t2, y2 - advec * t2), dim=-1)\n",
    "\n",
    "    coords1 = torch.cat((spat_coord1, (beta * t1).reshape(-1, 1)), dim=1)\n",
    "    coords2 = torch.cat((spat_coord2, (beta * t2).reshape(-1, 1)), dim=1)\n",
    "\n",
    "    U= coords1\n",
    "    V = coords2 \n",
    " \n",
    "    diff = U[:, :2].unsqueeze(1) - V[:, :2].unsqueeze(0)\n",
    "    spatial_diff = torch.norm(diff, dim=2)\n",
    "    temporal_diff = torch.abs(U[:, 2].unsqueeze(1) - V[:, 2].unsqueeze(0))\n",
    "    distance = (spatial_diff**2 + temporal_diff**2)\n",
    "\n",
    "    out = torch.zeros_like(distance)\n",
    "\n",
    "    non_zero_indices = distance != 0\n",
    "    if torch.any(non_zero_indices):\n",
    "        out[non_zero_indices] = sigmasq * torch.exp(- torch.sqrt(distance[non_zero_indices])/range_)\n",
    "    out[~non_zero_indices] = sigmasq\n",
    "\n",
    "    # Add a small jitter term to the diagonal for numerical stability\n",
    "    out += torch.eye(out.shape[0]) * nugget\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def full_likelihood( params: torch.Tensor, input_np: torch.Tensor, y: torch.Tensor, covariance_function) -> torch.Tensor:\n",
    "    input_arr = input_np[:, :4]\n",
    "    y_arr = y\n",
    "\n",
    "    # Compute the covariance matrix\n",
    "    cov_matrix = covariance_function(params=params, y=input_arr, x=input_arr)\n",
    "    \n",
    "    # Compute the log determinant of the covariance matrix\n",
    "    sign, log_det = torch.slogdet(cov_matrix)\n",
    "    # if sign <= 0:\n",
    "    #     raise ValueError(\"Covariance matrix is not positive definite\")\n",
    "    \n",
    "    # Extract locations\n",
    "    locs = input_arr[:, :2]\n",
    "\n",
    "    # Compute beta\n",
    "    tmp1 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, locs))\n",
    "    tmp2 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, y_arr))\n",
    "    beta = torch.linalg.solve(tmp1, tmp2)\n",
    "\n",
    "    # Compute the mean\n",
    "    mu = torch.matmul(locs, beta)\n",
    "    y_mu = y_arr - mu\n",
    "\n",
    "    # Compute the quadratic form\n",
    "    quad_form = torch.matmul(y_mu, torch.linalg.solve(cov_matrix, y_mu))\n",
    "\n",
    "    # Compute the negative log likelihood\n",
    "    neg_log_lik = 0.5 * (log_det + quad_form)\n",
    "\n",
    "    return log_det, quad_form, neg_log_lik\n",
    "params = [20, 8.25, 5.25, 0.2, 0.5, 5]\n",
    "params = torch.tensor(params, requires_grad=True)\n",
    "\n",
    "def matern_partial_sigma( params: torch.Tensor, y: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "    sigmasq, range_, range_lon, advec, beta, nugget = params\n",
    "    if y is None or x is None:\n",
    "        raise ValueError(\"Both y and x_df must be provided.\")\n",
    "\n",
    "    x1 = x[:, 0]\n",
    "    y1 = x[:, 1]\n",
    "    t1 = x[:, 3]\n",
    "\n",
    "    x2 = y[:, 0]\n",
    "    y2 = y[:, 1]\n",
    "    t2 = y[:, 3]\n",
    "\n",
    "    spat_coord1 = torch.stack((x1 - advec * t1, y1 - advec * t1), dim=-1)\n",
    "    spat_coord2 = torch.stack((x2 - advec * t2, y2 - advec * t2), dim=-1)\n",
    "\n",
    "    coords1 = torch.cat((spat_coord1, (beta * t1).reshape(-1, 1)), dim=1)\n",
    "    coords2 = torch.cat((spat_coord2, (beta * t2).reshape(-1, 1)), dim=1)\n",
    "\n",
    "    def custom_distance_matrix(U, V):\n",
    "        diff = U[:, :2].unsqueeze(1) - V[:, :2].unsqueeze(0)\n",
    "        spatial_diff = torch.norm(diff, dim=2)\n",
    "        temporal_diff = torch.abs(U[:, 2].unsqueeze(1) - V[:, 2].unsqueeze(0))\n",
    "        return torch.sqrt(spatial_diff**2 + temporal_diff**2)\n",
    "\n",
    "    distance = custom_distance_matrix(coords1, coords2)\n",
    "    out = torch.zeros_like(distance)\n",
    "\n",
    "    non_zero_indices = distance != 0\n",
    "    if torch.any(non_zero_indices):\n",
    "        out[non_zero_indices] = torch.exp(-distance[non_zero_indices]/range_)\n",
    "    out[~non_zero_indices] = 1\n",
    "\n",
    "    return out\n",
    "\n",
    "def matern_partial_range( params: torch.Tensor, y: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "    sigmasq, range_, range_lon, advec, beta, nugget = params\n",
    "    if y is None or x is None:\n",
    "        raise ValueError(\"Both y and x_df must be provided.\")\n",
    "\n",
    "    x1 = x[:, 0]\n",
    "    y1 = x[:, 1]\n",
    "    t1 = x[:, 3]\n",
    "\n",
    "    x2 = y[:, 0]\n",
    "    y2 = y[:, 1]\n",
    "    t2 = y[:, 3]\n",
    " \n",
    "    spat_coord1 = torch.stack((x1 - advec * t1, y1 - advec * t1), dim=-1)\n",
    "    spat_coord2 = torch.stack((x2 - advec * t2, y2 - advec * t2), dim=-1)\n",
    "\n",
    "    coords1 = torch.cat((spat_coord1, (beta * t1).reshape(-1, 1)), dim=1)\n",
    "    coords2 = torch.cat((spat_coord2, (beta * t2).reshape(-1, 1)), dim=1)\n",
    "\n",
    "    def custom_distance_matrix(U, V):\n",
    "        diff = U[:, :2].unsqueeze(1) - V[:, :2].unsqueeze(0)\n",
    "        spatial_diff = torch.norm(diff, dim=2)\n",
    "        temporal_diff = torch.abs(U[:, 2].unsqueeze(1) - V[:, 2].unsqueeze(0))\n",
    "        return torch.sqrt(spatial_diff**2 + temporal_diff**2)\n",
    "\n",
    "    distance = custom_distance_matrix(coords1, coords2)\n",
    "    out = torch.zeros_like(distance)\n",
    "\n",
    "    non_zero_indices = distance != 0\n",
    "    if torch.any(non_zero_indices):\n",
    "        out[non_zero_indices] = sigmasq * torch.exp(-distance[non_zero_indices] / range_)\n",
    "    out[~non_zero_indices] = sigmasq\n",
    "\n",
    "    # Compute the derivative with respect to the range parameter\n",
    "    derivative = torch.zeros_like(distance)\n",
    "    if torch.any(non_zero_indices):\n",
    "        derivative[non_zero_indices] = out[non_zero_indices] * distance[non_zero_indices] / range_**2\n",
    "\n",
    "    # Ensure the derivative on the diagonal is zero\n",
    "    derivative.fill_diagonal_(0)\n",
    "    \n",
    "    return derivative\n",
    "\n",
    "def matern_partial_nugget( params: torch.Tensor, y: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "    sigmasq, range_, range_lon, advec, beta, nugget = params\n",
    "    if y is None or x is None:\n",
    "        raise ValueError(\"Both y and x_df must be provided.\")\n",
    "\n",
    "    x1 = x[:, 0]\n",
    "    y1 = x[:, 1]\n",
    "    t1 = x[:, 3]\n",
    "\n",
    "    x2 = y[:, 0]\n",
    "    y2 = y[:, 1]\n",
    "    t2 = y[:, 3]\n",
    "\n",
    "    spat_coord1 = torch.stack((x1 - advec * t1, y1 - advec * t1), dim=-1)\n",
    "    spat_coord2 = torch.stack((x2 - advec * t2, y2 - advec * t2), dim=-1)\n",
    "\n",
    "    coords1 = torch.cat((spat_coord1, (beta * t1).reshape(-1, 1)), dim=1)\n",
    "    coords2 = torch.cat((spat_coord2, (beta * t2).reshape(-1, 1)), dim=1)\n",
    "\n",
    "    def custom_distance_matrix(U, V):\n",
    "        diff = U[:, :2].unsqueeze(1) - V[:, :2].unsqueeze(0)\n",
    "        spatial_diff = torch.norm(diff, dim=2)\n",
    "        temporal_diff = torch.abs(U[:, 2].unsqueeze(1) - V[:, 2].unsqueeze(0))\n",
    "        return torch.sqrt(spatial_diff**2 + temporal_diff**2)\n",
    "\n",
    "    distance = custom_distance_matrix(coords1, coords2)\n",
    "    out = torch.zeros_like(distance)\n",
    "\n",
    "    non_zero_indices = distance != 0\n",
    "    if torch.any(non_zero_indices):\n",
    "        out[non_zero_indices] = 0\n",
    "    out[~non_zero_indices] = 1\n",
    "\n",
    "    return out\n",
    "\n",
    "def matern_partial_beta( params: torch.Tensor, y: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "    sigmasq, range_, range_lon, advec, beta, nugget = params\n",
    "    if y is None or x is None:\n",
    "        raise ValueError(\"Both y and x_df must be provided.\")\n",
    "\n",
    "    x1 = x[:, 0]\n",
    "    y1 = x[:, 1]\n",
    "    t1 = x[:, 3]\n",
    "\n",
    "    x2 = y[:, 0]\n",
    "    y2 = y[:, 1]\n",
    "    t2 = y[:, 3]\n",
    "\n",
    "    spat_coord1 = torch.stack((x1 - advec * t1, y1 - advec * t1), dim=-1)\n",
    "    spat_coord2 = torch.stack((x2 - advec * t2, y2 - advec * t2), dim=-1)\n",
    "\n",
    "    coords1 = torch.cat((spat_coord1, (beta * t1).reshape(-1, 1)), dim=1)\n",
    "    coords2 = torch.cat((spat_coord2, (beta * t2).reshape(-1, 1)), dim=1)\n",
    "\n",
    "    def custom_distance_matrix(U, V):\n",
    "        diff = U[:, :2].unsqueeze(1) - V[:, :2].unsqueeze(0)\n",
    "        spatial_diff = torch.norm(diff, dim=2)\n",
    "        temporal_diff = torch.abs(U[:, 2].unsqueeze(1) - V[:, 2].unsqueeze(0))\n",
    "        return torch.sqrt(spatial_diff**2 + temporal_diff**2)\n",
    "\n",
    "\n",
    "    distance = custom_distance_matrix(coords1, coords2)\n",
    "    out = torch.zeros_like(distance)\n",
    "\n",
    "    non_zero_indices = distance != 0\n",
    "    if torch.any(non_zero_indices):\n",
    "        tmp1 = sigmasq * torch.exp(-distance[non_zero_indices]/range_)*(-1/range_)\n",
    "        \n",
    "        tmp2 = (beta/distance[non_zero_indices])\n",
    "\n",
    "                # Adjust t2 - t1 for non-zero indices\n",
    "        t1_non_zero = t1.unsqueeze(1).expand_as(distance)[non_zero_indices]\n",
    "        t2_non_zero = t2.unsqueeze(0).expand_as(distance)[non_zero_indices]\n",
    "\n",
    "        out[non_zero_indices] = tmp1 * tmp2 *(t2_non_zero - t1_non_zero)**2\n",
    "\n",
    "    out[~non_zero_indices] = 0\n",
    "\n",
    "    return out\n",
    "\n",
    "def matern_partial_advec( params: torch.Tensor, y: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "    sigmasq, range_, range_lon, advec, beta, nugget = params\n",
    "    if y is None or x is None:\n",
    "        raise ValueError(\"Both y and x_df must be provided.\")\n",
    "\n",
    "    x1 = x[:, 0]\n",
    "    y1 = x[:, 1]\n",
    "    t1 = x[:, 3]\n",
    "\n",
    "    x2 = y[:, 0]\n",
    "    y2 = y[:, 1]\n",
    "    t2 = y[:, 3]\n",
    "\n",
    "    spat_coord1 = torch.stack((x1 - advec * t1, y1 - advec * t1), dim=-1)\n",
    "    spat_coord2 = torch.stack((x2 - advec * t2, y2 - advec * t2), dim=-1)\n",
    "\n",
    "    coords1 = torch.cat((spat_coord1, (beta * t1).reshape(-1, 1)), dim=1)\n",
    "    coords2 = torch.cat((spat_coord2, (beta * t2).reshape(-1, 1)), dim=1)\n",
    "\n",
    "    def custom_distance_matrix(U, V):\n",
    "        diff = U[:, :2].unsqueeze(1) - V[:, :2].unsqueeze(0)\n",
    "        spatial_diff = torch.norm(diff, dim=2)\n",
    "        temporal_diff = torch.abs(U[:, 2].unsqueeze(1) - V[:, 2].unsqueeze(0))\n",
    "        return torch.sqrt(spatial_diff**2 + temporal_diff**2)\n",
    "\n",
    "\n",
    "    distance = custom_distance_matrix(coords1, coords2)\n",
    "    out = torch.zeros_like(distance)\n",
    "\n",
    "    non_zero_indices = distance != 0\n",
    "    if torch.any(non_zero_indices):\n",
    "        tmp1 = sigmasq * torch.exp(-distance[non_zero_indices]/range_)*(-1/range_)\n",
    "\n",
    "        # Adjust t2 - t1 for non-zero indices\n",
    "        t1_non_zero = t1.unsqueeze(1).expand_as(distance)[non_zero_indices]\n",
    "        t2_non_zero = t2.unsqueeze(0).expand_as(distance)[non_zero_indices]\n",
    "        x1_non_zero = x1.unsqueeze(1).expand_as(distance)[non_zero_indices]\n",
    "        x2_non_zero = x2.unsqueeze(0).expand_as(distance)[non_zero_indices]\n",
    "        y1_non_zero = y1.unsqueeze(1).expand_as(distance)[non_zero_indices]\n",
    "        y2_non_zero = y2.unsqueeze(0).expand_as(distance)[non_zero_indices]\n",
    "\n",
    "        tmp2 = (t2_non_zero - t1_non_zero )\n",
    "        tmp3 = tmp2* ( (x1_non_zero-x2_non_zero) - advec*(t1_non_zero - t2_non_zero) +  (y1_non_zero-y2_non_zero) - advec*(t1_non_zero - t2_non_zero) )\n",
    "\n",
    "        out[non_zero_indices] = tmp1* (1/ distance[non_zero_indices]) * tmp3\n",
    "\n",
    "    out[~non_zero_indices] = 0\n",
    "    return out\n",
    "\n",
    "def full_partial( params: torch.Tensor, input_np: torch.Tensor, y: torch.Tensor, covariance_function, partial_fun) -> torch.Tensor:\n",
    "    input_arr = input_np[:, :4]\n",
    "    y_arr = y\n",
    "\n",
    "    # Compute the covariance matrix\n",
    "    cov_matrix = covariance_function(params=params, y=input_arr, x=input_arr)\n",
    "    partial_der = partial_fun(params=params, y=input_arr, x=input_arr)\n",
    "    \n",
    "    # Compute the log determinant of the covariance matrix\n",
    "    # sign, log_det = torch.slogdet(cov_matrix)\n",
    "    # if sign <= 0:\n",
    "    #     raise ValueError(\"Covariance matrix is not positive definite\")\n",
    "    \n",
    "    # Extract locations\n",
    "    locs = input_arr[:, :2]\n",
    "\n",
    "    # Compute beta\n",
    "    tmp1 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, locs))\n",
    "    tmp2 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, y_arr))\n",
    "    beta = torch.linalg.solve(tmp1, tmp2)\n",
    "\n",
    "    # Compute the mean\n",
    "    mu = torch.matmul(locs, beta)\n",
    "    y_mu = y_arr - mu\n",
    "\n",
    "    # Compute the quadratic form\n",
    "    tmp1 = torch.linalg.solve(cov_matrix, partial_der)\n",
    "    tmp2 = torch.matmul(y_mu, tmp1)\n",
    "    tmp3 = torch.linalg.solve(cov_matrix, y_mu)\n",
    "    quad_form =  -torch.matmul(tmp2,tmp3 )\n",
    "    log_det = torch.trace(tmp1)\n",
    "    # Compute the negative log likelihood\n",
    "    neg_log_lik = 0.5 * (log_det + quad_form)\n",
    "    \n",
    "    return log_det, quad_form, neg_log_lik\n",
    "\n",
    "params = [20, 8.25, 5.25, 0.2, 0.5, 0.5]\n",
    "params = torch.tensor(params, requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "aggregated_np = aggregated_data\n",
    "a,b,c = full_partial(params, aggregated_np[:,:4], aggregated_np[:,2], matern_cov_yx, matern_partial_sigma)\n",
    "print(f'sigma log_det:{a}')\n",
    "print(f'sigma quad_form:{b}')\n",
    "print(f'sigma quad_form:{c}')\n",
    "\n",
    "a,b,c = full_partial(params, aggregated_np[:,:4], aggregated_np[:,2], matern_cov_yx, matern_partial_range)\n",
    "print(f'\\nrange log_det:{a}')\n",
    "print(f'range quad_form:{b}')\n",
    "print(f'range quad_form:{c}')\n",
    "\n",
    "a,b,c = full_partial(params, aggregated_np[:,:4], aggregated_np[:,2], matern_cov_yx, matern_partial_nugget)\n",
    "print(f'\\nnugget log_det:{a}')\n",
    "print(f'nugget quad_form:{b}')\n",
    "print(f'nugget quad_form:{c}')\n",
    "\n",
    "matern_partial_sigma(params, aggregated_np[:,:4], aggregated_np[:,:4])\n",
    "a,b,c = full_partial(params, aggregated_np[:,:4], aggregated_np[:,2], matern_cov_yx, matern_partial_beta)\n",
    "print(f'\\nbeta log_det:{a}')\n",
    "print(f'beta quad_form:{b}')\n",
    "print(f'beta quad_form:{c}')\n",
    "\n",
    "matern_partial_sigma(params, aggregated_np[:,:4], aggregated_np[:,:4])\n",
    "a,b,c = full_partial(params, aggregated_np[:,:4], aggregated_np[:,2], matern_cov_yx, matern_partial_advec)\n",
    "print(f'\\nadvec log_det:{a}')\n",
    "print(f'advec quad_form:{b}')\n",
    "print(f'advec quad_form:{c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great circle distance vs Euclidean distance \n",
    "   \n",
    "In small area, the difference between great circle distance (gcd) and euclidean distance should be small. However, model can try to account for curvature in small area resulting in overfitting. Haversine formula might cause numerical errors because it involves sin and cos for small distances. Also, incompatability between gcd for space and euclidean for time might be an issue.\n",
    "\n",
    "Based on my experiment, likelihood was way higher when using euclidean distance for space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def custom_distance_matrix(self, U, V):\n",
    "    # Efficient distance computation with broadcasting\n",
    "    spatial_diff = torch.norm(U[:, :2].unsqueeze(1) - V[:, :2].unsqueeze(0), dim=2)\n",
    "\n",
    "    temporal_diff = torch.abs(U[:, 2].unsqueeze(1) - V[:, 2].unsqueeze(0))\n",
    "    distance = (spatial_diff**2 + temporal_diff**2)  # move torch.sqrt to covariance function to track gradients of beta and avec\n",
    "    return distance\n",
    "\n",
    "#################################################################################\n",
    "#################################################################################\n",
    "\n",
    "def great_circle_distance(self, lat1, lon1, lat2, lon2):\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(torch.deg2rad, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = torch.sin(dlat / 2)**2 + torch.cos(lat1) * torch.cos(lat2) * torch.sin(dlon / 2)**2\n",
    "    c = 2 * torch.atan2(torch.sqrt(a), torch.sqrt(1 - a))\n",
    "    \n",
    "    # Radius of Earth in kilometers (mean radius)\n",
    "    R = 6371.0\n",
    "    return R * c\n",
    "def custom_distance_matrix(self, U, V):\n",
    "    # Efficient distance computation with broadcasting\n",
    "    lat1, lon1 = U[:, 0].unsqueeze(1), U[:, 1].unsqueeze(1)\n",
    "    lat2, lon2 = V[:, 0].unsqueeze(0), V[:, 1].unsqueeze(0)\n",
    "    \n",
    "    spatial_diff = self.great_circle_distance(lat1, lon1, lat2, lon2)\n",
    "    temporal_diff = torch.abs(U[:, 2].unsqueeze(1) - V[:, 2].unsqueeze(0))\n",
    "    distance = (spatial_diff**2 + temporal_diff**2)  # move torch.sqrt to covariance function to track gradients of beta and avec\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class using analytical derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss: 2623.149169921875, \n",
      "Params = tensor([ 2.4570e+01,  2.7900e+00, -2.3800e-02,  3.1100e-01,  2.1300e+00],\n",
      "       grad_fn=<SubBackward0>) \n",
      "Epoch 10: loss: 2619.02294921875, \n",
      "Params = tensor([24.6709,  2.6892, -0.1164,  0.3707,  2.2314], grad_fn=<SubBackward0>) \n",
      "Epoch 20: loss: 2604.1767578125, \n",
      "Params = tensor([24.7736,  2.5867, -0.0954,  0.3141,  2.3359], grad_fn=<SubBackward0>) \n",
      "Epoch 30: loss: 2590.366943359375, \n",
      "Params = tensor([24.8776,  2.4836, -0.0818,  0.3159,  2.4410], grad_fn=<SubBackward0>) \n",
      "Epoch 40: loss: 2584.289794921875, \n",
      "Params = tensor([24.9852,  2.3767, -0.0931,  0.3201,  2.5511], grad_fn=<SubBackward0>) \n",
      "Epoch 50: loss: 2573.384521484375, \n",
      "Params = tensor([25.0945,  2.2679, -0.0726,  0.2713,  2.6639], grad_fn=<SubBackward0>) \n",
      "Epoch 60: loss: 2574.65869140625, \n",
      "Params = tensor([25.2047,  2.1579, -0.1163,  0.3090,  2.7767], grad_fn=<SubBackward0>) \n",
      "Epoch 70: loss: 2564.40185546875, \n",
      "Params = tensor([25.3211,  2.0373, -0.0720,  0.2774,  2.9014], grad_fn=<SubBackward0>) \n",
      "Epoch 80: loss: 2560.0830078125, \n",
      "Params = tensor([25.4405,  1.9080, -0.0616,  0.2376,  3.0281], grad_fn=<SubBackward0>) \n",
      "Epoch 90: loss: 2567.586181640625, \n",
      "Params = tensor([25.5683,  1.8064, -0.1099,  0.2712,  3.1534], grad_fn=<SubBackward0>) \n",
      "Epoch 100: loss: 2561.503173828125, \n",
      "Params = tensor([25.7142,  1.8424, -0.0706,  0.2293,  3.1948], grad_fn=<SubBackward0>) \n",
      "Epoch 110: loss: 2561.10302734375, \n",
      "Params = tensor([25.8155,  1.9208, -0.0827,  0.2517,  3.2397], grad_fn=<SubBackward0>) \n",
      "Epoch 120: loss: 2563.56787109375, \n",
      "Params = tensor([25.9177,  2.0127, -0.0898,  0.2660,  3.2900], grad_fn=<SubBackward0>) \n",
      "Epoch 130: loss: 2559.03369140625, \n",
      "Params = tensor([26.0077,  2.0640, -0.0627,  0.2341,  3.3320], grad_fn=<SubBackward0>) \n",
      "Epoch 140: loss: 2564.91162109375, \n",
      "Params = tensor([26.1024,  2.0294, -0.1026,  0.2721,  3.4047], grad_fn=<SubBackward0>) \n",
      "Epoch 150: loss: 2559.25732421875, \n",
      "Params = tensor([26.2120,  2.0726, -0.0631,  0.2340,  3.4160], grad_fn=<SubBackward0>) \n",
      "Epoch 160: loss: 2560.583984375, \n",
      "Params = tensor([26.3052,  2.0700, -0.0847,  0.2535,  3.4626], grad_fn=<SubBackward0>) \n",
      "Epoch 170: loss: 2562.428955078125, \n",
      "Params = tensor([26.4133,  2.1018, -0.0866,  0.2580,  3.4758], grad_fn=<SubBackward0>) \n",
      "Epoch 180: loss: 2558.503662109375, \n",
      "Params = tensor([26.5079,  2.1346, -0.0649,  0.2349,  3.4706], grad_fn=<SubBackward0>) \n",
      "Epoch 190: loss: 2564.091552734375, \n",
      "Params = tensor([26.6059,  2.1158, -0.0989,  0.2696,  3.5146], grad_fn=<SubBackward0>) \n",
      "Epoch 200: loss: 2558.30029296875, \n",
      "Params = tensor([26.7109,  2.1632, -0.0594,  0.2295,  3.4860], grad_fn=<SubBackward0>) \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class matern_advec_beta:\n",
    "    def __init__(self, aggregated_np: torch.Tensor, params: torch.Tensor):\n",
    "        self.aggregated_np = aggregated_np[:,:4]\n",
    "        self.y = aggregated_np[:,2]\n",
    "        self.aggregated_locs = aggregated_np[:,:2]\n",
    "        aggregated_np_clone = copy.deepcopy(aggregated_np)\n",
    "        self.x1 = aggregated_np[:,0]\n",
    "  \n",
    "        self.y1 = aggregated_np[:,1]\n",
    "        self.t1 = aggregated_np[:,3]\n",
    "\n",
    "        self.x2 = aggregated_np_clone[:,0]\n",
    "        self.y2 = aggregated_np_clone[:,1]\n",
    "        self.t2 = aggregated_np_clone[:,3] \n",
    "        self.params = params \n",
    "\n",
    "        self.distance, self.non_zero_indices = self.precompute_coords(params)\n",
    "                # Adjust t2 - t1 for non-zero indices\n",
    "        self.t1_non_zero = self.t1.unsqueeze(1).expand_as(self.distance)[self.non_zero_indices]\n",
    "        self.t2_non_zero = self.t2.unsqueeze(0).expand_as(self.distance)[self.non_zero_indices]\n",
    "        self.x1_non_zero = self.x1.unsqueeze(1).expand_as(self.distance)[self.non_zero_indices]\n",
    "        self.x2_non_zero = self.x2.unsqueeze(0).expand_as(self.distance)[self.non_zero_indices]\n",
    "        self.y1_non_zero = self.y1.unsqueeze(1).expand_as(self.distance)[self.non_zero_indices]\n",
    "        self.y2_non_zero = self.y2.unsqueeze(0).expand_as(self.distance)[self.non_zero_indices]\n",
    "    \n",
    "    def custom_distance_matrix(self, U, V):\n",
    "        # Efficient distance computation with broadcasting\n",
    "        spatial_diff = torch.norm(U[:, :2].unsqueeze(1) - V[:, :2].unsqueeze(0), dim=2)\n",
    "        temporal_diff = torch.abs(U[:, 2].unsqueeze(1) - V[:, 2].unsqueeze(0))\n",
    "        distance = torch.sqrt(spatial_diff**2 + temporal_diff**2)  # Keeping torch.sqrt for proper gradients\n",
    "        return distance\n",
    "\n",
    "    def precompute_coords(self, params)-> torch.Tensor:\n",
    "        igmasq, range_,  advec, beta, nugget = params\n",
    "        # spat_coord1 = torch.stack((self.x1 , self.y1 - advec * self.t1), dim=-1)\n",
    "        spat_coord1 = torch.stack((self.x1 - advec * self.t1, self.y1 ), dim=-1)\n",
    "        spat_coord2 = torch.stack((self.x2 - advec * self.t2, self.y2 ), dim=-1)\n",
    "\n",
    "        U = torch.cat((spat_coord1, (beta * self.t1).reshape(-1, 1)), dim=1)\n",
    "        V = torch.cat((spat_coord2, (beta * self.t2).reshape(-1, 1)), dim=1)\n",
    "\n",
    "        distance = self.custom_distance_matrix(U,V)\n",
    "        non_zero_indices = distance != 0\n",
    "        return distance, non_zero_indices\n",
    "\n",
    "    def matern_advec_beta_cov(self,params) -> torch.Tensor:\n",
    "        sigmasq, range_,  advec, beta, nugget = params\n",
    "        \n",
    "        out = torch.zeros_like(self.distance)\n",
    "\n",
    "        out[self.non_zero_indices] = sigmasq * torch.exp(- (self.distance[self.non_zero_indices])/range_)\n",
    "        out[~self.non_zero_indices] = sigmasq\n",
    "\n",
    "        # Add a small jitter term to the diagonal for numerical stability\n",
    "        out += torch.eye(out.shape[0]) * nugget\n",
    "\n",
    "        return out\n",
    "\n",
    "    def matern_partial_sigma(self, params) -> torch.Tensor:\n",
    "        sigmasq, range_, advec, beta, nugget = params\n",
    "        out = torch.zeros_like(self.distance)\n",
    "\n",
    "        # Cache the non-zero indices distance\n",
    "        non_zero_distance = self.distance[self.non_zero_indices]\n",
    "\n",
    "        # Compute the exponential term\n",
    "        exp_term = torch.exp_(-non_zero_distance / range_)\n",
    "\n",
    "        # Assign the computed values to the output tensor\n",
    "        out[self.non_zero_indices] = exp_term\n",
    "        out[~self.non_zero_indices] = 1\n",
    "\n",
    "        return out\n",
    "\n",
    "    def matern_partial_range(self, params: torch.Tensor ) -> torch.Tensor:\n",
    "        sigmasq, range_,  advec, beta, nugget = params\n",
    "\n",
    "        out = torch.zeros_like(self.distance)\n",
    "    \n",
    "        tmp = sigmasq * torch.exp(-(self.distance[self.non_zero_indices]) / range_) \n",
    "        out[self.non_zero_indices] = tmp * (self.distance[self.non_zero_indices])  / range_**2\n",
    "\n",
    "        return out\n",
    "    def matern_partial_nugget(self, params: torch.Tensor) -> torch.Tensor:\n",
    "        sigmasq, range_,  advec, beta, nugget = params\n",
    "\n",
    "        out = torch.zeros_like(self.distance)\n",
    "\n",
    "   \n",
    "        out[self.non_zero_indices] = 0\n",
    "        out[~self.non_zero_indices] = 1\n",
    "\n",
    "        return out\n",
    "\n",
    "    def matern_partial_advec(self, params: torch.Tensor) -> torch.Tensor:\n",
    "        sigmasq, range_,  advec, beta, nugget = params\n",
    "\n",
    "\n",
    "        out = torch.zeros_like(self.distance)\n",
    "\n",
    "        tmp1 = sigmasq * torch.exp(-(self.distance[self.non_zero_indices]) /range_)*(-1/range_)\n",
    "\n",
    "\n",
    "        tmp2 = (self.t2_non_zero - self.t1_non_zero )\n",
    "        tmp3 = tmp2* ( (self.x1_non_zero-self.x2_non_zero) - advec*(self.t1_non_zero - self.t2_non_zero) +  (self.y1_non_zero-self.y2_non_zero) - advec*(self.t1_non_zero - self.t2_non_zero) )\n",
    "\n",
    "        out[self.non_zero_indices] = tmp1* (1/ (self.distance[self.non_zero_indices]) ) * tmp3\n",
    "\n",
    "        out[~self.non_zero_indices] = 0\n",
    "        return out\n",
    "    \n",
    "    def matern_partial_beta(self, params: torch.Tensor) -> torch.Tensor:\n",
    "        sigmasq, range_,  advec, beta, nugget = params\n",
    "    \n",
    "\n",
    "        out = torch.zeros_like(self.distance)\n",
    "\n",
    "        tmp1 = sigmasq * torch.exp(-(self.distance[self.non_zero_indices])/range_)*(-1/range_)\n",
    "        \n",
    "        tmp2 = (beta/(self.distance[self.non_zero_indices]))\n",
    "\n",
    "\n",
    "        out[self.non_zero_indices] = tmp1 * tmp2 *(self.t2_non_zero - self.t1_non_zero)**2\n",
    "\n",
    "        out[~self.non_zero_indices] = 0\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def cov_and_inv(self, params):\n",
    "        cov_matrix = self.matern_advec_beta_cov( params=params)\n",
    "        cov_inv = torch.linalg.inv(cov_matrix)\n",
    "                \n",
    "        # Compute beta\n",
    "        tmp1 = torch.matmul(self.aggregated_locs.T, torch.linalg.solve(cov_matrix, self.aggregated_locs))\n",
    "        tmp2 = torch.matmul(self.aggregated_locs.T, torch.linalg.solve(cov_matrix, self.y))\n",
    "        beta = torch.linalg.solve(tmp1, tmp2)\n",
    "\n",
    "        # Compute the mean\n",
    "        mu = torch.matmul(self.aggregated_locs, beta)\n",
    "        y_mu = self.y - mu\n",
    "        tmp3 = torch.matmul(cov_inv, y_mu)\n",
    "        return cov_matrix, cov_inv, mu, tmp3\n",
    "    \n",
    "    def full_partial(self, params: torch.Tensor, partial_der_fun, cov_matrix, cov_inv,mu, tmp3) -> torch.Tensor:\n",
    "        \n",
    "\n",
    "        partial_der = partial_der_fun(params)\n",
    "        \n",
    "        # Compute the log determinant of the covariance matrix\n",
    "        # sign, log_det = torch.slogdet(cov_matrix)\n",
    "        # if sign <= 0:\n",
    "        #     raise ValueError(\"Covariance matrix is not positive definite\")\n",
    "        \n",
    "        y_mu = self.y - mu\n",
    "\n",
    "        # Compute the quadratic form derivative\n",
    "        # tmp1 = torch.linalg.solve(cov_matrix, partial_der)\n",
    "        tmp1 = torch.matmul( cov_inv, partial_der)\n",
    "\n",
    "        tmp2 = torch.matmul(y_mu, tmp1)\n",
    "\n",
    "        # tmp3 = torch.matmul(cov_inv, y_mu)\n",
    "        # tmp3 = torch.linalg.solve(cov_matrix, y_mu)\n",
    "\n",
    "        quad_form_derivative  =  -torch.matmul(tmp2,tmp3 )\n",
    "        log_det_derivative = torch.trace(tmp1)\n",
    "        # Compute the negative log likelihood\n",
    "        neg_log_lik_derivative = 0.5 * (log_det_derivative  + quad_form_derivative )\n",
    "        \n",
    "        return log_det_derivative , quad_form_derivative , neg_log_lik_derivative \n",
    "    \n",
    "    def full_likelihood(self, params, covariance_function ,cov_matrix, cov_inv, mu) -> torch.Tensor:\n",
    "        input_arr = self.aggregated_np\n",
    "        y_arr = self.y\n",
    "\n",
    "        # Compute the covariance matrix\n",
    "        #cov_matrix = covariance_function(params=params)\n",
    "        \n",
    "        # Compute the log determinant of the covariance matrix\n",
    "        sign, log_det = torch.slogdet(cov_matrix)\n",
    "        # if sign <= 0:\n",
    "        #     raise ValueError(\"Covariance matrix is not positive definite\")\n",
    "        \n",
    "        # Extract locations\n",
    "        locs = input_arr[:, :2]\n",
    "\n",
    "        # Compute beta\n",
    "        # tmp1 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, locs))\n",
    "        # tmp2 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, y_arr))\n",
    "        # beta = torch.linalg.solve(tmp1, tmp2)\n",
    "\n",
    "        # Compute the mean\n",
    "        # mu = torch.matmul(locs, beta)\n",
    "        y_mu = y_arr - mu\n",
    "\n",
    "        # Compute the quadratic form\n",
    "        # quad_form = torch.matmul(y_mu, torch.linalg.solve(cov_matrix, y_mu))\n",
    "        tmp = torch.matmul( cov_inv, y_mu)\n",
    "        quad_form = torch.matmul( y_mu, tmp)\n",
    "\n",
    "        # Compute the negative log likelihood\n",
    "        neg_log_lik = 0.5 * (log_det + quad_form)\n",
    "\n",
    "        return neg_log_lik\n",
    "    \n",
    "    def matern_advec_beta_optim(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, epochs=10, tol=1e-3):\n",
    "        \n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        params = self.params.to(device)  # Assuming params is a tensor and moving it to the device\n",
    "        \n",
    "        # Initialize tensors and move to GPU if available\n",
    "        m = torch.zeros_like(params, device=device)\n",
    "        v = torch.zeros_like(params, device=device)\n",
    "        \n",
    "        b1 = torch.tensor(beta1, device=device)\n",
    "        b2 = torch.tensor(beta2, device=device)\n",
    "\n",
    "        prev_neg_log_lik = float('inf')\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.distance, self.non_zero_indices = self.precompute_coords(params)\n",
    "\n",
    "            if epoch ==0:\n",
    "                cov_matrix, cov_inv, mu, tmp3 = self.cov_and_inv(params)\n",
    "                \n",
    "            _, _, g_sigma = self.full_partial(params, self.matern_partial_sigma,cov_matrix, cov_inv, mu, tmp3)\n",
    "            _, _, g_range = self.full_partial(params, self.matern_partial_range,cov_matrix, cov_inv, mu, tmp3)\n",
    "            _, _, g_advec = self.full_partial(params, self.matern_partial_advec,cov_matrix, cov_inv, mu, tmp3)\n",
    "            _, _, g_beta = self.full_partial(params, self.matern_partial_beta,cov_matrix, cov_inv, mu, tmp3)\n",
    "            _, _, g_nugget = self.full_partial(params, self.matern_partial_nugget,cov_matrix, cov_inv, mu, tmp3)\n",
    "\n",
    "            g = torch.stack([g_sigma, g_range, g_advec, g_beta, g_nugget]).to(device)\n",
    "\n",
    "            # Update m and v\n",
    "            m = b1 * m + (1 - b1) * g\n",
    "            v = b2 * v + (1 - b2) * g**2\n",
    "\n",
    "            # Bias correction\n",
    "            m_hat = m / (1 - b1**(epoch + 1))\n",
    "            v_hat = v / (1 - b2**(epoch + 1))\n",
    "\n",
    "            # Update params\n",
    "            params = params - lr * m_hat / ( torch.sqrt(v_hat) + epsilon)\n",
    "\n",
    "            cov_matrix, cov_inv, mu, tmp3 = self.cov_and_inv(params)\n",
    "            # Compute the current negative log likelihood\n",
    "            neg_log_lik = self.full_likelihood(params, self.matern_advec_beta_cov,cov_matrix, cov_inv, mu)\n",
    "            \n",
    "            # Print params and likelihood every 10th epoch\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}: loss: {neg_log_lik}, \\nParams = {params} \")\n",
    "\n",
    "            # Check for convergence\n",
    "            if abs(prev_neg_log_lik - neg_log_lik) < tol:\n",
    "                print(f\"Converged at epoch {epoch}\")\n",
    "                break\n",
    "            \n",
    "            prev_neg_log_lik = neg_log_lik\n",
    "        return params\n",
    "\n",
    "\n",
    "params = [24.56, 2.8, -.0138, .301, 2.12]\n",
    "params = torch.tensor(params, requires_grad=True)\n",
    "\n",
    "instance = matern_advec_beta(aggregated_data, params)\n",
    "epochs=300\n",
    "instance.matern_advec_beta_optim(lr=0.01,beta1=0.9, beta2= 0.8, epsilon=1e-8, epochs=epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for pytorch + vecchia approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_for_datamap = [0,8]\n",
    "\n",
    "class matern_advec_beta_torch_vecchia:\n",
    "    def __init__(self, analaysis_data_map: torch.Tensor, params: torch.Tensor, nns_map=nns_map, mm_cond_number=mm_cond_number):\n",
    "        self.key_list = sorted(analysis_data_map)\n",
    "        self.input_map = analysis_data_map\n",
    "\n",
    "        self.mm_cond_number = mm_cond_number\n",
    "        self.nns_map = nns_map \n",
    "        self.analaysis_data_map = analaysis_data_map\n",
    "        self.smooth = 0.5  \n",
    "        sample_df = analaysis_data_map[self.key_list[0]]\n",
    "\n",
    "        self.size_per_hour = len(sample_df)\n",
    "\n",
    "    def custom_distance_matrix(self, U, V):\n",
    "        # Efficient distance computation with broadcasting\n",
    "        spatial_diff = torch.norm(U[:, :2].unsqueeze(1) - V[:, :2].unsqueeze(0), dim=2)\n",
    "\n",
    "        temporal_diff = torch.abs(U[:, 2].unsqueeze(1) - V[:, 2].unsqueeze(0))\n",
    "        distance = (spatial_diff**2 + temporal_diff**2)  # move torch.sqrt to covariance function to track gradients of beta and avec\n",
    "        return distance\n",
    "    \n",
    "    def precompute_coords_ani(self, params, y: torch.Tensor, x: torch.Tensor)-> torch.Tensor:\n",
    "        sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "\n",
    "        if y is None or x is None:\n",
    "            raise ValueError(\"Both y and x_df must be provided.\")\n",
    "\n",
    "        x1 = x[:, 0]\n",
    "        y1 = x[:, 1]\n",
    "        t1 = x[:, 3]\n",
    "\n",
    "        x2 = y[:, 0]\n",
    "        y2 = y[:, 1]\n",
    "        t2 = y[:, 3]\n",
    "\n",
    "        # spat_coord1 = torch.stack((self.x1 , self.y1 - advec * self.t1), dim=-1)\n",
    "        spat_coord1 = torch.stack(( (x1 - advec_lat * t1)/range_lat, (y1 - advec_lon * t1)/range_lon ), dim=-1)\n",
    "        spat_coord2 = torch.stack(( (x2 - advec_lat * t2)/range_lat, (y2 - advec_lon * t2)/range_lon ), dim=-1)\n",
    "\n",
    "        U = torch.cat((spat_coord1, (beta * t1).reshape(-1, 1)), dim=1)\n",
    "        V = torch.cat((spat_coord2, (beta * t2).reshape(-1, 1)), dim=1)\n",
    "\n",
    "        distance = self.custom_distance_matrix(U,V)\n",
    "        non_zero_indices = distance != 0\n",
    "        return distance, non_zero_indices\n",
    "    \n",
    "    # anisotropic in three \n",
    "    def matern_cov_ani(self,params: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "        \n",
    "\n",
    "        distance, non_zero_indices = self.precompute_coords_ani(params, x,y)\n",
    "        out = torch.zeros_like(distance)\n",
    "\n",
    "        non_zero_indices = distance != 0\n",
    "        if torch.any(non_zero_indices):\n",
    "            out[non_zero_indices] = sigmasq * torch.exp(- torch.sqrt(distance[non_zero_indices]))\n",
    "        out[~non_zero_indices] = sigmasq\n",
    "\n",
    "        # Add a small jitter term to the diagonal for numerical stability\n",
    "        out += torch.eye(out.shape[0]) * nugget\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def full_likelihood(self,params: torch.Tensor, input_np: torch.Tensor, y: torch.Tensor, covariance_function) -> torch.Tensor:\n",
    "        input_arr = input_np[:, :4]\n",
    "        y_arr = y\n",
    "\n",
    "        # Compute the covariance matrix\n",
    "        cov_matrix = covariance_function(params=params, y=input_arr, x=input_arr)\n",
    "        \n",
    "        # Compute the log determinant of the covariance matrix\n",
    "        sign, log_det = torch.slogdet(cov_matrix)\n",
    "        # if sign <= 0:\n",
    "        #     raise ValueError(\"Covariance matrix is not positive definite\")\n",
    "        \n",
    "        # Extract locations\n",
    "        locs = input_arr[:, :2]\n",
    "\n",
    "        # Compute beta\n",
    "        tmp1 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, locs))\n",
    "        tmp2 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, y_arr))\n",
    "        beta = torch.linalg.solve(tmp1, tmp2)\n",
    "\n",
    "        # Compute the mean\n",
    "        mu = torch.matmul(locs, beta)\n",
    "        y_mu = y_arr - mu\n",
    "\n",
    "        # Compute the quadratic form\n",
    "        quad_form = torch.matmul(y_mu, torch.linalg.solve(cov_matrix, y_mu))\n",
    "\n",
    "        # Compute the negative log likelihood\n",
    "        neg_log_lik = 0.5 * (log_det + quad_form)\n",
    "     \n",
    "        return  neg_log_lik \n",
    "    \n",
    "    def vecchia_like_local_computer(self, params: torch.Tensor, covariance_function) -> torch.Tensor:\n",
    "        self.cov_map = defaultdict(list)\n",
    "        neg_log_lik = 0.0\n",
    "        \n",
    "        for time_idx in range(len(self.analaysis_data_map)):\n",
    "            current_np = self.analaysis_data_map[self.key_list[time_idx]]\n",
    "\n",
    "            # Use below when working on local computer to avoid singular matrix\n",
    "            cur_heads = current_np[:21, :]\n",
    "            neg_log_lik += self.full_likelihood(params, cur_heads, cur_heads[:, 2], covariance_function)\n",
    "\n",
    "            for index in range(21, self.size_per_hour):\n",
    "\n",
    "                \n",
    "                current_row = current_np[index].reshape(1, -1)\n",
    "                current_y = current_row[0, 2]\n",
    "\n",
    "                # Construct conditioning set\n",
    "                mm_neighbors = self.nns_map[index]\n",
    "                past = list(mm_neighbors)\n",
    "                data_list = []\n",
    "\n",
    "                if past:\n",
    "                    data_list.append(current_np[past])\n",
    "\n",
    "                if time_idx > 1:\n",
    "                    cov_matrix = self.cov_map[index]['cov_matrix']\n",
    "                    tmp_for_beta = self.cov_map[index]['tmp_for_beta']\n",
    "                    cov_xx_inv = self.cov_map[index]['cov_xx_inv']\n",
    "                    L_inv = self.cov_map[index]['L_inv']\n",
    "                    cov_ygivenx = self.cov_map[index]['cov_ygivenx']\n",
    "                    cond_mean_tmp = self.cov_map[index]['cond_mean_tmp']\n",
    "                    log_det = self.cov_map[index]['log_det']\n",
    "                    locs = self.cov_map[index]['locs']\n",
    "                    \n",
    "                    last_hour_np = self.input_map[self.key_list[time_idx - 1]]\n",
    "                    past_conditioning_data = last_hour_np[past + [index], :]\n",
    "                    data_list.append(past_conditioning_data)\n",
    "\n",
    "                    if data_list:\n",
    "                        conditioning_data = torch.vstack(data_list)\n",
    "                    else:\n",
    "                        conditioning_data = torch.empty((0, current_row.shape[1]), dtype=torch.float32)\n",
    "\n",
    "                    np_arr = torch.vstack((current_row, conditioning_data))\n",
    "                    y_and_neighbors = np_arr[:, 2]\n",
    "\n",
    "                    cov_yx = cov_matrix[0, 1:]\n",
    "\n",
    "                    tmp2 = torch.matmul(torch.matmul(L_inv, locs).T, torch.matmul(L_inv, y_and_neighbors))\n",
    "                    beta = torch.linalg.solve(tmp_for_beta, tmp2)\n",
    "\n",
    "                    mu = torch.matmul(locs, beta)\n",
    "                    mu_current = mu[0]\n",
    "                    mu_neighbors = mu[1:]\n",
    "                    \n",
    "                    # Mean and variance of y|x\n",
    "                    cond_mean = mu_current + torch.matmul(cond_mean_tmp, (y_and_neighbors[1:] - mu_neighbors))\n",
    "                    alpha = current_y - cond_mean\n",
    "                    quad_form = alpha**2 * (1 / cov_ygivenx)\n",
    "                    neg_log_lik += 0.5 * (log_det + quad_form)\n",
    "\n",
    "                    continue\n",
    "\n",
    "                if time_idx > 0:\n",
    "                    last_hour_np = self.input_map[self.key_list[time_idx - 1]]\n",
    "                    past_conditioning_data = last_hour_np[past + [index], :]\n",
    "                    data_list.append(past_conditioning_data)\n",
    "\n",
    "                if data_list:\n",
    "                    conditioning_data = torch.vstack(data_list)\n",
    "                else:\n",
    "                    conditioning_data = torch.empty((0, current_row.shape[1]), dtype=torch.float32)\n",
    "\n",
    "                np_arr = torch.vstack((current_row, conditioning_data))\n",
    "                y_and_neighbors = np_arr[:, 2]\n",
    "                locs = np_arr[:, :2]\n",
    "\n",
    "                cov_matrix = covariance_function(params=params, y= np_arr, x= np_arr)\n",
    "                # print(f'Condition number: {torch.linalg.cond(cov_matrix)}')\n",
    "                L = torch.linalg.cholesky(cov_matrix)\n",
    "                L11 = L[:1, :1]\n",
    "                L12 = torch.zeros(L[:1, 1:].shape)\n",
    "                L21 = L[1:, :1]\n",
    "                L22 = L[1:, 1:]\n",
    "                L11_inv = torch.linalg.inv(L11)\n",
    "                L22_inv = torch.linalg.inv(L22)\n",
    "\n",
    "                L_inv = torch.block_diag(L11_inv, L22_inv)\n",
    "                L_inv[1:, :1] = -torch.matmul(L22_inv, L21) @ L11_inv\n",
    "\n",
    "                cov_yx = cov_matrix[0, 1:]\n",
    "\n",
    "                tmp1 = torch.matmul(L_inv, locs)\n",
    "                tmp2 = torch.matmul(torch.matmul(L_inv, locs).T, torch.matmul(L_inv, y_and_neighbors))\n",
    "                tmp_for_beta = torch.matmul(tmp1.T, tmp1)\n",
    "                beta = torch.linalg.solve(tmp_for_beta, tmp2)\n",
    "\n",
    "                mu = torch.matmul(locs, beta)\n",
    "                mu_current = mu[0]\n",
    "                mu_neighbors = mu[1:]\n",
    "\n",
    "                # Mean and variance of y|x\n",
    "                sigma = cov_matrix[0, 0]\n",
    "                cov_xx = cov_matrix[1:, 1:]\n",
    "                cov_xx_inv = torch.linalg.inv(cov_xx)\n",
    "\n",
    "                cov_ygivenx = sigma - torch.matmul(cov_yx, torch.matmul(cov_xx_inv, cov_yx))\n",
    "                cond_mean_tmp = torch.matmul(cov_yx, cov_xx_inv)\n",
    "                cond_mean = mu_current + torch.matmul(cond_mean_tmp, (y_and_neighbors[1:] - mu_neighbors))\n",
    "                \n",
    "                alpha = current_y - cond_mean\n",
    "                quad_form = alpha**2 * (1 / cov_ygivenx)\n",
    "                log_det = torch.log(cov_ygivenx)\n",
    "                neg_log_lik += 0.5 * (log_det + quad_form)\n",
    "\n",
    "             \n",
    "                if time_idx == 1:\n",
    "                    self.cov_map[index] = {\n",
    "                        'tmp_for_beta': tmp_for_beta,\n",
    "                        'cov_xx_inv': cov_xx_inv,\n",
    "                        'cov_matrix': cov_matrix,\n",
    "                        'L_inv': L_inv,\n",
    "                        'cov_ygivenx': cov_ygivenx,\n",
    "                        'cond_mean_tmp': cond_mean_tmp,\n",
    "                        'log_det': log_det,\n",
    "                        'locs': locs\n",
    "                    }\n",
    "\n",
    "        return neg_log_lik    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m instance = matern_advec_beta_torch_vecchia(analysis_data_map, params, nns_map, mm_cond_number)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# optimizer = optim.Adam([params], lr=0.01)  # For Adam\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m optimizer = \u001b[43moptim\u001b[49m.Adam([params],lr=\u001b[32m0.01\u001b[39m, betas=(\u001b[32m0.9\u001b[39m, \u001b[32m0.8\u001b[39m), eps=\u001b[32m1e-8\u001b[39m)\n\u001b[32m     14\u001b[39m scheduler = StepLR(optimizer, step_size=\u001b[32m20\u001b[39m, gamma=\u001b[32m0.9\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Example function to compute out1\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'optim' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Choose your optimizer\n",
    "params = [24.56, 2.8,  -0.0138, -0.0138, 0.301, 2.12]\n",
    "params = [20, 8.25, 5.25, 0.2, 0.2, 0.2]\n",
    "\n",
    "params = [20, 2, 2, 0.01, -0.1, 0.14, 3]\n",
    "\n",
    "params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "params = torch.tensor(params, requires_grad=True)\n",
    "\n",
    "instance = matern_advec_beta_torch_vecchia(analysis_data_map, params, nns_map, mm_cond_number)\n",
    "\n",
    "# optimizer = optim.Adam([params], lr=0.01)  # For Adam\n",
    "optimizer = optim.Adam([params],lr=0.01, betas=(0.9, 0.8), eps=1e-8)\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.9)\n",
    "\n",
    "# Example function to compute out1\n",
    "def compute_out1(params):\n",
    "    # Compute the output using your function\n",
    "    # nll = instance.full_likelihood(params, instance.matern_advec_beta_cov )\n",
    "    nll = instance.vecchia_like_local_computer(params, instance.matern_cov_ani )\n",
    "\n",
    "    return nll\n",
    "\n",
    "# Training loop\n",
    "prev_loss = float('inf')\n",
    "tol = 1e-4  # Convergence tolerance\n",
    "for epoch in range(3000):  # Number of epochs\n",
    "    optimizer.zero_grad()  # Zero the gradients \n",
    "    \n",
    "    loss = compute_out1(params)\n",
    "    loss.backward()  # Backpropagate the loss\n",
    "    \n",
    "    # Print gradients and parameters every 10th epoch\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "    \n",
    "    # print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "    \n",
    "    optimizer.step()  # Update the parameters\n",
    "    scheduler.step()\n",
    "    # Check for convergence\n",
    "    if abs(prev_loss - loss.item()) < tol:\n",
    "        print(f\"Converged at epoch {epoch}\")\n",
    "        break\n",
    "    \n",
    "    prev_loss = loss.item()\n",
    "\n",
    "print('Training complete.') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try my package oop using full likelihood "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Gradients: [  1.9402208  -8.884079   -8.339249   -2.2376022 -10.631287  126.89653\n",
      "   8.968163 ]\n",
      " Loss: 628.3262939453125, Parameters: [ 2.442e+01  1.920e+00  1.920e+00  1.000e-03 -4.500e-02  2.370e-01\n",
      "  3.340e+00]\n",
      "Epoch 101, Gradients: [ 0.40051484 -4.172155   -3.8580322  -0.147995   -0.50027466 -2.6572876\n",
      "  3.629163  ]\n",
      " Loss: 598.65771484375, Parameters: [23.563217    2.761465    2.7625687   0.10603883  0.04937236  0.10878895\n",
      "  2.5003712 ]\n",
      "Epoch 201, Gradients: [-0.08677709 -2.0934935  -1.3359222  -0.050354   -0.02059937 -0.6693115\n",
      "  0.93575895]\n",
      " Loss: 594.40234375, Parameters: [23.07261     3.2624993   3.2710943   0.04107396  0.075522    0.12459052\n",
      "  1.9889146 ]\n",
      "Epoch 301, Gradients: [-0.2727673  -1.3088493  -0.4664917  -0.02841187 -0.04309082 -0.4633789\n",
      " -0.03971767]\n",
      " Loss: 593.5638427734375, Parameters: [23.329903    3.556881    3.5775445   0.03232866  0.07745279  0.12197667\n",
      "  1.9365772 ]\n",
      "Epoch 401, Gradients: [-0.3030101  -1.0736284  -0.2778473   0.00257111  0.00158691 -0.01959229\n",
      " -0.0049932 ]\n",
      " Loss: 593.248291015625, Parameters: [23.497873    3.7271695   3.7500029   0.03472315  0.07729933  0.11669877\n",
      "  2.0316582 ]\n",
      "Epoch 501, Gradients: [-3.2152265e-01 -9.5022154e-01 -1.7694092e-01 -3.2043457e-04\n",
      "  4.1503906e-03 -1.2939453e-02 -2.5568604e-03]\n",
      " Loss: 593.092529296875, Parameters: [23.597105    3.8274157   3.8519604   0.03580173  0.0771253   0.11381087\n",
      "  2.0827272 ]\n",
      "Epoch 601, Gradients: [-3.3177370e-01 -8.8182521e-01 -1.2161255e-01  2.5634766e-03\n",
      " -2.5024414e-03  8.0017090e-02  6.6512823e-04]\n",
      " Loss: 593.01025390625, Parameters: [23.655807    3.8864534   3.9119275   0.03645198  0.07694593  0.11220228\n",
      "  2.1118193 ]\n",
      "Epoch 701, Gradients: [-0.33807594 -0.84411955 -0.09185791 -0.0040741  -0.00366211 -0.0456543\n",
      " -0.00164133]\n",
      " Loss: 592.9650268554688, Parameters: [23.690521    3.9212599   3.94719     0.03678487  0.07691052  0.11123377\n",
      "  2.1286952 ]\n",
      "Converged at epoch 742\n",
      "Epoch 743, Gradients: [-3.3977652e-01 -8.3334041e-01 -8.3297729e-02 -5.0354004e-04\n",
      "  2.4414062e-04 -4.4921875e-02 -1.6320944e-03]\n",
      " Loss: 592.9525146484375, full Parameters: [23.700626    3.931397    3.9574375   0.03695429  0.0769494   0.11098192\n",
      "  2.1335363 ]\n",
      "FINAL STATE: Epoch 743, Gradients: [-3.3977652e-01 -8.3334041e-01 -8.3297729e-02 -5.0354004e-04\n",
      "  2.4414062e-04 -4.4921875e-02 -1.6320944e-03]\n",
      " Loss: 592.9525146484375, full Parameters: [23.700626    3.931397    3.9574375   0.03695429  0.0769494   0.11098192\n",
      "  2.1335363 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([23.700626  ,  3.931397  ,  3.9574375 ,  0.03695429,  0.0769494 ,\n",
       "         0.11098192,  2.1335363 ], dtype=float32),\n",
       " 592.9525146484375]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "params = torch.tensor(params, requires_grad=True)\n",
    "\n",
    "instance = kernels.model_fitting(\n",
    "    smooth=0.5,\n",
    "    input_map=analysis_data_map,\n",
    "    aggregated_data=aggregated_data,\n",
    "    nns_map=nns_map,\n",
    "    mm_cond_number=mm_cond_number\n",
    ")\n",
    "\n",
    "# optimizer = optim.Adam([params], lr=0.01)  # For Adam\n",
    "optimizer, scheduler = instance.optimizer_fun( params, lr=0.01, betas=(0.9, 0.8), eps=1e-8, step_size=20, gamma=0.9)    \n",
    "instance.run_full(params, optimizer,scheduler, epochs=3000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
