{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed154afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "\n",
    "# --- Standard Libraries ---\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import copy\n",
    "import cmath\n",
    "import pickle\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Path configuration (only run once)\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "# --- Third-Party Libraries ---\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Tuple, Dict, Any, Callable\n",
    "from json import JSONEncoder\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "import typer\n",
    "\n",
    "# Torch and Numerical Libraries\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.fft\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# --- Custom (GEMS_TCO) Imports ---\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels_reparam_space_time \n",
    "from GEMS_TCO import data_preprocess, data_preprocess as dmbh\n",
    "from GEMS_TCO import orderings as _orderings \n",
    "\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "from GEMS_TCO.data_loader import load_data2\n",
    "from GEMS_TCO import debiased_whittle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa9b4bf",
   "metadata": {},
   "source": [
    "Load monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf1792b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting data to lat: [1, 3], lon: [129.0, 133.0]\n"
     ]
    }
   ],
   "source": [
    "space: List[str] = ['1', '1']\n",
    "lat_lon_resolution = [int(s) for s in space]\n",
    "mm_cond_number: int = 8\n",
    "years = ['2024']\n",
    "month_range = [7] \n",
    "\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "data_load_instance = load_data2(config.mac_data_load_path)\n",
    "\n",
    "lat_range_input = [1, 3]\n",
    "lon_range_input = [125.0, 129.0]\n",
    "lon_range_input = [129.0, 133.0]\n",
    "\n",
    "#lat_range_input=[0,5]      \n",
    "#lon_range_input=[123, 133.0] \n",
    "\n",
    "df_map, ord_mm, nns_map = data_load_instance.load_maxmin_ordered_data_bymonthyear(\n",
    "lat_lon_resolution=lat_lon_resolution, \n",
    "mm_cond_number=mm_cond_number,\n",
    "years_=years, \n",
    "months_=month_range,\n",
    "\n",
    "lat_range=lat_range_input,   \n",
    "lon_range=lon_range_input\n",
    "\n",
    ")\n",
    "\n",
    "#days: List[str] = ['0', '31']\n",
    "#days_s_e = [int(d) for d in days]\n",
    "#days_list = list(range(days_s_e[0], days_s_e[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7aec7102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23552, 4])\n"
     ]
    }
   ],
   "source": [
    "daily_aggregated_tensors_dw = [] \n",
    "daily_hourly_maps_dw = []      \n",
    "\n",
    "daily_aggregated_tensors_vecc = [] \n",
    "daily_hourly_maps_vecc = []   \n",
    "\n",
    "\n",
    "for day_index in range(31):\n",
    "    hour_start_index = day_index * 8\n",
    "    hour_end_index = (day_index + 1) * 8\n",
    "    #hour_end_index = day_index*8 + 1\n",
    "    hour_indices = [hour_start_index, hour_end_index]\n",
    "\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "    df_map, \n",
    "    hour_indices, \n",
    "    ord_mm= None,  # or just omit it\n",
    "    dtype=torch.float64, # or just omit it \n",
    "    keep_ori=False  #keep_exact_loc\n",
    "    )\n",
    "\n",
    "    daily_aggregated_tensors_dw.append( day_aggregated_tensor )\n",
    "    daily_hourly_maps_dw.append( day_hourly_map )\n",
    "\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "    df_map, \n",
    "    hour_indices, \n",
    "    ord_mm= ord_mm,  # or just omit it\n",
    "    dtype=torch.float64, # or just omit it \n",
    "    keep_ori=False  #keep_exact_loc\n",
    "    )\n",
    "\n",
    "    daily_aggregated_tensors_vecc.append( day_aggregated_tensor )\n",
    "    daily_hourly_maps_vecc.append( day_hourly_map )\n",
    "print(daily_aggregated_tensors_vecc[0].shape)\n",
    "#print(daily_hourly_maps[0])\n",
    "nn = daily_aggregated_tensors_vecc[0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef86d35",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "If you want to compute likleihood on small 8 boxes,\n",
    "then use "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0552cc",
   "metadata": {},
   "source": [
    "# 1-3 125-129"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce4db81",
   "metadata": {},
   "source": [
    "1-3 125 129\n",
    "day1\n",
    "\n",
    "23184\n",
    " full likelihood: 27497.49,\n",
    " vecchia: 27512.18, \n",
    " whittle de-biased: 1124.4\n",
    " full likelihood: 27516.94,\n",
    " vecchia: 27522.4, \n",
    " whittle de-biased: 1120.5\n",
    " full likelihood: 27502.56,\n",
    " vecchia: 27514.28, \n",
    " whittle de-biased: 1092.36\n",
    " full likelihood: 27506.84,\n",
    " vecchia: 27517.13, \n",
    " whittle de-biased: 1099.41\n",
    " full likelihood: 27665.1,\n",
    " vecchia: 27676.64, \n",
    " whittle de-biased: 1566.3\n",
    "\n",
    " mac line search 80\n",
    " 23184\n",
    " full likelihood: 27506.87,\n",
    " vecchia: 27516.98, \n",
    " whittle de-biased: 1100.3\n",
    "\n",
    "\n",
    " day2   \n",
    "\n",
    " 23184\n",
    " full likelihood: 31667.88,\n",
    " vecchia: 31737.68, \n",
    " whittle de-biased: 2833.55\n",
    " full likelihood: 31820.87,\n",
    " vecchia: 31869.75, \n",
    " whittle de-biased: 2392.34\n",
    " full likelihood: 31672.01,\n",
    " vecchia: 31737.81, \n",
    " whittle de-biased: 2846.26\n",
    " full likelihood: 31667.68,\n",
    " vecchia: 31736.62, \n",
    " whittle de-biased: 2849.41\n",
    " full likelihood: 31667.56,\n",
    " vecchia: 31736.63, \n",
    " whittle de-biased: 2849.82\n",
    " full likelihood: 31914.3,\n",
    " vecchia: 31955.94, \n",
    " whittle de-biased: 2579.62\n",
    "\n",
    "\n",
    " 3-5 129 133\n",
    "\n",
    " day1\n",
    "\n",
    " 23040\n",
    " full likelihood: 25851.96,\n",
    " vecchia: 25906.67, \n",
    " whittle de-biased: 1357.27\n",
    " full likelihood: 25825.77,\n",
    " vecchia: 25884.58, \n",
    " whittle de-biased: 1265.2\n",
    " full likelihood: 25840.32,\n",
    " vecchia: 25893.1, \n",
    " whittle de-biased: 1307.71\n",
    " full likelihood: 25839.27,\n",
    " vecchia: 25892.9, \n",
    " whittle de-biased: 1307.29\n",
    " full likelihood: 25947.66,\n",
    " vecchia: 26020.97, \n",
    " whittle de-biased: 1231.85\n",
    "\n",
    " mac_ slow version line search 80\n",
    " full likelihood: 25839.6,\n",
    " vecchia: 25893.36, \n",
    " whittle de-biased: 1308.45\n",
    "\n",
    "\n",
    "\n",
    " day2\n",
    "\n",
    " 23040\n",
    " full likelihood: 33063.62,\n",
    " vecchia: 33222.27, \n",
    " whittle de-biased: 3189.93\n",
    " full likelihood: 33205.76,\n",
    " vecchia: 33280.34, \n",
    " whittle de-biased: 2452.0\n",
    " full likelihood: 33073.54,\n",
    " vecchia: 33226.61, \n",
    " whittle de-biased: 3214.84\n",
    " full likelihood: 33067.24,\n",
    " vecchia: 33226.19, \n",
    " whittle de-biased: 3204.52\n",
    " full likelihood: 33066.99,    \n",
    " vecchia: 33226.04, \n",
    " whittle de-biased: 3204.76\n",
    " full likelihood: 33146.59,\n",
    " vecchia: 33267.35, \n",
    " whittle de-biased: 2215.82\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "day 2\n",
    "3-5 129 -133\n",
    "23040\n",
    " full likelihood: 26988.86,\n",
    " vecchia: 26872.44, \n",
    " whittle de-biased: 3189.93\n",
    " full likelihood: 26866.2,\n",
    " vecchia: 26814.83, \n",
    " whittle de-biased: 2988.98\n",
    " full likelihood: 26993.54,\n",
    " vecchia: 26876.0, \n",
    " whittle de-biased: 3204.76\n",
    " full likelihood: 26993.56,\n",
    " vecchia: 26875.93, \n",
    " whittle de-biased: 3204.64\n",
    " full likelihood: 26597.35,\n",
    " vecchia: 26625.36, \n",
    " whittle de-biased: 2215.82\n",
    "\n",
    "\n",
    " 3-5 124 128\n",
    "\n",
    " 22680\n",
    " full likelihood: 30780.73,\n",
    " vecchia: 30517.97, \n",
    " whittle de-biased: 11573.27\n",
    " full likelihood: 30610.24,     adams amarel\n",
    " vecchia: 30445.04, \n",
    " whittle de-biased: 11388.29\n",
    " full likelihood: 30790.4,\n",
    " vecchia: 30521.77, \n",
    " whittle de-biased: 11591.01\n",
    " full likelihood: 30790.61,\n",
    " vecchia: 30521.85, \n",
    " whittle de-biased: 11591.22\n",
    " full likelihood: 30538.36,     lbfgs amarel (more accurate, slow setting)\n",
    " vecchia: 30483.29, \n",
    " whittle de-biased: 11476.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "494f4d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23184\n",
      "Pre-computing Vecchia batches...\n",
      "Pre-computed 20784 batches.\n",
      " full likelihood: 35799.27,\n",
      " vecchia: 35313.67, \n",
      " whittle de-biased: 9220.28\n"
     ]
    }
   ],
   "source": [
    "print(nn)\n",
    "day_idx = 0\n",
    "lat_range=lat_range_input \n",
    "lon_range=lon_range_input\n",
    "\n",
    "day1_va = [4.22817, 1.664023, 0.481917, -3.77204, 0.02213, -0.16318, -1.737487]\n",
    "day1_va_amarel = [4.2766, 1.6846, 0.5049, -3.6748, 0.01975, -0.15765, -14.722]\n",
    "day1_vl = [4.2866, 1.7396, 0.4891, -3.777, 0.02048, -0.16411, -12.05573]\n",
    "day1_vl_amarel =[4.2843, 1.7136, 0.4887, -3.7712, 0.0202, -0.1616, -14.7220]\n",
    "day1_dwl = [4.2739, 1.8060, 0.7948, -3.3599, 0.0223, -0.1672, -11.8381]\n",
    "\n",
    "\n",
    "day1_dwl2 = [4.8141, 2.1179, 0.5037, -8.3823, 0.0120, 0.0077, 1.8768]\n",
    "\n",
    "\n",
    "day1_vl_mac2 = [4.2824, 1.7108, 0.4883, -3.7686, 0.0201, -0.1611, -5.1617]\n",
    "\n",
    "\n",
    "day2_va = [3.7634, 1.2864, 0.6458, -4.05860, 0.001777, -0.22191, 0.7242916]\n",
    "day2_va_amarel = [3.8929, 1.4547, 0.6384, -3.9767, 0.0002, -0.2129, 0.5708]\n",
    "day2_vl = [3.7503, 1.2538, 0.6472, -4.09016, 0.001728, -0.222897, 0.73606]\n",
    "day2_vl2 = [3.7440, 1.2168, 0.6473, -4.0569, 0.00105, -0.22017, 0.74023]\n",
    "day2_vl3 = [3.7440, 1.2160, 0.6473, -4.0566, 0.0011, -0.2202, 0.7403]\n",
    "day2_vl4 = [3.7440, 1.2167, 0.64726, -4.05665, 0.001068, -0.220179, 0.740284]\n",
    "day2_dwl =[4.1200, 1.6540, 0.8909, -3.4966, -0.0263, -0.2601, -0.0986]\n",
    "\n",
    "day3_va = [4.61865, 1.86892, 0.54694, -4.21337, -0.04020, -0.24562, -0.7427]\n",
    "day3_va_amarel = [4.4692, 1.6762, 0.4894, -4.25829, -0.0370, -0.24052, 0.0924]\n",
    "day3_vl = [4.39425, 1.60585, 0.50261, -4.30459, -0.03894, -0.2451, 0.26052]\n",
    "day2 = [ day2_va, day2_va_amarel, day2_vl3, day2_vl4, day2_dwl]\n",
    "\n",
    "day4_va = [4.1117, 1.6978, 0.7622, -4.0126, 0.028246, -0.14168, -0.27482]\n",
    "day4_vl = [3.962231, 1.4687, 0.7822, -4.0332, 0.03072, -0.14823, 0.0994]\n",
    "day4_dwl = [3.9351, 1.8070, 1.0980, -3.5154, 0.0214, -0.1712, -0.5348]\n",
    "\n",
    "#day1 = [day1_va, day1_va_amarel, day1_vl, day1_vl_amarel, day1_dwl, day1_dwl2,day1_vl_mac2]\n",
    "day3 = [day3_va, day3_va_amarel, day3_vl]\n",
    "day1 = [day1_dwl2]\n",
    "day4 = [ day4_dwl]\n",
    "day2 = [day2_va, day2_va_amarel, day2_vl3, day2_vl4, day2_dwl]\n",
    "\n",
    "for i,model_params in enumerate(day1):\n",
    "    instance = debiased_whittle.full_vecc_dw_likelihoods(daily_aggregated_tensors_vecc, daily_hourly_maps_vecc, day_idx=day_idx, params_list=model_params, lat_range=lat_range, lon_range=lon_range)\n",
    "    v = 0.5\n",
    "    nheads = 300\n",
    "    instance.initiate_model_instance_vecchia(v, nns_map, mm_cond_number, nheads)\n",
    "\n",
    "    model_params = [torch.tensor(x, dtype=torch.float64) for x in model_params]\n",
    "\n",
    "    res = instance.likelihood_wrapper(model_params, instance.model_instance.matern_cov_aniso_STABLE_log_reparam, daily_aggregated_tensors_dw, daily_hourly_maps_dw)\n",
    "    #print(res)\n",
    "    print(f' full likelihood: {torch.round(res[0]*nn, decimals=2)},\\n vecchia: {torch.round(res[1]*nn, decimals=2)}, \\n whittle de-biased: {torch.round(res[2], decimals = 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa69600",
   "metadata": {},
   "source": [
    "day1 dwl\n",
    "\n",
    "23184\n",
    "Pre-computing Vecchia batches...\n",
    "Pre-computed 20784 batches.\n",
    " full likelihood: 27665.1,\n",
    " vecchia: 27667.72, \n",
    " whittle de-biased: -8017.94\n",
    "\n",
    "23184\n",
    "Pre-computing Vecchia batches...\n",
    "Pre-computed 20784 batches.\n",
    " full likelihood: 35799.27,\n",
    " vecchia: 35313.67, \n",
    " whittle de-biased: 9220.28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4870a07e",
   "metadata": {},
   "source": [
    "Four days 1 to 3  125 to 129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b27daffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing Vecchia batches...\n",
      "Pre-computed 20784 batches.\n",
      "Vecchia Estimate using Adams\n",
      " full likelihood: 27784.07,\n",
      " vecchia: 27770.61, \n",
      " whittle de-biased: 6023.03\n",
      "Pre-computing Vecchia batches...\n",
      "Pre-computed 20784 batches.\n",
      "Vecchia estimate using L-BFGS\n",
      " full likelihood: 27808.64,\n",
      " vecchia: 27783.2, \n",
      " whittle de-biased: 6062.49\n",
      "Pre-computing Vecchia batches...\n",
      "Pre-computed 20784 batches.\n",
      "debiased_whittle estimate using L-BFGS\n",
      " full likelihood: 27702.8,\n",
      " vecchia: 27701.96, \n",
      " whittle de-biased: 6415.19\n",
      "Pre-computing Vecchia batches...\n",
      "Pre-computed 20784 batches.\n",
      "debiased_whittle estimate using L-BFGS\n",
      " full likelihood: 27431.5,\n",
      " vecchia: 27436.37, \n",
      " whittle de-biased: 5975.43\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "#day1_va = [3.8107, 1.4085, 2.7349, -3.0026, 0.0101, -0.1671, 0.1676] #epoch more than 200\n",
    "lat_range=lat_range_input \n",
    "lon_range=lon_range_input\n",
    "\n",
    "day1_va = [4.22817, 1.664023, 0.481917, -3.77204, 0.02213, -0.16318, -1.737487]\n",
    "day1_vl = [4.2866, 1.7396, 0.4891, -3.777, 0.02048, -0.16411, -12.05573]\n",
    "day1_dwl = [4.2739, 1.8060, 0.7948, -3.3599, 0.0223, -0.1672, -11.8381]\n",
    "\n",
    "day2_va = [3.7634, 1.2864, 0.6458, -4.05860, 0.001777, -0.22191, 0.7242916]\n",
    "day2_vl = [3.7503, 1.2538, 0.6472, -4.09016, 0.001728, -0.222897, 0.73606]\n",
    "day2_dwl =[4.1200, 1.6540, 0.8909, -3.4966, -0.0263, -0.2601, -0.0986]\n",
    "\n",
    "day3_va = [4.61865, 1.86892, 0.54694, -4.21337, -0.04020, -0.24562, -0.7427]\n",
    "day3_vl = [4.4038, 1.6321, 0.50344, -4.3653, -0.0417, -0.2480, 0.2393]\n",
    "day3_dwl = [4.0950, 1.6663, 0.6876, -3.3118, -0.0500, -0.2666, -0.5033]\n",
    "\n",
    "day4_va = [4.1117, 1.6978, 0.7622, -4.0126, 0.028246, -0.14168, -0.27482]\n",
    "day4_vl = [3.962231, 1.4687, 0.7822, -4.0332, 0.03072, -0.14823, 0.0994]\n",
    "day4_dwl = [3.9351, 1.8070, 1.0980, -3.5154, 0.0214, -0.1712, -0.5348]\n",
    "day4_dwl2 = [4.1875, 1.9465, 0.2492, -3.9739, 0.0146, -0.2040, -0.8567]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "day1 = [day1_va, day1_vl, day1_dwl]\n",
    "day2 = [day2_va, day2_vl, day2_dwl]\n",
    "day3 = [day3_va, day3_vl, day3_dwl]\n",
    "day4 = [day4_va, day4_vl, day4_dwl, day4_dwl2]\n",
    "\n",
    "#days = [day1, day2, day3, day4]\n",
    "days = [day4]\n",
    "v = 0.5\n",
    "nheads = 300\n",
    "\n",
    "\n",
    "\n",
    "for day_idx, day in enumerate(days):\n",
    "    for i,model_params in enumerate(day):\n",
    "        instance = debiased_whittle.full_vecc_dw_likelihoods(daily_aggregated_tensors_vecc, daily_hourly_maps_vecc, day_idx=day_idx, params_list=model_params, lat_range=lat_range, lon_range=lon_range)\n",
    "\n",
    "        instance.initiate_model_instance_vecchia(v, nns_map, mm_cond_number, nheads)\n",
    "        model_params = [torch.tensor(x, dtype=torch.float64) for x in model_params]\n",
    "        res = instance.likelihood_wrapper(model_params, instance.model_instance.matern_cov_aniso_STABLE_log_reparam, daily_aggregated_tensors_dw, daily_hourly_maps_dw)\n",
    "        #res = instance.likelihood_wrapper(daily_aggregated_tensors_dw, daily_hourly_maps_dw)\n",
    "        if i==0:\n",
    "            print(f'Vecchia Estimate using Adams')\n",
    "        elif i==1:\n",
    "            print(f'Vecchia estimate using L-BFGS')\n",
    "        else:\n",
    "            print(f'debiased_whittle estimate using L-BFGS')\n",
    "        \n",
    "        print(f' full likelihood: {torch.round(res[0]*nn, decimals=2)},\\n vecchia: {torch.round(res[1]*nn, decimals=2)}, \\n whittle de-biased: {torch.round(res[2], decimals = 2)}')\n",
    "    print(\"-----\")\n",
    "\n",
    "\n",
    "\n",
    "#instance = debiased_whittle.full_vecc_dw_likelihoods(daily_aggregated_tensors, daily_hourly_maps, day_idx=0, params_list=a)\n",
    "#v = 0.5\n",
    "#nheads = 300\n",
    "#instance.initiate_model_instance_vecchia(v, nns_map, mm_cond_number, nheads)\n",
    "#res = instance.likelihood_wrapper()\n",
    "#res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d4b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing Vecchia batches...\n",
      "Pre-computed 21152 batches.\n",
      "Vecchia Estimate using Adams\n",
      " full likelihood: 31058.52,\n",
      " vecchia: 31021.1, \n",
      " whittle de-biased: 6539.2\n",
      "Pre-computing Vecchia batches...\n",
      "Pre-computed 21152 batches.\n",
      "Vecchia estimate using L-BFGS\n",
      " full likelihood: 31145.22,\n",
      " vecchia: 31070.0, \n",
      " whittle de-biased: 6640.06\n",
      "Pre-computing Vecchia batches...\n",
      "Pre-computed 21152 batches.\n",
      "debiased_whittle estimate using L-BFGS\n",
      " full likelihood: 31256.6,\n",
      " vecchia: 31318.44, \n",
      " whittle de-biased: 7426.94\n",
      "Pre-computing Vecchia batches...\n",
      "Pre-computed 21152 batches.\n",
      "debiased_whittle estimate using L-BFGS\n",
      " full likelihood: 31276.31,\n",
      " vecchia: 31284.23, \n",
      " whittle de-biased: 6375.48\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "#day1_va = [3.8107, 1.4085, 2.7349, -3.0026, 0.0101, -0.1671, 0.1676] #epoch more than 200\n",
    "lat_range=lat_range_input \n",
    "lon_range=lon_range_input\n",
    "\n",
    "day1_va = [4.22817, 1.664023, 0.481917, -3.77204, 0.02213, -0.16318, -1.737487]\n",
    "day1_vl = [4.2866, 1.7396, 0.4891, -3.777, 0.02048, -0.16411, -12.05573]\n",
    "day1_dwl = [4.2739, 1.8060, 0.7948, -3.3599, 0.0223, -0.1672, -11.8381]\n",
    "\n",
    "day2_va = [3.7634, 1.2864, 0.6458, -4.05860, 0.001777, -0.22191, 0.7242916]\n",
    "day2_vl = [3.7503, 1.2538, 0.6472, -4.09016, 0.001728, -0.222897, 0.73606]\n",
    "day2_dwl =[4.1200, 1.6540, 0.8909, -3.4966, -0.0263, -0.2601, -0.0986]\n",
    "\n",
    "day3_va = [4.61865, 1.86892, 0.54694, -4.21337, -0.04020, -0.24562, -0.7427]\n",
    "day3_vl = [4.4038, 1.6321, 0.50344, -4.3653, -0.0417, -0.2480, 0.2393]\n",
    "day3_dwl = [4.0950, 1.6663, 0.6876, -3.3118, -0.0500, -0.2666, -0.5033]\n",
    "\n",
    "day4_va = [4.1117, 1.6978, 0.7622, -4.0126, 0.028246, -0.14168, -0.27482]\n",
    "day4_vl = [3.962231, 1.4687, 0.7822, -4.0332, 0.03072, -0.14823, 0.0994]\n",
    "day4_dwl = [3.9351, 1.8070, 1.0980, -3.5154, 0.0214, -0.1712, -0.5348]\n",
    "day4_dwl2 = [4.1875, 1.9465, 0.2492, -3.9739, 0.0146, -0.2040, -0.8567]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "day1 = [day1_va, day1_vl, day1_dwl]\n",
    "day2 = [day2_va, day2_vl, day2_dwl]\n",
    "day3 = [day3_va, day3_vl, day3_dwl]\n",
    "day4 = [day4_va, day4_vl, day4_dwl, day4_dwl2]\n",
    "\n",
    "#days = [day1, day2, day3, day4]\n",
    "days = [day4]\n",
    "v = 0.5\n",
    "nheads = 300\n",
    "\n",
    "for day_idx, day in enumerate(days):\n",
    "    for i,model_params in enumerate(day):\n",
    "        instance = debiased_whittle.full_vecc_dw_likelihoods(daily_aggregated_tensors_vecc, daily_hourly_maps_vecc, day_idx=day_idx, params_list=model_params, lat_range=lat_range, lon_range=lon_range)\n",
    "\n",
    "        instance.initiate_model_instance_vecchia(v, nns_map, mm_cond_number, nheads)\n",
    "        model_params = [torch.tensor(x, dtype=torch.float64) for x in model_params]\n",
    "        res = instance.likelihood_wrapper(model_params, instance.model_instance.matern_cov_aniso_STABLE_log_reparam, daily_aggregated_tensors_dw, daily_hourly_maps_dw)\n",
    "        #res = instance.likelihood_wrapper(daily_aggregated_tensors_dw, daily_hourly_maps_dw)\n",
    "        if i==0:\n",
    "            print(f'Vecchia Estimate using Adams')\n",
    "        elif i==1:\n",
    "            print(f'Vecchia estimate using L-BFGS')\n",
    "        else:\n",
    "            print(f'debiased_whittle estimate using L-BFGS')\n",
    "        \n",
    "        print(f' full likelihood: {torch.round(res[0]*nn, decimals=2)},\\n vecchia: {torch.round(res[1]*nn, decimals=2)}, \\n whittle de-biased: {torch.round(res[2], decimals = 2)}')\n",
    "    print(\"-----\")\n",
    "\n",
    "\n",
    "\n",
    "#instance = debiased_whittle.full_vecc_dw_likelihoods(daily_aggregated_tensors, daily_hourly_maps, day_idx=0, params_list=a)\n",
    "#v = 0.5\n",
    "#nheads = 300\n",
    "#instance.initiate_model_instance_vecchia(v, nns_map, mm_cond_number, nheads)\n",
    "#res = instance.likelihood_wrapper()\n",
    "#res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a67f9",
   "metadata": {},
   "source": [
    "difference data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df2b4e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([142832, 4])\n",
      "142832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.0000e-03,  1.2303e+02,  2.9422e+00,  2.1000e+01],\n",
       "        [ 4.0000e-03,  1.2309e+02,  1.9636e+00,  2.1000e+01],\n",
       "        [ 4.0000e-03,  1.2316e+02, -1.3187e+00,  2.1000e+01],\n",
       "        [ 4.0000e-03,  1.2322e+02, -3.1683e+00,  2.1000e+01],\n",
       "        [ 4.0000e-03,  1.2328e+02, -5.4924e-01,  2.1000e+01],\n",
       "        [ 4.0000e-03,  1.2335e+02,  1.7212e+00,  2.1000e+01],\n",
       "        [ 4.0000e-03,  1.2341e+02,  2.1317e+00,  2.1000e+01],\n",
       "        [ 4.0000e-03,  1.2347e+02,  2.3966e-01,  2.1000e+01],\n",
       "        [ 4.0000e-03,  1.2353e+02,  3.0116e+00,  2.1000e+01],\n",
       "        [ 4.0000e-03,  1.2360e+02, -5.0000e-01,  2.1000e+01],\n",
       "        [ 4.0000e-03,  1.2366e+02, -3.3742e-01,  2.1000e+01],\n",
       "        [ 4.0000e-03,  1.2372e+02, -1.5392e+00,  2.1000e+01],\n",
       "        [ 4.0000e-03,  1.2379e+02,  3.5266e+00,  2.1000e+01],\n",
       "        [ 4.0000e-03,  1.2385e+02, -1.3805e+00,  2.1000e+01],\n",
       "        [ 4.0000e-03,  1.2391e+02,  9.2229e-01,  2.1000e+01],\n",
       "        [ 4.0000e-03,  1.2398e+02, -8.0870e-01,  2.1000e+01],\n",
       "        [ 4.0000e-03,  1.2404e+02, -9.5982e+00,  2.1000e+01],\n",
       "        [ 4.0000e-03,  1.2410e+02, -7.6327e-01,  2.1000e+01],\n",
       "        [ 4.0000e-03,  1.2416e+02,  3.9375e+00,  2.1000e+01],\n",
       "        [ 4.0000e-03,  1.2423e+02, -3.2591e+00,  2.1000e+01]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [11.0474, 0.0623, 0.2445, 1.0972, 0.0101, -0.1671, 1.1825]\n",
    "day = 0 # 0 index\n",
    "lat_range= [0,5]\n",
    "lon_range= [123.0, 133.0]\n",
    "#lat_range= [1,3]\n",
    "#lon_range= [125, 129.0]\n",
    "db = debiased_whittle.debiased_whittle_preprocess(daily_aggregated_tensors_dw, daily_hourly_maps_dw, day_idx=day, params_list=a, lat_range=lat_range, lon_range=lon_range)\n",
    "\n",
    "\n",
    "subsetted_aggregated_day = db.generate_spatially_filtered_days(0,5,123,133)\n",
    "print(subsetted_aggregated_day.shape)\n",
    "N2= subsetted_aggregated_day.shape[0]\n",
    "print(N2)\n",
    "subsetted_aggregated_day[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5739b312",
   "metadata": {},
   "source": [
    "likelihood calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d53690a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Pre-computing J-vector (Hamming taper)...\n",
      "Starting with FIXED params (raw log-scale): [4.2739, 1.806, 0.7948, -3.3599, 0.0223, -0.1672, -11.8381]\n",
      "tensor(36212.0890, dtype=torch.float64, grad_fn=<MulBackward0>) (tensor(36252.7646, dtype=torch.float64, grad_fn=<SumBackward0>), 113, 158)\n"
     ]
    }
   ],
   "source": [
    "dwl = debiased_whittle.debiased_whittle_likelihood()\n",
    "\n",
    "# --- Configuration ---\n",
    "DAY_TO_RUN = 1\n",
    "TAPERING_FUNC = dwl.cgn_hamming # Use Hamming taper\n",
    "NUM_RUNS = 1\n",
    "EPOCHS = 2000\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "DELTA_LAT, DELTA_LON = 0.044, 0.063 \n",
    "\n",
    "LAT_COL, LON_COL = 0, 1\n",
    "VAL_COL = 2 # Spatially differenced value\n",
    "TIME_COL = 3\n",
    "lr = 0.01 \n",
    "\n",
    "cur_df = subsetted_aggregated_day\n",
    "unique_times = torch.unique(cur_df[:, TIME_COL])\n",
    "time_slices_list = [cur_df[cur_df[:, TIME_COL] == t_val] for t_val in unique_times]\n",
    "\n",
    "# --- 1. Pre-compute J-vector, Taper Grid, and Taper Autocorrelation ---\n",
    "print(\"Pre-computing J-vector (Hamming taper)...\")\n",
    "J_vec, n1, n2, p, taper_grid = dwl.generate_Jvector_tapered( \n",
    "    time_slices_list,\n",
    "    tapering_func=TAPERING_FUNC, \n",
    "    lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "I_sample = dwl.calculate_sample_periodogram_vectorized(J_vec)\n",
    "taper_autocorr_grid = dwl.calculate_taper_autocorrelation_fft(taper_grid, n1, n2, DEVICE)\n",
    "\n",
    "\n",
    "init_sigmasq   = 13.059\n",
    "init_range_lat = 0.154 \n",
    "init_range_lon = 0.195 \n",
    "init_nugget    = 0.247\n",
    "init_range_time = 1.28\n",
    "init_advec_lat = 0.0218\n",
    "init_advec_lon = -0.1689\n",
    "\n",
    "\n",
    "init_phi2 = 1.0 / init_range_lon\n",
    "init_phi1 = init_sigmasq * init_phi2\n",
    "init_phi3 = (init_range_lon / init_range_lat)**2\n",
    "init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "initial_params_values = [\n",
    "    np.log(init_phi1),    # [0] log_phi1\n",
    "    np.log(init_phi2),    # [1] log_phi2\n",
    "    np.log(init_phi3),    # [2] log_phi3\n",
    "    np.log(init_phi4),    # [3] log_phi4\n",
    "    init_advec_lat,       # [4] advec_lat (NOT log)\n",
    "    init_advec_lon,       # [5] advec_lon (NOT log)\n",
    "    np.log(init_nugget)   # [6] log_nugget\n",
    "]\n",
    "\n",
    "initial_params_values =[4.2739, 1.8060, 0.7948, -3.3599, 0.0223, -0.1672, -11.8381]\n",
    "print(f\"Starting with FIXED params (raw log-scale): {[round(p, 4) for p in initial_params_values]}\")\n",
    "\n",
    "params_list = [\n",
    "    Parameter(torch.tensor([val], dtype=torch.float64))\n",
    "    for val in initial_params_values\n",
    "]\n",
    "\n",
    "dwnll = dwl.whittle_likelihood_loss_tapered(\n",
    "    params=torch.cat(params_list),\n",
    "    I_sample=I_sample,\n",
    "    n1=n1,\n",
    "    n2=n2,\n",
    "    p_time=p,\n",
    "    taper_autocorr_grid=taper_autocorr_grid,\n",
    "    delta1=DELTA_LAT,\n",
    "    delta2=DELTA_LON\n",
    ")\n",
    "\n",
    "dwnll2 = dwl.whittle_likelihood_loss_tapered_sum(\n",
    "    params=torch.cat(params_list),\n",
    "    I_sample=I_sample,\n",
    "    n1=n1,\n",
    "    n2=n2,\n",
    "    p_time=p,\n",
    "    taper_autocorr_grid=taper_autocorr_grid,\n",
    "    delta1=DELTA_LAT,\n",
    "    delta2=DELTA_LON\n",
    ")\n",
    "\n",
    "print(dwnll* n1* n2, dwnll2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4be82c",
   "metadata": {},
   "source": [
    "debiased whittle optimization adams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0979f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# 6. Main Execution Script (ðŸ’¥ 7-PARAM MULTIVARIATE ðŸ’¥)\n",
    "# =========================================================================\n",
    "\n",
    "dwl = debiased_whittle.debiased_whittle_likelihood()\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    DAY_TO_RUN = 1\n",
    "    TAPERING_FUNC = dwl.cgn_hamming # Use Hamming taper\n",
    "    NUM_RUNS = 1\n",
    "    EPOCHS = 200\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063 \n",
    "\n",
    "    LAT_COL, LON_COL = 0, 1\n",
    "    VAL_COL = 2 # Spatially differenced value\n",
    "    TIME_COL = 3\n",
    "    lr = 0.1 \n",
    "\n",
    "    # --- Load Spatially Differenced Data ---\n",
    "\n",
    "    cur_df = subsetted_aggregated_day\n",
    "    \n",
    "\n",
    "    # --- 1. Pre-compute J-vector, Taper Grid, and Taper Autocorrelation ---\n",
    "    print(\"Pre-computing J-vector (Hamming taper)...\")\n",
    "    J_vec, n1, n2, p, taper_grid = dwl.generate_Jvector_tapered( \n",
    "        time_slices_list,\n",
    "        tapering_func=TAPERING_FUNC, \n",
    "        lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if J_vec is None or J_vec.numel() == 0 or n1 == 0 or n2 == 0 or p == 0:\n",
    "       print(f\"Error: J-vector generation failed for Day {DAY_TO_RUN}.\")\n",
    "       exit()\n",
    "       \n",
    "    print(\"Pre-computing sample periodogram...\")\n",
    "    I_sample = dwl.calculate_sample_periodogram_vectorized(J_vec)\n",
    "\n",
    "    print(\"Pre-computing Hamming taper autocorrelation...\")\n",
    "    taper_autocorr_grid = dwl.calculate_taper_autocorrelation_fft(taper_grid, n1, n2, DEVICE)\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Error: NaN/Inf in sample periodogram.\")\n",
    "        exit()\n",
    "    if torch.isnan(taper_autocorr_grid).any() or torch.isinf(taper_autocorr_grid).any():\n",
    "        print(\"Error: NaN/Inf in taper autocorrelation.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Data grid: {n1}x{n2}, {p} time points. J-vector, Periodogram, Taper Autocorr on {DEVICE}.\")\n",
    "\n",
    "    # --- 2. Optimization Loop ---\n",
    "    all_final_results = []\n",
    "    all_final_losses = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        print(f\"\\n{'='*30} Initialization Run {i+1}/{NUM_RUNS} {'='*30}\")\n",
    "\n",
    "        # --- 7-PARAMETER initialization ---\n",
    "        ''' \n",
    "        init_sigmasq   = 15.0\n",
    "        init_range_lat = 0.66 \n",
    "        init_range_lon = 0.7 \n",
    "        init_nugget    = 1.5\n",
    "        init_beta      = 0.1  # Temporal range ratio\n",
    "        init_advec_lat = 0.02\n",
    "        init_advec_lon = -0.08\n",
    "        '''\n",
    "        init_sigmasq   = 13.059\n",
    "        init_range_lat = 0.154 \n",
    "        init_range_lon = 0.195 \n",
    "        init_nugget    = 1.247\n",
    "        init_range_time = 1.28\n",
    "        init_advec_lat = 0.0218\n",
    "        init_advec_lon = -0.1689\n",
    "\n",
    "\n",
    "        \n",
    "        init_phi2 = 1.0 / init_range_lon\n",
    "        init_phi1 = init_sigmasq * init_phi2\n",
    "        init_phi3 = (init_range_lon / init_range_lat)**2\n",
    "        init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "        initial_params_values = [\n",
    "            np.log(init_phi1),    # [0] log_phi1\n",
    "            np.log(init_phi2),    # [1] log_phi2\n",
    "            np.log(init_phi3),    # [2] log_phi3\n",
    "            np.log(init_phi4),    # [3] log_phi4\n",
    "            init_advec_lat,       # [4] advec_lat (NOT log)\n",
    "            init_advec_lon,       # [5] advec_lon (NOT log)\n",
    "            np.log(init_nugget)   # [6] log_nugget\n",
    "        ]\n",
    "        \n",
    "        print(f\"Starting with FIXED params (raw log-scale): {[round(p, 4) for p in initial_params_values]}\")\n",
    "\n",
    "        params_list = [\n",
    "            Parameter(torch.tensor([val], dtype=torch.float32))\n",
    "            for val in initial_params_values\n",
    "        ]\n",
    "\n",
    "\n",
    "        optimizer = torch.optim.Adam(params_list, lr=lr)\n",
    "\n",
    "        # --- ðŸ’¥ REVISED: Use Plateau Scheduler ðŸ’¥ ---\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=10, # Wait 10 epochs for improvement\n",
    "            verbose=True\n",
    "        )\n",
    "        # --- END REVISION ---\n",
    "\n",
    "        print(f\"Starting optimization run {i+1} on device {DEVICE} (Hamming, 7-param ST kernel)...\")\n",
    "\n",
    "        nat_params_str, phi_params_str, raw_params_str, loss, epochs_run = dwl.run_full_tapered(\n",
    "            params_list=params_list,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            I_sample=I_sample,\n",
    "            n1=n1, n2=n2, p_time=p,\n",
    "            taper_autocorr_grid=taper_autocorr_grid, \n",
    "            epochs=EPOCHS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        \n",
    "        if loss is not None:\n",
    "            all_final_results.append((nat_params_str, phi_params_str, raw_params_str))\n",
    "            all_final_losses.append(loss)\n",
    "        else:\n",
    "            all_final_losses.append(float('inf'))\n",
    "\n",
    "    # --- ðŸ’¥ REVISED: Corrected f-string ðŸ’¥ ---\n",
    "    print(f\"\\n\\n{'='*25} Overall Result from Run {'='*25} {'='*25}\")\n",
    "    # --- END REVISION ---\n",
    "    \n",
    "    valid_losses = [l for l in all_final_losses if l is not None and l != float('inf')]\n",
    "\n",
    "    if not valid_losses:\n",
    "        print(f\"The run failed or resulted in an invalid loss for Day {DAY_TO_RUN}.\")\n",
    "    else:\n",
    "        best_loss = min(valid_losses)\n",
    "        best_run_index = all_final_losses.index(best_loss)\n",
    "        best_results = all_final_results[best_run_index]\n",
    "        \n",
    "        print(f\"Best Run Loss: {best_loss} (after {epochs_run} epochs)\")\n",
    "        print(f\"Final Parameters (Natural Scale): {best_results[0]}\")\n",
    "        print(f\"Final Parameters (Phi Scale)    : {best_results[1]}\")\n",
    "        print(f\"Final Parameters (Raw Log Scale): {best_results[2]}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2049ecc",
   "metadata": {},
   "source": [
    "debiased whittle optimization lbfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4f4bcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Pre-computing J-vector (Hamming taper)...\n",
      "Pre-computing sample periodogram...\n",
      "Pre-computing Hamming taper autocorrelation...\n",
      "Data grid: 113x158, 8 time points. J-vector, Periodogram, Taper Autocorr on cpu.\n",
      "\n",
      "============================== Initialization Run 1/1 ==============================\n",
      "Starting with FIXED params (raw log-scale): [4.2042, 1.6348, 0.4721, -2.5562, 0.0218, -0.1689, -1.3984]\n",
      "Starting optimization run 1 on device cpu (Hamming, 7-param ST kernel, L-BFGS)...\n",
      "--- Step 1/20 ---\n",
      " Loss: 1.254618 | Max Grad: 2.150713e-02\n",
      "  Params (Raw Log): log_phi1: 4.2025, log_phi2: 1.9605, log_phi3: 0.2627, log_phi4: -3.9885, advec_lat: 0.0152, advec_lon: -0.2036, log_nugget: -0.9763\n",
      "--- Step 2/20 ---\n",
      " Loss: 1.163201 | Max Grad: 6.975980e-06\n",
      "  Params (Raw Log): log_phi1: 4.1875, log_phi2: 1.9465, log_phi3: 0.2492, log_phi4: -3.9739, advec_lat: 0.0146, advec_lon: -0.2040, log_nugget: -0.8567\n",
      "--- Step 3/20 ---\n",
      " Loss: 1.163044 | Max Grad: 6.975980e-06\n",
      "  Params (Raw Log): log_phi1: 4.1875, log_phi2: 1.9465, log_phi3: 0.2492, log_phi4: -3.9739, advec_lat: 0.0146, advec_lon: -0.2040, log_nugget: -0.8567\n",
      "--- Step 4/20 ---\n",
      " Loss: 1.163044 | Max Grad: 6.975980e-06\n",
      "  Params (Raw Log): log_phi1: 4.1875, log_phi2: 1.9465, log_phi3: 0.2492, log_phi4: -3.9739, advec_lat: 0.0146, advec_lon: -0.2040, log_nugget: -0.8567\n",
      "\n",
      "--- Converged on gradient norm (max|grad| < 1e-05) at step 4 ---\n",
      "\n",
      "--- Training Complete ---\n",
      "\n",
      "FINAL BEST STATE ACHIEVED (during training):\n",
      "Best Loss: 1.163\n",
      "\n",
      "\n",
      "========================= Overall Result from Run ========================= =========================\n",
      "Best Run Loss: 1.163 (after 4 steps)\n",
      "Final Parameters (Natural Scale): sigmasq: 9.4033, range_lat: 0.1261, range_lon: 0.1428, range_time: 1.0413, advec_lat: 0.0146, advec_lon: -0.2040, nugget: 0.4246\n",
      "Final Parameters (Phi Scale)    : phi1: 65.8605, phi2: 7.0040, phi3: 1.2830, phi4: 0.0188, advec_lat: 0.0146, advec_lon: -0.2040, nugget: 0.4246\n",
      "Final Parameters (Raw Log Scale): log_phi1: 4.1875, log_phi2: 1.9465, log_phi3: 0.2492, log_phi4: -3.9739, advec_lat: 0.0146, advec_lon: -0.2040, log_nugget: -0.8567\n",
      "\n",
      "Total execution time: 46.20 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dwl = debiased_whittle.debiased_whittle_likelihood()\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    DAY_TO_RUN = 4 # data is decided above\n",
    "    TAPERING_FUNC = dwl.cgn_hamming # Use Hamming taper\n",
    "    NUM_RUNS = 1\n",
    "    MAX_STEPS = 20 # L-BFGS usually converges in far fewer steps\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063 \n",
    "\n",
    "    LAT_COL, LON_COL = 0, 1\n",
    "    VAL_COL = 2 # Spatially differenced value\n",
    "    TIME_COL = 3\n",
    "\n",
    "\n",
    "    cur_df =subsetted_aggregated_day\n",
    "    \n",
    "    if cur_df.numel() == 0 or cur_df.shape[1] <= max(LAT_COL, LON_COL, VAL_COL, TIME_COL):\n",
    "        print(f\"Error: Data for Day {DAY_TO_RUN} is empty or invalid.\")\n",
    "        exit()\n",
    "\n",
    "    unique_times = torch.unique(cur_df[:, TIME_COL])\n",
    "    time_slices_list = [cur_df[cur_df[:, TIME_COL] == t_val] for t_val in unique_times]\n",
    "\n",
    "    # --- 1. Pre-compute J-vector, Taper Grid, and Taper Autocorrelation ---\n",
    "    print(\"Pre-computing J-vector (Hamming taper)...\")\n",
    "    \n",
    "    # --- ðŸ’¥ REVISED: Renamed 'p' to 'p_time' ðŸ’¥ ---\n",
    "    J_vec, n1, n2, p_time, taper_grid = dwl.generate_Jvector_tapered( \n",
    "        time_slices_list,\n",
    "        tapering_func=TAPERING_FUNC, \n",
    "        lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if J_vec is None or J_vec.numel() == 0 or n1 == 0 or n2 == 0 or p_time == 0:\n",
    "       print(f\"Error: J-vector generation failed for Day {DAY_TO_RUN}.\")\n",
    "       exit()\n",
    "       \n",
    "    print(\"Pre-computing sample periodogram...\")\n",
    "    I_sample = dwl.calculate_sample_periodogram_vectorized(J_vec)\n",
    "\n",
    "    print(\"Pre-computing Hamming taper autocorrelation...\")\n",
    "    taper_autocorr_grid = dwl.calculate_taper_autocorrelation_fft(taper_grid, n1, n2, DEVICE)\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Error: NaN/Inf in sample periodogram.\")\n",
    "        exit()\n",
    "    if torch.isnan(taper_autocorr_grid).any() or torch.isinf(taper_autocorr_grid).any():\n",
    "        print(\"Error: NaN/Inf in taper autocorrelation.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Data grid: {n1}x{n2}, {p_time} time points. J-vector, Periodogram, Taper Autocorr on {DEVICE}.\")\n",
    "    # --- END REVISION ---\n",
    "\n",
    "    # --- 2. Optimization Loop ---\n",
    "    all_final_results = []\n",
    "    all_final_losses = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        print(f\"\\n{'='*30} Initialization Run {i+1}/{NUM_RUNS} {'='*30}\")\n",
    "\n",
    "        # --- 7-PARAMETER initialization ---\n",
    "        ''' \n",
    "        init_sigmasq   = 15.0\n",
    "        init_range_lat = 0.66 \n",
    "        init_range_lon = 0.7 \n",
    "        init_nugget    = 1.5\n",
    "        init_beta      = 0.1  # Temporal range ratio\n",
    "        init_advec_lat = 0.02\n",
    "        init_advec_lon = -0.08\n",
    "        '''\n",
    "        init_sigmasq   = 13.059\n",
    "        init_range_lat = 0.154 \n",
    "        init_range_lon = 0.195\n",
    "        init_advec_lat = 0.0218\n",
    "        init_range_time = 0.7\n",
    "        init_advec_lon = -0.1689\n",
    "        init_nugget    = 0.247\n",
    "\n",
    "        init_phi2 = 1.0 / init_range_lon\n",
    "        init_phi1 = init_sigmasq * init_phi2\n",
    "        init_phi3 = (init_range_lon / init_range_lat)**2\n",
    "        # Change needed to match the spatial-temporal distance formula:\n",
    "        init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "        initial_params_values = [\n",
    "            np.log(init_phi1),    # [0] log_phi1\n",
    "            np.log(init_phi2),    # [1] log_phi2\n",
    "            np.log(init_phi3),    # [2] log_phi3\n",
    "            np.log(init_phi4),    # [3] log_phi4\n",
    "            init_advec_lat,       # [4] advec_lat (NOT log)\n",
    "            init_advec_lon,       # [5] advec_lon (NOT log)\n",
    "            np.log(init_nugget)   # [6] log_nugget\n",
    "        ]\n",
    "        \n",
    "        print(f\"Starting with FIXED params (raw log-scale): {[round(p, 4) for p in initial_params_values]}\")\n",
    "\n",
    "        params_list = [\n",
    "            Parameter(torch.tensor([val], dtype=torch.float64))\n",
    "            for val in initial_params_values\n",
    "        ]\n",
    "\n",
    "        # Helper to define the boundary globally for clarity\n",
    "        NUGGET_LOWER_BOUND = 0.05\n",
    "        LOG_NUGGET_LOWER_BOUND = np.log(NUGGET_LOWER_BOUND) # Approx -2.9957\n",
    "\n",
    "        # --- ðŸ’¥ REVISED: Use L-BFGS Optimizer ðŸ’¥ ---\n",
    "        optimizer = torch.optim.LBFGS(\n",
    "            params_list,\n",
    "            lr=1.0,           # Initial step length for line search\n",
    "            max_iter=20,      # Iterations per step\n",
    "            history_size=100,\n",
    "            line_search_fn=\"strong_wolfe\", # Often more robust\n",
    "            tolerance_grad=1e-5\n",
    "        )\n",
    "        # --- END REVISION ---\n",
    "\n",
    "        print(f\"Starting optimization run {i+1} on device {DEVICE} (Hamming, 7-param ST kernel, L-BFGS)...\")\n",
    "        \n",
    "        # --- ðŸ’¥ REVISED: Call L-BFGS trainer, pass p_time ðŸ’¥ ---\n",
    "        nat_params_str, phi_params_str, raw_params_str, loss, steps_run = dwl.run_lbfgs_tapered(\n",
    "            params_list=params_list,\n",
    "            optimizer=optimizer,\n",
    "            I_sample=I_sample,\n",
    "            n1=n1, n2=n2, p_time=p_time,\n",
    "            taper_autocorr_grid=taper_autocorr_grid, \n",
    "            max_steps=MAX_STEPS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        # --- END REVISION ---\n",
    "        \n",
    "        if loss is not None:\n",
    "            all_final_results.append((nat_params_str, phi_params_str, raw_params_str))\n",
    "            all_final_losses.append(loss)\n",
    "        else:\n",
    "            all_final_losses.append(float('inf'))\n",
    "\n",
    "    print(f\"\\n\\n{'='*25} Overall Result from Run {'='*25} {'='*25}\")\n",
    "    \n",
    "    valid_losses = [l for l in all_final_losses if l is not None and l != float('inf')]\n",
    "\n",
    "    if not valid_losses:\n",
    "        print(f\"The run failed or resulted in an invalid loss for Day {DAY_TO_RUN}.\")\n",
    "    else:\n",
    "        best_loss = min(valid_losses)\n",
    "        best_run_index = all_final_losses.index(best_loss)\n",
    "        best_results = all_final_results[best_run_index]\n",
    "        \n",
    "        print(f\"Best Run Loss: {best_loss} (after {steps_run} steps)\")\n",
    "        print(f\"Final Parameters (Natural Scale): {best_results[0]}\")\n",
    "        print(f\"Final Parameters (Phi Scale)    : {best_results[1]}\")\n",
    "        print(f\"Final Parameters (Raw Log Scale): {best_results[2]}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
