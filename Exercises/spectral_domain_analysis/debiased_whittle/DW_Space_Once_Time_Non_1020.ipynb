{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "31170d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy                    # clone tensor\n",
    "import time\n",
    "\n",
    "# Custom imports\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels\n",
    "\n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "import typer\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO import configuration as config\n",
    "from GEMS_TCO import data_preprocess as dmbh\n",
    "\n",
    "import os\n",
    "from sklearn.neighbors import BallTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9f76296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load data from: /Users/joonwonlee/Documents/GEMS_DATA/pickle_2024/coarse_cen_map_without_decrement_latitude24_07.pkl\n",
      "\n",
      "Data loaded successfully! ✅\n",
      "Type of loaded data: <class 'dict'>\n",
      "Number of entries (hours) in the map: 248\n",
      "Example keys: ['y24m07day01_hm00:53', 'y24m07day01_hm01:53', 'y24m07day01_hm02:53', 'y24m07day01_hm03:53', 'y24m07day01_hm04:49']\n",
      "270\n",
      "273\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "# Assume your 'config' object is available\n",
    "# import config\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# Specify the year and month you want to load\n",
    "YEAR_TO_LOAD = 2024\n",
    "MONTH_TO_LOAD = 7\n",
    "\n",
    "# Use the same base path as your saving script\n",
    "BASE_PATH = config.mac_data_load_path\n",
    "\n",
    "# --- 2. Construct the File Path ---\n",
    "# This must exactly match the naming convention from your saving script\n",
    "month_str = f\"{MONTH_TO_LOAD:02d}\"\n",
    "pickle_path = os.path.join(BASE_PATH, f'pickle_{YEAR_TO_LOAD}')\n",
    "filename = f\"coarse_cen_map_without_decrement_latitude{str(YEAR_TO_LOAD)[2:]}_{month_str}.pkl\"\n",
    "filepath_to_load = os.path.join(pickle_path, filename)\n",
    "\n",
    "print(f\"Attempting to load data from: {filepath_to_load}\")\n",
    "\n",
    "# --- 3. Load the Data ---\n",
    "try:\n",
    "    with open(filepath_to_load, 'rb') as pickle_file:\n",
    "        # Use pickle.load() to read the data from the file\n",
    "        loaded_coarse_map = pickle.load(pickle_file)\n",
    "    \n",
    "    print(\"\\nData loaded successfully! ✅\")\n",
    "    \n",
    "    # --- 4. Verify the Loaded Data ---\n",
    "    # The loaded data is a dictionary. Let's inspect it.\n",
    "    print(f\"Type of loaded data: {type(loaded_coarse_map)}\")\n",
    "    if isinstance(loaded_coarse_map, dict):\n",
    "        print(f\"Number of entries (hours) in the map: {len(loaded_coarse_map)}\")\n",
    "        # Print the first 5 keys to see what they look like\n",
    "        first_five_keys = list(loaded_coarse_map.keys())[:5]\n",
    "        print(f\"Example keys: {first_five_keys}\")\n",
    "        \n",
    "        # You can now access the data for a specific hour, for example:\n",
    "        # first_hour_data = loaded_coarse_map[first_five_keys[0]]\n",
    "        # print(f\"\\nData for first hour is a tensor of shape: {first_hour_data.shape}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nError: File not found. Please check if the file exists at the specified path.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")\n",
    "\n",
    "\n",
    "print(loaded_coarse_map['y24m07day01_hm00:53']['Longitude'].nunique())\n",
    "print(loaded_coarse_map['y24m07day01_hm00:53']['Latitude'].nunique())\n",
    "\n",
    "import GEMS_TCO\n",
    "load_data_instance = GEMS_TCO.load_data('')\n",
    "\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "for i in range(31):\n",
    "    cur_map, cur_df =load_data_instance.load_working_data_byday_wo_mm(loaded_coarse_map,[i*8, (i+1)*8])\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "45723d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joonwonlee/Documents/GEMS_DATA/pickle_2024/orbit_map24_07.pkl\n"
     ]
    }
   ],
   "source": [
    "lon_s = 123\n",
    "lon_e = 133\n",
    "step_lat = 0.044\n",
    "step_lon = 0.063\n",
    "\n",
    "lat_coords = np.arange( 5 -0.044- 0.0002, 0 -0.044, -0.044)\n",
    "lon_coords = np.arange( lon_e-step_lon- 0.0002, lon_s-step_lon, -step_lon)\n",
    "\n",
    "# Apply the shift as in the original code\n",
    "# These are the unique lat/lon values for the \"center_points\" grid\n",
    "final_lat_values = lat_coords + step_lat \n",
    "final_lon_values = lon_coords + step_lon \n",
    "\n",
    "# Create 2D grid with broadcasting\n",
    "#decrement = 0.00012\n",
    "decrement = 0 \n",
    "lat_grid = final_lat_values[:, None] + np.arange(len(final_lon_values)) * decrement  # shape: (228, 152)\n",
    "\n",
    "\n",
    "mac_data_path = config.mac_data_load_path\n",
    "years = [2024]  # years = [2023,2024]\n",
    "months = list( range(7,8))\n",
    "year = years[0]\n",
    "month = months[0]\n",
    "month_str = f\"{month:02d}\"  \n",
    "filename = f\"pickle_2024/orbit_map{str(year)[2:]}_{month_str}.pkl\"\n",
    "picklefile_path = Path(mac_data_path) / filename\n",
    "print(picklefile_path)\n",
    "\n",
    "with open(picklefile_path, 'rb') as pickle_file:\n",
    "    data_map_hour = pickle.load(pickle_file)\n",
    "\n",
    "# Base file path and settings\n",
    "# base_path = \"C:\\\\Users\\\\joonw\\\\TCO\\\\GEMS_data\"    MSI notebook\n",
    "\n",
    "mac_data_path = config.mac_data_load_path\n",
    "lat_start, lat_end, lon_start, lon_end = 0, 5, 123, 133\n",
    "step_lat, step_lon = 0.044, 0.063\n",
    "\n",
    "# df = pd.read_csv(\"C:\\\\Users\\\\joonw\\\\TCO\\\\GEMS_data\\\\data_2024\\\\data_24_07_0131_N510_E110120.csv\")  MSI notebook\n",
    "df = pd.read_csv(\"/Users/joonwonlee/Documents/GEMS_DATA/data_2024/data_24_07_0131_N05_E123133.csv\")  # MAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7abfaa",
   "metadata": {},
   "source": [
    "# before preceding, change cur_map, _ = load_data_instance.load_working_data_byday_wo_mm(cbmap_ori, [i*8, i*8+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "13ee87b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/joonwonlee/Documents/GEMS_DATA/pickle_2024/coarse_cen_map_without_decrement_latitude24_07.pkl\n",
      "Loaded 31 days of raw data.\n",
      "\n",
      "--- Results ---\n",
      "Number of final spatially-differenced day tensors: 31\n",
      "Processed data saved to spatial_first_difference_data.pkl\n",
      "\n",
      "Shape of the first final tensor: torch.Size([17854, 4])\n",
      "First final tensor head:\n",
      "tensor([[ 4.0000e-03,  1.2303e+02,  2.9422e+00,  2.1000e+01],\n",
      "        [ 4.0000e-03,  1.2309e+02,  1.9636e+00,  2.1000e+01],\n",
      "        [ 4.0000e-03,  1.2316e+02, -1.3187e+00,  2.1000e+01],\n",
      "        [ 4.0000e-03,  1.2322e+02, -3.1683e+00,  2.1000e+01],\n",
      "        [ 4.0000e-03,  1.2328e+02, -5.4922e-01,  2.1000e+01]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Assume GEMS_TCO is a custom class/module you have available\n",
    "# from your_project import GEMS_TCO\n",
    "\n",
    "# =========================================================================\n",
    "# 1. Helper Functions\n",
    "# =========================================================================\n",
    "\n",
    "def subset_tensor(df_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Subsets a tensor to a specific lat/lon range.\"\"\"\n",
    "    #lat_mask = (df_tensor[:, 0] >= -5) & (df_tensor[:, 0] <= 6.3)\n",
    "    #lon_mask = (df_tensor[:, 1] >= 118) & (df_tensor[:, 1] <= 134.2)\n",
    "    lat_mask = (df_tensor[:, 0] >= 0) & (df_tensor[:, 0] <= 5)\n",
    "    lon_mask = (df_tensor[:, 1] >= 123) & (df_tensor[:, 1] <= 133)\n",
    "\n",
    "    df_sub = df_tensor[lat_mask & lon_mask].clone()\n",
    "    return df_sub\n",
    "\n",
    "def apply_first_difference_2d_tensor(df_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies a 2D first-order difference filter using convolution.\n",
    "    This approximates Z(s) = [X(s+d_lat) - X(s)] + [X(s+d_lon) - X(s)].\n",
    "    \"\"\"\n",
    "    if df_tensor.size(0) == 0:\n",
    "        return torch.empty(0, 4)\n",
    "\n",
    "    # 1. Get grid dimensions and validate\n",
    "    unique_lats = torch.unique(df_tensor[:, 0])\n",
    "    unique_lons = torch.unique(df_tensor[:, 1])\n",
    "    lat_count, lon_count = unique_lats.size(0), unique_lons.size(0)\n",
    "\n",
    "    if df_tensor.size(0) != lat_count * lon_count:\n",
    "        raise ValueError(\"Tensor size does not match grid dimensions. Must be a complete grid.\")\n",
    "    if lat_count < 2 or lon_count < 2:\n",
    "        return torch.empty(0, 4)\n",
    "\n",
    "    # 2. Reshape data and define the correct kernel\n",
    "    ozone_data = df_tensor[:, 2].reshape(1, 1, lat_count, lon_count)\n",
    "    \n",
    "    # ✅ CORRECT KERNEL: This kernel results in the standard first-order difference:\n",
    "    # Z(i,j) = X(i+1,j) + X(i,j+1) - 2*X(i,j)\n",
    "    # Note: F.conv2d in PyTorch actually performs cross-correlation. To get a true\n",
    "    # convolution result, the kernel would need to be flipped. However, for a \n",
    "    # forward difference operator, defining the kernel for cross-correlation is more direct.\n",
    "    # The kernel below is designed for cross-correlation to achieve the desired differencing.\n",
    "    diff_kernel = torch.tensor([[[[-2., 1.],\n",
    "                                  [ 1., 0.]]]], dtype=torch.float32)\n",
    "\n",
    "    # 3. Apply convolution (which acts as cross-correlation)\n",
    "    filtered_grid = F.conv2d(ozone_data, diff_kernel, padding='valid').squeeze()\n",
    "\n",
    "    # 4. Determine coordinates for the new, smaller grid\n",
    "    # The new grid corresponds to the anchor points of the kernel\n",
    "    new_lats = unique_lats[:-1]\n",
    "    new_lons = unique_lons[:-1]\n",
    "\n",
    "    # 5. Reconstruct the output tensor\n",
    "    new_lat_grid, new_lon_grid = torch.meshgrid(new_lats, new_lons, indexing='ij')\n",
    "    filtered_values = filtered_grid.flatten()\n",
    "    time_value = df_tensor[0, 3].repeat(filtered_values.size(0))\n",
    "\n",
    "    new_tensor = torch.stack([\n",
    "        new_lat_grid.flatten(),\n",
    "        new_lon_grid.flatten(),\n",
    "        filtered_values,\n",
    "        time_value\n",
    "    ], dim=1)\n",
    "    \n",
    "    return new_tensor\n",
    "\n",
    "# =========================================================================\n",
    "# 2. Data Loading (Unchanged)\n",
    "# =========================================================================\n",
    "# ⚠️ NOTE: You must define these variables\n",
    "# mac_data_path = \"...\"\n",
    "# year = 2022\n",
    "# month_str = \"01\"\n",
    "# class GEMS_TCO: # Placeholder\n",
    "#     def load_data(self, path): return self\n",
    "#     def load_working_data_byday_wo_mm(self, data, indices):\n",
    "#         return {'key': torch.randn(100, 4)}, torch.randn(100, 4)\n",
    "\n",
    "pickle_path = os.path.join(mac_data_path, f'pickle_{year}')\n",
    "output_filename = f\"coarse_cen_map_without_decrement_latitude{str(year)[2:]}_{month_str}.pkl\"\n",
    "output_filepath = os.path.join(pickle_path, output_filename)\n",
    "print(f\"Loading data from: {output_filepath}\")\n",
    "with open(output_filepath, 'rb') as pickle_file:\n",
    "    cbmap_ori = pickle.load(pickle_file)\n",
    "\n",
    "load_data_instance = GEMS_TCO.load_data('')\n",
    "df_day_map_list = []\n",
    "for i in range(31): # Adjust if necessary\n",
    "\n",
    "\n",
    "    \n",
    "    #cur_map, _ = load_data_instance.load_working_data_byday_wo_mm(cbmap_ori, [i*8, (i+1)*8])\n",
    "\n",
    "    # hour of data\n",
    "    cur_map, _ = load_data_instance.load_working_data_byday_wo_mm(cbmap_ori, [i*8, i*8+1])\n",
    "    df_day_map_list.append(cur_map)\n",
    "print(f\"Loaded {len(df_day_map_list)} days of raw data.\")\n",
    "\n",
    "# =========================================================================\n",
    "# 3. Main Processing Loop (Unchanged)\n",
    "# =========================================================================\n",
    "spatially_filtered_days = []\n",
    "for day_idx, day_map in enumerate(df_day_map_list):\n",
    "    tensors_to_aggregate = []\n",
    "    for key, tensor in day_map.items():\n",
    "        subsetted = subset_tensor(tensor)\n",
    "        if subsetted.size(0) > 0:\n",
    "            try:\n",
    "                diff_applied = apply_first_difference_2d_tensor(subsetted)\n",
    "                if diff_applied.size(0) > 0:\n",
    "                    tensors_to_aggregate.append(diff_applied)\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping data chunk on day {day_idx+1} due to error: {e}\")\n",
    "\n",
    "    if tensors_to_aggregate:\n",
    "        aggregated_day_tensor = torch.cat(tensors_to_aggregate, dim=0)\n",
    "        spatially_filtered_days.append(aggregated_day_tensor)\n",
    "\n",
    "# =========================================================================\n",
    "# 4. Verification (Unchanged)\n",
    "# =========================================================================\n",
    "print(\"\\n--- Results ---\")\n",
    "print(f\"Number of final spatially-differenced day tensors: {len(spatially_filtered_days)}\")\n",
    "if spatially_filtered_days:\n",
    "    # Save the processed data for the next script\n",
    "    processed_output_path = \"spatial_first_difference_data.pkl\"\n",
    "    with open(processed_output_path, 'wb') as f:\n",
    "        pickle.dump(spatially_filtered_days, f)\n",
    "    print(f\"Processed data saved to {processed_output_path}\")\n",
    "\n",
    "    print(f\"\\nShape of the first final tensor: {spatially_filtered_days[0].shape}\")\n",
    "    print(\"First final tensor head:\")\n",
    "    print(spatially_filtered_days[0][:5])\n",
    "else:\n",
    "    print(\"\\nNo final differenced tensors were created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a4337",
   "metadata": {},
   "source": [
    "## 10/20/25. No tapering, no approximate autocorrelation of taper by bartlett taper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0eb2e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # Keep if plotting might be added later\n",
    "import cmath\n",
    "import pickle\n",
    "import time # For timing\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import Parameter\n",
    "\n",
    "# =========================================================================\n",
    "# 1. Modeling Functions (Adapted for No Tapering, EXPONENTIAL Kernel)\n",
    "# =========================================================================\n",
    "\n",
    "# --- Bartlett Kernel (Used for c_gn when g_s=1) ---\n",
    "def cgn_2dbartlett_kernel(u1, u2, n1, n2):\n",
    "    \"\"\"\n",
    "    Computes the 2D Bartlett kernel: Product(1 - |ui|/ni).\n",
    "    This represents c_gn(u) when g_s=1.\n",
    "    \"\"\"\n",
    "    device = u1.device if isinstance(u1, torch.Tensor) else (u2.device if isinstance(u2, torch.Tensor) else torch.device('cpu'))\n",
    "    u1_tensor = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_tensor = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    n1_eff = float(n1) if n1 > 0 else 1.0\n",
    "    n2_eff = float(n2) if n2 > 0 else 1.0\n",
    "    kernel = (1.0 - torch.abs(u1_tensor) / n1_eff) * (1.0 - torch.abs(u2_tensor) / n2_eff)\n",
    "    return torch.clamp(kernel, min=0.0)\n",
    "\n",
    "# --- Covariance of the Original Field X (Corrected EXPONENTIAL Kernel) ---\n",
    "def cov_x_exponential(u1, u2, t, params):\n",
    "    \"\"\"\n",
    "    Computes the spatio-temporal autocovariance of the original, unfiltered process X\n",
    "    using an EXPONENTIAL kernel (sqrt distance in exponent).\n",
    "    Expects log-scale params [0,1,2,6].\n",
    "    \"\"\"\n",
    "    device = params.device # Assuming params is a tensor now\n",
    "    # Ensure inputs are tensors and on correct device\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    log_params_indices = [0, 1, 2, 6]\n",
    "    if torch.isnan(params[log_params_indices]).any() or torch.isinf(params[log_params_indices]).any():\n",
    "         print(\"Warning: NaN/Inf in log-params before exp in cov_x_exponential\")\n",
    "         out_shape = torch.broadcast_shapes(u1_dev.shape, u2_dev.shape, t_dev.shape)\n",
    "         return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "\n",
    "    sigmasq, r_lat, r_lon, nugget = torch.exp(params[log_params_indices])\n",
    "    a_lat, a_lon, beta = params[3], params[4], params[5]\n",
    "\n",
    "    if r_lat < 1e-6 or r_lon < 1e-6:\n",
    "        print(f\"Warning: Very small range detected (r_lat={r_lat:.2e}, r_lon={r_lon:.2e}). Clamping.\")\n",
    "        r_lat = torch.clamp(r_lat, min=1e-6)\n",
    "        r_lon = torch.clamp(r_lon, min=1e-6)\n",
    "\n",
    "    # Calculate Distance D for Exponential kernel\n",
    "    x1 = u1_dev / r_lat - a_lat * t_dev\n",
    "    x2 = u2_dev / r_lon - a_lon * t_dev\n",
    "    x3 = beta * t_dev\n",
    "    distance_sq = x1**2 + x2**2 + x3**2\n",
    "    epsilon = 1e-12\n",
    "    # Clamp before sqrt and add epsilon inside for stability\n",
    "    distance_sq_clamped = torch.clamp(distance_sq, min=0.0)\n",
    "    D = torch.sqrt(distance_sq_clamped + epsilon) # Use sqrt distance D\n",
    "\n",
    "    # Calculate covariance C_X = sigmasq * exp(-D)\n",
    "    cov_smooth = sigmasq * torch.exp(-D) # Use D here\n",
    "\n",
    "    is_zero_lag = (torch.abs(u1_dev) < 1e-9) & (torch.abs(u2_dev) < 1e-9) & (torch.abs(t_dev) < 1e-9)\n",
    "    final_cov = torch.where(is_zero_lag, cov_smooth + nugget, cov_smooth)\n",
    "\n",
    "    if torch.isnan(final_cov).any():\n",
    "        print(\"Warning: NaN detected in cov_x_exponential output.\")\n",
    "    return final_cov\n",
    "\n",
    "\n",
    "# --- Covariance of the Spatially Differenced Field Y ---\n",
    "def cov_spatial_difference(u1, u2, t, params, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Calculates covariance Cov(Y(s), Y(s+u)) for the field Y filtered by:\n",
    "    Y(s) = X(s+d1) + X(s+d2) - 2X(s)\n",
    "    Based on the underlying EXPONENTIAL covariance cov_x_exponential.\n",
    "    \"\"\"\n",
    "    weights = {(0, 0): -2.0, (1, 0): 1.0, (0, 1): 1.0}\n",
    "    device = params.device\n",
    "    out_shape = torch.broadcast_shapes(u1.shape if isinstance(u1, torch.Tensor) else (),\n",
    "                                     u2.shape if isinstance(u2, torch.Tensor) else (),\n",
    "                                     t.shape if isinstance(t, torch.Tensor) else ())\n",
    "    cov = torch.zeros(out_shape, device=device, dtype=torch.float32)\n",
    "\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    for (a_idx, b_idx), w_ab in weights.items():\n",
    "        offset_a1 = a_idx * delta1\n",
    "        offset_a2 = b_idx * delta2\n",
    "        for (c_idx, d_idx), w_cd in weights.items():\n",
    "            offset_c1 = c_idx * delta1\n",
    "            offset_c2 = d_idx * delta2\n",
    "            lag_u1 = u1_dev + (offset_a1 - offset_c1)\n",
    "            lag_u2 = u2_dev + (offset_a2 - offset_c2)\n",
    "            # --- Call the EXPONENTIAL version ---\n",
    "            term_cov = cov_x_exponential(lag_u1, lag_u2, t_dev, params)\n",
    "            # ---\n",
    "            if torch.isnan(term_cov).any():\n",
    "                 print(f\"Warning: NaN detected in term_cov within cov_spatial_difference.\")\n",
    "                 return torch.full_like(cov, float('nan'))\n",
    "            cov += w_ab * w_cd * term_cov\n",
    "\n",
    "    if torch.isnan(cov).any():\n",
    "        print(\"Warning: NaN detected in final cov_spatial_difference output.\")\n",
    "    return cov\n",
    "\n",
    "\n",
    "# --- Modified cn_bar for NO TAPERING (uses Bartlett kernel for c_gn) ---\n",
    "def cn_bar_no_taper(u1, u2, t, params, n1, n2, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Computes c_X(u) * c_gn(u) where c_X is cov_spatial_difference (using Exponential base)\n",
    "    and c_gn(u) is the Bartlett kernel (Eq. 16).\n",
    "    \"\"\"\n",
    "    device = params.device\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    # Calculate theoretical covariance of the differenced field Y (based on Exponential X)\n",
    "    cov_X_value = cov_spatial_difference(u1_dev, u2_dev, t_dev, params, delta1, delta2)\n",
    "\n",
    "    # Calculate Bartlett Kernel c_gn(u) for the no-taper case\n",
    "    c_gn_value = cgn_2dbartlett_kernel(u1_dev, u2_dev, n1, n2)\n",
    "\n",
    "    if torch.isnan(cov_X_value).any() or torch.isnan(c_gn_value).any():\n",
    "        print(\"Warning: NaN detected before multiplication in cn_bar_no_taper.\")\n",
    "        out_shape = torch.broadcast_shapes(cov_X_value.shape, c_gn_value.shape)\n",
    "        return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "\n",
    "    result = cov_X_value * c_gn_value\n",
    "    if torch.isnan(result).any():\n",
    "        print(\"Warning: NaN detected after multiplication in cn_bar_no_taper.\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# --- Expected Periodogram (uses cn_bar_no_taper) ---\n",
    "def expected_periodogram_fft_no_taper(params, n1, n2, p, cov_func, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Calculates the expected periodogram I(omega_s) (a pxp matrix in time)\n",
    "    for the spatially differenced process assuming NO data taper (g_s=1).\n",
    "    Uses the provided cov_func (cov_spatial_difference based on Exponential X).\n",
    "    Returns shape [n1_freq, n2_freq, p, p]\n",
    "    \"\"\"\n",
    "    device = params.device if isinstance(params, torch.Tensor) else params[0].device\n",
    "    if isinstance(params, list):\n",
    "        params_tensor = torch.cat([p.to(device) for p in params])\n",
    "    else:\n",
    "        params_tensor = params.to(device)\n",
    "\n",
    "    product_tensor = torch.zeros((n1, n2, p, p), dtype=torch.complex64, device=device)\n",
    "    t_lags = torch.arange(p, dtype=torch.float32, device=device)\n",
    "    u1_mesh_grid, u2_mesh_grid = torch.meshgrid(\n",
    "        torch.arange(n1, dtype=torch.float32, device=device),\n",
    "        torch.arange(n2, dtype=torch.float32, device=device),\n",
    "        indexing='ij'\n",
    "    )\n",
    "\n",
    "    for q in range(p):\n",
    "        for r in range(p):\n",
    "            t_diff = t_lags[q] - t_lags[r]\n",
    "            # Calls cn_bar_no_taper, which internally calls cov_spatial_difference\n",
    "            # based on the EXPONENTIAL cov_x_exponential\n",
    "            cov_times_bartlett = cn_bar_no_taper(\n",
    "                u1_mesh_grid, u2_mesh_grid, t_diff,\n",
    "                params_tensor, n1, n2, delta1, delta2\n",
    "            )\n",
    "            if torch.isnan(cov_times_bartlett).any():\n",
    "                 print(f\"Warning: NaN detected in cov_times_bartlett for t_lag {t_diff.item():.2f}.\")\n",
    "                 product_tensor[:, :, q, r] = float('nan')\n",
    "            else:\n",
    "                 product_tensor[:, :, q, r] = cov_times_bartlett.to(torch.complex64)\n",
    "\n",
    "    if torch.isnan(product_tensor).any():\n",
    "        print(\"Warning: NaN detected in product_tensor before FFT.\")\n",
    "        nan_shape = (n1, n2, p, p)\n",
    "        return torch.full(nan_shape, float('nan'), dtype=torch.complex64, device=device)\n",
    "\n",
    "    fft_result = torch.fft.fft2(product_tensor, dim=(0, 1))\n",
    "    normalization_factor = 1.0 / (4.0 * cmath.pi**2)\n",
    "    result = fft_result * normalization_factor\n",
    "\n",
    "    if torch.isnan(result).any():\n",
    "        print(\"Warning: NaN detected in expected_periodogram_fft_no_taper output after FFT.\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 2. Data Processing (MODIFIED for NO TAPERING)\n",
    "# =========================================================================\n",
    "def generate_Jvector_no_taper(tensor_list, lat_col, lon_col, val_col, device):\n",
    "    \"\"\"\n",
    "    Generates J-vector for a single component assuming g_s=1 (NO taper),\n",
    "    placing result on device.\n",
    "    \"\"\"\n",
    "    p = len(tensor_list)\n",
    "    if p == 0: return torch.empty(0, 0, 0, device=device), 0, 0, 0\n",
    "\n",
    "    valid_tensors = [t for t in tensor_list if t.numel() > 0 and t.shape[1] > max(lat_col, lon_col, val_col)]\n",
    "    if not valid_tensors:\n",
    "         print(\"Warning: No valid tensors found in tensor_list (empty or insufficient columns).\")\n",
    "         return torch.empty(0, 0, 0, device=device), 0, 0, 0\n",
    "\n",
    "    try:\n",
    "        all_lats_cpu = torch.cat([t[:, lat_col] for t in valid_tensors])\n",
    "        all_lons_cpu = torch.cat([t[:, lon_col] for t in valid_tensors])\n",
    "    except IndexError:\n",
    "        print(f\"Error: Invalid column index detected. lat_col={lat_col}, lon_col={lon_col}. Check tensor shapes.\")\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0\n",
    "\n",
    "    all_lats_cpu = all_lats_cpu[~torch.isnan(all_lats_cpu) & ~torch.isinf(all_lats_cpu)]\n",
    "    all_lons_cpu = all_lons_cpu[~torch.isnan(all_lons_cpu) & ~torch.isinf(all_lons_cpu)]\n",
    "    if all_lats_cpu.numel() == 0 or all_lons_cpu.numel() == 0:\n",
    "        print(\"Warning: No valid coordinates found after NaN/Inf filtering.\")\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0\n",
    "\n",
    "    unique_lats_cpu, unique_lons_cpu = torch.unique(all_lats_cpu), torch.unique(all_lons_cpu)\n",
    "    n1, n2 = len(unique_lats_cpu), len(unique_lons_cpu)\n",
    "    if n1 == 0 or n2 == 0:\n",
    "        print(\"Warning: Grid dimensions are zero after finding unique coordinates.\")\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0\n",
    "\n",
    "    lat_map = {lat.item(): i for i, lat in enumerate(unique_lats_cpu)}\n",
    "    lon_map = {lon.item(): i for i, lon in enumerate(unique_lons_cpu)}\n",
    "\n",
    "    fft_results = []\n",
    "    for tensor in tensor_list:\n",
    "        data_grid = torch.zeros((n1, n2), dtype=torch.float32, device=device)\n",
    "        if tensor.numel() > 0 and tensor.shape[1] > max(lat_col, lon_col, val_col):\n",
    "            for row in tensor:\n",
    "                lat_item, lon_item = row[lat_col].item(), row[lon_col].item()\n",
    "                if not (np.isnan(lat_item) or np.isnan(lon_item)):\n",
    "                    i = lat_map.get(lat_item)\n",
    "                    j = lon_map.get(lon_item)\n",
    "                    if i is not None and j is not None:\n",
    "                        val = row[val_col]\n",
    "                        val_num = val.item() if isinstance(val, torch.Tensor) else val\n",
    "                        if not np.isnan(val_num) and not np.isinf(val_num):\n",
    "                           data_grid[i, j] = val_num\n",
    "\n",
    "        if torch.isnan(data_grid).any() or torch.isinf(data_grid).any():\n",
    "             print(\"Warning: NaN/Inf detected in data_grid before FFT. Replacing with zeros.\")\n",
    "             data_grid = torch.nan_to_num(data_grid, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        fft_results.append(torch.fft.fft2(data_grid))\n",
    "\n",
    "    if not fft_results:\n",
    "         print(\"Warning: No FFT results generated.\")\n",
    "         return torch.empty(0, 0, 0, device=device), n1, n2, 0\n",
    "\n",
    "    J_vector_tensor = torch.stack(fft_results, dim=2).to(device)\n",
    "\n",
    "    H = float(n1 * n2)\n",
    "    if H < 1e-9:\n",
    "        print(\"Warning: Normalization factor H is near zero.\")\n",
    "        norm_factor = torch.tensor(0.0, device=device)\n",
    "    else:\n",
    "        norm_factor = (torch.sqrt(torch.tensor(1.0 / H, device=device)) / (2.0 * cmath.pi))\n",
    "\n",
    "    result = J_vector_tensor * norm_factor\n",
    "    if torch.isnan(result).any():\n",
    "        print(\"Warning: NaN detected in J_vector output.\")\n",
    "    return result, n1, n2, p\n",
    "\n",
    "\n",
    "def calculate_sample_periodogram_vectorized(J_vector_tensor):\n",
    "    \"\"\"Calculates sample periodogram I_n = J J^H (pxp matrix for each spatial freq).\"\"\"\n",
    "    if torch.isnan(J_vector_tensor).any() or torch.isinf(J_vector_tensor).any():\n",
    "        print(\"Warning: NaN/Inf detected in J_vector_tensor input.\")\n",
    "        n1, n2, p = J_vector_tensor.shape\n",
    "        return torch.full((n1, n2, p, p), float('nan'), dtype=torch.complex64, device=J_vector_tensor.device)\n",
    "\n",
    "    J_col = J_vector_tensor.unsqueeze(-1)\n",
    "    J_row_conj = J_vector_tensor.unsqueeze(-2).conj()\n",
    "    result = J_col @ J_row_conj\n",
    "\n",
    "    if torch.isnan(result).any():\n",
    "        print(\"Warning: NaN detected in calculate_sample_periodogram_vectorized output.\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 4. Likelihood Calculation (Adapted for No Tapering)\n",
    "# =========================================================================\n",
    "\n",
    "def whittle_likelihood_loss_no_taper(params, I_sample, n1, n2, p, delta1, delta2):\n",
    "    \"\"\"\n",
    "    ✅ Whittle Likelihood Loss assuming NO data taper (g_s=1).\n",
    "    Models a single field (the spatially differenced one).\n",
    "    Uses Exponential kernel based cov_spatial_difference.\n",
    "    \"\"\"\n",
    "    device = I_sample.device\n",
    "    params_tensor = params.to(device)\n",
    "\n",
    "    if torch.isnan(params_tensor).any() or torch.isinf(params_tensor).any():\n",
    "        print(\"Warning: NaN/Inf detected in input parameters to likelihood.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    # Use cov_spatial_difference (based on Exponential X) here\n",
    "    I_expected = expected_periodogram_fft_no_taper(\n",
    "        params_tensor, n1, n2, p, cov_spatial_difference, delta1, delta2\n",
    "    )\n",
    "\n",
    "    if torch.isnan(I_expected).any() or torch.isinf(I_expected).any():\n",
    "        print(\"Warning: NaN/Inf returned from expected_periodogram calculation.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    eye_matrix = torch.eye(p, dtype=torch.complex64, device=device)\n",
    "    # Adaptive diagonal loading based on diagonal magnitude\n",
    "    diag_vals = torch.abs(I_expected.diagonal(dim1=-2, dim2=-1))\n",
    "    # Handle case where p=0 or all diags are zero/nan\n",
    "    mean_diag_abs = diag_vals.mean().item() if diag_vals.numel() > 0 and not torch.isnan(diag_vals).all() else 1.0\n",
    "    diag_load = max(mean_diag_abs * 1e-8, 1e-9) # Ensure at least 1e-9\n",
    "    \n",
    "    I_expected_stable = I_expected + eye_matrix * diag_load\n",
    "\n",
    "    sign, logabsdet = torch.linalg.slogdet(I_expected_stable)\n",
    "    # Check determinant sign and use penalty for bad params\n",
    "    if torch.any(sign.real <= 1e-9):\n",
    "        print(\"Warning: Non-positive determinant encountered. Applying penalty.\")\n",
    "        # Apply penalty: large positive value for log_det_term\n",
    "        log_det_term = torch.where(sign.real > 1e-9, logabsdet, torch.tensor(1e10, device=device))\n",
    "    else:\n",
    "        log_det_term = logabsdet\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Warning: NaN/Inf detected in I_sample input to likelihood.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    try:\n",
    "        solved_term = torch.linalg.solve(I_expected_stable, I_sample)\n",
    "        trace_term = torch.einsum('...ii->...', solved_term).real\n",
    "    except torch.linalg.LinAlgError as e:\n",
    "        print(f\"Warning: LinAlgError during solve: {e}. Applying high loss penalty.\")\n",
    "        # Return a large loss value instead of NaN to guide optimizer away\n",
    "        return torch.tensor(float('inf'), device=device)\n",
    "\n",
    "    if torch.isnan(trace_term).any() or torch.isinf(trace_term).any():\n",
    "        print(\"Warning: NaN/Inf detected in trace_term. Returning NaN loss.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    likelihood_terms = log_det_term + trace_term\n",
    "\n",
    "    if torch.isnan(likelihood_terms).any():\n",
    "        print(\"Warning: NaN detected in likelihood_terms before summation. Returning NaN loss.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    # Sum over non-zero spatial frequencies\n",
    "    total_sum = torch.sum(likelihood_terms)\n",
    "    dc_term = likelihood_terms[0, 0] if n1 > 0 and n2 > 0 else torch.tensor(0.0, device=device)\n",
    "    if torch.isnan(dc_term).any() or torch.isinf(dc_term).any():\n",
    "        print(\"Warning: NaN/Inf detected in DC term. Setting to 0.\")\n",
    "        dc_term = torch.tensor(0.0, device=device)\n",
    "\n",
    "    loss = total_sum - dc_term if (n1 > 1 or n2 > 1) else total_sum\n",
    "\n",
    "    if torch.isnan(loss) or torch.isinf(loss):\n",
    "         print(\"Warning: NaN/Inf detected in final loss. Returning Inf penalty.\")\n",
    "         return torch.tensor(float('inf'), device=device) # Use Inf penalty\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 5. Training Loop (CORRECTED version)\n",
    "# =========================================================================\n",
    "def run_full(params_list, optimizer, scheduler, I_sample, n1, n2, p, epochs=600, device='cpu'):\n",
    "    \"\"\"Corrected training loop using parameter list and no taper likelihood.\"\"\"\n",
    "    best_loss = float('inf')\n",
    "    params_list = [p.to(device) for p in params_list]\n",
    "    best_params_state = [p.detach().clone() for p in params_list]\n",
    "    epochs_completed = 0\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063\n",
    "\n",
    "    def get_printable_params(p_list):\n",
    "        valid_tensors = [p for p in p_list if isinstance(p, torch.Tensor)]\n",
    "        if not valid_tensors: return \"Invalid params_list\"\n",
    "        p_cat = torch.cat([p.detach().clone().cpu() for p in valid_tensors])\n",
    "        log_indices = [0, 1, 2, 6]\n",
    "        if all(idx < len(p_cat) for idx in log_indices):\n",
    "            log_vals = p_cat[log_indices]\n",
    "            if not (torch.isnan(log_vals).any() or torch.isinf(log_vals).any()):\n",
    "                 p_cat[log_indices] = torch.exp(log_vals)\n",
    "            else:\n",
    "                 print(\"Warning: NaN/Inf in log-params for printing.\")\n",
    "                 # Set corresponding natural scale values to NaN for clarity\n",
    "                 p_cat[log_indices] = float('nan')\n",
    "        return p_cat.numpy().round(4)\n",
    "\n",
    "    I_sample_dev = I_sample.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epochs_completed = epoch + 1\n",
    "        optimizer.zero_grad()\n",
    "        params_tensor = torch.cat(params_list)\n",
    "\n",
    "        loss = whittle_likelihood_loss_no_taper(\n",
    "            params_tensor, I_sample_dev, n1, n2, p, DELTA_LAT, DELTA_LON\n",
    "        )\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"Loss became NaN or Inf at epoch {epoch+1}. Stopping.\")\n",
    "            if epoch == 0: best_params_state = None\n",
    "            epochs_completed = epoch\n",
    "            break # Stop this run\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        nan_grad = False\n",
    "        for param in params_list:\n",
    "            if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
    "                print(f\"Warning: NaN/Inf gradient detected at epoch {epoch+1}.\")\n",
    "                nan_grad = True\n",
    "                break\n",
    "        if nan_grad:\n",
    "             optimizer.zero_grad() # Zero bad gradients\n",
    "             print(\"Skipping optimizer step due to invalid gradients.\")\n",
    "             # Don't step scheduler if optimizer step is skipped\n",
    "             continue # Try next epoch\n",
    "\n",
    "        # Clip gradients only if they are valid\n",
    "        all_params_on_device = params_list\n",
    "        if all_params_on_device:\n",
    "            torch.nn.utils.clip_grad_norm_(all_params_on_device, max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step() # Step scheduler after optimizer\n",
    "\n",
    "        current_loss_item = loss.item()\n",
    "        # Save best state *after* successful step only if params are valid\n",
    "        if current_loss_item < best_loss:\n",
    "             # Check parameters *after* step\n",
    "            params_valid = not any(torch.isnan(p.data).any() or torch.isinf(p.data).any() for p in params_list)\n",
    "            if params_valid:\n",
    "                best_loss = current_loss_item\n",
    "                best_params_state = [p.detach().clone() for p in params_list]\n",
    "            else:\n",
    "                print(f\"Warning: NaN/Inf in params after step epoch {epoch+1}. Not saving state.\")\n",
    "\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr'] if optimizer.param_groups else 0.0\n",
    "\n",
    "        if epoch % 50 == 0 or epoch == epochs - 1:\n",
    "            print(f'--- Epoch {epoch+1}/{epochs} (LR: {current_lr:.6f}) ---')\n",
    "            print(f' Loss: {current_loss_item:.4f}')\n",
    "            print(f' Parameters (Natural Scale): {get_printable_params(params_list)}')\n",
    "\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    if best_params_state is None:\n",
    "        print(\"Training failed to find a valid model state.\")\n",
    "        return None, epochs_completed\n",
    "\n",
    "    # Prepare final output (move best state to CPU)\n",
    "    final_params_log_scale = torch.cat([p.cpu() for p in best_params_state])\n",
    "    final_params_natural_scale = final_params_log_scale.detach().clone()\n",
    "    log_indices = [0, 1, 2, 6]\n",
    "    # Final check for NaN/Inf before exp\n",
    "    if all(idx < len(final_params_natural_scale) for idx in log_indices):\n",
    "        log_vals = final_params_natural_scale[log_indices]\n",
    "        if not (torch.isnan(log_vals).any() or torch.isinf(log_vals).any()):\n",
    "            final_params_natural_scale[log_indices] = torch.exp(log_vals)\n",
    "        else:\n",
    "            print(\"Warning: Invalid values in best log-params before final exp. Setting natural scale to NaN.\")\n",
    "            final_params_natural_scale[log_indices] = float('nan')\n",
    "\n",
    "\n",
    "    final_params_rounded = [round(p.item(), 4) if not np.isnan(p.item()) else float('nan') for p in final_params_natural_scale]\n",
    "    final_loss_rounded = round(best_loss, 3) if best_loss != float('inf') else float('inf')\n",
    "\n",
    "    print(f'\\nFINAL BEST STATE ACHIEVED (during training):')\n",
    "    print(f'Best Loss: {final_loss_rounded}')\n",
    "    print(f'Parameters Corresponding to Best Loss (Natural Scale): {final_params_rounded}')\n",
    "\n",
    "    return final_params_rounded + [final_loss_rounded], epochs_completed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a3768d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Pre-computing sample periodogram (NO data taper)...\n",
      "Data grid: 113x158 spatial points, 1 time points. Sample Periodogram on cpu.\n",
      "\n",
      "============================== Initialization Run 1/1 ==============================\n",
      "Starting with FIXED params (log-scale for some): [3.091, 0.0, 0.4055, 0, 0, 0, 1.5621]\n",
      "Starting optimization run 1 on device cpu (NO data taper, EXP kernel)...\n",
      "--- Epoch 1/700 (LR: 0.005000) ---\n",
      " Loss: 7496.9814\n",
      " Parameters (Natural Scale): [21.8903  1.005   1.5075  0.      0.      0.      4.7452]\n",
      "--- Epoch 51/700 (LR: 0.004240) ---\n",
      " Loss: 5933.7134\n",
      " Parameters (Natural Scale): [17.381   1.266   1.8984  0.      0.      0.      3.7425]\n",
      "--- Epoch 101/700 (LR: 0.002461) ---\n",
      " Loss: 5405.8633\n",
      " Parameters (Natural Scale): [14.9064  1.4778  2.2111  0.      0.      0.      3.1535]\n",
      "--- Epoch 151/700 (LR: 0.000706) ---\n",
      " Loss: 5352.5537\n",
      " Parameters (Natural Scale): [13.9972  1.5883  2.3171  0.      0.      0.      2.9309]\n",
      "--- Epoch 201/700 (LR: 0.000001) ---\n",
      " Loss: 5352.3784\n",
      " Parameters (Natural Scale): [14.0408  1.6129  2.2636  0.      0.      0.      2.9368]\n",
      "--- Epoch 251/700 (LR: 0.000761) ---\n",
      " Loss: 5352.2695\n",
      " Parameters (Natural Scale): [14.1557  1.6497  2.2109  0.      0.      0.      2.9339]\n",
      "--- Epoch 301/700 (LR: 0.002540) ---\n",
      " Loss: 5351.7363\n",
      " Parameters (Natural Scale): [15.6807  1.8036  1.9507  0.      0.      0.      2.8898]\n",
      "--- Epoch 351/700 (LR: 0.004295) ---\n",
      " Loss: 5350.4189\n",
      " Parameters (Natural Scale): [22.2045  1.7683  1.5476  0.      0.      0.      2.6571]\n",
      "--- Epoch 401/700 (LR: 0.005000) ---\n",
      " Loss: 5347.4624\n",
      " Parameters (Natural Scale): [33.2687  1.4671  1.2169  0.      0.      0.      2.1364]\n",
      "--- Epoch 451/700 (LR: 0.004240) ---\n",
      " Loss: 5344.2397\n",
      " Parameters (Natural Scale): [41.6496  1.2708  1.0189  0.      0.      0.      1.6065]\n",
      "--- Epoch 501/700 (LR: 0.002461) ---\n",
      " Loss: 5342.3291\n",
      " Parameters (Natural Scale): [45.1979  1.1645  0.9242  0.      0.      0.      1.2929]\n",
      "--- Epoch 551/700 (LR: 0.000706) ---\n",
      " Loss: 5341.6069\n",
      " Parameters (Natural Scale): [46.3545  1.1255  0.8918  0.      0.      0.      1.1718]\n",
      "--- Epoch 601/700 (LR: 0.000001) ---\n",
      " Loss: 5341.5010\n",
      " Parameters (Natural Scale): [46.5179  1.1201  0.8873  0.      0.      0.      1.1542]\n",
      "--- Epoch 651/700 (LR: 0.000761) ---\n",
      " Loss: 5341.4038\n",
      " Parameters (Natural Scale): [46.6696  1.1145  0.8826  0.      0.      0.      1.1365]\n",
      "--- Epoch 700/700 (LR: 0.002500) ---\n",
      " Loss: 5340.8584\n",
      " Parameters (Natural Scale): [47.4981  1.0838  0.8574  0.      0.      0.      1.0374]\n",
      "\n",
      "--- Training Complete ---\n",
      "\n",
      "FINAL BEST STATE ACHIEVED (during training):\n",
      "Best Loss: 5340.858\n",
      "Parameters Corresponding to Best Loss (Natural Scale): [47.4981, 1.0838, 0.8574, 0.0, 0.0, 0.0, 1.0374]\n",
      "\n",
      "\n",
      "========================= Overall Result from Run =========================\n",
      "Run Loss: 5340.858\n",
      "Final Parameters (Natural Scale): [47.4981, 1.0838, 0.8574, 0.0, 0.0, 0.0, 1.0374]\n",
      "\n",
      "Total execution time: 4.76 seconds\n"
     ]
    }
   ],
   "source": [
    "processed_df = spatially_filtered_days\n",
    "\n",
    "# =========================================================================\n",
    "# 6. Main Execution Script (NO Tapering, Spatially Differenced Data, EXP Kernel)\n",
    "# =========================================================================\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    DAY_TO_RUN = 1\n",
    "    NUM_RUNS = 1 # Only 1 run with fixed start\n",
    "    EPOCHS = 700 # Use 700 as in reference\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    # --- Grid Spacing (Must match data differencing step) ---\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063 # From reference run_full\n",
    "\n",
    "    # --- Column Indices for processed_df ---\n",
    "    LAT_COL, LON_COL = 0, 1\n",
    "    VAL_COL = 2\n",
    "    TIME_COL = 3\n",
    "\n",
    "    # --- Load Spatially Differenced Data ---\n",
    "\n",
    "\n",
    "    processed_df = spatially_filtered_days\n",
    "    cur_df = processed_df[DAY_TO_RUN - 1]\n",
    "    if cur_df.numel() == 0 or cur_df.shape[1] <= max(LAT_COL, LON_COL, VAL_COL, TIME_COL):\n",
    "        print(f\"Error: Data for Day {DAY_TO_RUN} is empty or invalid.\")\n",
    "        exit()\n",
    "\n",
    "    unique_times = torch.unique(cur_df[:, TIME_COL])\n",
    "    time_slices_list = [cur_df[cur_df[:, TIME_COL] == t_val] for t_val in unique_times]\n",
    "\n",
    "    # --- 1. Pre-compute Sample Periodogram (NO Tapering) ---\n",
    "    print(\"Pre-computing sample periodogram (NO data taper)...\")\n",
    "    J_vec, n1, n2, p = generate_Jvector_no_taper(\n",
    "        time_slices_list,\n",
    "        lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if J_vec.numel() == 0 or n1 == 0 or n2 == 0 or p == 0:\n",
    "       print(f\"Error: J-vector generation failed for Day {DAY_TO_RUN}.\")\n",
    "       exit()\n",
    "\n",
    "    I_sample = calculate_sample_periodogram_vectorized(J_vec)\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Error: NaN or Inf detected in the sample periodogram. Cannot proceed.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Data grid: {n1}x{n2} spatial points, {p} time points. Sample Periodogram on {DEVICE}.\")\n",
    "\n",
    "    # --- 2. Optimization Loop ---\n",
    "    all_final_results = []\n",
    "    all_final_losses = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        print(f\"\\n{'='*30} Initialization Run {i+1}/{NUM_RUNS} {'='*30}\")\n",
    "\n",
    "        initial_params_values = [np.log(25), np.log(1),np.log(1.5), -0.05, -0.08, 0.05, np.log(2)]\n",
    "        initial_params_values = [np.log(22), np.log(1),np.log(1.5), 0, 0, 0, np.log(4.769)]\n",
    "        print(f\"Starting with FIXED params (log-scale for some): {[round(p, 4) for p in initial_params_values]}\")\n",
    "\n",
    "        params_list = [\n",
    "            Parameter(torch.tensor([val], dtype=torch.float32))\n",
    "            for val in initial_params_values\n",
    "        ]\n",
    "\n",
    "        lr_slow, lr_fast = 0.005, 0.02\n",
    "        slow_indices = [0, 1, 2, 6]\n",
    "        fast_indices = [3, 4, 5]\n",
    "\n",
    "        valid_slow_indices = [idx for idx in slow_indices if idx < len(params_list)]\n",
    "        valid_fast_indices = [idx for idx in fast_indices if idx < len(params_list)]\n",
    "\n",
    "        param_groups = [\n",
    "            {'params': [params_list[idx] for idx in valid_slow_indices], 'lr': lr_slow, 'name': 'slow_group'},\n",
    "            {'params': [params_list[idx] for idx in valid_fast_indices], 'lr': lr_fast, 'name': 'fast_group'}\n",
    "        ]\n",
    "\n",
    "        optimizer = torch.optim.Adam(param_groups)\n",
    "\n",
    "        T_MAX = 200\n",
    "        ETA_MIN = 1e-6\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=T_MAX, eta_min=ETA_MIN)\n",
    "\n",
    "        print(f\"Starting optimization run {i+1} on device {DEVICE} (NO data taper, EXP kernel)...\")\n",
    "        final_results, epochs_run = run_full(\n",
    "            params_list=params_list,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            I_sample=I_sample,\n",
    "            n1=n1, n2=n2, p=p,\n",
    "            epochs=EPOCHS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "        if final_results:\n",
    "            all_final_results.append(final_results)\n",
    "            all_final_losses.append(final_results[-1])\n",
    "        else:\n",
    "            all_final_results.append(None)\n",
    "            all_final_losses.append(float('inf'))\n",
    "\n",
    "    print(f\"\\n\\n{'='*25} Overall Result from Run {'='*25}\")\n",
    "    valid_losses = [l for l in all_final_losses if l is not None and l != float('inf')]\n",
    "\n",
    "    if not valid_losses:\n",
    "        print(f\"The run failed or resulted in an invalid loss for Day {DAY_TO_RUN}.\")\n",
    "    else:\n",
    "        best_loss = valid_losses[0]\n",
    "        best_run_index = 0\n",
    "        best_results = all_final_results[best_run_index]\n",
    "        print(f\"Run Loss: {best_results[-1]}\")\n",
    "        print(f\"Final Parameters (Natural Scale): {best_results[:-1]}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dfa128",
   "metadata": {},
   "source": [
    "## 10/20/25. Hamming tapering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "45000134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Pre-computing J-vector (Hamming taper)...\n",
      "Pre-computing sample periodogram...\n",
      "Pre-computing Hamming taper autocorrelation...\n",
      "Data grid: 113x158, 1 time points. J-vector, Periodogram, Taper Autocorr on cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # Keep if plotting might be added later\n",
    "import cmath\n",
    "import pickle\n",
    "import time # For timing\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import Parameter\n",
    "import torch.fft # Explicit import for fft functions\n",
    "\n",
    "# =========================================================================\n",
    "# 1. Tapering, Autocorrelation, and Covariance Functions\n",
    "# =========================================================================\n",
    "\n",
    "# --- Tapering Functions (Hamming is used) ---\n",
    "def cgn_hamming(u, n1, n2):\n",
    "    \"\"\"Computes a 2D Hamming window.\"\"\"\n",
    "    u1, u2 = u\n",
    "    # Ensure inputs are tensors and on the correct device\n",
    "    device = u1.device if isinstance(u1, torch.Tensor) else (u2.device if isinstance(u2, torch.Tensor) else torch.device('cpu'))\n",
    "    u1_tensor = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_tensor = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "\n",
    "    n1_eff = float(n1) if n1 > 0 else 1.0\n",
    "    n2_eff = float(n2) if n2 > 0 else 1.0\n",
    "    # Add epsilon to denominator to avoid potential division by zero if n=0 or 1?\n",
    "    # Though meshgrid usually starts from 0 to n-1.\n",
    "    hamming1 = 0.54 + 0.46 * torch.cos(2.0 * torch.pi * u1_tensor / n1_eff)\n",
    "    hamming2 = 0.54 + 0.46 * torch.cos(2.0 * torch.pi * u2_tensor / n2_eff)\n",
    "    return hamming1 * hamming2\n",
    "\n",
    "def cgn_2dbartlett(u, n1, n2): # Kept for potential future use\n",
    "    \"\"\"Computes a 2D Bartlett window function.\"\"\"\n",
    "    u1, u2 = u\n",
    "    device = u1.device if isinstance(u1, torch.Tensor) else (u2.device if isinstance(u2, torch.Tensor) else torch.device('cpu'))\n",
    "    u1_tensor = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_tensor = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    n1_eff = float(n1) if n1 > 0 else 1.0\n",
    "    n2_eff = float(n2) if n2 > 0 else 1.0\n",
    "    kernel = (1.0 - torch.abs(u1_tensor) / n1_eff) * (1.0 - torch.abs(u2_tensor) / n2_eff)\n",
    "    return torch.clamp(kernel, min=0.0)\n",
    "\n",
    "# --- NEW: Function to Calculate Taper Autocorrelation ---\n",
    "def calculate_taper_autocorrelation_fft(taper_grid, n1, n2, device):\n",
    "    \"\"\"\n",
    "    Computes the normalized taper autocorrelation function c_gn(u) using FFT.\n",
    "\n",
    "    Args:\n",
    "        taper_grid (torch.Tensor): The 2D taper function g_s on the grid [n1, n2].\n",
    "        n1, n2 (int): Dimensions of the original grid.\n",
    "        device: The torch device.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Normalized autocorrelation c_gn(u), shifted so lag u=(0,0)\n",
    "                      is at index [n1-1, n2-1]. Shape [2*n1-1, 2*n2-1].\n",
    "    \"\"\"\n",
    "    taper_grid = taper_grid.to(device) # Ensure input is on device\n",
    "    H = torch.sum(taper_grid**2)\n",
    "    if H < 1e-12:\n",
    "        print(\"Warning: Sum of squared taper weights (H) is near zero.\")\n",
    "        return torch.zeros((2*n1-1, 2*n2-1), device=device, dtype=taper_grid.dtype)\n",
    "\n",
    "    # Pad for linear autocorrelation via FFT\n",
    "    N1, N2 = 2 * n1 - 1, 2 * n2 - 1\n",
    "    taper_fft = torch.fft.fft2(taper_grid, s=(N1, N2))\n",
    "    power_spectrum = torch.abs(taper_fft)**2\n",
    "    autocorr_unnormalized = torch.fft.ifft2(power_spectrum).real\n",
    "    # Shift zero lag (originally at [0,0]) to the center [n1-1, n2-1]\n",
    "    autocorr_shifted = torch.fft.fftshift(autocorr_unnormalized)\n",
    "\n",
    "    # Normalize by H (value at zero lag)\n",
    "    # Add small epsilon to prevent potential division by zero if H is calculated slightly differently\n",
    "    c_gn_grid = autocorr_shifted / (H + 1e-12)\n",
    "\n",
    "    return c_gn_grid # Already on device\n",
    "\n",
    "# --- Covariance of Original Field X (EXPONENTIAL Kernel) ---\n",
    "def cov_x_exponential(u1, u2, t, params):\n",
    "    \"\"\"\n",
    "    Computes spatio-temporal autocovariance of X using EXPONENTIAL kernel.\n",
    "    Expects log-scale params [0,1,2,6]. Handles device internally.\n",
    "    \"\"\"\n",
    "    device = params.device\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    log_params_indices = [0, 1, 2, 6]\n",
    "    if torch.isnan(params[log_params_indices]).any() or torch.isinf(params[log_params_indices]).any():\n",
    "         print(\"Warning: NaN/Inf in log-params before exp in cov_x_exponential\")\n",
    "         out_shape = torch.broadcast_shapes(u1_dev.shape, u2_dev.shape, t_dev.shape)\n",
    "         return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "\n",
    "    sigmasq, r_lat, r_lon, nugget = torch.exp(params[log_params_indices])\n",
    "    a_lat, a_lon, beta = params[3], params[4], params[5]\n",
    "\n",
    "    if r_lat < 1e-6 or r_lon < 1e-6:\n",
    "        r_lat = torch.clamp(r_lat, min=1e-6)\n",
    "        r_lon = torch.clamp(r_lon, min=1e-6)\n",
    "\n",
    "    x1 = u1_dev / r_lat - a_lat * t_dev\n",
    "    x2 = u2_dev / r_lon - a_lon * t_dev\n",
    "    x3 = beta * t_dev\n",
    "    distance_sq = x1**2 + x2**2 + x3**2\n",
    "    epsilon = 1e-12\n",
    "    distance_sq_clamped = torch.clamp(distance_sq, min=0.0)\n",
    "    D = torch.sqrt(distance_sq_clamped + epsilon)\n",
    "\n",
    "    cov_smooth = sigmasq * torch.exp(-D)\n",
    "    is_zero_lag = (torch.abs(u1_dev) < 1e-9) & (torch.abs(u2_dev) < 1e-9) & (torch.abs(t_dev) < 1e-9)\n",
    "    final_cov = torch.where(is_zero_lag, cov_smooth + nugget, cov_smooth)\n",
    "\n",
    "    if torch.isnan(final_cov).any(): print(\"Warning: NaN detected in cov_x_exponential output.\")\n",
    "    return final_cov\n",
    "\n",
    "# --- Covariance of Spatially Differenced Field Y (Unchanged structure) ---\n",
    "def cov_spatial_difference(u1, u2, t, params, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Calculates covariance Cov(Y(s), Y(s+u)) for Y(s) = X(s+d1) + X(s+d2) - 2X(s)\n",
    "    Based on the underlying EXPONENTIAL covariance cov_x_exponential.\n",
    "    \"\"\"\n",
    "    weights = {(0, 0): -2.0, (1, 0): 1.0, (0, 1): 1.0}\n",
    "    device = params.device\n",
    "    out_shape = torch.broadcast_shapes(u1.shape if isinstance(u1, torch.Tensor) else (),\n",
    "                                     u2.shape if isinstance(u2, torch.Tensor) else (),\n",
    "                                     t.shape if isinstance(t, torch.Tensor) else ())\n",
    "    cov = torch.zeros(out_shape, device=device, dtype=torch.float32)\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    for (a_idx, b_idx), w_ab in weights.items():\n",
    "        offset_a1 = a_idx * delta1\n",
    "        offset_a2 = b_idx * delta2\n",
    "        for (c_idx, d_idx), w_cd in weights.items():\n",
    "            offset_c1 = c_idx * delta1\n",
    "            offset_c2 = d_idx * delta2\n",
    "            lag_u1 = u1_dev + (offset_a1 - offset_c1)\n",
    "            lag_u2 = u2_dev + (offset_a2 - offset_c2)\n",
    "            term_cov = cov_x_exponential(lag_u1, lag_u2, t_dev, params) # Calls EXPONENTIAL version\n",
    "            if torch.isnan(term_cov).any():\n",
    "                 print(f\"Warning: NaN in term_cov within cov_spatial_difference.\")\n",
    "                 return torch.full_like(cov, float('nan'))\n",
    "            cov += w_ab * w_cd * term_cov\n",
    "\n",
    "    if torch.isnan(cov).any(): print(\"Warning: NaN in final cov_spatial_difference output.\")\n",
    "    return cov\n",
    "\n",
    "# --- NEW cn_bar using Taper Autocorrelation ---\n",
    "def cn_bar_tapered(u1, u2, t, params, n1, n2, taper_autocorr_grid, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Computes c_X(u) * c_gn(u) where c_X is cov_spatial_difference\n",
    "    and c_gn(u) is looked up from the pre-computed taper_autocorr_grid.\n",
    "    \"\"\"\n",
    "    device = params.device\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    # Calculate theoretical covariance c_X(u)\n",
    "    cov_X_value = cov_spatial_difference(u1_dev, u2_dev, t_dev, params, delta1, delta2)\n",
    "\n",
    "    # --- Get Taper Autocorrelation Value c_gn(u) from grid ---\n",
    "    # Lags u1, u2 are grid indices (0..n1-1, 0..n2-1) from meshgrid\n",
    "    # Center of autocorr grid (n1-1, n2-1) corresponds to lag (0,0)\n",
    "    # Index for lag (u1, u2) IS (n1-1 + u1, n2-1 + u2) if u1,u2 are relative lags centered at 0\n",
    "    # BUT u1_mesh_grid, u2_mesh_grid are 0..n1-1, 0..n2-1.\n",
    "    # We need autocorrelation for lags u = s1 - s2, where s1, s2 are grid indices.\n",
    "    # The FFT method gives autocorr C(k) = Sum_n g[n]g[n-k]^*\n",
    "    # The shifted grid taper_autocorr_grid[n1-1+u1_lag, n2-1+u2_lag] gives C(u1_lag, u2_lag)\n",
    "    # where u1_lag = -(n1-1)...0...(n1-1).\n",
    "    # Since u1_mesh_grid represents the ABSOLUTE separation s1-s2 (0 to n1-1),\n",
    "    # we need to map this to the indices of the centered autocorrelation grid.\n",
    "    # A lag of 0 corresponds to index n1-1. A lag of 1 corresponds to index n1.\n",
    "    # A lag of u1 corresponds to index n1-1 + u1.\n",
    "    # However, FFT assumes periodicity. Need care with interpretation of lags vs indices.\n",
    "    # Let's test: If u1_mesh_grid=0, we need lag 0 => index n1-1. If u1_mesh_grid=1, need lag 1 => index n1.\n",
    "    # If u1_mesh_grid=u1, need lag u1 => index n1-1 + u1. This seems correct.\n",
    "    # But wait, meshgrid is 0..n1-1. What lags does product_tensor represent?\n",
    "    # product_tensor[i,j,q,r] = cn_bar(u1=i, u2=j, t=tq-tr). u1, u2 are interpreted as spatial lags.\n",
    "    # So we need c_gn at spatial lags u1=i, u2=j.\n",
    "\n",
    "    # Ensure u1, u2 are integer indices for lookup\n",
    "    u1_idx = u1_dev.long()\n",
    "    u2_idx = u2_dev.long()\n",
    "\n",
    "    # Calculate indices into the centered autocorrelation grid\n",
    "    idx1 = (n1 - 1 + u1_idx)\n",
    "    idx2 = (n2 - 1 + u2_idx)\n",
    "\n",
    "    # Clamp indices (although they should be within bounds if u1, u2 are from meshgrid 0..n-1?)\n",
    "    # Max lag is n-1. Max index needed is n-1 + n-1 = 2n-2. Grid size is 2n-1. Correct.\n",
    "    idx1 = torch.clamp(idx1, 0, 2 * n1 - 2)\n",
    "    idx2 = torch.clamp(idx2, 0, 2 * n2 - 2)\n",
    "\n",
    "    taper_autocorr_value = taper_autocorr_grid[idx1, idx2] # Indexing with meshes\n",
    "\n",
    "    if torch.isnan(cov_X_value).any() or torch.isnan(taper_autocorr_value).any():\n",
    "        print(\"Warning: NaN detected before multiplication in cn_bar_tapered.\")\n",
    "        out_shape = torch.broadcast_shapes(cov_X_value.shape, taper_autocorr_value.shape)\n",
    "        return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "\n",
    "    result = cov_X_value * taper_autocorr_value\n",
    "    if torch.isnan(result).any(): print(\"Warning: NaN in cn_bar_tapered output.\")\n",
    "    return result\n",
    "\n",
    "# --- Expected Periodogram (uses cn_bar_tapered) ---\n",
    "def expected_periodogram_fft_tapered(params, n1, n2, p, taper_autocorr_grid, cov_func, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Calculates the expected periodogram I(omega_s) (a pxp matrix in time)\n",
    "    using the exact taper autocorrelation c_gn(u).\n",
    "    Returns shape [n1_freq, n2_freq, p, p]\n",
    "    \"\"\"\n",
    "    device = params.device if isinstance(params, torch.Tensor) else params[0].device\n",
    "    if isinstance(params, list):\n",
    "        params_tensor = torch.cat([p.to(device) for p in params])\n",
    "    else:\n",
    "        params_tensor = params.to(device)\n",
    "\n",
    "    product_tensor = torch.zeros((n1, n2, p, p), dtype=torch.complex64, device=device)\n",
    "    t_lags = torch.arange(p, dtype=torch.float32, device=device)\n",
    "    u1_mesh_grid, u2_mesh_grid = torch.meshgrid(\n",
    "        torch.arange(n1, dtype=torch.float32, device=device),\n",
    "        torch.arange(n2, dtype=torch.float32, device=device),\n",
    "        indexing='ij'\n",
    "    )\n",
    "\n",
    "    for q in range(p):\n",
    "        for r in range(p):\n",
    "            t_diff = t_lags[q] - t_lags[r]\n",
    "            # Calculate c_X(u) * c_gn(u) using the autocorrelation grid\n",
    "            cov_times_autocorr = cn_bar_tapered(\n",
    "                u1_mesh_grid, u2_mesh_grid, t_diff,\n",
    "                params_tensor, n1, n2, taper_autocorr_grid, delta1, delta2\n",
    "            )\n",
    "            if torch.isnan(cov_times_autocorr).any():\n",
    "                 print(f\"Warning: NaN detected in cov_times_autocorr for t_lag {t_diff.item():.2f}.\")\n",
    "                 product_tensor[:, :, q, r] = float('nan')\n",
    "            else:\n",
    "                 product_tensor[:, :, q, r] = cov_times_autocorr.to(torch.complex64)\n",
    "\n",
    "    if torch.isnan(product_tensor).any():\n",
    "        print(\"Warning: NaN detected in product_tensor before FFT.\")\n",
    "        nan_shape = (n1, n2, p, p)\n",
    "        return torch.full(nan_shape, float('nan'), dtype=torch.complex64, device=device)\n",
    "\n",
    "    fft_result = torch.fft.fft2(product_tensor, dim=(0, 1))\n",
    "    normalization_factor = 1.0 / (4.0 * cmath.pi**2)\n",
    "    result = fft_result * normalization_factor\n",
    "\n",
    "    if torch.isnan(result).any(): print(\"Warning: NaN in expected_periodogram_fft_tapered output.\")\n",
    "    return result\n",
    "\n",
    "# =========================================================================\n",
    "# 2. Data Processing (MODIFIED for HAMMING TAPERING)\n",
    "# =========================================================================\n",
    "def generate_Jvector_tapered(tensor_list, tapering_func, lat_col, lon_col, val_col, device):\n",
    "    \"\"\"\n",
    "    Generates J-vector for a single component using the specified taper,\n",
    "    placing result on device.\n",
    "    \"\"\"\n",
    "    p = len(tensor_list)\n",
    "    if p == 0: return torch.empty(0, 0, 0, device=device), 0, 0, 0, None # Return None for taper grid\n",
    "\n",
    "    valid_tensors = [t for t in tensor_list if t.numel() > 0 and t.shape[1] > max(lat_col, lon_col, val_col)]\n",
    "    if not valid_tensors:\n",
    "         print(\"Warning: No valid tensors found in tensor_list.\")\n",
    "         return torch.empty(0, 0, 0, device=device), 0, 0, 0, None\n",
    "\n",
    "    try:\n",
    "        all_lats_cpu = torch.cat([t[:, lat_col] for t in valid_tensors])\n",
    "        all_lons_cpu = torch.cat([t[:, lon_col] for t in valid_tensors])\n",
    "    except IndexError:\n",
    "        print(f\"Error: Invalid column index. Check tensor shapes.\")\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0, None\n",
    "\n",
    "    all_lats_cpu = all_lats_cpu[~torch.isnan(all_lats_cpu) & ~torch.isinf(all_lats_cpu)]\n",
    "    all_lons_cpu = all_lons_cpu[~torch.isnan(all_lons_cpu) & ~torch.isinf(all_lons_cpu)]\n",
    "    if all_lats_cpu.numel() == 0 or all_lons_cpu.numel() == 0:\n",
    "        print(\"Warning: No valid coordinates after NaN/Inf filtering.\")\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0, None\n",
    "\n",
    "    unique_lats_cpu, unique_lons_cpu = torch.unique(all_lats_cpu), torch.unique(all_lons_cpu)\n",
    "    n1, n2 = len(unique_lats_cpu), len(unique_lons_cpu)\n",
    "    if n1 == 0 or n2 == 0:\n",
    "        print(\"Warning: Grid dimensions are zero.\")\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0, None\n",
    "\n",
    "    lat_map = {lat.item(): i for i, lat in enumerate(unique_lats_cpu)}\n",
    "    lon_map = {lon.item(): i for i, lon in enumerate(unique_lons_cpu)}\n",
    "\n",
    "    # --- Create Taper Grid ---\n",
    "    u1_mesh_cpu, u2_mesh_cpu = torch.meshgrid(\n",
    "        torch.arange(n1, dtype=torch.float32),\n",
    "        torch.arange(n2, dtype=torch.float32),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    taper_grid = tapering_func((u1_mesh_cpu, u2_mesh_cpu), n1, n2).to(device) # Taper on device\n",
    "\n",
    "    fft_results = []\n",
    "    for tensor in tensor_list:\n",
    "        data_grid = torch.zeros((n1, n2), dtype=torch.float32, device=device)\n",
    "        if tensor.numel() > 0 and tensor.shape[1] > max(lat_col, lon_col, val_col):\n",
    "            for row in tensor:\n",
    "                lat_item, lon_item = row[lat_col].item(), row[lon_col].item()\n",
    "                if not (np.isnan(lat_item) or np.isnan(lon_item)):\n",
    "                    i = lat_map.get(lat_item)\n",
    "                    j = lon_map.get(lon_item)\n",
    "                    if i is not None and j is not None:\n",
    "                        val = row[val_col]\n",
    "                        val_num = val.item() if isinstance(val, torch.Tensor) else val\n",
    "                        if not np.isnan(val_num) and not np.isinf(val_num):\n",
    "                           data_grid[i, j] = val_num\n",
    "\n",
    "        # --- Apply Tapering ---\n",
    "        data_grid_tapered = data_grid * taper_grid # Both on device\n",
    "\n",
    "        if torch.isnan(data_grid_tapered).any() or torch.isinf(data_grid_tapered).any():\n",
    "             print(\"Warning: NaN/Inf detected in data_grid_tapered before FFT. Replacing with zeros.\")\n",
    "             data_grid_tapered = torch.nan_to_num(data_grid_tapered, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        fft_results.append(torch.fft.fft2(data_grid_tapered))\n",
    "\n",
    "    if not fft_results:\n",
    "         print(\"Warning: No FFT results generated.\")\n",
    "         return torch.empty(0, 0, 0, device=device), n1, n2, 0, taper_grid # Return taper grid\n",
    "\n",
    "    J_vector_tensor = torch.stack(fft_results, dim=2).to(device)\n",
    "\n",
    "    # --- Normalization using Taper ---\n",
    "    H = torch.sum(taper_grid**2)\n",
    "    if H < 1e-12:\n",
    "        print(\"Warning: Normalization factor H is near zero.\")\n",
    "        norm_factor = torch.tensor(0.0, device=device)\n",
    "    else:\n",
    "        norm_factor = (torch.sqrt(1.0 / H) / (2.0 * cmath.pi)).to(device)\n",
    "\n",
    "    result = J_vector_tensor * norm_factor\n",
    "    if torch.isnan(result).any(): print(\"Warning: NaN in J_vector output.\")\n",
    "    # Also return the calculated taper_grid\n",
    "    return result, n1, n2, p, taper_grid\n",
    "\n",
    "\n",
    "def calculate_sample_periodogram_vectorized(J_vector_tensor):\n",
    "    \"\"\"Calculates sample periodogram I_n = J J^H (pxp matrix for each spatial freq).\"\"\"\n",
    "    if torch.isnan(J_vector_tensor).any() or torch.isinf(J_vector_tensor).any():\n",
    "        print(\"Warning: NaN/Inf detected in J_vector_tensor input.\")\n",
    "        n1, n2, p = J_vector_tensor.shape\n",
    "        return torch.full((n1, n2, p, p), float('nan'), dtype=torch.complex64, device=J_vector_tensor.device)\n",
    "\n",
    "    J_col = J_vector_tensor.unsqueeze(-1)\n",
    "    J_row_conj = J_vector_tensor.unsqueeze(-2).conj()\n",
    "    result = J_col @ J_row_conj\n",
    "\n",
    "    if torch.isnan(result).any(): print(\"Warning: NaN in periodogram matrix output.\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 4. Likelihood Calculation (Adapted for Tapering with Autocorrelation)\n",
    "# =========================================================================\n",
    "\n",
    "def whittle_likelihood_loss_tapered(params, I_sample, n1, n2, p, taper_autocorr_grid, delta1, delta2):\n",
    "    \"\"\"\n",
    "    ✅ Whittle Likelihood Loss using data tapering and exact taper autocorrelation c_gn.\n",
    "    Models a single field (the spatially differenced one). Uses Exponential kernel.\n",
    "    \"\"\"\n",
    "    device = I_sample.device\n",
    "    params_tensor = params.to(device)\n",
    "\n",
    "    if torch.isnan(params_tensor).any() or torch.isinf(params_tensor).any():\n",
    "        print(\"Warning: NaN/Inf detected in input parameters to likelihood.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    # Use expected_periodogram_fft_tapered which uses cn_bar_tapered\n",
    "    I_expected = expected_periodogram_fft_tapered(\n",
    "        params_tensor, n1, n2, p, taper_autocorr_grid, cov_spatial_difference, delta1, delta2\n",
    "    )\n",
    "\n",
    "    if torch.isnan(I_expected).any() or torch.isinf(I_expected).any():\n",
    "        print(\"Warning: NaN/Inf returned from expected_periodogram calculation.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    eye_matrix = torch.eye(p, dtype=torch.complex64, device=device)\n",
    "    diag_vals = torch.abs(I_expected.diagonal(dim1=-2, dim2=-1))\n",
    "    mean_diag_abs = diag_vals.mean().item() if diag_vals.numel() > 0 and not torch.isnan(diag_vals).all() else 1.0\n",
    "    diag_load = max(mean_diag_abs * 1e-8, 1e-9)\n",
    "    I_expected_stable = I_expected + eye_matrix * diag_load\n",
    "\n",
    "    sign, logabsdet = torch.linalg.slogdet(I_expected_stable)\n",
    "    if torch.any(sign.real <= 1e-9):\n",
    "        print(\"Warning: Non-positive determinant encountered. Applying penalty.\")\n",
    "        log_det_term = torch.where(sign.real > 1e-9, logabsdet, torch.tensor(1e10, device=device))\n",
    "    else:\n",
    "        log_det_term = logabsdet\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Warning: NaN/Inf detected in I_sample input to likelihood.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    try:\n",
    "        solved_term = torch.linalg.solve(I_expected_stable, I_sample)\n",
    "        trace_term = torch.einsum('...ii->...', solved_term).real\n",
    "    except torch.linalg.LinAlgError as e:\n",
    "        print(f\"Warning: LinAlgError during solve: {e}. Applying high loss penalty.\")\n",
    "        return torch.tensor(float('inf'), device=device)\n",
    "\n",
    "    if torch.isnan(trace_term).any() or torch.isinf(trace_term).any():\n",
    "        print(\"Warning: NaN/Inf detected in trace_term. Returning NaN loss.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    likelihood_terms = log_det_term + trace_term\n",
    "\n",
    "    if torch.isnan(likelihood_terms).any():\n",
    "        print(\"Warning: NaN detected in likelihood_terms before summation. Returning NaN loss.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    total_sum = torch.sum(likelihood_terms)\n",
    "    dc_term = likelihood_terms[0, 0] if n1 > 0 and n2 > 0 else torch.tensor(0.0, device=device)\n",
    "    if torch.isnan(dc_term).any() or torch.isinf(dc_term).any():\n",
    "        print(\"Warning: NaN/Inf detected in DC term. Setting to 0.\")\n",
    "        dc_term = torch.tensor(0.0, device=device)\n",
    "\n",
    "    loss = total_sum - dc_term if (n1 > 1 or n2 > 1) else total_sum\n",
    "\n",
    "    if torch.isnan(loss) or torch.isinf(loss):\n",
    "         print(\"Warning: NaN/Inf detected in final loss. Returning Inf penalty.\")\n",
    "         return torch.tensor(float('inf'), device=device)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 5. Training Loop (CORRECTED version, adapted for tapering)\n",
    "# =========================================================================\n",
    "def run_full_tapered(params_list, optimizer, scheduler, I_sample, n1, n2, p, taper_autocorr_grid, epochs=600, device='cpu'):\n",
    "    \"\"\"Corrected training loop using parameter list and tapered likelihood.\"\"\"\n",
    "    best_loss = float('inf')\n",
    "    params_list = [p.to(device) for p in params_list]\n",
    "    best_params_state = [p.detach().clone() for p in params_list]\n",
    "    epochs_completed = 0\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063 # Grid spacing needed for cov func\n",
    "\n",
    "    def get_printable_params(p_list):\n",
    "        valid_tensors = [p for p in p_list if isinstance(p, torch.Tensor)]\n",
    "        if not valid_tensors: return \"Invalid params_list\"\n",
    "        p_cat = torch.cat([p.detach().clone().cpu() for p in valid_tensors])\n",
    "        log_indices = [0, 1, 2, 6]\n",
    "        if all(idx < len(p_cat) for idx in log_indices):\n",
    "            log_vals = p_cat[log_indices]\n",
    "            if not (torch.isnan(log_vals).any() or torch.isinf(log_vals).any()):\n",
    "                 p_cat[log_indices] = torch.exp(log_vals)\n",
    "            else:\n",
    "                 p_cat[log_indices] = float('nan')\n",
    "        return p_cat.numpy().round(4)\n",
    "\n",
    "    I_sample_dev = I_sample.to(device)\n",
    "    taper_autocorr_grid_dev = taper_autocorr_grid.to(device) # Ensure autocorr grid is on device\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epochs_completed = epoch + 1\n",
    "        optimizer.zero_grad()\n",
    "        params_tensor = torch.cat(params_list) # Create tensor on device\n",
    "\n",
    "        # Use the tapered likelihood function\n",
    "        loss = whittle_likelihood_loss_tapered(\n",
    "            params_tensor, I_sample_dev, n1, n2, p, taper_autocorr_grid_dev, DELTA_LAT, DELTA_LON\n",
    "        )\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"Loss became NaN or Inf at epoch {epoch+1}. Stopping.\")\n",
    "            if epoch == 0: best_params_state = None\n",
    "            epochs_completed = epoch\n",
    "            break\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        nan_grad = False\n",
    "        for param in params_list:\n",
    "            if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
    "                nan_grad = True; break\n",
    "        if nan_grad:\n",
    "             print(f\"Warning: NaN/Inf gradient at epoch {epoch+1}. Skipping step.\")\n",
    "             optimizer.zero_grad()\n",
    "             continue\n",
    "\n",
    "        all_params_on_device = params_list\n",
    "        if all_params_on_device:\n",
    "            torch.nn.utils.clip_grad_norm_(all_params_on_device, max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        current_loss_item = loss.item()\n",
    "        if current_loss_item < best_loss:\n",
    "            params_valid = not any(torch.isnan(p.data).any() or torch.isinf(p.data).any() for p in params_list)\n",
    "            if params_valid:\n",
    "                best_loss = current_loss_item\n",
    "                best_params_state = [p.detach().clone() for p in params_list]\n",
    "            else:\n",
    "                print(f\"Warning: NaN/Inf in params after step epoch {epoch+1}. Not saving.\")\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr'] if optimizer.param_groups else 0.0\n",
    "\n",
    "        if epoch % 50 == 0 or epoch == epochs - 1:\n",
    "            print(f'--- Epoch {epoch+1}/{epochs} (LR: {current_lr:.6f}) ---')\n",
    "            print(f' Loss: {current_loss_item:.4f}')\n",
    "            print(f' Parameters (Natural Scale): {get_printable_params(params_list)}')\n",
    "\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    if best_params_state is None:\n",
    "        print(\"Training failed to find a valid model state.\")\n",
    "        return None, epochs_completed\n",
    "\n",
    "    final_params_log_scale = torch.cat([p.cpu() for p in best_params_state])\n",
    "    final_params_natural_scale = final_params_log_scale.detach().clone()\n",
    "    log_indices = [0, 1, 2, 6]\n",
    "    if all(idx < len(final_params_natural_scale) for idx in log_indices):\n",
    "        log_vals = final_params_natural_scale[log_indices]\n",
    "        if not (torch.isnan(log_vals).any() or torch.isinf(log_vals).any()):\n",
    "            final_params_natural_scale[log_indices] = torch.exp(log_vals)\n",
    "        else:\n",
    "            final_params_natural_scale[log_indices] = float('nan')\n",
    "\n",
    "    final_params_rounded = [round(p.item(), 4) if not np.isnan(p.item()) else float('nan') for p in final_params_natural_scale]\n",
    "    final_loss_rounded = round(best_loss, 3) if best_loss != float('inf') else float('inf')\n",
    "\n",
    "    print(f'\\nFINAL BEST STATE ACHIEVED (during training):')\n",
    "    print(f'Best Loss: {final_loss_rounded}')\n",
    "    print(f'Parameters Corresponding to Best Loss (Natural Scale): {final_params_rounded}')\n",
    "\n",
    "    return final_params_rounded + [final_loss_rounded], epochs_completed\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    DAY_TO_RUN = 1\n",
    "    TAPERING_FUNC = cgn_hamming # Use Hamming taper\n",
    "    NUM_RUNS = 1\n",
    "    EPOCHS = 700\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063\n",
    "\n",
    "    LAT_COL, LON_COL = 0, 1\n",
    "    VAL_COL = 2 # Spatially differenced value\n",
    "    TIME_COL = 3\n",
    "\n",
    "    processed_df = spatially_filtered_days\n",
    "\n",
    "    cur_df = processed_df[DAY_TO_RUN - 1]\n",
    "    if cur_df.numel() == 0 or cur_df.shape[1] <= max(LAT_COL, LON_COL, VAL_COL, TIME_COL):\n",
    "        print(f\"Error: Data for Day {DAY_TO_RUN} is empty or invalid.\")\n",
    "        exit()\n",
    "\n",
    "    unique_times = torch.unique(cur_df[:, TIME_COL])\n",
    "    time_slices_list = [cur_df[cur_df[:, TIME_COL] == t_val] for t_val in unique_times]\n",
    "\n",
    "    # --- 1. Pre-compute J-vector, Taper Grid, and Taper Autocorrelation ---\n",
    "    print(\"Pre-computing J-vector (Hamming taper)...\")\n",
    "    J_vec, n1, n2, p, taper_grid = generate_Jvector_tapered( # Use tapered version\n",
    "        time_slices_list,\n",
    "        tapering_func=TAPERING_FUNC, # Pass Hamming\n",
    "        lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if J_vec is None or J_vec.numel() == 0 or n1 == 0 or n2 == 0 or p == 0:\n",
    "       print(f\"Error: J-vector generation failed for Day {DAY_TO_RUN}.\")\n",
    "       exit()\n",
    "\n",
    "    print(\"Pre-computing sample periodogram...\")\n",
    "    I_sample = calculate_sample_periodogram_vectorized(J_vec)\n",
    "\n",
    "    print(\"Pre-computing Hamming taper autocorrelation...\")\n",
    "    taper_autocorr_grid = calculate_taper_autocorrelation_fft(taper_grid, n1, n2, DEVICE)\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Error: NaN/Inf in sample periodogram.\")\n",
    "        exit()\n",
    "    if torch.isnan(taper_autocorr_grid).any() or torch.isinf(taper_autocorr_grid).any():\n",
    "        print(\"Error: NaN/Inf in taper autocorrelation.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Data grid: {n1}x{n2}, {p} time points. J-vector, Periodogram, Taper Autocorr on {DEVICE}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929d08d8",
   "metadata": {},
   "source": [
    "# best output below 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8862b719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded 31 days from spatial_first_difference_data.pkl.\n",
      "Pre-computing J-vector (Hamming taper)...\n",
      "Pre-computing sample periodogram...\n",
      "Pre-computing Hamming taper autocorrelation...\n",
      "Data grid: 113x158, 1 time points. J-vector, Periodogram, Taper Autocorr on cpu.\n",
      "\n",
      "============================== Initialization Run 1/1 ==============================\n",
      "Starting with FIXED params (log-scale): [3.091, 0.0, 0.4055, 0, 0, 0, 1.5621]\n",
      "Starting optimization run 1 on device cpu (Hamming taper, EXP kernel)...\n",
      "--- Epoch 1/700 (LR: 0.005000) ---\n",
      " Loss: 7640.4780\n",
      " Parameters (Natural Scale): [21.8903  1.005   1.5075  0.      0.      0.      4.7452]\n",
      "--- Epoch 51/700 (LR: 0.004240) ---\n",
      " Loss: 6121.1533\n",
      " Parameters (Natural Scale): [17.3807  1.266   1.8986  0.      0.      0.      3.7425]\n",
      "--- Epoch 101/700 (LR: 0.002461) ---\n",
      " Loss: 5632.1089\n",
      " Parameters (Natural Scale): [14.9045  1.477   2.2124  0.      0.      0.      3.1535]\n",
      "--- Epoch 151/700 (LR: 0.000706) ---\n",
      " Loss: 5595.5435\n",
      " Parameters (Natural Scale): [14.1552  1.5608  2.2843  0.      0.      0.      2.9785]\n",
      "--- Epoch 201/700 (LR: 0.000001) ---\n",
      " Loss: 5595.4917\n",
      " Parameters (Natural Scale): [14.0696  1.5767  2.2369  0.      0.      0.      2.9751]\n",
      "--- Epoch 251/700 (LR: 0.000761) ---\n",
      " Loss: 5595.4443\n",
      " Parameters (Natural Scale): [13.8296  1.5997  2.183   0.      0.      0.      2.9793]\n",
      "--- Epoch 301/700 (LR: 0.002540) ---\n",
      " Loss: 5595.2197\n",
      " Parameters (Natural Scale): [12.6543  1.4693  1.9221  0.      0.      0.      2.9738]\n",
      "--- Epoch 351/700 (LR: 0.004295) ---\n",
      " Loss: 5594.7329\n",
      " Parameters (Natural Scale): [12.6472  1.1939  1.5173  0.      0.      0.      2.8914]\n",
      "--- Epoch 401/700 (LR: 0.005000) ---\n",
      " Loss: 5593.5786\n",
      " Parameters (Natural Scale): [16.7219  0.9397  1.1348  0.      0.      0.      2.5899]\n",
      "--- Epoch 451/700 (LR: 0.004240) ---\n",
      " Loss: 5591.5928\n",
      " Parameters (Natural Scale): [23.2541  0.7859  0.8908  0.      0.      0.      2.0774]\n",
      "--- Epoch 501/700 (LR: 0.002461) ---\n",
      " Loss: 5590.0879\n",
      " Parameters (Natural Scale): [27.2676  0.714   0.7797  0.      0.      0.      1.6921]\n",
      "--- Epoch 551/700 (LR: 0.000706) ---\n",
      " Loss: 5589.4805\n",
      " Parameters (Natural Scale): [28.6964  0.688   0.742   0.      0.      0.      1.5334]\n",
      "--- Epoch 601/700 (LR: 0.000001) ---\n",
      " Loss: 5589.3892\n",
      " Parameters (Natural Scale): [28.8981  0.6844  0.7368  0.      0.      0.      1.51  ]\n",
      "--- Epoch 651/700 (LR: 0.000761) ---\n",
      " Loss: 5589.3086\n",
      " Parameters (Natural Scale): [29.0938  0.6808  0.7313  0.      0.      0.      1.4864]\n",
      "--- Epoch 700/700 (LR: 0.002500) ---\n",
      " Loss: 5588.8403\n",
      " Parameters (Natural Scale): [30.1311  0.6599  0.7021  0.      0.      0.      1.3537]\n",
      "\n",
      "--- Training Complete ---\n",
      "\n",
      "FINAL BEST STATE ACHIEVED (during training):\n",
      "Best Loss: 5588.84\n",
      "Parameters Corresponding to Best Loss (Natural Scale): [30.1311, 0.6599, 0.7021, 0.0, 0.0, 0.0, 1.3537]\n",
      "\n",
      "\n",
      "========================= Overall Result from Run =========================\n",
      "Run Loss: 5588.84\n",
      "Final Parameters (Natural Scale): [30.1311, 0.6599, 0.7021, 0.0, 0.0, 0.0, 1.3537]\n",
      "\n",
      "Total execution time: 4.74 seconds\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# 6. Main Execution Script (HAMMING Tapering, Spatially Differenced Data, EXP Kernel)\n",
    "# =========================================================================\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    DAY_TO_RUN = 1\n",
    "    TAPERING_FUNC = cgn_hamming # Use Hamming taper\n",
    "    NUM_RUNS = 1\n",
    "    EPOCHS = 700\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063\n",
    "\n",
    "    LAT_COL, LON_COL = 0, 1\n",
    "    VAL_COL = 2 # Spatially differenced value\n",
    "    TIME_COL = 3\n",
    "\n",
    "    # --- Load Spatially Differenced Data ---\n",
    "    try:\n",
    "        with open(\"spatial_first_difference_data.pkl\", 'rb') as f:\n",
    "            processed_df = pickle.load(f)\n",
    "        print(f\"Loaded {len(processed_df)} days from spatial_first_difference_data.pkl.\")\n",
    "        processed_df = [\n",
    "            torch.tensor(arr, dtype=torch.float32).cpu() if not isinstance(arr, torch.Tensor)\n",
    "            else arr.cpu().to(torch.float32)\n",
    "            for arr in processed_df\n",
    "        ]\n",
    "        if not processed_df: raise ValueError(\"'processed_df' is empty.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: `spatial_first_difference_data.pkl` not found.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing 'processed_df': {e}\")\n",
    "        exit()\n",
    "\n",
    "    if DAY_TO_RUN > len(processed_df) or DAY_TO_RUN <= 0:\n",
    "        print(f\"Error: DAY_TO_RUN ({DAY_TO_RUN}) out of bounds.\")\n",
    "        exit()\n",
    "\n",
    "    cur_df = processed_df[DAY_TO_RUN - 1]\n",
    "    cur_df = cur_df[cur_df[:,3]==cur_df[:,3].min()]\n",
    "    if cur_df.numel() == 0 or cur_df.shape[1] <= max(LAT_COL, LON_COL, VAL_COL, TIME_COL):\n",
    "        print(f\"Error: Data for Day {DAY_TO_RUN} is empty or invalid.\")\n",
    "        exit()\n",
    "\n",
    "    unique_times = torch.unique(cur_df[:, TIME_COL])\n",
    "    time_slices_list = [cur_df[cur_df[:, TIME_COL] == t_val] for t_val in unique_times]\n",
    "\n",
    "    # --- 1. Pre-compute J-vector, Taper Grid, and Taper Autocorrelation ---\n",
    "    print(\"Pre-computing J-vector (Hamming taper)...\")\n",
    "    J_vec, n1, n2, p, taper_grid = generate_Jvector_tapered( # Use tapered version\n",
    "        time_slices_list,\n",
    "        tapering_func=TAPERING_FUNC, # Pass Hamming\n",
    "        lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if J_vec is None or J_vec.numel() == 0 or n1 == 0 or n2 == 0 or p == 0:\n",
    "       print(f\"Error: J-vector generation failed for Day {DAY_TO_RUN}.\")\n",
    "       exit()\n",
    "\n",
    "    print(\"Pre-computing sample periodogram...\")\n",
    "    I_sample = calculate_sample_periodogram_vectorized(J_vec)\n",
    "\n",
    "    print(\"Pre-computing Hamming taper autocorrelation...\")\n",
    "    taper_autocorr_grid = calculate_taper_autocorrelation_fft(taper_grid, n1, n2, DEVICE)\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Error: NaN/Inf in sample periodogram.\")\n",
    "        exit()\n",
    "    if torch.isnan(taper_autocorr_grid).any() or torch.isinf(taper_autocorr_grid).any():\n",
    "        print(\"Error: NaN/Inf in taper autocorrelation.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Data grid: {n1}x{n2}, {p} time points. J-vector, Periodogram, Taper Autocorr on {DEVICE}.\")\n",
    "\n",
    "    # --- 2. Optimization Loop ---\n",
    "    all_final_results = []\n",
    "    all_final_losses = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        print(f\"\\n{'='*30} Initialization Run {i+1}/{NUM_RUNS} {'='*30}\")\n",
    "\n",
    "        initial_params_values = [np.log(22), np.log(1),np.log(1.5), 0, 0, 0, np.log(4.769)]\n",
    "        print(f\"Starting with FIXED params (log-scale): {[round(p, 4) for p in initial_params_values]}\")\n",
    "\n",
    "        params_list = [\n",
    "            Parameter(torch.tensor([val], dtype=torch.float32))\n",
    "            for val in initial_params_values\n",
    "        ]\n",
    "\n",
    "        lr_slow, lr_fast = 0.005, 0.02\n",
    "        slow_indices = [0, 1, 2, 6]\n",
    "        fast_indices = [3, 4, 5]\n",
    "\n",
    "        valid_slow_indices = [idx for idx in slow_indices if idx < len(params_list)]\n",
    "        valid_fast_indices = [idx for idx in fast_indices if idx < len(params_list)]\n",
    "\n",
    "        param_groups = [\n",
    "            {'params': [params_list[idx] for idx in valid_slow_indices], 'lr': lr_slow, 'name': 'slow_group'},\n",
    "            {'params': [params_list[idx] for idx in valid_fast_indices], 'lr': lr_fast, 'name': 'fast_group'}\n",
    "        ]\n",
    "\n",
    "        optimizer = torch.optim.Adam(param_groups)\n",
    "\n",
    "        T_MAX = 200\n",
    "        ETA_MIN = 1e-6\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=T_MAX, eta_min=ETA_MIN)\n",
    "\n",
    "        print(f\"Starting optimization run {i+1} on device {DEVICE} (Hamming taper, EXP kernel)...\")\n",
    "        # --- Use the TAPERED training loop ---\n",
    "        final_results, epochs_run = run_full_tapered(\n",
    "            params_list=params_list,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            I_sample=I_sample,\n",
    "            n1=n1, n2=n2, p=p,\n",
    "            taper_autocorr_grid=taper_autocorr_grid, # Pass the autocorrelation grid\n",
    "            epochs=EPOCHS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "        if final_results:\n",
    "            all_final_results.append(final_results)\n",
    "            all_final_losses.append(final_results[-1])\n",
    "        else:\n",
    "            all_final_results.append(None)\n",
    "            all_final_losses.append(float('inf'))\n",
    "\n",
    "    print(f\"\\n\\n{'='*25} Overall Result from Run {'='*25}\")\n",
    "    valid_losses = [l for l in all_final_losses if l is not None and l != float('inf')]\n",
    "\n",
    "    if not valid_losses:\n",
    "        print(f\"The run failed or resulted in an invalid loss for Day {DAY_TO_RUN}.\")\n",
    "    else:\n",
    "        best_loss = valid_losses[0]\n",
    "        best_run_index = 0\n",
    "        best_results = all_final_results[best_run_index]\n",
    "        print(f\"Run Loss: {best_results[-1]}\")\n",
    "        print(f\"Final Parameters (Natural Scale): {best_results[:-1]}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c54e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Pre-computing J-vector (Hamming taper)...\n",
      "Pre-computing sample periodogram...\n",
      "Pre-computing Hamming taper autocorrelation...\n",
      "Data grid: 113x158, 1 time points. J-vector, Periodogram, Taper Autocorr on cpu.\n",
      "\n",
      "============================== Initialization Run 1/1 ==============================\n",
      "Starting with FIXED params (log-scale): [3.091, 0.0, 0.4055, 0, 0, 0, 1.5621]\n",
      "Starting optimization run 1 on device cpu (Hamming taper, EXP kernel)...\n",
      "--- Epoch 1/700 (LR: 0.005000) ---\n",
      " Loss: 7640.4780\n",
      " Parameters (Natural Scale): [21.8903  1.005   1.5075  0.      0.      0.      4.7452]\n",
      "--- Epoch 51/700 (LR: 0.004240) ---\n",
      " Loss: 6121.1533\n",
      " Parameters (Natural Scale): [17.3807  1.266   1.8986  0.      0.      0.      3.7425]\n",
      "--- Epoch 101/700 (LR: 0.002461) ---\n",
      " Loss: 5632.1089\n",
      " Parameters (Natural Scale): [14.9045  1.477   2.2124  0.      0.      0.      3.1535]\n",
      "--- Epoch 151/700 (LR: 0.000706) ---\n",
      " Loss: 5595.5435\n",
      " Parameters (Natural Scale): [14.1552  1.5608  2.2843  0.      0.      0.      2.9785]\n",
      "--- Epoch 201/700 (LR: 0.000001) ---\n",
      " Loss: 5595.4917\n",
      " Parameters (Natural Scale): [14.0696  1.5767  2.2369  0.      0.      0.      2.9751]\n",
      "--- Epoch 251/700 (LR: 0.000761) ---\n",
      " Loss: 5595.4443\n",
      " Parameters (Natural Scale): [13.8296  1.5997  2.183   0.      0.      0.      2.9793]\n",
      "--- Epoch 301/700 (LR: 0.002540) ---\n",
      " Loss: 5595.2197\n",
      " Parameters (Natural Scale): [12.6543  1.4693  1.9221  0.      0.      0.      2.9738]\n",
      "--- Epoch 351/700 (LR: 0.004295) ---\n",
      " Loss: 5594.7329\n",
      " Parameters (Natural Scale): [12.6472  1.1939  1.5173  0.      0.      0.      2.8914]\n",
      "--- Epoch 401/700 (LR: 0.005000) ---\n",
      " Loss: 5593.5786\n",
      " Parameters (Natural Scale): [16.7219  0.9397  1.1348  0.      0.      0.      2.5899]\n",
      "--- Epoch 451/700 (LR: 0.004240) ---\n",
      " Loss: 5591.5928\n",
      " Parameters (Natural Scale): [23.2541  0.7859  0.8908  0.      0.      0.      2.0774]\n",
      "--- Epoch 501/700 (LR: 0.002461) ---\n",
      " Loss: 5590.0879\n",
      " Parameters (Natural Scale): [27.2676  0.714   0.7797  0.      0.      0.      1.6921]\n",
      "--- Epoch 551/700 (LR: 0.000706) ---\n",
      " Loss: 5589.4805\n",
      " Parameters (Natural Scale): [28.6964  0.688   0.742   0.      0.      0.      1.5334]\n",
      "--- Epoch 601/700 (LR: 0.000001) ---\n",
      " Loss: 5589.3892\n",
      " Parameters (Natural Scale): [28.8981  0.6844  0.7368  0.      0.      0.      1.51  ]\n",
      "--- Epoch 651/700 (LR: 0.000761) ---\n",
      " Loss: 5589.3086\n",
      " Parameters (Natural Scale): [29.0938  0.6808  0.7313  0.      0.      0.      1.4864]\n",
      "--- Epoch 700/700 (LR: 0.002500) ---\n",
      " Loss: 5588.8403\n",
      " Parameters (Natural Scale): [30.1311  0.6599  0.7021  0.      0.      0.      1.3537]\n",
      "\n",
      "--- Training Complete ---\n",
      "\n",
      "FINAL BEST STATE ACHIEVED (during training):\n",
      "Best Loss: 5588.84\n",
      "Parameters Corresponding to Best Loss (Natural Scale): [30.1311, 0.6599, 0.7021, 0.0, 0.0, 0.0, 1.3537]\n",
      "\n",
      "\n",
      "========================= Overall Result from Run =========================\n",
      "Run Loss: 5588.84\n",
      "Final Parameters (Natural Scale): [30.1311, 0.6599, 0.7021, 0.0, 0.0, 0.0, 1.3537]\n",
      "\n",
      "Total execution time: 4.72 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================================================================\n",
    "# 6. Main Execution Script (HAMMING Tapering, Spatially Differenced Data, EXP Kernel)\n",
    "# =========================================================================\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    DAY_TO_RUN = 1\n",
    "    TAPERING_FUNC = cgn_hamming # Use Hamming taper\n",
    "    NUM_RUNS = 1\n",
    "    EPOCHS = 700\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063\n",
    "\n",
    "    LAT_COL, LON_COL = 0, 1\n",
    "    VAL_COL = 2 # Spatially differenced value\n",
    "    TIME_COL = 3\n",
    "\n",
    "    # --- Load Spatially Differenced Data ---\n",
    "    processed_df = spatially_filtered_days\n",
    "\n",
    "    cur_df = processed_df[DAY_TO_RUN - 1]\n",
    "    cur_df = cur_df[cur_df[:,3]==cur_df[:,3].min()]\n",
    "    if cur_df.numel() == 0 or cur_df.shape[1] <= max(LAT_COL, LON_COL, VAL_COL, TIME_COL):\n",
    "        print(f\"Error: Data for Day {DAY_TO_RUN} is empty or invalid.\")\n",
    "        exit()\n",
    "\n",
    "    unique_times = torch.unique(cur_df[:, TIME_COL])\n",
    "    time_slices_list = [cur_df[cur_df[:, TIME_COL] == t_val] for t_val in unique_times]\n",
    "\n",
    "    # --- 1. Pre-compute J-vector, Taper Grid, and Taper Autocorrelation ---\n",
    "    print(\"Pre-computing J-vector (Hamming taper)...\")\n",
    "    J_vec, n1, n2, p, taper_grid = generate_Jvector_tapered( # Use tapered version\n",
    "        time_slices_list,\n",
    "        tapering_func=TAPERING_FUNC, # Pass Hamming\n",
    "        lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if J_vec is None or J_vec.numel() == 0 or n1 == 0 or n2 == 0 or p == 0:\n",
    "       print(f\"Error: J-vector generation failed for Day {DAY_TO_RUN}.\")\n",
    "       exit()\n",
    "\n",
    "    print(\"Pre-computing sample periodogram...\")\n",
    "    I_sample = calculate_sample_periodogram_vectorized(J_vec)\n",
    "\n",
    "    print(\"Pre-computing Hamming taper autocorrelation...\")\n",
    "    taper_autocorr_grid = calculate_taper_autocorrelation_fft(taper_grid, n1, n2, DEVICE)\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Error: NaN/Inf in sample periodogram.\")\n",
    "        exit()\n",
    "    if torch.isnan(taper_autocorr_grid).any() or torch.isinf(taper_autocorr_grid).any():\n",
    "        print(\"Error: NaN/Inf in taper autocorrelation.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Data grid: {n1}x{n2}, {p} time points. J-vector, Periodogram, Taper Autocorr on {DEVICE}.\")\n",
    "\n",
    "    # --- 2. Optimization Loop ---\n",
    "    all_final_results = []\n",
    "    all_final_losses = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        print(f\"\\n{'='*30} Initialization Run {i+1}/{NUM_RUNS} {'='*30}\")\n",
    "\n",
    "        #initial_params_values = [np.log(21.303), np.log(1.007),np.log(1.563), 0, 0, 0, np.log(3.890)]\n",
    "        initial_params_values = [np.log(21), np.log(1),np.log(1.5), 0, 0, 0, np.log(4.769)]   \n",
    "  \n",
    "        print(f\"Starting with FIXED params (log-scale): {[round(p, 4) for p in initial_params_values]}\")\n",
    "\n",
    "        params_list = [\n",
    "            Parameter(torch.tensor([val], dtype=torch.float32))\n",
    "            for val in initial_params_values\n",
    "        ]\n",
    "\n",
    "        lr_slow, lr_fast = 0.005, 0.02\n",
    "        slow_indices = [0, 1, 2,6]\n",
    "        fast_indices = [3, 4, 5]\n",
    "\n",
    "        valid_slow_indices = [idx for idx in slow_indices if idx < len(params_list)]\n",
    "        valid_fast_indices = [idx for idx in fast_indices if idx < len(params_list)]\n",
    "\n",
    "        param_groups = [\n",
    "            {'params': [params_list[idx] for idx in valid_slow_indices], 'lr': lr_slow, 'name': 'slow_group'},\n",
    "            {'params': [params_list[idx] for idx in valid_fast_indices], 'lr': lr_fast, 'name': 'fast_group'}\n",
    "        ]\n",
    "\n",
    "        optimizer = torch.optim.Adam(param_groups)\n",
    "\n",
    "        T_MAX = 200\n",
    "        ETA_MIN = 1e-6\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=T_MAX, eta_min=ETA_MIN)\n",
    "\n",
    "        print(f\"Starting optimization run {i+1} on device {DEVICE} (Hamming taper, EXP kernel)...\")\n",
    "        # --- Use the TAPERED training loop ---\n",
    "        final_results, epochs_run = run_full_tapered(\n",
    "            params_list=params_list,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            I_sample=I_sample,\n",
    "            n1=n1, n2=n2, p=p,\n",
    "            taper_autocorr_grid=taper_autocorr_grid, # Pass the autocorrelation grid\n",
    "            epochs=EPOCHS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "        if final_results:\n",
    "            all_final_results.append(final_results)\n",
    "            all_final_losses.append(final_results[-1])\n",
    "        else:\n",
    "            all_final_results.append(None)\n",
    "            all_final_losses.append(float('inf'))\n",
    "\n",
    "    print(f\"\\n\\n{'='*25} Overall Result from Run {'='*25}\")\n",
    "    valid_losses = [l for l in all_final_losses if l is not None and l != float('inf')]\n",
    "\n",
    "    if not valid_losses:\n",
    "        print(f\"The run failed or resulted in an invalid loss for Day {DAY_TO_RUN}.\")\n",
    "    else:\n",
    "        best_loss = valid_losses[0]\n",
    "        best_run_index = 0\n",
    "        best_results = all_final_results[best_run_index]\n",
    "        print(f\"Run Loss: {best_results[-1]}\")\n",
    "        print(f\"Final Parameters (Natural Scale): {best_results[:-1]}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abef74d",
   "metadata": {},
   "source": [
    "### whittle likelihood function for this hamming + spatial differenced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e6593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded 31 days from spatial_first_difference_data.pkl.\n",
      "Pre-computing J-vector (cgn_hamming taper)...\n",
      "Pre-computing sample periodogram...\n",
      "Pre-computing cgn_hamming taper autocorrelation...\n",
      "Data grid: 113x158, 8 time points. J-vector, Periodogram, Taper Autocorr on cpu.\n",
      "\n",
      "Calculating likelihood for initial parameters (natural scale):\n",
      "-> Whittle Likelihood ('Vecchia Optimized' Params): 57998.410\n",
      "\n",
      "Calculating likelihood for 'optimized' parameters (natural scale):\n",
      "-> Whittle Likelihood ('Whittle Optimized' Params): 41623.910\n",
      "\n",
      "'Optimized' parameters better by 16374.500.\n",
      "\n",
      "Total execution time: 1.68 seconds\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# 2. Likelihood Evaluation Function (Accepts Natural Scale Parameters)\n",
    "# =========================================================================\n",
    "\n",
    "def calculate_whittle_likelihood(\n",
    "    param_estimates_natural_scale: list or np.ndarray, # <<< Renamed input\n",
    "    I_sample: torch.Tensor,\n",
    "    taper_autocorr_grid: torch.Tensor,\n",
    "    n1: int,\n",
    "    n2: int,\n",
    "    p: int,\n",
    "    delta_lat: float,\n",
    "    delta_lon: float,\n",
    "    device: torch.device = torch.device('cpu')\n",
    "    ) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the Whittle likelihood (negative log-likelihood) for given estimates\n",
    "    provided in NATURAL SCALE.\n",
    "\n",
    "    Assumes Hamming tapering applied to spatially differenced data,\n",
    "    using exact taper autocorrelation and an underlying Exponential kernel for X.\n",
    "\n",
    "    Args:\n",
    "        param_estimates_natural_scale: List/array of 7 params in NATURAL SCALE\n",
    "                                       [sigmasq, r_lat, r_lon, a_lat, a_lon, beta, nugget].\n",
    "        I_sample: Pre-computed sample periodogram tensor [n1, n2, p, p].\n",
    "        taper_autocorr_grid: Pre-computed taper autocorrelation grid [2n1-1, 2n2-1].\n",
    "        n1, n2, p: Grid and time dimensions.\n",
    "        delta_lat, delta_lon: Grid spacings used for differencing.\n",
    "        device: Torch device for calculation.\n",
    "\n",
    "    Returns:\n",
    "        Scalar negative log-likelihood value (float), or float('inf')/float('nan').\n",
    "    \"\"\"\n",
    "    start_calc_time = time.time()\n",
    "\n",
    "    # --- Input Validation and Tensor Conversion ---\n",
    "    try:\n",
    "        # Convert natural scale input to tensor\n",
    "        if not isinstance(param_estimates_natural_scale, torch.Tensor):\n",
    "            params_natural_tensor = torch.tensor(param_estimates_natural_scale, dtype=torch.float32)\n",
    "        else:\n",
    "            params_natural_tensor = param_estimates_natural_scale.float()\n",
    "        params_natural_tensor = params_natural_tensor.to(device)\n",
    "\n",
    "        if params_natural_tensor.shape != (7,):\n",
    "             raise ValueError(f\"Expected 7 params, got {params_natural_tensor.shape}\")\n",
    "        if torch.isnan(params_natural_tensor).any() or torch.isinf(params_natural_tensor).any():\n",
    "             raise ValueError(\"NaN/Inf detected in input natural scale params.\")\n",
    "\n",
    "        # <<< NEW: Convert relevant parameters BACK to log-scale >>>\n",
    "        params_log_scale_tensor = params_natural_tensor.clone()\n",
    "        log_indices = [0, 1, 2, 6] # sigmasq, r_lat, r_lon, nugget\n",
    "        # Check for non-positive values before taking log\n",
    "        if torch.any(params_natural_tensor[log_indices] <= 0):\n",
    "            raise ValueError(\"Parameters sigmasq, r_lat, r_lon, nugget must be positive in natural scale.\")\n",
    "        params_log_scale_tensor[log_indices] = torch.log(params_natural_tensor[log_indices])\n",
    "        # <<< END NEW SECTION >>>\n",
    "\n",
    "        # Ensure other inputs are tensors on the correct device\n",
    "        I_sample = I_sample.to(device)\n",
    "        taper_autocorr_grid = taper_autocorr_grid.to(device)\n",
    "        if torch.isnan(I_sample).any() or torch.isinf(I_sample).any(): raise ValueError(\"NaN/Inf in I_sample.\")\n",
    "        if torch.isnan(taper_autocorr_grid).any() or torch.isinf(taper_autocorr_grid).any(): raise ValueError(\"NaN/Inf in taper_autocorr_grid.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during input validation/conversion: {e}\")\n",
    "        return float('nan')\n",
    "\n",
    "    # --- Likelihood Calculation ---\n",
    "    with torch.no_grad():\n",
    "        # Pass the LOG-SCALE tensor to the loss function\n",
    "        likelihood_value = whittle_likelihood_loss_tapered(\n",
    "            params=params_log_scale_tensor, # <<< Use log-scale tensor here\n",
    "            I_sample=I_sample,\n",
    "            n1=n1, n2=n2, p=p,\n",
    "            taper_autocorr_grid=taper_autocorr_grid,\n",
    "            delta1=delta_lat,\n",
    "            delta2=delta_lon\n",
    "        )\n",
    "\n",
    "    end_calc_time = time.time()\n",
    "\n",
    "    # --- Return Scalar Value ---\n",
    "    if isinstance(likelihood_value, torch.Tensor):\n",
    "        if torch.isinf(likelihood_value) and likelihood_value > 0: return float('inf')\n",
    "        if torch.isnan(likelihood_value): return float('nan')\n",
    "        return likelihood_value.item()\n",
    "    else: return float(likelihood_value)\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 6. Main Execution Script (HAMMING Tapering, Spatially Differenced Data, EXP Kernel)\n",
    "# =========================================================================\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    DAY_TO_RUN = 1\n",
    "    TAPERING_FUNC = cgn_hamming # Use Hamming taper\n",
    "    # NUM_RUNS = 1 # Not needed if not optimizing\n",
    "    # EPOCHS = 700 # Not needed if not optimizing\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063\n",
    "\n",
    "    LAT_COL, LON_COL = 0, 1\n",
    "    VAL_COL = 2 # Spatially differenced value\n",
    "    TIME_COL = 3\n",
    "\n",
    "    # --- Load Spatially Differenced Data ---\n",
    "    try:\n",
    "        with open(\"spatial_first_difference_data.pkl\", 'rb') as f:\n",
    "            processed_df = pickle.load(f)\n",
    "        print(f\"Loaded {len(processed_df)} days from spatial_first_difference_data.pkl.\")\n",
    "        processed_df = [\n",
    "            torch.tensor(arr, dtype=torch.float32).cpu() if not isinstance(arr, torch.Tensor)\n",
    "            else arr.cpu().to(torch.float32)\n",
    "            for arr in processed_df\n",
    "        ]\n",
    "        if not processed_df: raise ValueError(\"'processed_df' is empty.\")\n",
    "    except FileNotFoundError: print(\"Error: `spatial_first_difference_data.pkl` not found.\"); exit()\n",
    "    except Exception as e: print(f\"Error loading 'processed_df': {e}\"); exit()\n",
    "\n",
    "    if DAY_TO_RUN > len(processed_df) or DAY_TO_RUN <= 0: print(f\"Error: DAY_TO_RUN out of bounds.\"); exit()\n",
    "\n",
    "    cur_df = processed_df[DAY_TO_RUN - 1]\n",
    "    if cur_df.numel() == 0 or cur_df.shape[1] <= max(LAT_COL, LON_COL, VAL_COL, TIME_COL): print(f\"Error: Data for Day {DAY_TO_RUN} empty/invalid.\"); exit()\n",
    "\n",
    "    unique_times = torch.unique(cur_df[:, TIME_COL])\n",
    "    time_slices_list = [cur_df[cur_df[:, TIME_COL] == t_val] for t_val in unique_times]\n",
    "\n",
    "    # --- 1. Pre-compute J-vector, Taper Grid, and Taper Autocorrelation ---\n",
    "    print(f\"Pre-computing J-vector ({TAPERING_FUNC.__name__} taper)...\")\n",
    "    J_vec, n1, n2, p, taper_grid = generate_Jvector_tapered(\n",
    "        time_slices_list,\n",
    "        tapering_func=TAPERING_FUNC,\n",
    "        lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if J_vec is None or J_vec.numel() == 0 or n1 == 0 or n2 == 0 or p == 0: print(f\"Error: J-vector generation failed.\"); exit()\n",
    "    if taper_grid is None: print(\"Error: Taper grid not generated.\"); exit()\n",
    "\n",
    "    print(\"Pre-computing sample periodogram...\")\n",
    "    I_sample = calculate_sample_periodogram_vectorized(J_vec) # Now defined here\n",
    "\n",
    "    print(f\"Pre-computing {TAPERING_FUNC.__name__} taper autocorrelation...\")\n",
    "    taper_autocorr_grid = calculate_taper_autocorrelation_fft(taper_grid, n1, n2, DEVICE) # Now defined here\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any(): print(\"Error: NaN/Inf in sample periodogram.\"); exit()\n",
    "    if torch.isnan(taper_autocorr_grid).any() or torch.isinf(taper_autocorr_grid).any(): print(\"Error: NaN/Inf in taper autocorrelation.\"); exit()\n",
    "\n",
    "    print(f\"Data grid: {n1}x{n2}, {p} time points. J-vector, Periodogram, Taper Autocorr on {DEVICE}.\")\n",
    "\n",
    "    # --- 2. Define Parameter Estimates to Evaluate (NATURAL SCALE) ---\n",
    "    # Example 1: Your fixed initial parameters (converted to natural scale)\n",
    "    param_estimates_initial_natural = [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769] \n",
    "\n",
    "    # Example 2: Hypothetical optimized parameters (NATURAL SCALE)\n",
    "    param_estimates_optimized_natural =  [31.2594, 0.665, 1.8981, 0.0, 0.1317, -0.0, 1.9785]\n",
    "\n",
    "    # --- 3. Calculate Likelihood Directly ---\n",
    "    print(\"\\nCalculating likelihood for initial parameters (natural scale):\")\n",
    "    # Convert natural to log scale for the likelihood function\n",
    "    params_initial_log = torch.tensor(param_estimates_initial_natural, dtype=torch.float32)\n",
    "    log_indices = [0, 1, 2, 6]\n",
    "    params_initial_log[log_indices] = torch.log(params_initial_log[log_indices])\n",
    "\n",
    "    with torch.no_grad(): # Disable gradients for evaluation\n",
    "        likelihood_initial = whittle_likelihood_loss_tapered(\n",
    "            params=params_initial_log.to(DEVICE), # Pass log-scale tensor\n",
    "            I_sample=I_sample.to(DEVICE),\n",
    "            n1=n1, n2=n2, p=p,\n",
    "            taper_autocorr_grid=taper_autocorr_grid.to(DEVICE),\n",
    "            delta1=DELTA_LAT, delta2=DELTA_LON\n",
    "        )\n",
    "    print(f\"-> Whittle Likelihood ('Vecchia Optimized' Params): {likelihood_initial.item():.3f}\")\n",
    "\n",
    "\n",
    "    print(\"\\nCalculating likelihood for 'optimized' parameters (natural scale):\")\n",
    "    # Convert natural to log scale for the likelihood function\n",
    "    params_optimized_log = torch.tensor(param_estimates_optimized_natural, dtype=torch.float32)\n",
    "    # Check for positivity before log\n",
    "    if torch.any(params_optimized_log[log_indices] <= 0):\n",
    "         print(\"Error: Optimized params have non-positive values for log conversion.\")\n",
    "         likelihood_optimized = torch.tensor(float('nan')) # Or handle appropriately\n",
    "    else:\n",
    "         params_optimized_log[log_indices] = torch.log(params_optimized_log[log_indices])\n",
    "\n",
    "         with torch.no_grad():\n",
    "             likelihood_optimized = whittle_likelihood_loss_tapered(\n",
    "                 params=params_optimized_log.to(DEVICE), # Pass log-scale tensor\n",
    "                 I_sample=I_sample.to(DEVICE),\n",
    "                 n1=n1, n2=n2, p=p,\n",
    "                 taper_autocorr_grid=taper_autocorr_grid.to(DEVICE),\n",
    "                 delta1=DELTA_LAT, delta2=DELTA_LON\n",
    "             )\n",
    "    print(f\"-> Whittle Likelihood ('Whittle Optimized' Params): {likelihood_optimized.item():.3f}\")\n",
    "\n",
    "\n",
    "    # --- 4. Compare Results ---\n",
    "    likelihood_initial_val = likelihood_initial.item()\n",
    "    likelihood_optimized_val = likelihood_optimized.item()\n",
    "    if not (np.isnan(likelihood_initial_val) or np.isnan(likelihood_optimized_val) or np.isinf(likelihood_initial_val) or np.isinf(likelihood_optimized_val)):\n",
    "        if likelihood_optimized_val < likelihood_initial_val: print(f\"\\n'Optimized' parameters better by {likelihood_initial_val - likelihood_optimized_val:.3f}.\")\n",
    "        elif likelihood_initial_val < likelihood_optimized_val: print(f\"\\nInitial parameters better by {likelihood_optimized_val - likelihood_initial_val:.3f}.\")\n",
    "        else: print(\"\\nBoth parameter sets yield the same likelihood.\")\n",
    "    else: print(\"\\nCould not compare likelihoods due to NaN or Inf results.\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7a1309",
   "metadata": {},
   "source": [
    "## bartlett tapering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3016920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded 31 days from spatial_first_difference_data.pkl.\n",
      "Pre-computing J-vector (Bartlett taper)...\n",
      "Pre-computing sample periodogram...\n",
      "Pre-computing Bartlett taper autocorrelation...\n",
      "Data grid: 113x158, 8 time points. J-vector, Periodogram, Taper Autocorr on cpu.\n",
      "\n",
      "============================== Initialization Run 1/1 ==============================\n",
      "Starting with FIXED params (log-scale): [3.2189, 0.0, 0.4055, -0.05, -0.08, 0.05, 0.6931]\n",
      "Starting optimization run 1 on device cpu (Bartlett taper, EXP kernel)...\n",
      "--- Epoch 1/700 (LR: 0.005000) ---\n",
      " Loss: 8180.4858\n",
      " Parameters (Natural Scale): [24.8753  1.005   1.5075 -0.03   -0.06    0.07    1.99  ]\n",
      "--- Epoch 51/700 (LR: 0.004240) ---\n",
      " Loss: 6522.7622\n",
      " Parameters (Natural Scale): [2.30309e+01 1.06020e+00 1.71420e+00 2.33300e-01 1.49600e-01 4.70000e-03\n",
      " 1.81810e+00]\n",
      "--- Epoch 101/700 (LR: 0.002461) ---\n",
      " Loss: 6332.5908\n",
      " Parameters (Natural Scale): [2.30089e+01 1.05530e+00 1.72840e+00 2.37400e-01 1.43200e-01 1.60000e-03\n",
      " 1.80290e+00]\n",
      "--- Epoch 151/700 (LR: 0.000706) ---\n",
      " Loss: 6360.0117\n",
      " Parameters (Natural Scale): [2.29922e+01 1.05470e+00 1.73190e+00 2.36900e-01 1.44200e-01 6.00000e-04\n",
      " 1.79540e+00]\n",
      "--- Epoch 201/700 (LR: 0.000001) ---\n",
      " Loss: 6284.4399\n",
      " Parameters (Natural Scale): [22.9922  1.0547  1.7323  0.237   0.1443  0.      1.7942]\n",
      "--- Epoch 251/700 (LR: 0.000761) ---\n",
      " Loss: 6300.7603\n",
      " Parameters (Natural Scale): [22.9934  1.0546  1.7328  0.2371  0.1446  0.      1.7928]\n",
      "--- Epoch 301/700 (LR: 0.002540) ---\n",
      " Loss: 6339.1333\n",
      " Parameters (Natural Scale): [23.0111  1.0536  1.7341  0.2373  0.1433  0.      1.7845]\n",
      "--- Epoch 351/700 (LR: 0.004295) ---\n",
      " Loss: 6356.3364\n",
      " Parameters (Natural Scale): [23.1145  1.0496  1.7359  0.2381  0.1454  0.      1.7705]\n",
      "--- Epoch 401/700 (LR: 0.005000) ---\n",
      " Loss: 6358.6958\n",
      " Parameters (Natural Scale): [23.3465  1.042   1.7333  0.24    0.1427  0.      1.7566]\n",
      "--- Epoch 451/700 (LR: 0.004240) ---\n",
      " Loss: 6335.4287\n",
      " Parameters (Natural Scale): [23.6188  1.0334  1.7323  0.2419  0.1455  0.      1.7435]\n",
      "--- Epoch 501/700 (LR: 0.002461) ---\n",
      " Loss: 6316.5396\n",
      " Parameters (Natural Scale): [23.832   1.0273  1.7305  0.2441  0.1444 -0.      1.7335]\n",
      "--- Epoch 551/700 (LR: 0.000706) ---\n",
      " Loss: 6257.9326\n",
      " Parameters (Natural Scale): [23.9354  1.0248  1.73    0.2436  0.1445  0.      1.7285]\n",
      "--- Epoch 601/700 (LR: 0.000001) ---\n",
      " Loss: 6230.4395\n",
      " Parameters (Natural Scale): [23.9528  1.0246  1.73    0.244   0.1445 -0.      1.7276]\n",
      "--- Epoch 651/700 (LR: 0.000761) ---\n",
      " Loss: 6271.2568\n",
      " Parameters (Natural Scale): [ 2.39732e+01  1.02430e+00  1.73000e+00  2.44100e-01  1.44800e-01\n",
      " -1.00000e-04  1.72660e+00]\n",
      "--- Epoch 700/700 (LR: 0.002500) ---\n",
      " Loss: 6297.4873\n",
      " Parameters (Natural Scale): [24.1013  1.0223  1.7288  0.2447  0.1434  0.      1.721 ]\n",
      "\n",
      "--- Training Complete ---\n",
      "\n",
      "FINAL BEST STATE ACHIEVED:\n",
      "Best Loss: 6229.785\n",
      "Parameters (Natural Scale): [23.9528, 1.0246, 1.73, 0.244, 0.1445, 0.0, 1.7276]\n",
      "\n",
      "\n",
      "========================= Overall Result from Run =========================\n",
      "Run Loss: 6229.785\n",
      "Final Parameters (Natural Scale): [23.9528, 1.0246, 1.73, 0.244, 0.1445, 0.0, 1.7276]\n",
      "\n",
      "Total execution time: 262.77 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # Keep if plotting might be added later\n",
    "import cmath\n",
    "import pickle\n",
    "import time # For timing\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import Parameter\n",
    "import torch.fft # Explicit import for fft functions\n",
    "\n",
    "# =========================================================================\n",
    "# 1. Tapering, Autocorrelation, and Covariance Functions\n",
    "# =========================================================================\n",
    "\n",
    "# --- Tapering Functions (Bartlett is used) ---\n",
    "def cgn_hamming(u, n1, n2): # Kept for potential future use\n",
    "    \"\"\"Computes a 2D Hamming window.\"\"\"\n",
    "    u1, u2 = u\n",
    "    device = u1.device if isinstance(u1, torch.Tensor) else (u2.device if isinstance(u2, torch.Tensor) else torch.device('cpu'))\n",
    "    u1_tensor = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_tensor = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    n1_eff = float(n1) if n1 > 0 else 1.0\n",
    "    n2_eff = float(n2) if n2 > 0 else 1.0\n",
    "    hamming1 = 0.54 + 0.46 * torch.cos(2.0 * torch.pi * u1_tensor / n1_eff)\n",
    "    hamming2 = 0.54 + 0.46 * torch.cos(2.0 * torch.pi * u2_tensor / n2_eff)\n",
    "    return hamming1 * hamming2\n",
    "\n",
    "def cgn_2dbartlett(u, n1, n2):\n",
    "    \"\"\"Computes a 2D Bartlett window function.\"\"\"\n",
    "    u1, u2 = u\n",
    "    device = u1.device if isinstance(u1, torch.Tensor) else (u2.device if isinstance(u2, torch.Tensor) else torch.device('cpu'))\n",
    "    u1_tensor = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_tensor = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    n1_eff = float(n1) if n1 > 0 else 1.0\n",
    "    n2_eff = float(n2) if n2 > 0 else 1.0\n",
    "    kernel = (1.0 - torch.abs(u1_tensor) / n1_eff) * (1.0 - torch.abs(u2_tensor) / n2_eff)\n",
    "    return torch.clamp(kernel, min=0.0)\n",
    "\n",
    "# --- Function to Calculate Taper Autocorrelation (Unchanged) ---\n",
    "def calculate_taper_autocorrelation_fft(taper_grid, n1, n2, device):\n",
    "    \"\"\"\n",
    "    Computes the normalized taper autocorrelation function c_gn(u) using FFT.\n",
    "    \"\"\"\n",
    "    taper_grid = taper_grid.to(device)\n",
    "    H = torch.sum(taper_grid**2)\n",
    "    if H < 1e-12:\n",
    "        print(\"Warning: Sum of squared taper weights (H) is near zero.\")\n",
    "        return torch.zeros((2*n1-1, 2*n2-1), device=device, dtype=taper_grid.dtype)\n",
    "    N1, N2 = 2 * n1 - 1, 2 * n2 - 1\n",
    "    taper_fft = torch.fft.fft2(taper_grid, s=(N1, N2))\n",
    "    power_spectrum = torch.abs(taper_fft)**2\n",
    "    autocorr_unnormalized = torch.fft.ifft2(power_spectrum).real\n",
    "    autocorr_shifted = torch.fft.fftshift(autocorr_unnormalized)\n",
    "    c_gn_grid = autocorr_shifted / (H + 1e-12)\n",
    "    return c_gn_grid\n",
    "\n",
    "# --- Covariance of Original Field X (EXPONENTIAL Kernel) (Unchanged) ---\n",
    "def cov_x_exponential(u1, u2, t, params):\n",
    "    \"\"\"\n",
    "    Computes spatio-temporal autocovariance of X using EXPONENTIAL kernel.\n",
    "    Expects log-scale params [0,1,2,6]. Handles device internally.\n",
    "    \"\"\"\n",
    "    device = params.device\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "    log_params_indices = [0, 1, 2, 6]\n",
    "    if torch.isnan(params[log_params_indices]).any() or torch.isinf(params[log_params_indices]).any():\n",
    "         print(\"Warning: NaN/Inf in log-params before exp in cov_x_exponential\")\n",
    "         out_shape = torch.broadcast_shapes(u1_dev.shape, u2_dev.shape, t_dev.shape)\n",
    "         return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "    sigmasq, r_lat, r_lon, nugget = torch.exp(params[log_params_indices])\n",
    "    a_lat, a_lon, beta = params[3], params[4], params[5]\n",
    "    if r_lat < 1e-6 or r_lon < 1e-6:\n",
    "        r_lat = torch.clamp(r_lat, min=1e-6); r_lon = torch.clamp(r_lon, min=1e-6)\n",
    "    x1 = u1_dev / r_lat - a_lat * t_dev\n",
    "    x2 = u2_dev / r_lon - a_lon * t_dev\n",
    "    x3 = beta * t_dev\n",
    "    distance_sq = x1**2 + x2**2 + x3**2\n",
    "    epsilon = 1e-12\n",
    "    distance_sq_clamped = torch.clamp(distance_sq, min=0.0)\n",
    "    D = torch.sqrt(distance_sq_clamped + epsilon)\n",
    "    cov_smooth = sigmasq * torch.exp(-D)\n",
    "    is_zero_lag = (torch.abs(u1_dev) < 1e-9) & (torch.abs(u2_dev) < 1e-9) & (torch.abs(t_dev) < 1e-9)\n",
    "    final_cov = torch.where(is_zero_lag, cov_smooth + nugget, cov_smooth)\n",
    "    if torch.isnan(final_cov).any(): print(\"Warning: NaN detected in cov_x_exponential output.\")\n",
    "    return final_cov\n",
    "\n",
    "# --- Covariance of Spatially Differenced Field Y (Unchanged structure) ---\n",
    "def cov_spatial_difference(u1, u2, t, params, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Calculates covariance Cov(Y(s), Y(s+u)) for Y(s) = X(s+d1) + X(s+d2) - 2X(s)\n",
    "    Based on the underlying EXPONENTIAL covariance cov_x_exponential.\n",
    "    \"\"\"\n",
    "    weights = {(0, 0): -2.0, (1, 0): 1.0, (0, 1): 1.0}\n",
    "    device = params.device\n",
    "    out_shape = torch.broadcast_shapes(u1.shape if isinstance(u1, torch.Tensor) else (),\n",
    "                                     u2.shape if isinstance(u2, torch.Tensor) else (),\n",
    "                                     t.shape if isinstance(t, torch.Tensor) else ())\n",
    "    cov = torch.zeros(out_shape, device=device, dtype=torch.float32)\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "    for (a_idx, b_idx), w_ab in weights.items():\n",
    "        offset_a1 = a_idx * delta1; offset_a2 = b_idx * delta2\n",
    "        for (c_idx, d_idx), w_cd in weights.items():\n",
    "            offset_c1 = c_idx * delta1; offset_c2 = d_idx * delta2\n",
    "            lag_u1 = u1_dev + (offset_a1 - offset_c1)\n",
    "            lag_u2 = u2_dev + (offset_a2 - offset_c2)\n",
    "            term_cov = cov_x_exponential(lag_u1, lag_u2, t_dev, params) # Calls EXPONENTIAL version\n",
    "            if torch.isnan(term_cov).any():\n",
    "                 print(f\"Warning: NaN in term_cov within cov_spatial_difference.\")\n",
    "                 return torch.full_like(cov, float('nan'))\n",
    "            cov += w_ab * w_cd * term_cov\n",
    "    if torch.isnan(cov).any(): print(\"Warning: NaN in final cov_spatial_difference output.\")\n",
    "    return cov\n",
    "\n",
    "# --- cn_bar using Taper Autocorrelation (Unchanged structure) ---\n",
    "def cn_bar_tapered(u1, u2, t, params, n1, n2, taper_autocorr_grid, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Computes c_X(u) * c_gn(u) using the pre-computed taper_autocorr_grid.\n",
    "    \"\"\"\n",
    "    device = params.device\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "    cov_X_value = cov_spatial_difference(u1_dev, u2_dev, t_dev, params, delta1, delta2)\n",
    "    u1_idx = u1_dev.long(); u2_idx = u2_dev.long()\n",
    "    # Indices into the centered autocorrelation grid [2n1-1, 2n2-1]\n",
    "    idx1 = (n1 - 1 + u1_idx); idx2 = (n2 - 1 + u2_idx)\n",
    "    # Clamp indices to ensure they are within the valid range\n",
    "    idx1 = torch.clamp(idx1, 0, 2 * n1 - 2); idx2 = torch.clamp(idx2, 0, 2 * n2 - 2)\n",
    "    # Ensure taper_autocorr_grid is on the correct device\n",
    "    taper_autocorr_value = taper_autocorr_grid.to(device)[idx1, idx2]\n",
    "    if torch.isnan(cov_X_value).any() or torch.isnan(taper_autocorr_value).any():\n",
    "        print(\"Warning: NaN detected before multiplication in cn_bar_tapered.\")\n",
    "        out_shape = torch.broadcast_shapes(cov_X_value.shape, taper_autocorr_value.shape)\n",
    "        return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "    result = cov_X_value * taper_autocorr_value\n",
    "    if torch.isnan(result).any(): print(\"Warning: NaN in cn_bar_tapered output.\")\n",
    "    return result\n",
    "\n",
    "# --- Expected Periodogram (uses cn_bar_tapered) (Unchanged structure) ---\n",
    "def expected_periodogram_fft_tapered(params, n1, n2, p, taper_autocorr_grid, cov_func, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Calculates the expected periodogram I(omega_s) using c_gn(u).\n",
    "    \"\"\"\n",
    "    device = params.device if isinstance(params, torch.Tensor) else params[0].device\n",
    "    if isinstance(params, list): params_tensor = torch.cat([p.to(device) for p in params])\n",
    "    else: params_tensor = params.to(device)\n",
    "    product_tensor = torch.zeros((n1, n2, p, p), dtype=torch.complex64, device=device)\n",
    "    t_lags = torch.arange(p, dtype=torch.float32, device=device)\n",
    "    u1_mesh_grid, u2_mesh_grid = torch.meshgrid(\n",
    "        torch.arange(n1, dtype=torch.float32, device=device),\n",
    "        torch.arange(n2, dtype=torch.float32, device=device), indexing='ij'\n",
    "    )\n",
    "    for q in range(p):\n",
    "        for r in range(p):\n",
    "            t_diff = t_lags[q] - t_lags[r]\n",
    "            cov_times_autocorr = cn_bar_tapered(\n",
    "                u1_mesh_grid, u2_mesh_grid, t_diff,\n",
    "                params_tensor, n1, n2, taper_autocorr_grid, delta1, delta2\n",
    "            )\n",
    "            if torch.isnan(cov_times_autocorr).any():\n",
    "                 print(f\"Warning: NaN in cov_times_autocorr t_lag {t_diff.item():.2f}.\")\n",
    "                 product_tensor[:, :, q, r] = float('nan')\n",
    "            else:\n",
    "                 product_tensor[:, :, q, r] = cov_times_autocorr.to(torch.complex64)\n",
    "    if torch.isnan(product_tensor).any():\n",
    "        print(\"Warning: NaN in product_tensor before FFT.\")\n",
    "        nan_shape = (n1, n2, p, p); return torch.full(nan_shape, float('nan'), dtype=torch.complex64, device=device)\n",
    "    fft_result = torch.fft.fft2(product_tensor, dim=(0, 1))\n",
    "    normalization_factor = 1.0 / (4.0 * cmath.pi**2)\n",
    "    result = fft_result * normalization_factor\n",
    "    if torch.isnan(result).any(): print(\"Warning: NaN in expected_periodogram_fft_tapered output.\")\n",
    "    return result\n",
    "\n",
    "# =========================================================================\n",
    "# 2. Data Processing (Uses Tapering)\n",
    "# =========================================================================\n",
    "def generate_Jvector_tapered(tensor_list, tapering_func, lat_col, lon_col, val_col, device):\n",
    "    \"\"\"\n",
    "    Generates J-vector using the specified taper, returns J-vector and taper grid.\n",
    "    \"\"\"\n",
    "    p = len(tensor_list)\n",
    "    taper_grid_out = None # Initialize\n",
    "    if p == 0: return torch.empty(0, 0, 0, device=device), 0, 0, 0, taper_grid_out\n",
    "\n",
    "    valid_tensors = [t for t in tensor_list if t.numel() > 0 and t.shape[1] > max(lat_col, lon_col, val_col)]\n",
    "    if not valid_tensors:\n",
    "         print(\"Warning: No valid tensors found in tensor_list.\"); return torch.empty(0, 0, 0, device=device), 0, 0, 0, taper_grid_out\n",
    "\n",
    "    try:\n",
    "        all_lats_cpu = torch.cat([t[:, lat_col] for t in valid_tensors])\n",
    "        all_lons_cpu = torch.cat([t[:, lon_col] for t in valid_tensors])\n",
    "    except IndexError: print(f\"Error: Invalid column index.\"); return torch.empty(0, 0, 0, device=device), 0, 0, 0, taper_grid_out\n",
    "\n",
    "    all_lats_cpu = all_lats_cpu[~torch.isnan(all_lats_cpu) & ~torch.isinf(all_lats_cpu)]\n",
    "    all_lons_cpu = all_lons_cpu[~torch.isnan(all_lons_cpu) & ~torch.isinf(all_lons_cpu)]\n",
    "    if all_lats_cpu.numel() == 0 or all_lons_cpu.numel() == 0:\n",
    "        print(\"Warning: No valid coords.\"); return torch.empty(0, 0, 0, device=device), 0, 0, 0, taper_grid_out\n",
    "\n",
    "    unique_lats_cpu, unique_lons_cpu = torch.unique(all_lats_cpu), torch.unique(all_lons_cpu)\n",
    "    n1, n2 = len(unique_lats_cpu), len(unique_lons_cpu)\n",
    "    if n1 == 0 or n2 == 0: print(\"Warning: Grid dims zero.\"); return torch.empty(0, 0, 0, device=device), 0, 0, 0, taper_grid_out\n",
    "\n",
    "    lat_map = {lat.item(): i for i, lat in enumerate(unique_lats_cpu)}\n",
    "    lon_map = {lon.item(): i for i, lon in enumerate(unique_lons_cpu)}\n",
    "\n",
    "    u1_mesh_cpu, u2_mesh_cpu = torch.meshgrid(torch.arange(n1, dtype=torch.float32), torch.arange(n2, dtype=torch.float32), indexing='ij')\n",
    "    taper_grid_out = tapering_func((u1_mesh_cpu, u2_mesh_cpu), n1, n2).to(device) # Taper on device\n",
    "\n",
    "    fft_results = []\n",
    "    for tensor in tensor_list:\n",
    "        data_grid = torch.zeros((n1, n2), dtype=torch.float32, device=device)\n",
    "        if tensor.numel() > 0 and tensor.shape[1] > max(lat_col, lon_col, val_col):\n",
    "            for row in tensor:\n",
    "                lat_item, lon_item = row[lat_col].item(), row[lon_col].item()\n",
    "                if not (np.isnan(lat_item) or np.isnan(lon_item)):\n",
    "                    i = lat_map.get(lat_item); j = lon_map.get(lon_item)\n",
    "                    if i is not None and j is not None:\n",
    "                        val = row[val_col]; val_num = val.item() if isinstance(val, torch.Tensor) else val\n",
    "                        if not np.isnan(val_num) and not np.isinf(val_num): data_grid[i, j] = val_num\n",
    "\n",
    "        data_grid_tapered = data_grid * taper_grid_out # Apply taper\n",
    "        if torch.isnan(data_grid_tapered).any() or torch.isinf(data_grid_tapered).any():\n",
    "             print(\"Warning: NaN/Inf in tapered data grid. Replacing zeros.\"); data_grid_tapered = torch.nan_to_num(data_grid_tapered)\n",
    "        fft_results.append(torch.fft.fft2(data_grid_tapered))\n",
    "\n",
    "    if not fft_results: print(\"Warning: No FFT results.\"); return torch.empty(0, 0, 0, device=device), n1, n2, 0, taper_grid_out\n",
    "\n",
    "    J_vector_tensor = torch.stack(fft_results, dim=2).to(device)\n",
    "    H = torch.sum(taper_grid_out**2)\n",
    "    if H < 1e-12: print(\"Warning: H near zero.\"); norm_factor = torch.tensor(0.0, device=device)\n",
    "    else: norm_factor = (torch.sqrt(1.0 / H) / (2.0 * cmath.pi)).to(device)\n",
    "\n",
    "    result = J_vector_tensor * norm_factor\n",
    "    if torch.isnan(result).any(): print(\"Warning: NaN in J_vector output.\")\n",
    "    return result, n1, n2, p, taper_grid_out\n",
    "\n",
    "def calculate_sample_periodogram_vectorized(J_vector_tensor):\n",
    "    \"\"\"Calculates sample periodogram I_n = J J^H.\"\"\"\n",
    "    if torch.isnan(J_vector_tensor).any() or torch.isinf(J_vector_tensor).any():\n",
    "        print(\"Warning: NaN/Inf in J_vector input.\"); n1,n2,p=J_vector_tensor.shape; return torch.full((n1,n2,p,p), float('nan'), dtype=torch.complex64, device=J_vector_tensor.device)\n",
    "    J_col = J_vector_tensor.unsqueeze(-1); J_row_conj = J_vector_tensor.unsqueeze(-2).conj()\n",
    "    result = J_col @ J_row_conj\n",
    "    if torch.isnan(result).any(): print(\"Warning: NaN in periodogram matrix output.\")\n",
    "    return result\n",
    "\n",
    "# =========================================================================\n",
    "# 4. Likelihood Calculation (Adapted for Tapering with Autocorrelation)\n",
    "# =========================================================================\n",
    "\n",
    "def whittle_likelihood_loss_tapered(params, I_sample, n1, n2, p, taper_autocorr_grid, delta1, delta2):\n",
    "    \"\"\" Whittle Likelihood using data tapering and exact taper autocorrelation c_gn.\"\"\"\n",
    "    device = I_sample.device\n",
    "    params_tensor = params.to(device)\n",
    "    if torch.isnan(params_tensor).any() or torch.isinf(params_tensor).any(): print(\"Warning: NaN/Inf in likelihood params.\"); return torch.tensor(float('nan'), device=device)\n",
    "    I_expected = expected_periodogram_fft_tapered(\n",
    "        params_tensor, n1, n2, p, taper_autocorr_grid, cov_spatial_difference, delta1, delta2\n",
    "    )\n",
    "    if torch.isnan(I_expected).any() or torch.isinf(I_expected).any(): print(\"Warning: NaN/Inf from expected periodogram.\"); return torch.tensor(float('nan'), device=device)\n",
    "    eye_matrix = torch.eye(p, dtype=torch.complex64, device=device)\n",
    "    diag_vals = torch.abs(I_expected.diagonal(dim1=-2, dim2=-1))\n",
    "    mean_diag_abs = diag_vals.mean().item() if diag_vals.numel() > 0 and not torch.isnan(diag_vals).all() else 1.0\n",
    "    diag_load = max(mean_diag_abs * 1e-8, 1e-9)\n",
    "    I_expected_stable = I_expected + eye_matrix * diag_load\n",
    "    sign, logabsdet = torch.linalg.slogdet(I_expected_stable)\n",
    "    if torch.any(sign.real <= 1e-9): print(\"Warning: Non-positive determinant.\"); log_det_term = torch.where(sign.real > 1e-9, logabsdet, torch.tensor(1e10, device=device))\n",
    "    else: log_det_term = logabsdet\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any(): print(\"Warning: NaN/Inf in I_sample.\"); return torch.tensor(float('nan'), device=device)\n",
    "    try:\n",
    "        solved_term = torch.linalg.solve(I_expected_stable, I_sample)\n",
    "        trace_term = torch.einsum('...ii->...', solved_term).real\n",
    "    except torch.linalg.LinAlgError as e: print(f\"Warning: LinAlgError: {e}.\"); return torch.tensor(float('inf'), device=device)\n",
    "    if torch.isnan(trace_term).any() or torch.isinf(trace_term).any(): print(\"Warning: NaN/Inf in trace_term.\"); return torch.tensor(float('nan'), device=device)\n",
    "    likelihood_terms = log_det_term + trace_term\n",
    "    if torch.isnan(likelihood_terms).any(): print(\"Warning: NaN in likelihood_terms.\"); return torch.tensor(float('nan'), device=device)\n",
    "    total_sum = torch.sum(likelihood_terms)\n",
    "    dc_term = likelihood_terms[0, 0] if n1 > 0 and n2 > 0 else torch.tensor(0.0, device=device)\n",
    "    if torch.isnan(dc_term).any() or torch.isinf(dc_term).any(): print(\"Warning: NaN/Inf DC term.\"); dc_term = torch.tensor(0.0, device=device)\n",
    "    loss = total_sum - dc_term if (n1 > 1 or n2 > 1) else total_sum\n",
    "    if torch.isnan(loss) or torch.isinf(loss): print(\"Warning: NaN/Inf final loss.\"); return torch.tensor(float('inf'), device=device)\n",
    "    return loss\n",
    "\n",
    "# =========================================================================\n",
    "# 5. Training Loop (CORRECTED version, adapted for tapering)\n",
    "# =========================================================================\n",
    "def run_full_tapered(params_list, optimizer, scheduler, I_sample, n1, n2, p, taper_autocorr_grid, epochs=600, device='cpu'):\n",
    "    \"\"\"Corrected training loop using parameter list and tapered likelihood.\"\"\"\n",
    "    best_loss = float('inf')\n",
    "    params_list = [p.to(device) for p in params_list]\n",
    "    best_params_state = [p.detach().clone() for p in params_list]\n",
    "    epochs_completed = 0\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063\n",
    "    def get_printable_params(p_list):\n",
    "        valid_tensors = [p for p in p_list if isinstance(p, torch.Tensor)];\n",
    "        if not valid_tensors: return \"Invalid params_list\"\n",
    "        p_cat = torch.cat([p.detach().clone().cpu() for p in valid_tensors])\n",
    "        log_indices = [0, 1, 2, 6]\n",
    "        if all(idx < len(p_cat) for idx in log_indices):\n",
    "            log_vals = p_cat[log_indices]\n",
    "            if not (torch.isnan(log_vals).any() or torch.isinf(log_vals).any()): p_cat[log_indices] = torch.exp(log_vals)\n",
    "            else: p_cat[log_indices] = float('nan')\n",
    "        return p_cat.numpy().round(4)\n",
    "    I_sample_dev = I_sample.to(device)\n",
    "    taper_autocorr_grid_dev = taper_autocorr_grid.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        epochs_completed = epoch + 1\n",
    "        optimizer.zero_grad()\n",
    "        params_tensor = torch.cat(params_list)\n",
    "        loss = whittle_likelihood_loss_tapered(\n",
    "            params_tensor, I_sample_dev, n1, n2, p, taper_autocorr_grid_dev, DELTA_LAT, DELTA_LON\n",
    "        )\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"Loss NaN/Inf epoch {epoch+1}. Stop.\");\n",
    "            if epoch == 0: best_params_state = None;\n",
    "            epochs_completed = epoch; break\n",
    "        loss.backward()\n",
    "        nan_grad = False\n",
    "        for param in params_list:\n",
    "            if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()): nan_grad = True; break\n",
    "        if nan_grad: print(f\"Warning: NaN/Inf grad epoch {epoch+1}. Skip step.\"); optimizer.zero_grad(); continue\n",
    "        all_params_on_device = params_list\n",
    "        if all_params_on_device: torch.nn.utils.clip_grad_norm_(all_params_on_device, max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        current_loss_item = loss.item()\n",
    "        if current_loss_item < best_loss:\n",
    "            params_valid = not any(torch.isnan(p.data).any() or torch.isinf(p.data).any() for p in params_list)\n",
    "            if params_valid: best_loss = current_loss_item; best_params_state = [p.detach().clone() for p in params_list]\n",
    "            else: print(f\"Warning: NaN/Inf params epoch {epoch+1}. Not saving.\")\n",
    "        current_lr = optimizer.param_groups[0]['lr'] if optimizer.param_groups else 0.0\n",
    "        if epoch % 50 == 0 or epoch == epochs - 1:\n",
    "            print(f'--- Epoch {epoch+1}/{epochs} (LR: {current_lr:.6f}) ---'); print(f' Loss: {current_loss_item:.4f}')\n",
    "            print(f' Parameters (Natural Scale): {get_printable_params(params_list)}')\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    if best_params_state is None: print(\"Training failed.\"); return None, epochs_completed\n",
    "    final_params_log_scale = torch.cat([p.cpu() for p in best_params_state])\n",
    "    final_params_natural_scale = final_params_log_scale.detach().clone()\n",
    "    log_indices = [0, 1, 2, 6]\n",
    "    if all(idx < len(final_params_natural_scale) for idx in log_indices):\n",
    "        log_vals = final_params_natural_scale[log_indices]\n",
    "        if not (torch.isnan(log_vals).any() or torch.isinf(log_vals).any()): final_params_natural_scale[log_indices] = torch.exp(log_vals)\n",
    "        else: final_params_natural_scale[log_indices] = float('nan')\n",
    "    final_params_rounded = [round(p.item(), 4) if not np.isnan(p.item()) else float('nan') for p in final_params_natural_scale]\n",
    "    final_loss_rounded = round(best_loss, 3) if best_loss != float('inf') else float('inf')\n",
    "    print(f'\\nFINAL BEST STATE ACHIEVED:'); print(f'Best Loss: {final_loss_rounded}')\n",
    "    print(f'Parameters (Natural Scale): {final_params_rounded}')\n",
    "    return final_params_rounded + [final_loss_rounded], epochs_completed\n",
    "\n",
    "# =========================================================================\n",
    "# 6. Main Execution Script (BARTLETT Tapering, Spatially Differenced Data, EXP Kernel)\n",
    "# =========================================================================\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    DAY_TO_RUN = 1\n",
    "    TAPERING_FUNC = cgn_2dbartlett # <<< Use Bartlett taper\n",
    "    NUM_RUNS = 1\n",
    "    EPOCHS = 700\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063\n",
    "\n",
    "    LAT_COL, LON_COL = 0, 1\n",
    "    VAL_COL = 2 # Spatially differenced value\n",
    "    TIME_COL = 3\n",
    "\n",
    "    # --- Load Spatially Differenced Data ---\n",
    "    try:\n",
    "        with open(\"spatial_first_difference_data.pkl\", 'rb') as f:\n",
    "            processed_df = pickle.load(f)\n",
    "        print(f\"Loaded {len(processed_df)} days from spatial_first_difference_data.pkl.\")\n",
    "        processed_df = [\n",
    "            torch.tensor(arr, dtype=torch.float32).cpu() if not isinstance(arr, torch.Tensor)\n",
    "            else arr.cpu().to(torch.float32)\n",
    "            for arr in processed_df\n",
    "        ]\n",
    "        if not processed_df: raise ValueError(\"'processed_df' is empty.\")\n",
    "    except FileNotFoundError: print(\"Error: `spatial_first_difference_data.pkl` not found.\"); exit()\n",
    "    except Exception as e: print(f\"Error loading 'processed_df': {e}\"); exit()\n",
    "\n",
    "    if DAY_TO_RUN > len(processed_df) or DAY_TO_RUN <= 0: print(f\"Error: DAY_TO_RUN out of bounds.\"); exit()\n",
    "\n",
    "    cur_df = processed_df[DAY_TO_RUN - 1]\n",
    "    if cur_df.numel() == 0 or cur_df.shape[1] <= max(LAT_COL, LON_COL, VAL_COL, TIME_COL): print(f\"Error: Data for Day {DAY_TO_RUN} empty/invalid.\"); exit()\n",
    "\n",
    "    unique_times = torch.unique(cur_df[:, TIME_COL])\n",
    "    time_slices_list = [cur_df[cur_df[:, TIME_COL] == t_val] for t_val in unique_times]\n",
    "\n",
    "    # --- 1. Pre-compute J-vector, Taper Grid, and Taper Autocorrelation ---\n",
    "    print(\"Pre-computing J-vector (Bartlett taper)...\")\n",
    "    J_vec, n1, n2, p, taper_grid = generate_Jvector_tapered( # Use tapered version\n",
    "        time_slices_list,\n",
    "        tapering_func=TAPERING_FUNC, # Pass Bartlett\n",
    "        lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if J_vec is None or J_vec.numel() == 0 or n1 == 0 or n2 == 0 or p == 0: print(f\"Error: J-vector generation failed.\"); exit()\n",
    "\n",
    "    print(\"Pre-computing sample periodogram...\")\n",
    "    I_sample = calculate_sample_periodogram_vectorized(J_vec)\n",
    "\n",
    "    print(\"Pre-computing Bartlett taper autocorrelation...\")\n",
    "    taper_autocorr_grid = calculate_taper_autocorrelation_fft(taper_grid, n1, n2, DEVICE)\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any(): print(\"Error: NaN/Inf in sample periodogram.\"); exit()\n",
    "    if torch.isnan(taper_autocorr_grid).any() or torch.isinf(taper_autocorr_grid).any(): print(\"Error: NaN/Inf in taper autocorrelation.\"); exit()\n",
    "\n",
    "    print(f\"Data grid: {n1}x{n2}, {p} time points. J-vector, Periodogram, Taper Autocorr on {DEVICE}.\")\n",
    "\n",
    "    # --- 2. Optimization Loop ---\n",
    "    all_final_results = []\n",
    "    all_final_losses = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        print(f\"\\n{'='*30} Initialization Run {i+1}/{NUM_RUNS} {'='*30}\")\n",
    "\n",
    "        initial_params_values = [np.log(25), np.log(1),np.log(1.5), -0.05, -0.08, 0.05, np.log(2)]\n",
    "        print(f\"Starting with FIXED params (log-scale): {[round(p, 4) for p in initial_params_values]}\")\n",
    "        params_list = [Parameter(torch.tensor([val], dtype=torch.float32)) for val in initial_params_values]\n",
    "\n",
    "        lr_slow, lr_fast = 0.005, 0.02\n",
    "        slow_indices = [0, 1, 2, 6]; fast_indices = [3, 4, 5]\n",
    "        valid_slow_indices = [idx for idx in slow_indices if idx < len(params_list)]\n",
    "        valid_fast_indices = [idx for idx in fast_indices if idx < len(params_list)]\n",
    "        param_groups = [\n",
    "            {'params': [params_list[idx] for idx in valid_slow_indices], 'lr': lr_slow, 'name': 'slow_group'},\n",
    "            {'params': [params_list[idx] for idx in valid_fast_indices], 'lr': lr_fast, 'name': 'fast_group'}\n",
    "        ]\n",
    "        optimizer = torch.optim.Adam(param_groups)\n",
    "        T_MAX = 200; ETA_MIN = 1e-6\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=T_MAX, eta_min=ETA_MIN)\n",
    "\n",
    "        print(f\"Starting optimization run {i+1} on device {DEVICE} (Bartlett taper, EXP kernel)...\")\n",
    "        # --- Use the TAPERED training loop ---\n",
    "        final_results, epochs_run = run_full_tapered(\n",
    "            params_list=params_list,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            I_sample=I_sample,\n",
    "            n1=n1, n2=n2, p=p,\n",
    "            taper_autocorr_grid=taper_autocorr_grid, # Pass the autocorrelation grid\n",
    "            epochs=EPOCHS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "        if final_results: all_final_results.append(final_results); all_final_losses.append(final_results[-1])\n",
    "        else: all_final_results.append(None); all_final_losses.append(float('inf'))\n",
    "\n",
    "    print(f\"\\n\\n{'='*25} Overall Result from Run {'='*25}\")\n",
    "    valid_losses = [l for l in all_final_losses if l is not None and l != float('inf')]\n",
    "    if not valid_losses: print(f\"Run failed for Day {DAY_TO_RUN}.\")\n",
    "    else:\n",
    "        best_loss = valid_losses[0]; best_run_index = 0\n",
    "        best_results = all_final_results[best_run_index]\n",
    "        print(f\"Run Loss: {best_results[-1]}\")\n",
    "        print(f\"Final Parameters (Natural Scale): {best_results[:-1]}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
