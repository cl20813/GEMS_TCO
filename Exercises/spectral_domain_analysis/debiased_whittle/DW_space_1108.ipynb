{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0aabd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy                    # clone tensor\n",
    "import time\n",
    "\n",
    "# Custom imports\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "import typer\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO import configuration as config\n",
    "from GEMS_TCO import data_preprocess as dmbh\n",
    "from GEMS_TCO.data_loader import load_data2\n",
    "import os\n",
    "from sklearn.neighbors import BallTree\n",
    "from GEMS_TCO.data_loader import load_data2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ea323",
   "metadata": {},
   "source": [
    "Load monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a330e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting data to lat: [0.0, 5.0], lon: [123.0, 133.0]\n"
     ]
    }
   ],
   "source": [
    "space: List[str] = ['1', '1']\n",
    "lat_lon_resolution = [int(s) for s in space]\n",
    "mm_cond_number: int = 20\n",
    "years = ['2024']\n",
    "month_range = [7] \n",
    "\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "data_load_instance = load_data2(config.mac_data_load_path)\n",
    "\n",
    "\n",
    "df_map, ord_mm, nns_map = data_load_instance.load_maxmin_ordered_data_bymonthyear(\n",
    "lat_lon_resolution=lat_lon_resolution, \n",
    "mm_cond_number=mm_cond_number,\n",
    "years_=years, \n",
    "months_=month_range,\n",
    "lat_range=[0.0, 5.0],      \n",
    "lon_range=[123.0, 133.0] \n",
    ")\n",
    "\n",
    "#days: List[str] = ['0', '31']\n",
    "#days_s_e = [int(d) for d in days]\n",
    "#days_list = list(range(days_s_e[0], days_s_e[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a316a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18126, 4])\n"
     ]
    }
   ],
   "source": [
    "daily_aggregated_tensors = [] \n",
    "daily_hourly_maps = []        \n",
    "\n",
    "for day_index in range(31):\n",
    "  \n",
    "    hour_start_index = day_index * 8\n",
    "    #hour_end_index = (day_index + 1) * 8\n",
    "    hour_end_index = day_index*8 + 1\n",
    "    hour_indices = [hour_start_index, hour_end_index]\n",
    "    \n",
    "    # Load the data for the current day\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "        df_map, \n",
    "        hour_indices, \n",
    "        ord_mm= None,  \n",
    "        dtype=torch.float \n",
    "    )\n",
    "    # Append the day's data to their respective lists\n",
    "    daily_aggregated_tensors.append(day_aggregated_tensor)\n",
    "    daily_hourly_maps.append(day_hourly_map) \n",
    "\n",
    "print(daily_aggregated_tensors[0].shape)\n",
    "#print(daily_hourly_maps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93f6b4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results ---\n",
      "Number of final spatially-differenced day tensors: 31\n",
      "Processed data saved to spatial_first_difference_data.pkl\n",
      "\n",
      "Shape of the first final tensor: torch.Size([17854, 4])\n",
      "First final tensor head:\n",
      "tensor([[ 4.0000e-03,  1.2303e+02,  2.9422e+00,  2.1000e+01],\n",
      "        [ 4.0000e-03,  1.2309e+02,  1.9636e+00,  2.1000e+01],\n",
      "        [ 4.0000e-03,  1.2316e+02, -1.3187e+00,  2.1000e+01],\n",
      "        [ 4.0000e-03,  1.2322e+02, -3.1683e+00,  2.1000e+01],\n",
      "        [ 4.0000e-03,  1.2328e+02, -5.4922e-01,  2.1000e+01]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assume GEMS_TCO is a custom class/module you have available\n",
    "# from your_project import GEMS_TCO\n",
    "\n",
    "# =========================================================================\n",
    "# 1. Helper Functions\n",
    "# =========================================================================\n",
    "\n",
    "def subset_tensor(df_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Subsets a tensor to a specific lat/lon range.\"\"\"\n",
    "    #lat_mask = (df_tensor[:, 0] >= -5) & (df_tensor[:, 0] <= 6.3)\n",
    "    #lon_mask = (df_tensor[:, 1] >= 118) & (df_tensor[:, 1] <= 134.2)\n",
    "    lat_mask = (df_tensor[:, 0] >= 0) & (df_tensor[:, 0] <= 5)\n",
    "    lon_mask = (df_tensor[:, 1] >= 123) & (df_tensor[:, 1] <= 133)\n",
    "\n",
    "    df_sub = df_tensor[lat_mask & lon_mask].clone()\n",
    "    return df_sub\n",
    "\n",
    "def apply_first_difference_2d_tensor(df_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies a 2D first-order difference filter using convolution.\n",
    "    This approximates Z(s) = [X(s+d_lat) - X(s)] + [X(s+d_lon) - X(s)].\n",
    "    \"\"\"\n",
    "    if df_tensor.size(0) == 0:\n",
    "        return torch.empty(0, 4)\n",
    "\n",
    "    # 1. Get grid dimensions and validate\n",
    "    unique_lats = torch.unique(df_tensor[:, 0])\n",
    "    unique_lons = torch.unique(df_tensor[:, 1])\n",
    "    lat_count, lon_count = unique_lats.size(0), unique_lons.size(0)\n",
    "\n",
    "    if df_tensor.size(0) != lat_count * lon_count:\n",
    "        raise ValueError(\"Tensor size does not match grid dimensions. Must be a complete grid.\")\n",
    "    if lat_count < 2 or lon_count < 2:\n",
    "        return torch.empty(0, 4)\n",
    "\n",
    "    # 2. Reshape data and define the correct kernel\n",
    "    ozone_data = df_tensor[:, 2].reshape(1, 1, lat_count, lon_count)\n",
    "    \n",
    "    # âœ… CORRECT KERNEL: This kernel results in the standard first-order difference:\n",
    "    # Z(i,j) = X(i+1,j) + X(i,j+1) - 2*X(i,j)\n",
    "    # Note: F.conv2d in PyTorch actually performs cross-correlation. To get a true\n",
    "    # convolution result, the kernel would need to be flipped. However, for a \n",
    "    # forward difference operator, defining the kernel for cross-correlation is more direct.\n",
    "    # The kernel below is designed for cross-correlation to achieve the desired differencing.\n",
    "    diff_kernel = torch.tensor([[[[-2., 1.],\n",
    "                                  [ 1., 0.]]]], dtype=torch.float32)\n",
    "\n",
    "    # 3. Apply convolution (which acts as cross-correlation)\n",
    "    filtered_grid = F.conv2d(ozone_data, diff_kernel, padding='valid').squeeze()\n",
    "\n",
    "    # 4. Determine coordinates for the new, smaller grid\n",
    "    # The new grid corresponds to the anchor points of the kernel\n",
    "    new_lats = unique_lats[:-1]\n",
    "    new_lons = unique_lons[:-1]\n",
    "\n",
    "    # 5. Reconstruct the output tensor\n",
    "    new_lat_grid, new_lon_grid = torch.meshgrid(new_lats, new_lons, indexing='ij')\n",
    "    filtered_values = filtered_grid.flatten()\n",
    "    time_value = df_tensor[0, 3].repeat(filtered_values.size(0))\n",
    "\n",
    "    new_tensor = torch.stack([\n",
    "        new_lat_grid.flatten(),\n",
    "        new_lon_grid.flatten(),\n",
    "        filtered_values,\n",
    "        time_value\n",
    "    ], dim=1)\n",
    "    \n",
    "    return new_tensor\n",
    "\n",
    "# =========================================================================\n",
    "# 2. Data Loading (Unchanged)\n",
    "# =========================================================================\n",
    "# âš ï¸ NOTE: You must define these variables\n",
    "# mac_data_path = \"...\"\n",
    "# year = 2022\n",
    "# month_str = \"01\"\n",
    "# class GEMS_TCO: # Placeholder\n",
    "#     def load_data(self, path): return self\n",
    "#     def load_working_data_byday_wo_mm(self, data, indices):\n",
    "#         return {'key': torch.randn(100, 4)}, torch.randn(100, 4)\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 3. Main Processing Loop (Unchanged)\n",
    "# =========================================================================\n",
    "spatially_filtered_days = []\n",
    "for day_idx, day_map in enumerate(daily_hourly_maps):\n",
    "    tensors_to_aggregate = []\n",
    "    for key, tensor in day_map.items():\n",
    "        subsetted = subset_tensor(tensor)\n",
    "        if subsetted.size(0) > 0:\n",
    "            try:\n",
    "                diff_applied = apply_first_difference_2d_tensor(subsetted)\n",
    "                if diff_applied.size(0) > 0:\n",
    "                    tensors_to_aggregate.append(diff_applied)\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping data chunk on day {day_idx+1} due to error: {e}\")\n",
    "\n",
    "    if tensors_to_aggregate:\n",
    "        aggregated_day_tensor = torch.cat(tensors_to_aggregate, dim=0)\n",
    "        spatially_filtered_days.append(aggregated_day_tensor)\n",
    "\n",
    "# =========================================================================\n",
    "# 4. Verification (Unchanged)\n",
    "# =========================================================================\n",
    "print(\"\\n--- Results ---\")\n",
    "print(f\"Number of final spatially-differenced day tensors: {len(spatially_filtered_days)}\")\n",
    "if spatially_filtered_days:\n",
    "    # Save the processed data for the next script\n",
    "    processed_output_path = \"spatial_first_difference_data.pkl\"\n",
    "    with open(processed_output_path, 'wb') as f:\n",
    "        pickle.dump(spatially_filtered_days, f)\n",
    "    print(f\"Processed data saved to {processed_output_path}\")\n",
    "\n",
    "    print(f\"\\nShape of the first final tensor: {spatially_filtered_days[0].shape}\")\n",
    "    print(\"First final tensor head:\")\n",
    "    print(spatially_filtered_days[0][:5])\n",
    "else:\n",
    "    print(\"\\nNo final differenced tensors were created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4d2ab4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded 31 days from spatial_first_difference_data.pkl.\n",
      "Filtered data to single time point. New shape: torch.Size([17854, 4])\n",
      "Pre-computing J-vector (Hamming taper)...\n",
      "Pre-computing sample periodogram...\n",
      "Pre-computing Hamming taper autocorrelation...\n",
      "Data grid: 113x158, 1 time points. J-vector, Periodogram, Taper Autocorr on cpu.\n",
      "\n",
      "============================== Initialization Run 1/1 ==============================\n",
      "Starting with FIXED params (log-scale): [2.7081, -0.4155, -0.3567, 0.4055]\n",
      "Starting optimization run 1 on device cpu (Hamming, SpatialModel kernel)...\n",
      "--- Epoch 1/5000 (LR: 0.009999) ---\n",
      " Loss: 8893.6602\n",
      " Parameters (Natural Scale): sigmasq: 23.1864, range_lat: 1.8200, range_lon: 1.5304, nugget: 1.5151\n",
      "--- Epoch 51/5000 (LR: 0.008480) ---\n",
      " Loss: 4808.0454\n",
      " Parameters (Natural Scale): sigmasq: 54.4521, range_lat: 2.0226, range_lon: 2.1998, nugget: 2.3851\n",
      "--- Epoch 101/5000 (LR: 0.004922) ---\n",
      " Loss: 4508.7007\n",
      " Parameters (Natural Scale): sigmasq: 38.2246, range_lat: 0.9178, range_lon: 1.2544, nugget: 1.9901\n",
      "--- Epoch 151/5000 (LR: 0.001410) ---\n",
      " Loss: 4456.0894\n",
      " Parameters (Natural Scale): sigmasq: 28.0975, range_lat: 0.5804, range_lon: 0.8814, nugget: 1.7944\n",
      "--- Epoch 201/5000 (LR: 0.000002) ---\n",
      " Loss: 4452.0010\n",
      " Parameters (Natural Scale): sigmasq: 26.5475, range_lat: 0.5420, range_lon: 0.8343, nugget: 1.7851\n",
      "--- Epoch 251/5000 (LR: 0.001521) ---\n",
      " Loss: 4449.4814\n",
      " Parameters (Natural Scale): sigmasq: 25.3628, range_lat: 0.5141, range_lon: 0.8001, nugget: 1.7826\n",
      "--- Epoch 301/5000 (LR: 0.005079) ---\n",
      " Loss: 4435.4438\n",
      " Parameters (Natural Scale): sigmasq: 19.7069, range_lat: 0.3911, range_lon: 0.6192, nugget: 1.7681\n",
      "--- Epoch 351/5000 (LR: 0.008591) ---\n",
      " Loss: 4403.9829\n",
      " Parameters (Natural Scale): sigmasq: 12.7833, range_lat: 0.2374, range_lon: 0.3743, nugget: 1.6697\n",
      "--- Epoch 401/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9282\n",
      " Parameters (Natural Scale): sigmasq: 10.1517, range_lat: 0.1723, range_lon: 0.2668, nugget: 1.5116\n",
      "--- Epoch 451/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9175\n",
      " Parameters (Natural Scale): sigmasq: 10.1714, range_lat: 0.1723, range_lon: 0.2669, nugget: 1.5129\n",
      "--- Epoch 501/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9087\n",
      " Parameters (Natural Scale): sigmasq: 10.1719, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5131\n",
      "--- Epoch 551/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9043\n",
      " Parameters (Natural Scale): sigmasq: 10.1652, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5123\n",
      "--- Epoch 601/5000 (LR: 0.000002) ---\n",
      " Loss: 4391.9033\n",
      " Parameters (Natural Scale): sigmasq: 10.1695, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 651/5000 (LR: 0.001521) ---\n",
      " Loss: 4391.9048\n",
      " Parameters (Natural Scale): sigmasq: 10.1694, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 701/5000 (LR: 0.005079) ---\n",
      " Loss: 4391.9048\n",
      " Parameters (Natural Scale): sigmasq: 10.1710, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5130\n",
      "--- Epoch 751/5000 (LR: 0.008591) ---\n",
      " Loss: 4391.9062\n",
      " Parameters (Natural Scale): sigmasq: 10.1724, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5132\n",
      "--- Epoch 801/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9048\n",
      " Parameters (Natural Scale): sigmasq: 10.1727, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5132\n",
      "--- Epoch 851/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9048\n",
      " Parameters (Natural Scale): sigmasq: 10.1720, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5131\n",
      "--- Epoch 901/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9038\n",
      " Parameters (Natural Scale): sigmasq: 10.1705, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5129\n",
      "--- Epoch 951/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9038\n",
      " Parameters (Natural Scale): sigmasq: 10.1694, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 1001/5000 (LR: 0.000002) ---\n",
      " Loss: 4391.9043\n",
      " Parameters (Natural Scale): sigmasq: 10.1694, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 1051/5000 (LR: 0.001521) ---\n",
      " Loss: 4391.9033\n",
      " Parameters (Natural Scale): sigmasq: 10.1694, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 1101/5000 (LR: 0.005079) ---\n",
      " Loss: 4391.9028\n",
      " Parameters (Natural Scale): sigmasq: 10.1646, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5122\n",
      "--- Epoch 1151/5000 (LR: 0.008591) ---\n",
      " Loss: 4391.9253\n",
      " Parameters (Natural Scale): sigmasq: 10.1722, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5132\n",
      "--- Epoch 1201/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9072\n",
      " Parameters (Natural Scale): sigmasq: 10.1661, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5124\n",
      "--- Epoch 1251/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9043\n",
      " Parameters (Natural Scale): sigmasq: 10.1616, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5119\n",
      "--- Epoch 1301/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9082\n",
      " Parameters (Natural Scale): sigmasq: 10.1615, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5119\n",
      "--- Epoch 1351/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9033\n",
      " Parameters (Natural Scale): sigmasq: 10.1722, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5131\n",
      "--- Epoch 1401/5000 (LR: 0.000002) ---\n",
      " Loss: 4391.9023\n",
      " Parameters (Natural Scale): sigmasq: 10.1694, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 1451/5000 (LR: 0.001521) ---\n",
      " Loss: 4391.9033\n",
      " Parameters (Natural Scale): sigmasq: 10.1695, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 1501/5000 (LR: 0.005079) ---\n",
      " Loss: 4391.9038\n",
      " Parameters (Natural Scale): sigmasq: 10.1706, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5130\n",
      "--- Epoch 1551/5000 (LR: 0.008591) ---\n",
      " Loss: 4391.9058\n",
      " Parameters (Natural Scale): sigmasq: 10.1722, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5131\n",
      "--- Epoch 1601/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9062\n",
      " Parameters (Natural Scale): sigmasq: 10.1727, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5132\n",
      "--- Epoch 1651/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9053\n",
      " Parameters (Natural Scale): sigmasq: 10.1720, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5131\n",
      "--- Epoch 1701/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9033\n",
      " Parameters (Natural Scale): sigmasq: 10.1707, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5129\n",
      "--- Epoch 1751/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9038\n",
      " Parameters (Natural Scale): sigmasq: 10.1707, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5130\n",
      "--- Epoch 1801/5000 (LR: 0.000002) ---\n",
      " Loss: 4391.9028\n",
      " Parameters (Natural Scale): sigmasq: 10.1694, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 1851/5000 (LR: 0.001521) ---\n",
      " Loss: 4391.9033\n",
      " Parameters (Natural Scale): sigmasq: 10.1693, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 1901/5000 (LR: 0.005079) ---\n",
      " Loss: 4391.9082\n",
      " Parameters (Natural Scale): sigmasq: 10.1606, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5117\n",
      "--- Epoch 1951/5000 (LR: 0.008591) ---\n",
      " Loss: 4391.9185\n",
      " Parameters (Natural Scale): sigmasq: 10.2028, range_lat: 0.1732, range_lon: 0.2681, nugget: 1.5113\n",
      "--- Epoch 2001/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9102\n",
      " Parameters (Natural Scale): sigmasq: 10.1627, range_lat: 0.1726, range_lon: 0.2672, nugget: 1.5139\n",
      "--- Epoch 2051/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9131\n",
      " Parameters (Natural Scale): sigmasq: 10.1792, range_lat: 0.1730, range_lon: 0.2678, nugget: 1.5102\n",
      "--- Epoch 2101/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9053\n",
      " Parameters (Natural Scale): sigmasq: 10.1797, range_lat: 0.1726, range_lon: 0.2674, nugget: 1.5147\n",
      "--- Epoch 2151/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9033\n",
      " Parameters (Natural Scale): sigmasq: 10.1702, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5127\n",
      "--- Epoch 2201/5000 (LR: 0.000002) ---\n",
      " Loss: 4391.9033\n",
      " Parameters (Natural Scale): sigmasq: 10.1694, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 2251/5000 (LR: 0.001521) ---\n",
      " Loss: 4391.9038\n",
      " Parameters (Natural Scale): sigmasq: 10.1696, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 2301/5000 (LR: 0.005079) ---\n",
      " Loss: 4391.9053\n",
      " Parameters (Natural Scale): sigmasq: 10.1659, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5124\n",
      "--- Epoch 2351/5000 (LR: 0.008591) ---\n",
      " Loss: 4391.9082\n",
      " Parameters (Natural Scale): sigmasq: 10.1669, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5126\n",
      "--- Epoch 2401/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9170\n",
      " Parameters (Natural Scale): sigmasq: 10.1597, range_lat: 0.1729, range_lon: 0.2674, nugget: 1.5118\n",
      "--- Epoch 2451/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9062\n",
      " Parameters (Natural Scale): sigmasq: 10.1713, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5130\n",
      "--- Epoch 2501/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9136\n",
      " Parameters (Natural Scale): sigmasq: 10.1614, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5120\n",
      "--- Epoch 2551/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9038\n",
      " Parameters (Natural Scale): sigmasq: 10.1714, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5131\n",
      "--- Epoch 2601/5000 (LR: 0.000002) ---\n",
      " Loss: 4391.9023\n",
      " Parameters (Natural Scale): sigmasq: 10.1694, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 2651/5000 (LR: 0.001521) ---\n",
      " Loss: 4391.9038\n",
      " Parameters (Natural Scale): sigmasq: 10.1695, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 2701/5000 (LR: 0.005079) ---\n",
      " Loss: 4391.9062\n",
      " Parameters (Natural Scale): sigmasq: 10.1718, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5130\n",
      "--- Epoch 2751/5000 (LR: 0.008591) ---\n",
      " Loss: 4391.9478\n",
      " Parameters (Natural Scale): sigmasq: 10.1880, range_lat: 0.1727, range_lon: 0.2676, nugget: 1.5147\n",
      "--- Epoch 2801/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9048\n",
      " Parameters (Natural Scale): sigmasq: 10.1561, range_lat: 0.1727, range_lon: 0.2673, nugget: 1.5115\n",
      "--- Epoch 2851/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9194\n",
      " Parameters (Natural Scale): sigmasq: 10.1746, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5134\n",
      "--- Epoch 2901/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9067\n",
      " Parameters (Natural Scale): sigmasq: 10.1898, range_lat: 0.1729, range_lon: 0.2678, nugget: 1.5134\n",
      "--- Epoch 2951/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9038\n",
      " Parameters (Natural Scale): sigmasq: 10.1702, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5129\n",
      "--- Epoch 3001/5000 (LR: 0.000002) ---\n",
      " Loss: 4391.9043\n",
      " Parameters (Natural Scale): sigmasq: 10.1695, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 3051/5000 (LR: 0.001521) ---\n",
      " Loss: 4391.9033\n",
      " Parameters (Natural Scale): sigmasq: 10.1694, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 3101/5000 (LR: 0.005079) ---\n",
      " Loss: 4391.9087\n",
      " Parameters (Natural Scale): sigmasq: 10.1674, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5126\n",
      "--- Epoch 3151/5000 (LR: 0.008591) ---\n",
      " Loss: 4391.9062\n",
      " Parameters (Natural Scale): sigmasq: 10.1568, range_lat: 0.1725, range_lon: 0.2672, nugget: 1.5128\n",
      "--- Epoch 3201/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9077\n",
      " Parameters (Natural Scale): sigmasq: 10.1609, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5120\n",
      "--- Epoch 3251/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9067\n",
      " Parameters (Natural Scale): sigmasq: 10.1766, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5135\n",
      "--- Epoch 3301/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9058\n",
      " Parameters (Natural Scale): sigmasq: 10.1639, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5123\n",
      "--- Epoch 3351/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9033\n",
      " Parameters (Natural Scale): sigmasq: 10.1715, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5130\n",
      "--- Epoch 3401/5000 (LR: 0.000002) ---\n",
      " Loss: 4391.9048\n",
      " Parameters (Natural Scale): sigmasq: 10.1694, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 3451/5000 (LR: 0.001521) ---\n",
      " Loss: 4391.9033\n",
      " Parameters (Natural Scale): sigmasq: 10.1693, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 3501/5000 (LR: 0.005079) ---\n",
      " Loss: 4391.9043\n",
      " Parameters (Natural Scale): sigmasq: 10.1723, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5129\n",
      "--- Epoch 3551/5000 (LR: 0.008591) ---\n",
      " Loss: 4391.9111\n",
      " Parameters (Natural Scale): sigmasq: 10.1473, range_lat: 0.1727, range_lon: 0.2672, nugget: 1.5108\n",
      "--- Epoch 3601/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9888\n",
      " Parameters (Natural Scale): sigmasq: 10.1307, range_lat: 0.1726, range_lon: 0.2671, nugget: 1.5093\n",
      "--- Epoch 3651/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9121\n",
      " Parameters (Natural Scale): sigmasq: 10.1907, range_lat: 0.1727, range_lon: 0.2677, nugget: 1.5147\n",
      "--- Epoch 3701/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9224\n",
      " Parameters (Natural Scale): sigmasq: 10.1884, range_lat: 0.1727, range_lon: 0.2676, nugget: 1.5145\n",
      "--- Epoch 3751/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9023\n",
      " Parameters (Natural Scale): sigmasq: 10.1654, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5125\n",
      "--- Epoch 3801/5000 (LR: 0.000002) ---\n",
      " Loss: 4391.9028\n",
      " Parameters (Natural Scale): sigmasq: 10.1696, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 3851/5000 (LR: 0.001521) ---\n",
      " Loss: 4391.9043\n",
      " Parameters (Natural Scale): sigmasq: 10.1692, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 3901/5000 (LR: 0.005079) ---\n",
      " Loss: 4391.9043\n",
      " Parameters (Natural Scale): sigmasq: 10.1646, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5124\n",
      "--- Epoch 3951/5000 (LR: 0.008591) ---\n",
      " Loss: 4391.9087\n",
      " Parameters (Natural Scale): sigmasq: 10.1788, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5136\n",
      "--- Epoch 4001/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9077\n",
      " Parameters (Natural Scale): sigmasq: 10.1597, range_lat: 0.1727, range_lon: 0.2673, nugget: 1.5120\n",
      "--- Epoch 4051/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9082\n",
      " Parameters (Natural Scale): sigmasq: 10.1789, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5136\n",
      "--- Epoch 4101/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9038\n",
      " Parameters (Natural Scale): sigmasq: 10.1650, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5125\n",
      "--- Epoch 4151/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9033\n",
      " Parameters (Natural Scale): sigmasq: 10.1712, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5130\n",
      "--- Epoch 4201/5000 (LR: 0.000002) ---\n",
      " Loss: 4391.9053\n",
      " Parameters (Natural Scale): sigmasq: 10.1695, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 4251/5000 (LR: 0.001521) ---\n",
      " Loss: 4391.9038\n",
      " Parameters (Natural Scale): sigmasq: 10.1693, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 4301/5000 (LR: 0.005079) ---\n",
      " Loss: 4391.9028\n",
      " Parameters (Natural Scale): sigmasq: 10.1731, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5131\n",
      "--- Epoch 4351/5000 (LR: 0.008591) ---\n",
      " Loss: 4391.9116\n",
      " Parameters (Natural Scale): sigmasq: 10.1727, range_lat: 0.1726, range_lon: 0.2673, nugget: 1.5142\n",
      "--- Epoch 4401/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9097\n",
      " Parameters (Natural Scale): sigmasq: 10.1550, range_lat: 0.1726, range_lon: 0.2672, nugget: 1.5119\n",
      "--- Epoch 4451/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9053\n",
      " Parameters (Natural Scale): sigmasq: 10.1757, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5134\n",
      "--- Epoch 4501/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9053\n",
      " Parameters (Natural Scale): sigmasq: 10.1655, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5122\n",
      "--- Epoch 4551/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9033\n",
      " Parameters (Natural Scale): sigmasq: 10.1717, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5129\n",
      "--- Epoch 4601/5000 (LR: 0.000002) ---\n",
      " Loss: 4391.9043\n",
      " Parameters (Natural Scale): sigmasq: 10.1694, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 4651/5000 (LR: 0.001521) ---\n",
      " Loss: 4391.9033\n",
      " Parameters (Natural Scale): sigmasq: 10.1692, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 4701/5000 (LR: 0.005079) ---\n",
      " Loss: 4391.9087\n",
      " Parameters (Natural Scale): sigmasq: 10.1854, range_lat: 0.1728, range_lon: 0.2676, nugget: 1.5140\n",
      "--- Epoch 4751/5000 (LR: 0.008591) ---\n",
      " Loss: 4391.9092\n",
      " Parameters (Natural Scale): sigmasq: 10.2233, range_lat: 0.1733, range_lon: 0.2685, nugget: 1.5137\n",
      "--- Epoch 4801/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9336\n",
      " Parameters (Natural Scale): sigmasq: 10.1835, range_lat: 0.1727, range_lon: 0.2676, nugget: 1.5140\n",
      "--- Epoch 4851/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9131\n",
      " Parameters (Natural Scale): sigmasq: 10.1463, range_lat: 0.1726, range_lon: 0.2672, nugget: 1.5109\n",
      "--- Epoch 4901/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9048\n",
      " Parameters (Natural Scale): sigmasq: 10.1787, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5136\n",
      "--- Epoch 4951/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9048\n",
      " Parameters (Natural Scale): sigmasq: 10.1717, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5130\n",
      "--- Epoch 5000/5000 (LR: 0.000001) ---\n",
      " Loss: 4391.9028\n",
      " Parameters (Natural Scale): sigmasq: 10.1695, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "\n",
      "--- Training Complete ---\n",
      "\n",
      "FINAL BEST STATE ACHIEVED (during training):\n",
      "Best Loss: 4391.901\n",
      "Parameters (Natural Scale): sigmasq: 10.1554, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5111\n",
      "Parameters (Phi Scale)    : phi1: 37.9799, phi2: 3.7399, phi3: 2.3969, nugget: 1.5111\n",
      "\n",
      "\n",
      "========================= Overall Result from Run =========================\n",
      "Best Run Loss: 4391.901\n",
      "Final Parameters (Natural Scale): sigmasq: 10.1554, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5111\n",
      "Final Parameters (Phi Scale)    : phi1: 37.9799, phi2: 3.7399, phi3: 2.3969, nugget: 1.5111\n",
      "\n",
      "Total execution time: 73.96 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import cmath\n",
    "import pickle\n",
    "import time \n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import Parameter\n",
    "import torch.fft \n",
    "from typing import List, Dict, Any, Callable\n",
    "\n",
    "# =========================================================================\n",
    "# 1. Tapering, Autocorrelation, and Covariance Functions\n",
    "# =========================================================================\n",
    "\n",
    "def cgn_hamming(u, n1, n2):\n",
    "    \"\"\"Computes a 2D Hamming window.\"\"\"\n",
    "    u1, u2 = u\n",
    "    device = u1.device if isinstance(u1, torch.Tensor) else (u2.device if isinstance(u2, torch.Tensor) else torch.device('cpu'))\n",
    "    u1_tensor = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_tensor = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "\n",
    "    n1_eff = float(n1) if n1 > 0 else 1.0\n",
    "    n2_eff = float(n2) if n2 > 0 else 1.0\n",
    "    hamming1 = 0.54 + 0.46 * torch.cos(2.0 * torch.pi * u1_tensor / n1_eff)\n",
    "    hamming2 = 0.54 + 0.46 * torch.cos(2.0 * torch.pi * u2_tensor / n2_eff)\n",
    "    return hamming1 * hamming2\n",
    "\n",
    "def cgn_2dbartlett(u, n1, n2): # Kept for potential future use\n",
    "    \"\"\"Computes a 2D Bartlett window function.\"\"\"\n",
    "    u1, u2 = u\n",
    "    device = u1.device if isinstance(u1, torch.Tensor) else (u2.device if isinstance(u2, torch.Tensor) else torch.device('cpu'))\n",
    "    u1_tensor = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_tensor = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    n1_eff = float(n1) if n1 > 0 else 1.0\n",
    "    n2_eff = float(n2) if n2 > 0 else 1.0\n",
    "    kernel = (1.0 - torch.abs(u1_tensor) / n1_eff) * (1.0 - torch.abs(u2_tensor) / n2_eff)\n",
    "    return torch.clamp(kernel, min=0.0)\n",
    "\n",
    "def calculate_taper_autocorrelation_fft(taper_grid, n1, n2, device):\n",
    "    \"\"\"\n",
    "    Computes the normalized taper autocorrelation function c_gn(u) using FFT.\n",
    "    \"\"\"\n",
    "    taper_grid = taper_grid.to(device) \n",
    "    H = torch.sum(taper_grid**2)\n",
    "    if H < 1e-12:\n",
    "        print(\"Warning: Sum of squared taper weights (H) is near zero.\")\n",
    "        return torch.zeros((2*n1-1, 2*n2-1), device=device, dtype=taper_grid.dtype)\n",
    "\n",
    "    N1, N2 = 2 * n1 - 1, 2 * n2 - 1\n",
    "    taper_fft = torch.fft.fft2(taper_grid, s=(N1, N2))\n",
    "    power_spectrum = torch.abs(taper_fft)**2\n",
    "    autocorr_unnormalized = torch.fft.ifft2(power_spectrum).real\n",
    "    autocorr_shifted = torch.fft.fftshift(autocorr_unnormalized)\n",
    "\n",
    "    c_gn_grid = autocorr_shifted / (H + 1e-12)\n",
    "\n",
    "    return c_gn_grid \n",
    "\n",
    "def cov_x_spatial_model_kernel(u1, u2, t, params):\n",
    "    \"\"\"\n",
    "    Computes autocovariance of X using the SPATIAL-ONLY kernel.\n",
    "    u1, u2 are PHYSICAL lags (already scaled by deltas).\n",
    "    \n",
    "    params index map (4 params):\n",
    "    [0] log_phi1 (log(sigmasq / range_lon))\n",
    "    [1] log_phi2 (log(1 / range_lon))\n",
    "    [2] log_phi3 (log((range_lon / range_lat)^2))\n",
    "    [3] log_nugget\n",
    "    \"\"\"\n",
    "    device = params.device\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    if torch.isnan(params).any() or torch.isinf(params).any():\n",
    "         print(\"Warning: NaN/Inf in log-params before exp in cov_x_spatial_model_kernel\")\n",
    "         out_shape = torch.broadcast_shapes(u1_dev.shape, u2_dev.shape, t_dev.shape)\n",
    "         return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "\n",
    "    phi1   = torch.exp(params[0])\n",
    "    phi2   = torch.exp(params[1]) \n",
    "    phi3   = torch.exp(params[2]) \n",
    "    nugget = torch.exp(params[3])\n",
    "\n",
    "    epsilon = 1e-12\n",
    "    sigmasq = phi1 / (phi2 + epsilon)  \n",
    "    range_lon_inv = phi2\n",
    "    range_lat_inv = torch.sqrt(phi3 + epsilon) * phi2\n",
    "\n",
    "    dist_sq = (u1_dev * range_lat_inv).pow(2) + (u2_dev * range_lon_inv).pow(2)\n",
    "    distance = torch.sqrt(dist_sq + epsilon) \n",
    "\n",
    "    cov_smooth = sigmasq * torch.exp(-distance)\n",
    "\n",
    "    is_zero_lag = (torch.abs(u1_dev) < 1e-9) & (torch.abs(u2_dev) < 1e-9) & (torch.abs(t_dev) < 1e-9)\n",
    "    final_cov = torch.where(is_zero_lag, cov_smooth + nugget, cov_smooth)\n",
    "\n",
    "    if torch.isnan(final_cov).any(): print(\"Warning: NaN detected in cov_x_spatial_model_kernel output.\")\n",
    "    return final_cov\n",
    "\n",
    "\n",
    "def cov_spatial_difference(u1, u2, t, params, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Calculates covariance Cov(Y(s), Y(s+u)) for Y(s) = X(s+d1) + X(s+d2) - 2X(s)\n",
    "    Based on the underlying spatial kernel.\n",
    "    u1, u2 are PHYSICAL lags.\n",
    "    \"\"\"\n",
    "    weights = {(0, 0): -2.0, (1, 0): 1.0, (0, 1): 1.0}\n",
    "    device = params.device\n",
    "    out_shape = torch.broadcast_shapes(u1.shape if isinstance(u1, torch.Tensor) else (),\n",
    "                                     u2.shape if isinstance(u2, torch.Tensor) else (),\n",
    "                                     t.shape if isinstance(t, torch.Tensor) else ())\n",
    "    cov = torch.zeros(out_shape, device=device, dtype=torch.float32)\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    for (a_idx, b_idx), w_ab in weights.items():\n",
    "        offset_a1 = a_idx * delta1\n",
    "        offset_a2 = b_idx * delta2\n",
    "        for (c_idx, d_idx), w_cd in weights.items():\n",
    "            offset_c1 = c_idx * delta1\n",
    "            offset_c2 = d_idx * delta2\n",
    "            \n",
    "            # All lags are physical lags\n",
    "            lag_u1 = u1_dev + (offset_a1 - offset_c1)\n",
    "            lag_u2 = u2_dev + (offset_a2 - offset_c2)\n",
    "            \n",
    "            term_cov = cov_x_spatial_model_kernel(lag_u1, lag_u2, t_dev, params) \n",
    "            \n",
    "            if torch.isnan(term_cov).any():\n",
    "                 print(f\"Warning: NaN in term_cov within cov_spatial_difference.\")\n",
    "                 return torch.full_like(cov, float('nan'))\n",
    "            cov += w_ab * w_cd * term_cov\n",
    "\n",
    "    if torch.isnan(cov).any(): print(\"Warning: NaN in final cov_spatial_difference output.\")\n",
    "    return cov\n",
    "\n",
    "def cn_bar_tapered(u1, u2, t, params, n1, n2, taper_autocorr_grid, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Computes c_Y(u) * c_gn(u).\n",
    "    u1, u2 are GRID index lags (e.g., -n1..0..n1)\n",
    "    \"\"\"\n",
    "    device = params.device\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    # --- Convert GRID lags to PHYSICAL lags ---\n",
    "    lag_u1 = u1_dev * delta1\n",
    "    lag_u2 = u2_dev * delta2\n",
    "    \n",
    "    # Calculate theoretical covariance c_Y(u) using physical lags\n",
    "    cov_X_value = cov_spatial_difference(lag_u1, lag_u2, t_dev, params, delta1, delta2)\n",
    "\n",
    "    # --- Get Taper Autocorrelation Value c_gn(u) from grid ---\n",
    "    # Use GRID lags for indexing the taper grid\n",
    "    u1_idx = u1_dev.long()\n",
    "    u2_idx = u2_dev.long()\n",
    "\n",
    "    idx1 = (n1 - 1 + u1_idx) # Centering index\n",
    "    idx2 = (n2 - 1 + u2_idx) # Centering index\n",
    "    \n",
    "    idx1 = torch.clamp(idx1, 0, 2 * n1 - 2)\n",
    "    idx2 = torch.clamp(idx2, 0, 2 * n2 - 2)\n",
    "\n",
    "    taper_autocorr_value = taper_autocorr_grid[idx1, idx2]\n",
    "\n",
    "    if torch.isnan(cov_X_value).any() or torch.isnan(taper_autocorr_value).any():\n",
    "        print(\"Warning: NaN detected before multiplication in cn_bar_tapered.\")\n",
    "        out_shape = torch.broadcast_shapes(cov_X_value.shape, taper_autocorr_value.shape)\n",
    "        return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "\n",
    "    result = cov_X_value * taper_autocorr_value\n",
    "    if torch.isnan(result).any(): print(\"Warning: NaN in cn_bar_tapered output.\")\n",
    "    return result\n",
    "\n",
    "# --- ðŸ’¥ RIGOROUSLY REVISED FUNCTION ðŸ’¥ ---\n",
    "def expected_periodogram_fft_tapered(params, n1, n2, p, taper_autocorr_grid, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Calculates the expected periodogram I(omega_s) (a pxp matrix in time)\n",
    "    using the exact taper autocorrelation c_gn(u) and\n",
    "    CORRECTLY implementing the aliasing sum (Lemma 2).\n",
    "    \"\"\"\n",
    "    device = params.device if isinstance(params, torch.Tensor) else params[0].device\n",
    "    if isinstance(params, list):\n",
    "        params_tensor = torch.cat([p.to(device) for p in params])\n",
    "    else:\n",
    "        params_tensor = params.to(device)\n",
    "\n",
    "    # 1. Create the grid for u from [0, n-1]\n",
    "    # This is the grid for the *output* of the aliased function tilde_c_n(u)\n",
    "    u1_lags = torch.arange(n1, dtype=torch.float32, device=device)\n",
    "    u2_lags = torch.arange(n2, dtype=torch.float32, device=device)\n",
    "    u1_mesh, u2_mesh = torch.meshgrid(u1_lags, u2_lags, indexing='ij')\n",
    "\n",
    "    t_lags = torch.arange(p, dtype=torch.float32, device=device)\n",
    "    tilde_cn_tensor = torch.zeros((n1, n2, p, p), dtype=torch.complex64, device=device)\n",
    "\n",
    "    # 2. Loop over the p x p matrix (for this spatial code, p=1)\n",
    "    for q in range(p):\n",
    "        for r in range(p):\n",
    "            t_diff = t_lags[q] - t_lags[r]\n",
    "            \n",
    "            # --- Calculate 4 aliasing terms from Lemma 2 ---\n",
    "            # cn_bar_tapered handles any GRID lag (positive or negative)\n",
    "            \n",
    "            # Term 1: c_n(u1, u2, t_diff)\n",
    "            term1 = cn_bar_tapered(u1_mesh, u2_mesh, t_diff, \n",
    "                                   params_tensor, n1, n2, taper_autocorr_grid, delta1, delta2)\n",
    "\n",
    "            # Term 2: c_n(u1 - n1, u2, t_diff)\n",
    "            term2 = cn_bar_tapered(u1_mesh - n1, u2_mesh, t_diff, \n",
    "                                   params_tensor, n1, n2, taper_autocorr_grid, delta1, delta2)\n",
    "\n",
    "            # Term 3: c_n(u1, u2 - n2, t_diff)\n",
    "            term3 = cn_bar_tapered(u1_mesh, u2_mesh - n2, t_diff, \n",
    "                                   params_tensor, n1, n2, taper_autocorr_grid, delta1, delta2)\n",
    "\n",
    "            # Term 4: c_n(u1 - n1, u2 - n2, t_diff)\n",
    "            term4 = cn_bar_tapered(u1_mesh - n1, u2_mesh - n2, t_diff,\n",
    "                                   params_tensor, n1, n2, taper_autocorr_grid, delta1, delta2)\n",
    "            \n",
    "            # 3. Sum them to create tilde_c_n(u, t_diff)\n",
    "            tilde_cn_grid_qr = (term1 + term2 + term3 + term4)\n",
    "            \n",
    "            if torch.isnan(tilde_cn_grid_qr).any():\n",
    "                 tilde_cn_tensor[:, :, q, r] = float('nan')\n",
    "            else:\n",
    "                 tilde_cn_tensor[:, :, q, r] = tilde_cn_grid_qr.to(torch.complex64)\n",
    "\n",
    "    if torch.isnan(tilde_cn_tensor).any():\n",
    "        print(\"Warning: NaN detected in tilde_cn_tensor before FFT.\")\n",
    "        nan_shape = (n1, n2, p, p)\n",
    "        return torch.full(nan_shape, float('nan'), dtype=torch.complex64, device=device)\n",
    "\n",
    "    # 4. Compute FFT of the *aliased* covariance\n",
    "    fft_result = torch.fft.fft2(tilde_cn_tensor, dim=(0, 1))\n",
    "    \n",
    "    # Result should be real (up to numerical noise)\n",
    "    fft_result_real = fft_result.real \n",
    "    \n",
    "    normalization_factor = 1.0 / (4.0 * cmath.pi**2)\n",
    "    result = fft_result_real * normalization_factor\n",
    "\n",
    "    if torch.isnan(result).any(): print(\"Warning: NaN in expected_periodogram_fft_tapered output.\")\n",
    "    return result\n",
    "# --- ðŸ’¥ END REVISION ðŸ’¥ ---\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 3. Data Processing (MODIFIED for HAMMING TAPERING)\n",
    "# =========================================================================\n",
    "def generate_Jvector_tapered(tensor_list, tapering_func, lat_col, lon_col, val_col, device):\n",
    "    \"\"\"\n",
    "    Generates J-vector for a single component using the specified taper,\n",
    "    placing result on device.\n",
    "    \"\"\"\n",
    "    p = len(tensor_list)\n",
    "    if p == 0: return torch.empty(0, 0, 0, device=device), 0, 0, 0, None \n",
    "\n",
    "    valid_tensors = [t for t in tensor_list if t.numel() > 0 and t.shape[1] > max(lat_col, lon_col, val_col)]\n",
    "    if not valid_tensors:\n",
    "         print(\"Warning: No valid tensors found in tensor_list.\")\n",
    "         return torch.empty(0, 0, 0, device=device), 0, 0, 0, None\n",
    "\n",
    "    try:\n",
    "        all_lats_cpu = torch.cat([t[:, lat_col] for t in valid_tensors])\n",
    "        all_lons_cpu = torch.cat([t[:, lon_col] for t in valid_tensors])\n",
    "    except IndexError:\n",
    "        print(f\"Error: Invalid column index. Check tensor shapes.\")\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0, None\n",
    "\n",
    "    all_lats_cpu = all_lats_cpu[~torch.isnan(all_lats_cpu) & ~torch.isinf(all_lats_cpu)]\n",
    "    all_lons_cpu = all_lons_cpu[~torch.isnan(all_lons_cpu) & ~torch.isinf(all_lons_cpu)]\n",
    "    if all_lats_cpu.numel() == 0 or all_lons_cpu.numel() == 0:\n",
    "        print(\"Warning: No valid coordinates after NaN/Inf filtering.\")\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0, None\n",
    "\n",
    "    unique_lats_cpu, unique_lons_cpu = torch.unique(all_lats_cpu), torch.unique(all_lons_cpu)\n",
    "    n1, n2 = len(unique_lats_cpu), len(unique_lons_cpu)\n",
    "    if n1 == 0 or n2 == 0:\n",
    "        print(\"Warning: Grid dimensions are zero.\")\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0, None\n",
    "\n",
    "    lat_map = {lat.item(): i for i, lat in enumerate(unique_lats_cpu)}\n",
    "    lon_map = {lon.item(): i for i, lon in enumerate(unique_lons_cpu)}\n",
    "\n",
    "    # --- Create Taper Grid ---\n",
    "    u1_mesh_cpu, u2_mesh_cpu = torch.meshgrid(\n",
    "        torch.arange(n1, dtype=torch.float32),\n",
    "        torch.arange(n2, dtype=torch.float32),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    taper_grid = tapering_func((u1_mesh_cpu, u2_mesh_cpu), n1, n2).to(device) # Taper on device\n",
    "\n",
    "    fft_results = []\n",
    "    for tensor in tensor_list:\n",
    "        data_grid = torch.zeros((n1, n2), dtype=torch.float32, device=device)\n",
    "        if tensor.numel() > 0 and tensor.shape[1] > max(lat_col, lon_col, val_col):\n",
    "            for row in tensor:\n",
    "                lat_item, lon_item = row[lat_col].item(), row[lon_col].item()\n",
    "                if not (np.isnan(lat_item) or np.isnan(lon_item)):\n",
    "                    i = lat_map.get(lat_item)\n",
    "                    j = lon_map.get(lon_item)\n",
    "                    if i is not None and j is not None:\n",
    "                        val = row[val_col]\n",
    "                        val_num = val.item() if isinstance(val, torch.Tensor) else val\n",
    "                        if not np.isnan(val_num) and not np.isinf(val_num):\n",
    "                           data_grid[i, j] = val_num\n",
    "\n",
    "        data_grid_tapered = data_grid * taper_grid \n",
    "\n",
    "        if torch.isnan(data_grid_tapered).any() or torch.isinf(data_grid_tapered).any():\n",
    "             print(\"Warning: NaN/Inf detected in data_grid_tapered before FFT. Replacing with zeros.\")\n",
    "             data_grid_tapered = torch.nan_to_num(data_grid_tapered, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        fft_results.append(torch.fft.fft2(data_grid_tapered))\n",
    "\n",
    "    if not fft_results:\n",
    "         print(\"Warning: No FFT results generated.\")\n",
    "         return torch.empty(0, 0, 0, device=device), n1, n2, 0, taper_grid\n",
    "\n",
    "    J_vector_tensor = torch.stack(fft_results, dim=2).to(device)\n",
    "\n",
    "    H = torch.sum(taper_grid**2)\n",
    "    if H < 1e-12:\n",
    "        print(\"Warning: Normalization factor H is near zero.\")\n",
    "        norm_factor = torch.tensor(0.0, device=device)\n",
    "    else:\n",
    "        norm_factor = (torch.sqrt(1.0 / H) / (2.0 * cmath.pi)).to(device)\n",
    "\n",
    "    result = J_vector_tensor * norm_factor\n",
    "    if torch.isnan(result).any(): print(\"Warning: NaN in J_vector output.\")\n",
    "    return result, n1, n2, p, taper_grid\n",
    "\n",
    "\n",
    "def calculate_sample_periodogram_vectorized(J_vector_tensor):\n",
    "    \"\"\"Calculates sample periodogram I_n = J J^H (pxp matrix for each spatial freq).\"\"\"\n",
    "    if torch.isnan(J_vector_tensor).any() or torch.isinf(J_vector_tensor).any():\n",
    "        print(\"Warning: NaN/Inf detected in J_vector_tensor input.\")\n",
    "        n1, n2, p = J_vector_tensor.shape\n",
    "        return torch.full((n1, n2, p, p), float('nan'), dtype=torch.complex64, device=J_vector_tensor.device)\n",
    "\n",
    "    J_col = J_vector_tensor.unsqueeze(-1)\n",
    "    J_row_conj = J_vector_tensor.unsqueeze(-2).conj()\n",
    "    result = J_col @ J_row_conj\n",
    "\n",
    "    if torch.isnan(result).any(): print(\"Warning: NaN in periodogram matrix output.\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 4. Likelihood Calculation (Adapted for Tapering with Autocorrelation)\n",
    "# =========================================================================\n",
    "\n",
    "def whittle_likelihood_loss_tapered(params, I_sample, n1, n2, p, taper_autocorr_grid, delta1, delta2):\n",
    "    \"\"\"\n",
    "    âœ… Whittle Likelihood Loss using data tapering and exact taper autocorrelation c_gn.\n",
    "    Models a single field (the spatially differenced one). \n",
    "    Uses SpatialModel kernel.\n",
    "    \"\"\"\n",
    "    device = I_sample.device\n",
    "    params_tensor = params.to(device)\n",
    "\n",
    "    if torch.isnan(params_tensor).any() or torch.isinf(params_tensor).any():\n",
    "        print(\"Warning: NaN/Inf detected in input parameters to likelihood.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    # --- ðŸ’¥ MODIFIED: Call the corrected function ðŸ’¥ ---\n",
    "    # The 'cov_func' argument is removed as it's no longer needed.\n",
    "    I_expected = expected_periodogram_fft_tapered(\n",
    "        params_tensor, n1, n2, p, taper_autocorr_grid, \n",
    "        delta1, delta2\n",
    "    )\n",
    "    # --- END MODIFICATION ---\n",
    "\n",
    "    if torch.isnan(I_expected).any() or torch.isinf(I_expected).any():\n",
    "        print(\"Warning: NaN/Inf returned from expected_periodogram calculation.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    eye_matrix = torch.eye(p, dtype=torch.complex64, device=device)\n",
    "    diag_vals = torch.abs(I_expected.diagonal(dim1=-2, dim2=-1))\n",
    "    mean_diag_abs = diag_vals.mean().item() if diag_vals.numel() > 0 and not torch.isnan(diag_vals).all() else 1.0\n",
    "    diag_load = max(mean_diag_abs * 1e-8, 1e-9)\n",
    "    I_expected_stable = I_expected + eye_matrix * diag_load\n",
    "\n",
    "    sign, logabsdet = torch.linalg.slogdet(I_expected_stable)\n",
    "    if torch.any(sign.real <= 1e-9):\n",
    "        print(\"Warning: Non-positive determinant encountered. Applying penalty.\")\n",
    "        log_det_term = torch.where(sign.real > 1e-9, logabsdet, torch.tensor(1e10, device=device))\n",
    "    else:\n",
    "        log_det_term = logabsdet\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Warning: NaN/Inf detected in I_sample input to likelihood.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    try:\n",
    "        solved_term = torch.linalg.solve(I_expected_stable, I_sample)\n",
    "        trace_term = torch.einsum('...ii->...', solved_term).real\n",
    "    except torch.linalg.LinAlgError as e:\n",
    "        print(f\"Warning: LinAlgError during solve: {e}. Applying high loss penalty.\")\n",
    "        return torch.tensor(float('inf'), device=device)\n",
    "\n",
    "    if torch.isnan(trace_term).any() or torch.isinf(trace_term).any():\n",
    "        print(\"Warning: NaN/Inf detected in trace_term. Returning NaN loss.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    likelihood_terms = log_det_term + trace_term\n",
    "\n",
    "    if torch.isnan(likelihood_terms).any():\n",
    "        print(\"Warning: NaN detected in likelihood_terms before summation. Returning NaN loss.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    total_sum = torch.sum(likelihood_terms)\n",
    "    dc_term = likelihood_terms[0, 0] if n1 > 0 and n2 > 0 else torch.tensor(0.0, device=device)\n",
    "    if torch.isnan(dc_term).any() or torch.isinf(dc_term).any():\n",
    "        print(\"Warning: NaN/Inf detected in DC term. Setting to 0.\")\n",
    "        dc_term = torch.tensor(0.0, device=device)\n",
    "\n",
    "    loss = total_sum - dc_term if (n1 > 1 or n2 > 1) else total_sum\n",
    "\n",
    "    if torch.isnan(loss) or torch.isinf(loss):\n",
    "         print(\"Warning: NaN/Inf detected in final loss. Returning Inf penalty.\")\n",
    "         return torch.tensor(float('inf'), device=device)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 5. Training Loop & Helpers (ðŸ’¥ MODIFIED for 2-style param printing ðŸ’¥)\n",
    "# =========================================================================\n",
    "\n",
    "def get_printable_params(p_list):\n",
    "    \"\"\"Helper to convert log-params to natural scale for printing.\"\"\"\n",
    "    valid_tensors = [p for p in p_list if isinstance(p, torch.Tensor)]\n",
    "    if not valid_tensors: return \"Invalid params_list\"\n",
    "    p_cat = torch.cat([p.detach().clone().cpu() for p in valid_tensors])\n",
    "    \n",
    "    if len(p_cat) != 4:\n",
    "        return f\"Expected 4 params, got {len(p_cat)}. Data: {p_cat.numpy().round(4)}\"\n",
    "\n",
    "    log_params = p_cat\n",
    "    if torch.isnan(log_params).any() or torch.isinf(log_params).any():\n",
    "        return \"[NaN/Inf in log_params]\"\n",
    "        \n",
    "    try:\n",
    "        phi1 = torch.exp(log_params[0])\n",
    "        phi2 = torch.exp(log_params[1]) # range_lon_inv\n",
    "        phi3 = torch.exp(log_params[2]) # (range_lon / range_lat)^2\n",
    "        nugget = torch.exp(log_params[3])\n",
    "\n",
    "        epsilon = 1e-12\n",
    "        sigmasq = phi1 / (phi2 + epsilon)\n",
    "        range_lon = 1.0 / (phi2 + epsilon)\n",
    "        range_lat = 1.0 / (torch.sqrt(phi3 + epsilon) * phi2 + epsilon)\n",
    "        \n",
    "        return (f\"sigmasq: {sigmasq.item():.4f}, range_lat: {range_lat.item():.4f}, \"\n",
    "                f\"range_lon: {range_lon.item():.4f}, nugget: {nugget.item():.4f}\")\n",
    "    except Exception as e:\n",
    "        return f\"[Error in param conversion: {e}]\"\n",
    "\n",
    "# --- ðŸ’¥ NEW HELPER ðŸ’¥ ---\n",
    "def get_phi_params(log_params_list):\n",
    "    \"\"\"Helper to print reparameterized (phi-scale) params.\"\"\"\n",
    "    try:\n",
    "        p_cat = torch.cat([p.detach().clone().cpu() for p in log_params_list])\n",
    "        phi1 = torch.exp(p_cat[0])\n",
    "        phi2 = torch.exp(p_cat[1])\n",
    "        phi3 = torch.exp(p_cat[2])\n",
    "        nugget = torch.exp(p_cat[3]) \n",
    "        \n",
    "        return (f\"phi1: {phi1.item():.4f}, phi2: {phi2.item():.4f}, \"\n",
    "                f\"phi3: {phi3.item():.4f}, nugget: {nugget.item():.4f}\")\n",
    "    except Exception:\n",
    "        return \"[Error in reparam conversion]\"\n",
    "# --- END NEW HELPER ---\n",
    "\n",
    "\n",
    "def run_full_tapered(params_list, optimizer, scheduler, I_sample, n1, n2, p, taper_autocorr_grid, epochs=600, device='cpu'):\n",
    "    \"\"\"Corrected training loop using 4-parameter list and tapered likelihood.\"\"\"\n",
    "    best_loss = float('inf')\n",
    "    params_list = [p.to(device) for p in params_list]\n",
    "    best_params_state = [p.detach().clone() for p in params_list]\n",
    "    epochs_completed = 0\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063 # Grid spacing needed for cov func\n",
    "\n",
    "    I_sample_dev = I_sample.to(device)\n",
    "    taper_autocorr_grid_dev = taper_autocorr_grid.to(device) \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epochs_completed = epoch + 1\n",
    "        optimizer.zero_grad()\n",
    "        params_tensor = torch.cat(params_list) \n",
    "\n",
    "        loss = whittle_likelihood_loss_tapered(\n",
    "            params_tensor, I_sample_dev, n1, n2, p, taper_autocorr_grid_dev, DELTA_LAT, DELTA_LON\n",
    "        )\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"Loss became NaN or Inf at epoch {epoch+1}. Stopping.\")\n",
    "            if epoch == 0: best_params_state = None\n",
    "            epochs_completed = epoch\n",
    "            break\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        nan_grad = False\n",
    "        for param in params_list:\n",
    "            if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
    "                nan_grad = True; break\n",
    "        if nan_grad:\n",
    "             print(f\"Warning: NaN/Inf gradient at epoch {epoch+1}. Skipping step.\")\n",
    "             optimizer.zero_grad()\n",
    "             continue\n",
    "\n",
    "        all_params_on_device = params_list\n",
    "        if all_params_on_device:\n",
    "            torch.nn.utils.clip_grad_norm_(all_params_on_device, max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        current_loss_item = loss.item()\n",
    "        if current_loss_item < best_loss:\n",
    "            params_valid = not any(torch.isnan(p.data).any() or torch.isinf(p.data).any() for p in params_list)\n",
    "            if params_valid:\n",
    "                best_loss = current_loss_item\n",
    "                best_params_state = [p.detach().clone() for p in params_list]\n",
    "            else:\n",
    "                print(f\"Warning: NaN/Inf in params after step epoch {epoch+1}. Not saving.\")\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr'] if optimizer.param_groups else 0.0\n",
    "\n",
    "        if epoch % 50 == 0 or epoch == epochs - 1:\n",
    "            print(f'--- Epoch {epoch+1}/{epochs} (LR: {current_lr:.6f}) ---')\n",
    "            print(f' Loss: {current_loss_item:.4f}')\n",
    "            print(f' Parameters (Natural Scale): {get_printable_params(params_list)}')\n",
    "\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    if best_params_state is None:\n",
    "        print(\"Training failed to find a valid model state.\")\n",
    "        return None, None, epochs_completed # Return None for params\n",
    "\n",
    "    final_params_log_scale = torch.cat([p.cpu() for p in best_params_state])\n",
    "    \n",
    "    # Get final params in both formats\n",
    "    final_natural_params_str = get_printable_params(best_params_state)\n",
    "    final_phi_params_str = get_phi_params(best_params_state)\n",
    "    final_loss_rounded = round(best_loss, 3) if best_loss != float('inf') else float('inf')\n",
    "\n",
    "    print(f'\\nFINAL BEST STATE ACHIEVED (during training):')\n",
    "    print(f'Best Loss: {final_loss_rounded}')\n",
    "    print(f'Parameters (Natural Scale): {final_natural_params_str}')\n",
    "    print(f'Parameters (Phi Scale)    : {final_phi_params_str}')\n",
    "    \n",
    "    # Return strings for final printing\n",
    "    return final_natural_params_str, final_phi_params_str, final_loss_rounded, epochs_completed\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 6. Main Execution Script (ðŸ’¥ MODIFIED ðŸ’¥)\n",
    "# =========================================================================\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    DAY_TO_RUN = 1\n",
    "    TAPERING_FUNC = cgn_hamming # Use Hamming taper\n",
    "    NUM_RUNS = 1\n",
    "    EPOCHS = 5000 \n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063 \n",
    "\n",
    "    LAT_COL, LON_COL = 0, 1\n",
    "    VAL_COL = 2 # Spatially differenced value\n",
    "    TIME_COL = 3\n",
    "\n",
    "    # --- Load Spatially Differenced Data ---\n",
    "    try:\n",
    "        with open(\"spatial_first_difference_data.pkl\", 'rb') as f:\n",
    "            processed_df = pickle.load(f)\n",
    "        print(f\"Loaded {len(processed_df)} days from spatial_first_difference_data.pkl.\")\n",
    "        processed_df = [\n",
    "            torch.tensor(arr, dtype=torch.float32).cpu() if not isinstance(arr, torch.Tensor)\n",
    "            else arr.cpu().to(torch.float32)\n",
    "            for arr in processed_df\n",
    "        ]\n",
    "        if not processed_df: raise ValueError(\"'processed_df' is empty.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: `spatial_first_difference_data.pkl` not found.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing 'processed_df': {e}\")\n",
    "        exit()\n",
    "\n",
    "    if DAY_TO_RUN > len(processed_df) or DAY_TO_RUN <= 0:\n",
    "        print(f\"Error: DAY_TO_RUN ({DAY_TO_RUN}) out of bounds.\")\n",
    "        exit()\n",
    "\n",
    "    cur_df = processed_df[DAY_TO_RUN - 1]\n",
    "    \n",
    "    # --- Filter to a single time point (p=1) ---\n",
    "    cur_df = cur_df[cur_df[:,3] == cur_df[:,3].min()]\n",
    "    print(f\"Filtered data to single time point. New shape: {cur_df.shape}\")\n",
    "    \n",
    "    if cur_df.numel() == 0 or cur_df.shape[1] <= max(LAT_COL, LON_COL, VAL_COL, TIME_COL):\n",
    "        print(f\"Error: Data for Day {DAY_TO_RUN} (single time point) is empty or invalid.\")\n",
    "        exit()\n",
    "\n",
    "    time_slices_list = [cur_df] # List containing the single time slice\n",
    "    \n",
    "    # --- 1. Pre-compute J-vector, Taper Grid, and Taper Autocorrelation ---\n",
    "    print(\"Pre-computing J-vector (Hamming taper)...\")\n",
    "    J_vec, n1, n2, p, taper_grid = generate_Jvector_tapered( \n",
    "        time_slices_list,\n",
    "        tapering_func=TAPERING_FUNC, \n",
    "        lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if J_vec is None or J_vec.numel() == 0 or n1 == 0 or n2 == 0 or p == 0:\n",
    "       print(f\"Error: J-vector generation failed for Day {DAY_TO_RUN}.\")\n",
    "       exit()\n",
    "       \n",
    "    print(\"Pre-computing sample periodogram...\")\n",
    "    I_sample = calculate_sample_periodogram_vectorized(J_vec)\n",
    "\n",
    "    print(\"Pre-computing Hamming taper autocorrelation...\")\n",
    "    taper_autocorr_grid = calculate_taper_autocorrelation_fft(taper_grid, n1, n2, DEVICE)\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Error: NaN/Inf in sample periodogram.\")\n",
    "        exit()\n",
    "    if torch.isnan(taper_autocorr_grid).any() or torch.isinf(taper_autocorr_grid).any():\n",
    "        print(\"Error: NaN/Inf in taper autocorrelation.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Data grid: {n1}x{n2}, {p} time points. J-vector, Periodogram, Taper Autocorr on {DEVICE}.\")\n",
    "\n",
    "    # --- 2. Optimization Loop ---\n",
    "    all_final_results = []\n",
    "    all_final_losses = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        print(f\"\\n{'='*30} Initialization Run {i+1}/{NUM_RUNS} {'='*30}\")\n",
    "\n",
    "        # [log_phi1, log_phi2, log_phi3, log_nugget]\n",
    "        initial_params_values = [np.log(15.0), np.log(0.66), np.log(0.7), np.log(1.5)]\n",
    "        \n",
    "        print(f\"Starting with FIXED params (log-scale): {[round(p, 4) for p in initial_params_values]}\")\n",
    "\n",
    "        params_list = [\n",
    "            Parameter(torch.tensor([val], dtype=torch.float32))\n",
    "            for val in initial_params_values\n",
    "        ]\n",
    "\n",
    "        lr = 0.01 \n",
    "        optimizer = torch.optim.Adam(params_list, lr=lr)\n",
    "\n",
    "        T_MAX = 200\n",
    "        ETA_MIN = 1e-6\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=T_MAX, eta_min=ETA_MIN)\n",
    "\n",
    "        print(f\"Starting optimization run {i+1} on device {DEVICE} (Hamming, SpatialModel kernel)...\")\n",
    "        \n",
    "        nat_params_str, phi_params_str, loss, epochs_run = run_full_tapered(\n",
    "            params_list=params_list,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            I_sample=I_sample,\n",
    "            n1=n1, n2=n2, p=p,\n",
    "            taper_autocorr_grid=taper_autocorr_grid, \n",
    "            epochs=EPOCHS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        \n",
    "        # Store results for final summary\n",
    "        if loss is not None:\n",
    "            all_final_results.append((nat_params_str, phi_params_str))\n",
    "            all_final_losses.append(loss)\n",
    "        else:\n",
    "            all_final_losses.append(float('inf'))\n",
    "\n",
    "    print(f\"\\n\\n{'='*25} Overall Result from Run {'='*25}\")\n",
    "    valid_losses = [l for l in all_final_losses if l is not None and l != float('inf')]\n",
    "\n",
    "    if not valid_losses:\n",
    "        print(f\"The run failed or resulted in an invalid loss for Day {DAY_TO_RUN}.\")\n",
    "    else:\n",
    "        best_loss = min(valid_losses)\n",
    "        best_run_index = all_final_losses.index(best_loss)\n",
    "        best_results = all_final_results[best_run_index]\n",
    "        \n",
    "        print(f\"Best Run Loss: {best_loss}\")\n",
    "        print(f\"Final Parameters (Natural Scale): {best_results[0]}\")\n",
    "        print(f\"Final Parameters (Phi Scale)    : {best_results[1]}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e82d62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:676: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<>:676: SyntaxWarning: invalid escape sequence '\\g'\n",
      "/var/folders/9p/53hd4c7d2fl193h4jwp194wc0000gn/T/ipykernel_41423/2319905123.py:676: SyntaxWarning: invalid escape sequence '\\g'\n",
      "  print(f\"\\n\\g{'='*25} Overall Result from Run {'='*25}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded 31 days from spatial_first_difference_data.pkl.\n",
      "Filtered data to single time point. New shape: torch.Size([17854, 4])\n",
      "Pre-computing J-vector (Hamming taper)...\n",
      "Pre-computing sample periodogram...\n",
      "Pre-computing Hamming taper autocorrelation...\n",
      "Data grid: 113x158, 1 time points. J-vector, Periodogram, Taper Autocorr on cpu.\n",
      "\n",
      "============================== Initialization Run 1/1 ==============================\n",
      "Starting with FIXED params (log-scale): [2.7081, -0.4155, -0.3567, 0.4055]\n",
      "Starting optimization run 1 on device cpu (Hamming, SpatialModel kernel)...\n",
      "--- Epoch 1/5000 (LR: 0.009999) ---\n",
      " Loss: 8893.6602 | Max Grad: 8.774561e+03\n",
      " Params (Raw Log Scale): log_phi1: 2.7181, log_phi2: -0.4255, log_phi3: -0.3467, log_nugget: 0.4155\n",
      " Params (Natural Scale): sigmasq: 23.1864, range_lat: 1.8200, range_lon: 1.5304, nugget: 1.5151\n",
      "--- Epoch 51/5000 (LR: 0.008480) ---\n",
      " Loss: 4808.0454 | Max Grad: 6.999688e+02\n",
      " Params (Raw Log Scale): log_phi1: 3.2090, log_phi2: -0.7884, log_phi3: 0.1680, log_nugget: 0.8692\n",
      " Params (Natural Scale): sigmasq: 54.4521, range_lat: 2.0226, range_lon: 2.1998, nugget: 2.3851\n",
      "--- Epoch 101/5000 (LR: 0.004922) ---\n",
      " Loss: 4508.7007 | Max Grad: 2.323040e+02\n",
      " Params (Raw Log Scale): log_phi1: 3.4168, log_phi2: -0.2266, log_phi3: 0.6249, log_nugget: 0.6882\n",
      " Params (Natural Scale): sigmasq: 38.2246, range_lat: 0.9178, range_lon: 1.2544, nugget: 1.9901\n",
      "--- Epoch 151/5000 (LR: 0.001410) ---\n",
      " Loss: 4456.0894 | Max Grad: 5.948474e+01\n",
      " Params (Raw Log Scale): log_phi1: 3.4619, log_phi2: 0.1262, log_phi3: 0.8357, log_nugget: 0.5847\n",
      " Params (Natural Scale): sigmasq: 28.0975, range_lat: 0.5804, range_lon: 0.8814, nugget: 1.7944\n",
      "--- Epoch 201/5000 (LR: 0.000002) ---\n",
      " Loss: 4452.0010 | Max Grad: 4.695312e+01\n",
      " Params (Raw Log Scale): log_phi1: 3.4601, log_phi2: 0.1811, log_phi3: 0.8628, log_nugget: 0.5795\n",
      " Params (Natural Scale): sigmasq: 26.5475, range_lat: 0.5420, range_lon: 0.8343, nugget: 1.7851\n",
      "--- Epoch 251/5000 (LR: 0.001521) ---\n",
      " Loss: 4449.4814 | Max Grad: 4.896680e+01\n",
      " Params (Raw Log Scale): log_phi1: 3.4563, log_phi2: 0.2230, log_phi3: 0.8845, log_nugget: 0.5781\n",
      " Params (Natural Scale): sigmasq: 25.3628, range_lat: 0.5141, range_lon: 0.8001, nugget: 1.7826\n",
      "--- Epoch 301/5000 (LR: 0.005079) ---\n",
      " Loss: 4435.4438 | Max Grad: 5.931641e+01\n",
      " Params (Raw Log Scale): log_phi1: 3.4603, log_phi2: 0.4793, log_phi3: 0.9187, log_nugget: 0.5699\n",
      " Params (Natural Scale): sigmasq: 19.7069, range_lat: 0.3911, range_lon: 0.6192, nugget: 1.7681\n",
      "--- Epoch 351/5000 (LR: 0.008591) ---\n",
      " Loss: 4403.9829 | Max Grad: 5.503516e+01\n",
      " Params (Raw Log Scale): log_phi1: 3.5309, log_phi2: 0.9828, log_phi3: 0.9106, log_nugget: 0.5126\n",
      " Params (Natural Scale): sigmasq: 12.7833, range_lat: 0.2374, range_lon: 0.3743, nugget: 1.6697\n",
      "--- Epoch 401/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9282 | Max Grad: 1.362207e+01\n",
      " Params (Raw Log Scale): log_phi1: 3.6391, log_phi2: 1.3214, log_phi3: 0.8745, log_nugget: 0.4131\n",
      " Params (Natural Scale): sigmasq: 10.1517, range_lat: 0.1723, range_lon: 0.2668, nugget: 1.5116\n",
      "--- Epoch 451/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9175 | Max Grad: 1.277832e+01\n",
      " Params (Raw Log Scale): log_phi1: 3.6406, log_phi2: 1.3210, log_phi3: 0.8747, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1714, range_lat: 0.1723, range_lon: 0.2669, nugget: 1.5129\n",
      "--- Epoch 501/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9087 | Max Grad: 8.416016e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6386, log_phi2: 1.3189, log_phi3: 0.8749, log_nugget: 0.4141\n",
      " Params (Natural Scale): sigmasq: 10.1719, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5131\n",
      "--- Epoch 551/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9043 | Max Grad: 2.666016e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6378, log_phi2: 1.3189, log_phi3: 0.8747, log_nugget: 0.4137\n",
      " Params (Natural Scale): sigmasq: 10.1652, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5123\n",
      "--- Epoch 601/5000 (LR: 0.000002) ---\n",
      " Loss: 4391.9033 | Max Grad: 2.929688e-02\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3189, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1695, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 651/5000 (LR: 0.001521) ---\n",
      " Loss: 4391.9048 | Max Grad: 4.003906e-02\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3189, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1694, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 701/5000 (LR: 0.005079) ---\n",
      " Loss: 4391.9048 | Max Grad: 3.057617e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6384, log_phi2: 1.3188, log_phi3: 0.8749, log_nugget: 0.4141\n",
      " Params (Natural Scale): sigmasq: 10.1710, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5130\n",
      "--- Epoch 751/5000 (LR: 0.008591) ---\n",
      " Loss: 4391.9062 | Max Grad: 4.564453e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6385, log_phi2: 1.3188, log_phi3: 0.8749, log_nugget: 0.4142\n",
      " Params (Natural Scale): sigmasq: 10.1724, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5132\n",
      "--- Epoch 801/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9048 | Max Grad: 5.023438e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6385, log_phi2: 1.3188, log_phi3: 0.8750, log_nugget: 0.4142\n",
      " Params (Natural Scale): sigmasq: 10.1727, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5132\n",
      "--- Epoch 851/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9048 | Max Grad: 4.323242e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6385, log_phi2: 1.3188, log_phi3: 0.8749, log_nugget: 0.4142\n",
      " Params (Natural Scale): sigmasq: 10.1720, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5131\n",
      "--- Epoch 901/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9038 | Max Grad: 2.881836e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6383, log_phi2: 1.3188, log_phi3: 0.8749, log_nugget: 0.4141\n",
      " Params (Natural Scale): sigmasq: 10.1705, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5129\n",
      "--- Epoch 951/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9038 | Max Grad: 9.082031e-02\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3189, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1694, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 1001/5000 (LR: 0.000002) ---\n",
      " Loss: 4391.9043 | Max Grad: 1.953125e-03\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3189, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1694, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 1051/5000 (LR: 0.001521) ---\n",
      " Loss: 4391.9033 | Max Grad: 9.765625e-03\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3189, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1694, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 1101/5000 (LR: 0.005079) ---\n",
      " Loss: 4391.9028 | Max Grad: 2.285156e-01\n",
      " Params (Raw Log Scale): log_phi1: 3.6378, log_phi2: 1.3189, log_phi3: 0.8746, log_nugget: 0.4136\n",
      " Params (Natural Scale): sigmasq: 10.1646, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5122\n",
      "--- Epoch 1151/5000 (LR: 0.008591) ---\n",
      " Loss: 4391.9253 | Max Grad: 1.636621e+01\n",
      " Params (Raw Log Scale): log_phi1: 3.6385, log_phi2: 1.3188, log_phi3: 0.8750, log_nugget: 0.4142\n",
      " Params (Natural Scale): sigmasq: 10.1722, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5132\n",
      "--- Epoch 1201/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9072 | Max Grad: 7.599609e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6379, log_phi2: 1.3189, log_phi3: 0.8747, log_nugget: 0.4137\n",
      " Params (Natural Scale): sigmasq: 10.1661, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5124\n",
      "--- Epoch 1251/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9043 | Max Grad: 1.296875e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6376, log_phi2: 1.3190, log_phi3: 0.8744, log_nugget: 0.4134\n",
      " Params (Natural Scale): sigmasq: 10.1616, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5119\n",
      "--- Epoch 1301/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9082 | Max Grad: 7.652344e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6376, log_phi2: 1.3190, log_phi3: 0.8744, log_nugget: 0.4133\n",
      " Params (Natural Scale): sigmasq: 10.1615, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5119\n",
      "--- Epoch 1351/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9033 | Max Grad: 3.925781e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6385, log_phi2: 1.3188, log_phi3: 0.8750, log_nugget: 0.4142\n",
      " Params (Natural Scale): sigmasq: 10.1722, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5131\n",
      "--- Epoch 1401/5000 (LR: 0.000002) ---\n",
      " Loss: 4391.9023 | Max Grad: 5.175781e-02\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3189, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1694, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 1451/5000 (LR: 0.001521) ---\n",
      " Loss: 4391.9033 | Max Grad: 3.027344e-02\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3189, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1695, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 1501/5000 (LR: 0.005079) ---\n",
      " Loss: 4391.9038 | Max Grad: 2.785156e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6384, log_phi2: 1.3189, log_phi3: 0.8749, log_nugget: 0.4141\n",
      " Params (Natural Scale): sigmasq: 10.1706, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5130\n",
      "--- Epoch 1551/5000 (LR: 0.008591) ---\n",
      " Loss: 4391.9058 | Max Grad: 4.211914e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6385, log_phi2: 1.3188, log_phi3: 0.8750, log_nugget: 0.4142\n",
      " Params (Natural Scale): sigmasq: 10.1722, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5131\n",
      "--- Epoch 1601/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9062 | Max Grad: 4.770508e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6385, log_phi2: 1.3188, log_phi3: 0.8750, log_nugget: 0.4142\n",
      " Params (Natural Scale): sigmasq: 10.1727, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5132\n",
      "--- Epoch 1651/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9053 | Max Grad: 4.140625e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6384, log_phi2: 1.3188, log_phi3: 0.8750, log_nugget: 0.4142\n",
      " Params (Natural Scale): sigmasq: 10.1720, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5131\n",
      "--- Epoch 1701/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9033 | Max Grad: 2.763672e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6383, log_phi2: 1.3188, log_phi3: 0.8749, log_nugget: 0.4141\n",
      " Params (Natural Scale): sigmasq: 10.1707, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5129\n",
      "--- Epoch 1751/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9038 | Max Grad: 1.163086e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6383, log_phi2: 1.3188, log_phi3: 0.8749, log_nugget: 0.4141\n",
      " Params (Natural Scale): sigmasq: 10.1707, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5130\n",
      "--- Epoch 1801/5000 (LR: 0.000002) ---\n",
      " Loss: 4391.9028 | Max Grad: 1.953125e-02\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3189, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1694, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 1851/5000 (LR: 0.001521) ---\n",
      " Loss: 4391.9033 | Max Grad: 1.367188e-02\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3189, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1693, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 1901/5000 (LR: 0.005079) ---\n",
      " Loss: 4391.9082 | Max Grad: 8.192383e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6375, log_phi2: 1.3190, log_phi3: 0.8743, log_nugget: 0.4133\n",
      " Params (Natural Scale): sigmasq: 10.1606, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5117\n",
      "--- Epoch 1951/5000 (LR: 0.008591) ---\n",
      " Loss: 4391.9185 | Max Grad: 1.419531e+01\n",
      " Params (Raw Log Scale): log_phi1: 3.6389, log_phi2: 1.3163, log_phi3: 0.8741, log_nugget: 0.4130\n",
      " Params (Natural Scale): sigmasq: 10.2028, range_lat: 0.1732, range_lon: 0.2681, nugget: 1.5113\n",
      "--- Epoch 2001/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9102 | Max Grad: 4.038712e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6383, log_phi2: 1.3196, log_phi3: 0.8749, log_nugget: 0.4147\n",
      " Params (Natural Scale): sigmasq: 10.1627, range_lat: 0.1726, range_lon: 0.2672, nugget: 1.5139\n",
      "--- Epoch 2051/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9131 | Max Grad: 9.650391e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6377, log_phi2: 1.3174, log_phi3: 0.8741, log_nugget: 0.4122\n",
      " Params (Natural Scale): sigmasq: 10.1792, range_lat: 0.1730, range_lon: 0.2678, nugget: 1.5102\n",
      "--- Epoch 2101/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9053 | Max Grad: 3.113281e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6393, log_phi2: 1.3189, log_phi3: 0.8758, log_nugget: 0.4152\n",
      " Params (Natural Scale): sigmasq: 10.1797, range_lat: 0.1726, range_lon: 0.2674, nugget: 1.5147\n",
      "--- Epoch 2151/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9033 | Max Grad: 9.228516e-01\n",
      " Params (Raw Log Scale): log_phi1: 3.6383, log_phi2: 1.3188, log_phi3: 0.8749, log_nugget: 0.4139\n",
      " Params (Natural Scale): sigmasq: 10.1702, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5127\n",
      "--- Epoch 2201/5000 (LR: 0.000002) ---\n",
      " Loss: 4391.9033 | Max Grad: 7.812500e-03\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3189, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1694, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 2251/5000 (LR: 0.001521) ---\n",
      " Loss: 4391.9038 | Max Grad: 2.929688e-03\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3188, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1696, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 2301/5000 (LR: 0.005079) ---\n",
      " Loss: 4391.9053 | Max Grad: 5.524414e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6380, log_phi2: 1.3189, log_phi3: 0.8746, log_nugget: 0.4137\n",
      " Params (Natural Scale): sigmasq: 10.1659, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5124\n",
      "--- Epoch 2351/5000 (LR: 0.008591) ---\n",
      " Loss: 4391.9082 | Max Grad: 7.869141e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6381, log_phi2: 1.3190, log_phi3: 0.8749, log_nugget: 0.4138\n",
      " Params (Natural Scale): sigmasq: 10.1669, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5126\n",
      "--- Epoch 2401/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9170 | Max Grad: 1.210449e+01\n",
      " Params (Raw Log Scale): log_phi1: 3.6374, log_phi2: 1.3189, log_phi3: 0.8726, log_nugget: 0.4133\n",
      " Params (Natural Scale): sigmasq: 10.1597, range_lat: 0.1729, range_lon: 0.2674, nugget: 1.5118\n",
      "--- Epoch 2451/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9062 | Max Grad: 3.315430e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6384, log_phi2: 1.3188, log_phi3: 0.8752, log_nugget: 0.4141\n",
      " Params (Natural Scale): sigmasq: 10.1713, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5130\n",
      "--- Epoch 2501/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9136 | Max Grad: 1.207910e+01\n",
      " Params (Raw Log Scale): log_phi1: 3.6377, log_phi2: 1.3191, log_phi3: 0.8739, log_nugget: 0.4134\n",
      " Params (Natural Scale): sigmasq: 10.1614, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5120\n",
      "--- Epoch 2551/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9038 | Max Grad: 9.130859e-01\n",
      " Params (Raw Log Scale): log_phi1: 3.6384, log_phi2: 1.3188, log_phi3: 0.8748, log_nugget: 0.4142\n",
      " Params (Natural Scale): sigmasq: 10.1714, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5131\n",
      "--- Epoch 2601/5000 (LR: 0.000002) ---\n",
      " Loss: 4391.9023 | Max Grad: 3.613281e-02\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3189, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1694, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 2651/5000 (LR: 0.001521) ---\n",
      " Loss: 4391.9038 | Max Grad: 1.464844e-02\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3189, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1695, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 2701/5000 (LR: 0.005079) ---\n",
      " Loss: 4391.9062 | Max Grad: 2.926758e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6384, log_phi2: 1.3188, log_phi3: 0.8749, log_nugget: 0.4141\n",
      " Params (Natural Scale): sigmasq: 10.1718, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5130\n",
      "--- Epoch 2751/5000 (LR: 0.008591) ---\n",
      " Loss: 4391.9478 | Max Grad: 2.368652e+01\n",
      " Params (Raw Log Scale): log_phi1: 3.6395, log_phi2: 1.3183, log_phi3: 0.8761, log_nugget: 0.4152\n",
      " Params (Natural Scale): sigmasq: 10.1880, range_lat: 0.1727, range_lon: 0.2676, nugget: 1.5147\n",
      "--- Epoch 2801/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9048 | Max Grad: 4.724609e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6374, log_phi2: 1.3193, log_phi3: 0.8740, log_nugget: 0.4131\n",
      " Params (Natural Scale): sigmasq: 10.1561, range_lat: 0.1727, range_lon: 0.2673, nugget: 1.5115\n",
      "--- Epoch 2851/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9194 | Max Grad: 1.417871e+01\n",
      " Params (Raw Log Scale): log_phi1: 3.6386, log_phi2: 1.3187, log_phi3: 0.8753, log_nugget: 0.4144\n",
      " Params (Natural Scale): sigmasq: 10.1746, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5134\n",
      "--- Epoch 2901/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9067 | Max Grad: 4.224609e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6390, log_phi2: 1.3176, log_phi3: 0.8746, log_nugget: 0.4143\n",
      " Params (Natural Scale): sigmasq: 10.1898, range_lat: 0.1729, range_lon: 0.2678, nugget: 1.5134\n",
      "--- Epoch 2951/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9038 | Max Grad: 2.077148e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6383, log_phi2: 1.3188, log_phi3: 0.8749, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1702, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5129\n",
      "--- Epoch 3001/5000 (LR: 0.000002) ---\n",
      " Loss: 4391.9043 | Max Grad: 9.765625e-03\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3188, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1695, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 3051/5000 (LR: 0.001521) ---\n",
      " Loss: 4391.9033 | Max Grad: 9.765625e-03\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3189, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1694, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 3101/5000 (LR: 0.005079) ---\n",
      " Loss: 4391.9087 | Max Grad: 8.753906e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6381, log_phi2: 1.3189, log_phi3: 0.8747, log_nugget: 0.4139\n",
      " Params (Natural Scale): sigmasq: 10.1674, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5126\n",
      "--- Epoch 3151/5000 (LR: 0.008591) ---\n",
      " Loss: 4391.9062 | Max Grad: 6.063477e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6380, log_phi2: 1.3198, log_phi3: 0.8754, log_nugget: 0.4139\n",
      " Params (Natural Scale): sigmasq: 10.1568, range_lat: 0.1725, range_lon: 0.2672, nugget: 1.5128\n",
      "--- Epoch 3201/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9077 | Max Grad: 7.368164e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6377, log_phi2: 1.3192, log_phi3: 0.8746, log_nugget: 0.4134\n",
      " Params (Natural Scale): sigmasq: 10.1609, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5120\n",
      "--- Epoch 3251/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9067 | Max Grad: 6.330078e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6387, log_phi2: 1.3186, log_phi3: 0.8753, log_nugget: 0.4144\n",
      " Params (Natural Scale): sigmasq: 10.1766, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5135\n",
      "--- Epoch 3301/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9058 | Max Grad: 4.561523e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6379, log_phi2: 1.3191, log_phi3: 0.8745, log_nugget: 0.4136\n",
      " Params (Natural Scale): sigmasq: 10.1639, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5123\n",
      "--- Epoch 3351/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9033 | Max Grad: 1.512695e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6384, log_phi2: 1.3188, log_phi3: 0.8749, log_nugget: 0.4141\n",
      " Params (Natural Scale): sigmasq: 10.1715, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5130\n",
      "--- Epoch 3401/5000 (LR: 0.000002) ---\n",
      " Loss: 4391.9048 | Max Grad: 4.003906e-02\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3189, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1694, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 3451/5000 (LR: 0.001521) ---\n",
      " Loss: 4391.9033 | Max Grad: 9.719849e-03\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3189, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1693, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 3501/5000 (LR: 0.005079) ---\n",
      " Loss: 4391.9043 | Max Grad: 1.004791e-01\n",
      " Params (Raw Log Scale): log_phi1: 3.6384, log_phi2: 1.3187, log_phi3: 0.8749, log_nugget: 0.4141\n",
      " Params (Natural Scale): sigmasq: 10.1723, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5129\n",
      "--- Epoch 3551/5000 (LR: 0.008591) ---\n",
      " Loss: 4391.9111 | Max Grad: 9.712891e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6369, log_phi2: 1.3197, log_phi3: 0.8736, log_nugget: 0.4127\n",
      " Params (Natural Scale): sigmasq: 10.1473, range_lat: 0.1727, range_lon: 0.2672, nugget: 1.5108\n",
      "--- Epoch 3601/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9888 | Max Grad: 3.275391e+01\n",
      " Params (Raw Log Scale): log_phi1: 3.6359, log_phi2: 1.3203, log_phi3: 0.8726, log_nugget: 0.4117\n",
      " Params (Natural Scale): sigmasq: 10.1307, range_lat: 0.1726, range_lon: 0.2671, nugget: 1.5093\n",
      "--- Epoch 3651/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9121 | Max Grad: 1.025586e+01\n",
      " Params (Raw Log Scale): log_phi1: 3.6395, log_phi2: 1.3180, log_phi3: 0.8760, log_nugget: 0.4152\n",
      " Params (Natural Scale): sigmasq: 10.1907, range_lat: 0.1727, range_lon: 0.2677, nugget: 1.5147\n",
      "--- Epoch 3701/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9224 | Max Grad: 1.557031e+01\n",
      " Params (Raw Log Scale): log_phi1: 3.6394, log_phi2: 1.3181, log_phi3: 0.8759, log_nugget: 0.4151\n",
      " Params (Natural Scale): sigmasq: 10.1884, range_lat: 0.1727, range_lon: 0.2676, nugget: 1.5145\n",
      "--- Epoch 3751/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9023 | Max Grad: 1.476562e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6380, log_phi2: 1.3190, log_phi3: 0.8746, log_nugget: 0.4137\n",
      " Params (Natural Scale): sigmasq: 10.1654, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5125\n",
      "--- Epoch 3801/5000 (LR: 0.000002) ---\n",
      " Loss: 4391.9028 | Max Grad: 1.298828e-01\n",
      " Params (Raw Log Scale): log_phi1: 3.6383, log_phi2: 1.3189, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1696, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 3851/5000 (LR: 0.001521) ---\n",
      " Loss: 4391.9043 | Max Grad: 4.760742e-03\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3189, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1692, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 3901/5000 (LR: 0.005079) ---\n",
      " Loss: 4391.9043 | Max Grad: 3.992188e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6380, log_phi2: 1.3191, log_phi3: 0.8746, log_nugget: 0.4137\n",
      " Params (Natural Scale): sigmasq: 10.1646, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5124\n",
      "--- Epoch 3951/5000 (LR: 0.008591) ---\n",
      " Loss: 4391.9087 | Max Grad: 7.566406e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6388, log_phi2: 1.3185, log_phi3: 0.8753, log_nugget: 0.4145\n",
      " Params (Natural Scale): sigmasq: 10.1788, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5136\n",
      "--- Epoch 4001/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9077 | Max Grad: 7.619141e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6377, log_phi2: 1.3193, log_phi3: 0.8743, log_nugget: 0.4134\n",
      " Params (Natural Scale): sigmasq: 10.1597, range_lat: 0.1727, range_lon: 0.2673, nugget: 1.5120\n",
      "--- Epoch 4051/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9082 | Max Grad: 7.465820e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6388, log_phi2: 1.3185, log_phi3: 0.8753, log_nugget: 0.4145\n",
      " Params (Natural Scale): sigmasq: 10.1789, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5136\n",
      "--- Epoch 4101/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9038 | Max Grad: 3.489258e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6380, log_phi2: 1.3191, log_phi3: 0.8746, log_nugget: 0.4137\n",
      " Params (Natural Scale): sigmasq: 10.1650, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5125\n",
      "--- Epoch 4151/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9033 | Max Grad: 1.379883e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6383, log_phi2: 1.3188, log_phi3: 0.8749, log_nugget: 0.4141\n",
      " Params (Natural Scale): sigmasq: 10.1712, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5130\n",
      "--- Epoch 4201/5000 (LR: 0.000002) ---\n",
      " Loss: 4391.9053 | Max Grad: 2.929688e-02\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3188, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1695, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 4251/5000 (LR: 0.001521) ---\n",
      " Loss: 4391.9038 | Max Grad: 1.464844e-02\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3189, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1693, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 4301/5000 (LR: 0.005079) ---\n",
      " Loss: 4391.9028 | Max Grad: 9.443359e-01\n",
      " Params (Raw Log Scale): log_phi1: 3.6384, log_phi2: 1.3187, log_phi3: 0.8750, log_nugget: 0.4142\n",
      " Params (Natural Scale): sigmasq: 10.1731, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5131\n",
      "--- Epoch 4351/5000 (LR: 0.008591) ---\n",
      " Loss: 4391.9116 | Max Grad: 9.919922e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6389, log_phi2: 1.3192, log_phi3: 0.8757, log_nugget: 0.4149\n",
      " Params (Natural Scale): sigmasq: 10.1727, range_lat: 0.1726, range_lon: 0.2673, nugget: 1.5142\n",
      "--- Epoch 4401/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9097 | Max Grad: 9.378906e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6376, log_phi2: 1.3196, log_phi3: 0.8743, log_nugget: 0.4133\n",
      " Params (Natural Scale): sigmasq: 10.1550, range_lat: 0.1726, range_lon: 0.2672, nugget: 1.5119\n",
      "--- Epoch 4451/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9053 | Max Grad: 5.679688e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6386, log_phi2: 1.3186, log_phi3: 0.8752, log_nugget: 0.4144\n",
      " Params (Natural Scale): sigmasq: 10.1757, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5134\n",
      "--- Epoch 4501/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9053 | Max Grad: 4.925781e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6379, log_phi2: 1.3189, log_phi3: 0.8744, log_nugget: 0.4136\n",
      " Params (Natural Scale): sigmasq: 10.1655, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5122\n",
      "--- Epoch 4551/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9033 | Max Grad: 1.168945e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6383, log_phi2: 1.3187, log_phi3: 0.8749, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1717, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5129\n",
      "--- Epoch 4601/5000 (LR: 0.000002) ---\n",
      " Loss: 4391.9043 | Max Grad: 3.222656e-02\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3189, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1694, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 4651/5000 (LR: 0.001521) ---\n",
      " Loss: 4391.9033 | Max Grad: 7.812500e-03\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3189, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1692, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "--- Epoch 4701/5000 (LR: 0.005079) ---\n",
      " Loss: 4391.9087 | Max Grad: 8.241211e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6390, log_phi2: 1.3181, log_phi3: 0.8756, log_nugget: 0.4147\n",
      " Params (Natural Scale): sigmasq: 10.1854, range_lat: 0.1728, range_lon: 0.2676, nugget: 1.5140\n",
      "--- Epoch 4751/5000 (LR: 0.008591) ---\n",
      " Loss: 4391.9092 | Max Grad: 4.845276e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6396, log_phi2: 1.3150, log_phi3: 0.8752, log_nugget: 0.4146\n",
      " Params (Natural Scale): sigmasq: 10.2233, range_lat: 0.1733, range_lon: 0.2685, nugget: 1.5137\n",
      "--- Epoch 4801/5000 (LR: 0.009999) ---\n",
      " Loss: 4391.9336 | Max Grad: 1.946777e+01\n",
      " Params (Raw Log Scale): log_phi1: 3.6390, log_phi2: 1.3183, log_phi3: 0.8756, log_nugget: 0.4148\n",
      " Params (Natural Scale): sigmasq: 10.1835, range_lat: 0.1727, range_lon: 0.2676, nugget: 1.5140\n",
      "--- Epoch 4851/5000 (LR: 0.008480) ---\n",
      " Loss: 4391.9131 | Max Grad: 1.113379e+01\n",
      " Params (Raw Log Scale): log_phi1: 3.6370, log_phi2: 1.3198, log_phi3: 0.8736, log_nugget: 0.4127\n",
      " Params (Natural Scale): sigmasq: 10.1463, range_lat: 0.1726, range_lon: 0.2672, nugget: 1.5109\n",
      "--- Epoch 4901/5000 (LR: 0.004922) ---\n",
      " Loss: 4391.9048 | Max Grad: 5.028320e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6388, log_phi2: 1.3185, log_phi3: 0.8753, log_nugget: 0.4145\n",
      " Params (Natural Scale): sigmasq: 10.1787, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5136\n",
      "--- Epoch 4951/5000 (LR: 0.001410) ---\n",
      " Loss: 4391.9048 | Max Grad: 2.917969e+00\n",
      " Params (Raw Log Scale): log_phi1: 3.6384, log_phi2: 1.3188, log_phi3: 0.8749, log_nugget: 0.4141\n",
      " Params (Natural Scale): sigmasq: 10.1717, range_lat: 0.1727, range_lon: 0.2675, nugget: 1.5130\n",
      "--- Epoch 5000/5000 (LR: 0.000001) ---\n",
      " Loss: 4391.9028 | Max Grad: 3.808594e-02\n",
      " Params (Raw Log Scale): log_phi1: 3.6382, log_phi2: 1.3189, log_phi3: 0.8748, log_nugget: 0.4140\n",
      " Params (Natural Scale): sigmasq: 10.1695, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5128\n",
      "\n",
      "--- Training Complete ---\n",
      "\n",
      "FINAL BEST STATE ACHIEVED (during training):\n",
      "Best Loss: 4391.901\n",
      "Parameters (Natural Scale): sigmasq: 10.1554, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5111\n",
      "Parameters (Phi Scale)    : phi1: 37.9799, phi2: 3.7399, phi3: 2.3969, nugget: 1.5111\n",
      "Parameters (Raw Log Scale): log_phi1: 3.6371, log_phi2: 1.3191, log_phi3: 0.8742, log_nugget: 0.4129\n",
      "\n",
      "\\g========================= Overall Result from Run =========================\n",
      "Best Run Loss: 4391.901\n",
      "Final Parameters (Natural Scale): sigmasq: 10.1554, range_lat: 0.1727, range_lon: 0.2674, nugget: 1.5111\n",
      "Final Parameters (Phi Scale)    : phi1: 37.9799, phi2: 3.7399, phi3: 2.3969, nugget: 1.5111\n",
      "Final Parameters (Raw Log Scale): log_phi1: 3.6371, log_phi2: 1.3191, log_phi3: 0.8742, log_nugget: 0.4129\n",
      "\n",
      "Total execution time: 74.03 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import cmath\n",
    "import pickle\n",
    "import time \n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import Parameter\n",
    "import torch.fft \n",
    "from typing import List, Dict, Any, Callable\n",
    "\n",
    "# =========================================================================\n",
    "# 1. Tapering, Autocorrelation, and Covariance Functions\n",
    "# =========================================================================\n",
    "\n",
    "def cgn_hamming(u, n1, n2):\n",
    "    \"\"\"Computes a 2D Hamming window.\"\"\"\n",
    "    u1, u2 = u\n",
    "    device = u1.device if isinstance(u1, torch.Tensor) else (u2.device if isinstance(u2, torch.Tensor) else torch.device('cpu'))\n",
    "    u1_tensor = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_tensor = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "\n",
    "    n1_eff = float(n1) if n1 > 0 else 1.0\n",
    "    n2_eff = float(n2) if n2 > 0 else 1.0\n",
    "    hamming1 = 0.54 + 0.46 * torch.cos(2.0 * torch.pi * u1_tensor / n1_eff)\n",
    "    hamming2 = 0.54 + 0.46 * torch.cos(2.0 * torch.pi * u2_tensor / n2_eff)\n",
    "    return hamming1 * hamming2\n",
    "\n",
    "def cgn_2dbartlett(u, n1, n2): # Kept for potential future use\n",
    "    \"\"\"Computes a 2D Bartlett window function.\"\"\"\n",
    "    u1, u2 = u\n",
    "    device = u1.device if isinstance(u1, torch.Tensor) else (u2.device if isinstance(u2, torch.Tensor) else torch.device('cpu'))\n",
    "    u1_tensor = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_tensor = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    n1_eff = float(n1) if n1 > 0 else 1.0\n",
    "    n2_eff = float(n2) if n2 > 0 else 1.0\n",
    "    kernel = (1.0 - torch.abs(u1_tensor) / n1_eff) * (1.0 - torch.abs(u2_tensor) / n2_eff)\n",
    "    return torch.clamp(kernel, min=0.0)\n",
    "\n",
    "def calculate_taper_autocorrelation_fft(taper_grid, n1, n2, device):\n",
    "    \"\"\"\n",
    "    Computes the normalized taper autocorrelation function c_gn(u) using FFT.\n",
    "    \"\"\"\n",
    "    taper_grid = taper_grid.to(device) \n",
    "    H = torch.sum(taper_grid**2)\n",
    "    if H < 1e-12:\n",
    "        print(\"Warning: Sum of squared taper weights (H) is near zero.\")\n",
    "        return torch.zeros((2*n1-1, 2*n2-1), device=device, dtype=taper_grid.dtype)\n",
    "\n",
    "    N1, N2 = 2 * n1 - 1, 2 * n2 - 1\n",
    "    taper_fft = torch.fft.fft2(taper_grid, s=(N1, N2))\n",
    "    power_spectrum = torch.abs(taper_fft)**2\n",
    "    autocorr_unnormalized = torch.fft.ifft2(power_spectrum).real\n",
    "    autocorr_shifted = torch.fft.fftshift(autocorr_unnormalized)\n",
    "\n",
    "    c_gn_grid = autocorr_shifted / (H + 1e-12)\n",
    "\n",
    "    return c_gn_grid \n",
    "\n",
    "def cov_x_spatial_model_kernel(u1, u2, t, params):\n",
    "    \"\"\"\n",
    "    Computes autocovariance of X using the SPATIAL-ONLY kernel.\n",
    "    u1, u2 are PHYSICAL lags (already scaled by deltas).\n",
    "    \"\"\"\n",
    "    device = params.device\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    if torch.isnan(params).any() or torch.isinf(params).any():\n",
    "         out_shape = torch.broadcast_shapes(u1_dev.shape, u2_dev.shape, t_dev.shape)\n",
    "         return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "\n",
    "    phi1   = torch.exp(params[0])\n",
    "    phi2   = torch.exp(params[1]) \n",
    "    phi3   = torch.exp(params[2]) \n",
    "    nugget = torch.exp(params[3])\n",
    "\n",
    "    epsilon = 1e-12\n",
    "    sigmasq = phi1 / (phi2 + epsilon)  \n",
    "    range_lon_inv = phi2\n",
    "    range_lat_inv = torch.sqrt(phi3 + epsilon) * phi2\n",
    "\n",
    "    dist_sq = (u1_dev * range_lat_inv).pow(2) + (u2_dev * range_lon_inv).pow(2)\n",
    "    distance = torch.sqrt(dist_sq + epsilon) \n",
    "\n",
    "    cov_smooth = sigmasq * torch.exp(-distance)\n",
    "\n",
    "    is_zero_lag = (torch.abs(u1_dev) < 1e-9) & (torch.abs(u2_dev) < 1e-9) & (torch.abs(t_dev) < 1e-9)\n",
    "    final_cov = torch.where(is_zero_lag, cov_smooth + nugget, cov_smooth)\n",
    "\n",
    "    if torch.isnan(final_cov).any(): print(\"Warning: NaN detected in cov_x_spatial_model_kernel output.\")\n",
    "    return final_cov\n",
    "\n",
    "\n",
    "def cov_spatial_difference(u1, u2, t, params, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Calculates covariance Cov(Y(s), Y(s+u)) for Y(s) = X(s+d1) + X(s+d2) - 2X(s)\n",
    "    Based on the underlying spatial kernel.\n",
    "    u1, u2 are PHYSICAL lags.\n",
    "    \"\"\"\n",
    "    weights = {(0, 0): -2.0, (1, 0): 1.0, (0, 1): 1.0}\n",
    "    device = params.device\n",
    "    out_shape = torch.broadcast_shapes(u1.shape if isinstance(u1, torch.Tensor) else (),\n",
    "                                     u2.shape if isinstance(u2, torch.Tensor) else (),\n",
    "                                     t.shape if isinstance(t, torch.Tensor) else ())\n",
    "    cov = torch.zeros(out_shape, device=device, dtype=torch.float32)\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    for (a_idx, b_idx), w_ab in weights.items():\n",
    "        offset_a1 = a_idx * delta1\n",
    "        offset_a2 = b_idx * delta2\n",
    "        for (c_idx, d_idx), w_cd in weights.items():\n",
    "            offset_c1 = c_idx * delta1\n",
    "            offset_c2 = d_idx * delta2\n",
    "            \n",
    "            lag_u1 = u1_dev + (offset_a1 - offset_c1)\n",
    "            lag_u2 = u2_dev + (offset_a2 - offset_c2)\n",
    "            \n",
    "            term_cov = cov_x_spatial_model_kernel(lag_u1, lag_u2, t_dev, params) \n",
    "            \n",
    "            if torch.isnan(term_cov).any():\n",
    "                 print(f\"Warning: NaN in term_cov within cov_spatial_difference.\")\n",
    "                 return torch.full_like(cov, float('nan'))\n",
    "            cov += w_ab * w_cd * term_cov\n",
    "\n",
    "    if torch.isnan(cov).any(): print(\"Warning: NaN in final cov_spatial_difference output.\")\n",
    "    return cov\n",
    "\n",
    "def cn_bar_tapered(u1, u2, t, params, n1, n2, taper_autocorr_grid, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Computes c_Y(u) * c_gn(u).\n",
    "    u1, u2 are GRID index lags (e.g., -n1..0..n1)\n",
    "    \"\"\"\n",
    "    device = params.device\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    # --- Convert GRID lags to PHYSICAL lags ---\n",
    "    lag_u1 = u1_dev * delta1\n",
    "    lag_u2 = u2_dev * delta2\n",
    "    \n",
    "    cov_X_value = cov_spatial_difference(lag_u1, lag_u2, t_dev, params, delta1, delta2)\n",
    "\n",
    "    # --- Get Taper Autocorrelation Value c_gn(u) from grid ---\n",
    "    u1_idx = u1_dev.long()\n",
    "    u2_idx = u2_dev.long()\n",
    "\n",
    "    idx1 = (n1 - 1 + u1_idx) # Centering index\n",
    "    idx2 = (n2 - 1 + u2_idx) # Centering index\n",
    "    \n",
    "    idx1 = torch.clamp(idx1, 0, 2 * n1 - 2)\n",
    "    idx2 = torch.clamp(idx2, 0, 2 * n2 - 2)\n",
    "\n",
    "    taper_autocorr_value = taper_autocorr_grid[idx1, idx2]\n",
    "\n",
    "    if torch.isnan(cov_X_value).any() or torch.isnan(taper_autocorr_value).any():\n",
    "        out_shape = torch.broadcast_shapes(cov_X_value.shape, taper_autocorr_value.shape)\n",
    "        return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "\n",
    "    result = cov_X_value * taper_autocorr_value\n",
    "    if torch.isnan(result).any(): print(\"Warning: NaN in cn_bar_tapered output.\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def expected_periodogram_fft_tapered(params, n1, n2, p, taper_autocorr_grid, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Calculates the expected periodogram I(omega_s) (a pxp matrix in time)\n",
    "    using the exact taper autocorrelation c_gn(u) and\n",
    "    CORRECTLY implementing the aliasing sum (Lemma 2).\n",
    "    \"\"\"\n",
    "    device = params.device if isinstance(params, torch.Tensor) else params[0].device\n",
    "    if isinstance(params, list):\n",
    "        params_tensor = torch.cat([p.to(device) for p in params])\n",
    "    else:\n",
    "        params_tensor = params.to(device)\n",
    "\n",
    "    u1_lags = torch.arange(n1, dtype=torch.float32, device=device)\n",
    "    u2_lags = torch.arange(n2, dtype=torch.float32, device=device)\n",
    "    u1_mesh, u2_mesh = torch.meshgrid(u1_lags, u2_lags, indexing='ij')\n",
    "\n",
    "    t_lags = torch.arange(p, dtype=torch.float32, device=device)\n",
    "    tilde_cn_tensor = torch.zeros((n1, n2, p, p), dtype=torch.complex64, device=device)\n",
    "\n",
    "    for q in range(p):\n",
    "        for r in range(p):\n",
    "            t_diff = t_lags[q] - t_lags[r]\n",
    "            \n",
    "            term1 = cn_bar_tapered(u1_mesh, u2_mesh, t_diff, \n",
    "                                   params_tensor, n1, n2, taper_autocorr_grid, delta1, delta2)\n",
    "            term2 = cn_bar_tapered(u1_mesh - n1, u2_mesh, t_diff, \n",
    "                                   params_tensor, n1, n2, taper_autocorr_grid, delta1, delta2)\n",
    "            term3 = cn_bar_tapered(u1_mesh, u2_mesh - n2, t_diff, \n",
    "                                   params_tensor, n1, n2, taper_autocorr_grid, delta1, delta2)\n",
    "            term4 = cn_bar_tapered(u1_mesh - n1, u2_mesh - n2, t_diff,\n",
    "                                   params_tensor, n1, n2, taper_autocorr_grid, delta1, delta2)\n",
    "            \n",
    "            tilde_cn_grid_qr = (term1 + term2 + term3 + term4)\n",
    "            \n",
    "            if torch.isnan(tilde_cn_grid_qr).any():\n",
    "                 tilde_cn_tensor[:, :, q, r] = float('nan')\n",
    "            else:\n",
    "                 tilde_cn_tensor[:, :, q, r] = tilde_cn_grid_qr.to(torch.complex64)\n",
    "\n",
    "    if torch.isnan(tilde_cn_tensor).any():\n",
    "        print(\"Warning: NaN detected in tilde_cn_tensor before FFT.\")\n",
    "        nan_shape = (n1, n2, p, p)\n",
    "        return torch.full(nan_shape, float('nan'), dtype=torch.complex64, device=device)\n",
    "\n",
    "    fft_result = torch.fft.fft2(tilde_cn_tensor, dim=(0, 1))\n",
    "    fft_result_real = fft_result.real \n",
    "    normalization_factor = 1.0 / (4.0 * cmath.pi**2)\n",
    "    result = fft_result_real * normalization_factor\n",
    "\n",
    "    if torch.isnan(result).any(): print(\"Warning: NaN in expected_periodogram_fft_tapered output.\")\n",
    "    return result\n",
    "\n",
    "# =========================================================================\n",
    "# 3. Data Processing (Tapering)\n",
    "# =========================================================================\n",
    "def generate_Jvector_tapered(tensor_list, tapering_func, lat_col, lon_col, val_col, device):\n",
    "    \"\"\"\n",
    "    Generates J-vector for a single component using the specified taper,\n",
    "    placing result on device.\n",
    "    \"\"\"\n",
    "    p = len(tensor_list)\n",
    "    if p == 0: return torch.empty(0, 0, 0, device=device), 0, 0, 0, None \n",
    "\n",
    "    valid_tensors = [t for t in tensor_list if t.numel() > 0 and t.shape[1] > max(lat_col, lon_col, val_col)]\n",
    "    if not valid_tensors:\n",
    "         print(\"Warning: No valid tensors found in tensor_list.\")\n",
    "         return torch.empty(0, 0, 0, device=device), 0, 0, 0, None\n",
    "\n",
    "    try:\n",
    "        all_lats_cpu = torch.cat([t[:, lat_col] for t in valid_tensors])\n",
    "        all_lons_cpu = torch.cat([t[:, lon_col] for t in valid_tensors])\n",
    "    except IndexError:\n",
    "        print(f\"Error: Invalid column index. Check tensor shapes.\")\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0, None\n",
    "\n",
    "    all_lats_cpu = all_lats_cpu[~torch.isnan(all_lats_cpu) & ~torch.isinf(all_lats_cpu)]\n",
    "    all_lons_cpu = all_lons_cpu[~torch.isnan(all_lons_cpu) & ~torch.isinf(all_lons_cpu)]\n",
    "    if all_lats_cpu.numel() == 0 or all_lons_cpu.numel() == 0:\n",
    "        print(\"Warning: No valid coordinates after NaN/Inf filtering.\")\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0, None\n",
    "\n",
    "    unique_lats_cpu, unique_lons_cpu = torch.unique(all_lats_cpu), torch.unique(all_lons_cpu)\n",
    "    n1, n2 = len(unique_lats_cpu), len(unique_lons_cpu)\n",
    "    if n1 == 0 or n2 == 0:\n",
    "        print(\"Warning: Grid dimensions are zero.\")\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0, None\n",
    "\n",
    "    lat_map = {lat.item(): i for i, lat in enumerate(unique_lats_cpu)}\n",
    "    lon_map = {lon.item(): i for i, lon in enumerate(unique_lons_cpu)}\n",
    "\n",
    "    u1_mesh_cpu, u2_mesh_cpu = torch.meshgrid(\n",
    "        torch.arange(n1, dtype=torch.float32),\n",
    "        torch.arange(n2, dtype=torch.float32),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    taper_grid = tapering_func((u1_mesh_cpu, u2_mesh_cpu), n1, n2).to(device) # Taper on device\n",
    "\n",
    "    fft_results = []\n",
    "    for tensor in tensor_list:\n",
    "        data_grid = torch.zeros((n1, n2), dtype=torch.float32, device=device)\n",
    "        if tensor.numel() > 0 and tensor.shape[1] > max(lat_col, lon_col, val_col):\n",
    "            for row in tensor:\n",
    "                lat_item, lon_item = row[lat_col].item(), row[lon_col].item()\n",
    "                if not (np.isnan(lat_item) or np.isnan(lon_item)):\n",
    "                    i = lat_map.get(lat_item)\n",
    "                    j = lon_map.get(lon_item)\n",
    "                    if i is not None and j is not None:\n",
    "                        val = row[val_col]\n",
    "                        val_num = val.item() if isinstance(val, torch.Tensor) else val\n",
    "                        if not np.isnan(val_num) and not np.isinf(val_num):\n",
    "                           data_grid[i, j] = val_num\n",
    "\n",
    "        data_grid_tapered = data_grid * taper_grid \n",
    "\n",
    "        if torch.isnan(data_grid_tapered).any() or torch.isinf(data_grid_tapered).any():\n",
    "             print(\"Warning: NaN/Inf detected in data_grid_tapered before FFT. Replacing with zeros.\")\n",
    "             data_grid_tapered = torch.nan_to_num(data_grid_tapered, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        fft_results.append(torch.fft.fft2(data_grid_tapered))\n",
    "\n",
    "    if not fft_results:\n",
    "         print(\"Warning: No FFT results generated.\")\n",
    "         return torch.empty(0, 0, 0, device=device), n1, n2, 0, taper_grid\n",
    "\n",
    "    J_vector_tensor = torch.stack(fft_results, dim=2).to(device)\n",
    "\n",
    "    H = torch.sum(taper_grid**2)\n",
    "    if H < 1e-12:\n",
    "        print(\"Warning: Normalization factor H is near zero.\")\n",
    "        norm_factor = torch.tensor(0.0, device=device)\n",
    "    else:\n",
    "        norm_factor = (torch.sqrt(1.0 / H) / (2.0 * cmath.pi)).to(device)\n",
    "\n",
    "    result = J_vector_tensor * norm_factor\n",
    "    if torch.isnan(result).any(): print(\"Warning: NaN in J_vector output.\")\n",
    "    return result, n1, n2, p, taper_grid\n",
    "\n",
    "\n",
    "def calculate_sample_periodogram_vectorized(J_vector_tensor):\n",
    "    \"\"\"Calculates sample periodogram I_n = J J^H (pxp matrix for each spatial freq).\"\"\"\n",
    "    if torch.isnan(J_vector_tensor).any() or torch.isinf(J_vector_tensor).any():\n",
    "        print(\"Warning: NaN/Inf detected in J_vector_tensor input.\")\n",
    "        n1, n2, p = J_vector_tensor.shape\n",
    "        return torch.full((n1, n2, p, p), float('nan'), dtype=torch.complex64, device=J_vector_tensor.device)\n",
    "\n",
    "    J_col = J_vector_tensor.unsqueeze(-1)\n",
    "    J_row_conj = J_vector_tensor.unsqueeze(-2).conj()\n",
    "    result = J_col @ J_row_conj\n",
    "\n",
    "    if torch.isnan(result).any(): print(\"Warning: NaN in periodogram matrix output.\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 4. Likelihood Calculation (Tapered)\n",
    "# =========================================================================\n",
    "\n",
    "def whittle_likelihood_loss_tapered(params, I_sample, n1, n2, p, taper_autocorr_grid, delta1, delta2):\n",
    "    \"\"\"\n",
    "    âœ… Whittle Likelihood Loss using data tapering and exact taper autocorrelation c_gn.\n",
    "    \"\"\"\n",
    "    device = I_sample.device\n",
    "    params_tensor = params.to(device)\n",
    "\n",
    "    if torch.isnan(params_tensor).any() or torch.isinf(params_tensor).any():\n",
    "        print(\"Warning: NaN/Inf detected in input parameters to likelihood.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    I_expected = expected_periodogram_fft_tapered(\n",
    "        params_tensor, n1, n2, p, taper_autocorr_grid, \n",
    "        delta1, delta2\n",
    "    )\n",
    "\n",
    "    if torch.isnan(I_expected).any() or torch.isinf(I_expected).any():\n",
    "        print(\"Warning: NaN/Inf returned from expected_periodogram calculation.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    eye_matrix = torch.eye(p, dtype=torch.complex64, device=device)\n",
    "    diag_vals = torch.abs(I_expected.diagonal(dim1=-2, dim2=-1))\n",
    "    mean_diag_abs = diag_vals.mean().item() if diag_vals.numel() > 0 and not torch.isnan(diag_vals).all() else 1.0\n",
    "    diag_load = max(mean_diag_abs * 1e-8, 1e-9)\n",
    "    I_expected_stable = I_expected + eye_matrix * diag_load\n",
    "\n",
    "    sign, logabsdet = torch.linalg.slogdet(I_expected_stable)\n",
    "    if torch.any(sign.real <= 1e-9):\n",
    "        print(\"Warning: Non-positive determinant encountered. Applying penalty.\")\n",
    "        log_det_term = torch.where(sign.real > 1e-9, logabsdet, torch.tensor(1e10, device=device))\n",
    "    else:\n",
    "        log_det_term = logabsdet\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Warning: NaN/Inf detected in I_sample input to likelihood.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    try:\n",
    "        solved_term = torch.linalg.solve(I_expected_stable, I_sample)\n",
    "        trace_term = torch.einsum('...ii->...', solved_term).real\n",
    "    except torch.linalg.LinAlgError as e:\n",
    "        print(f\"Warning: LinAlgError during solve: {e}. Applying high loss penalty.\")\n",
    "        return torch.tensor(float('inf'), device=device)\n",
    "\n",
    "    if torch.isnan(trace_term).any() or torch.isinf(trace_term).any():\n",
    "        print(\"Warning: NaN/Inf detected in trace_term. Returning NaN loss.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    likelihood_terms = log_det_term + trace_term\n",
    "\n",
    "    if torch.isnan(likelihood_terms).any():\n",
    "        print(\"Warning: NaN detected in likelihood_terms before summation. Returning NaN loss.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    total_sum = torch.sum(likelihood_terms)\n",
    "    dc_term = likelihood_terms[0, 0] if n1 > 0 and n2 > 0 else torch.tensor(0.0, device=device)\n",
    "    if torch.isnan(dc_term).any() or torch.isinf(dc_term).any():\n",
    "        print(\"Warning: NaN/Inf detected in DC term. Setting to 0.\")\n",
    "        dc_term = torch.tensor(0.0, device=device)\n",
    "\n",
    "    loss = total_sum - dc_term if (n1 > 1 or n2 > 1) else total_sum\n",
    "\n",
    "    if torch.isnan(loss) or torch.isinf(loss):\n",
    "         print(\"Warning: NaN/Inf detected in final loss. Returning Inf penalty.\")\n",
    "         return torch.tensor(float('inf'), device=device)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 5. Training Loop & Helpers (ðŸ’¥ REVISED ðŸ’¥)\n",
    "# =========================================================================\n",
    "\n",
    "def get_printable_params(p_list):\n",
    "    \"\"\"Helper to convert log-params to natural scale for printing.\"\"\"\n",
    "    valid_tensors = [p for p in p_list if isinstance(p, torch.Tensor)]\n",
    "    if not valid_tensors: return \"Invalid params_list\"\n",
    "    p_cat = torch.cat([p.detach().clone().cpu() for p in valid_tensors])\n",
    "    \n",
    "    if len(p_cat) != 4:\n",
    "        return f\"Expected 4 params, got {len(p_cat)}.\"\n",
    "\n",
    "    log_params = p_cat\n",
    "    if torch.isnan(log_params).any() or torch.isinf(log_params).any():\n",
    "        return \"[NaN/Inf in log_params]\"\n",
    "        \n",
    "    try:\n",
    "        phi1 = torch.exp(log_params[0])\n",
    "        phi2 = torch.exp(log_params[1]) # range_lon_inv\n",
    "        phi3 = torch.exp(log_params[2]) # (range_lon / range_lat)^2\n",
    "        nugget = torch.exp(log_params[3])\n",
    "\n",
    "        epsilon = 1e-12\n",
    "        sigmasq = phi1 / (phi2 + epsilon)\n",
    "        range_lon = 1.0 / (phi2 + epsilon)\n",
    "        range_lat = 1.0 / (torch.sqrt(phi3 + epsilon) * phi2 + epsilon)\n",
    "        \n",
    "        return (f\"sigmasq: {sigmasq.item():.4f}, range_lat: {range_lat.item():.4f}, \"\n",
    "                f\"range_lon: {range_lon.item():.4f}, nugget: {nugget.item():.4f}\")\n",
    "    except Exception as e:\n",
    "        return f\"[Error in param conversion: {e}]\"\n",
    "\n",
    "def get_phi_params(log_params_list):\n",
    "    \"\"\"Helper to print reparameterized (phi-scale) params.\"\"\"\n",
    "    try:\n",
    "        p_cat = torch.cat([p.detach().clone().cpu() for p in log_params_list])\n",
    "        phi1 = torch.exp(p_cat[0])\n",
    "        phi2 = torch.exp(p_cat[1])\n",
    "        phi3 = torch.exp(p_cat[2])\n",
    "        nugget = torch.exp(p_cat[3]) \n",
    "        \n",
    "        return (f\"phi1: {phi1.item():.4f}, phi2: {phi2.item():.4f}, \"\n",
    "                f\"phi3: {phi3.item():.4f}, nugget: {nugget.item():.4f}\")\n",
    "    except Exception:\n",
    "        return \"[Error in reparam conversion]\"\n",
    "\n",
    "# --- ðŸ’¥ NEW HELPER ðŸ’¥ ---\n",
    "def get_raw_log_params(log_params_list):\n",
    "    \"\"\"Helper to print the raw log-phi params being optimized.\"\"\"\n",
    "    try:\n",
    "        p_cat = torch.cat([p.detach().clone().cpu() for p in log_params_list])\n",
    "        return (f\"log_phi1: {p_cat[0].item():.4f}, log_phi2: {p_cat[1].item():.4f}, \"\n",
    "                f\"log_phi3: {p_cat[2].item():.4f}, log_nugget: {p_cat[3].item():.4f}\")\n",
    "    except Exception:\n",
    "        return \"[Error in raw param conversion]\"\n",
    "# --- END NEW HELPER ---\n",
    "\n",
    "\n",
    "def run_full_tapered(params_list, optimizer, scheduler, I_sample, n1, n2, p, taper_autocorr_grid, epochs=600, device='cpu'):\n",
    "    \"\"\"Corrected training loop with gradient-based convergence.\"\"\"\n",
    "    best_loss = float('inf')\n",
    "    params_list = [p.to(device) for p in params_list]\n",
    "    best_params_state = [p.detach().clone() for p in params_list]\n",
    "    epochs_completed = 0\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063 \n",
    "    \n",
    "    # --- ðŸ’¥ NEW: Convergence tolerance ðŸ’¥ ---\n",
    "    grad_tol = 1e-5\n",
    "\n",
    "    I_sample_dev = I_sample.to(device)\n",
    "    taper_autocorr_grid_dev = taper_autocorr_grid.to(device) \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epochs_completed = epoch + 1\n",
    "        optimizer.zero_grad()\n",
    "        params_tensor = torch.cat(params_list) \n",
    "\n",
    "        loss = whittle_likelihood_loss_tapered(\n",
    "            params_tensor, I_sample_dev, n1, n2, p, taper_autocorr_grid_dev, DELTA_LAT, DELTA_LON\n",
    "        )\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"Loss became NaN or Inf at epoch {epoch+1}. Stopping.\")\n",
    "            if epoch == 0: best_params_state = None\n",
    "            epochs_completed = epoch\n",
    "            break\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        nan_grad = False\n",
    "        max_abs_grad = 0.0\n",
    "        for param in params_list:\n",
    "            if param.grad is not None:\n",
    "                if (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
    "                    nan_grad = True\n",
    "                    break\n",
    "                # --- ðŸ’¥ NEW: Track max gradient ðŸ’¥ ---\n",
    "                max_abs_grad = max(max_abs_grad, param.grad.abs().item())\n",
    "            \n",
    "        if nan_grad:\n",
    "             print(f\"Warning: NaN/Inf gradient at epoch {epoch+1}. Skipping step.\")\n",
    "             optimizer.zero_grad()\n",
    "             continue\n",
    "\n",
    "        all_params_on_device = params_list\n",
    "        if all_params_on_device:\n",
    "            torch.nn.utils.clip_grad_norm_(all_params_on_device, max_norm=1.0)\n",
    "        \n",
    "        # --- ðŸ’¥ NEW: Convergence Check ðŸ’¥ ---\n",
    "        if epoch > 10 and max_abs_grad < grad_tol: # 10-epoch warmup\n",
    "            print(f\"\\n--- Converged on gradient norm (max|grad| < {grad_tol}) at epoch {epoch+1} ---\")\n",
    "            epochs_completed = epoch + 1\n",
    "            break # Exit the training loop\n",
    "        # --- END NEW ---\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        current_loss_item = loss.item()\n",
    "        if current_loss_item < best_loss:\n",
    "            params_valid = not any(torch.isnan(p.data).any() or torch.isinf(p.data).any() for p in params_list)\n",
    "            if params_valid:\n",
    "                best_loss = current_loss_item\n",
    "                best_params_state = [p.detach().clone() for p in params_list]\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr'] if optimizer.param_groups else 0.0\n",
    "\n",
    "        if epoch % 50 == 0 or epoch == epochs - 1:\n",
    "            print(f'--- Epoch {epoch+1}/{epochs} (LR: {current_lr:.6f}) ---')\n",
    "            print(f' Loss: {current_loss_item:.4f} | Max Grad: {max_abs_grad:.6e}')\n",
    "            print(f' Params (Raw Log Scale): {get_raw_log_params(params_list)}')\n",
    "            print(f' Params (Natural Scale): {get_printable_params(params_list)}')\n",
    "\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    if best_params_state is None:\n",
    "        print(\"Training failed to find a valid model state.\")\n",
    "        return None, None, None, None, epochs_completed # Return Nones\n",
    "\n",
    "    # Get final params in all 3 formats\n",
    "    final_natural_params_str = get_printable_params(best_params_state)\n",
    "    final_phi_params_str = get_phi_params(best_params_state)\n",
    "    final_raw_params_str = get_raw_log_params(best_params_state)\n",
    "    final_loss_rounded = round(best_loss, 3) if best_loss != float('inf') else float('inf')\n",
    "\n",
    "    print(f'\\nFINAL BEST STATE ACHIEVED (during training):')\n",
    "    print(f'Best Loss: {final_loss_rounded}')\n",
    "    print(f'Parameters (Natural Scale): {final_natural_params_str}')\n",
    "    print(f'Parameters (Phi Scale)    : {final_phi_params_str}')\n",
    "    print(f'Parameters (Raw Log Scale): {final_raw_params_str}')\n",
    "    \n",
    "    # Return strings for final printing\n",
    "    return final_natural_params_str, final_phi_params_str, final_raw_params_str, final_loss_rounded, epochs_completed\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 6. Main Execution Script (ðŸ’¥ REVISED ðŸ’¥)\n",
    "# =========================================================================\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    DAY_TO_RUN = 1\n",
    "    TAPERING_FUNC = cgn_hamming # Use Hamming taper\n",
    "    NUM_RUNS = 1\n",
    "    EPOCHS = 5000 \n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063 \n",
    "\n",
    "    LAT_COL, LON_COL = 0, 1\n",
    "    VAL_COL = 2 # Spatially differenced value\n",
    "    TIME_COL = 3\n",
    "\n",
    "    # --- Load Spatially Differenced Data ---\n",
    "    try:\n",
    "        with open(\"spatial_first_difference_data.pkl\", 'rb') as f:\n",
    "            processed_df = pickle.load(f)\n",
    "        print(f\"Loaded {len(processed_df)} days from spatial_first_difference_data.pkl.\")\n",
    "        processed_df = [\n",
    "            torch.tensor(arr, dtype=torch.float32).cpu() if not isinstance(arr, torch.Tensor)\n",
    "            else arr.cpu().to(torch.float32)\n",
    "            for arr in processed_df\n",
    "        ]\n",
    "        if not processed_df: raise ValueError(\"'processed_df' is empty.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: `spatial_first_difference_data.pkl` not found.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing 'processed_df': {e}\")\n",
    "        exit()\n",
    "\n",
    "    if DAY_TO_RUN > len(processed_df) or DAY_TO_RUN <= 0:\n",
    "        print(f\"Error: DAY_TO_RUN ({DAY_TO_RUN}) out of bounds.\")\n",
    "        exit()\n",
    "\n",
    "    cur_df = processed_df[DAY_TO_RUN - 1]\n",
    "    \n",
    "    cur_df = cur_df[cur_df[:,3] == cur_df[:,3].min()]\n",
    "    print(f\"Filtered data to single time point. New shape: {cur_df.shape}\")\n",
    "    \n",
    "    if cur_df.numel() == 0 or cur_df.shape[1] <= max(LAT_COL, LON_COL, VAL_COL, TIME_COL):\n",
    "        print(f\"Error: Data for Day {DAY_TO_RUN} (single time point) is empty or invalid.\")\n",
    "        exit()\n",
    "\n",
    "    time_slices_list = [cur_df] # List containing the single time slice\n",
    "    \n",
    "    # --- 1. Pre-compute J-vector, Taper Grid, and Taper Autocorrelation ---\n",
    "    print(\"Pre-computing J-vector (Hamming taper)...\")\n",
    "    J_vec, n1, n2, p, taper_grid = generate_Jvector_tapered( \n",
    "        time_slices_list,\n",
    "        tapering_func=TAPERING_FUNC, \n",
    "        lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if J_vec is None or J_vec.numel() == 0 or n1 == 0 or n2 == 0 or p == 0:\n",
    "       print(f\"Error: J-vector generation failed for Day {DAY_TO_RUN}.\")\n",
    "       exit()\n",
    "       \n",
    "    print(\"Pre-computing sample periodogram...\")\n",
    "    I_sample = calculate_sample_periodogram_vectorized(J_vec)\n",
    "\n",
    "    print(\"Pre-computing Hamming taper autocorrelation...\")\n",
    "    taper_autocorr_grid = calculate_taper_autocorrelation_fft(taper_grid, n1, n2, DEVICE)\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Error: NaN/Inf in sample periodogram.\")\n",
    "        exit()\n",
    "    if torch.isnan(taper_autocorr_grid).any() or torch.isinf(taper_autocorr_grid).any():\n",
    "        print(\"Error: NaN/Inf in taper autocorrelation.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Data grid: {n1}x{n2}, {p} time points. J-vector, Periodogram, Taper Autocorr on {DEVICE}.\")\n",
    "\n",
    "    # --- 2. Optimization Loop ---\n",
    "    all_final_results = []\n",
    "    all_final_losses = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        print(f\"\\n{'='*30} Initialization Run {i+1}/{NUM_RUNS} {'='*30}\")\n",
    "\n",
    "        # [log_phi1, log_phi2, log_phi3, log_nugget]\n",
    "        initial_params_values = [np.log(15.0), np.log(0.66), np.log(0.7), np.log(1.5)]\n",
    "        \n",
    "        print(f\"Starting with FIXED params (log-scale): {[round(p, 4) for p in initial_params_values]}\")\n",
    "\n",
    "        params_list = [\n",
    "            Parameter(torch.tensor([val], dtype=torch.float32))\n",
    "            for val in initial_params_values\n",
    "        ]\n",
    "\n",
    "        lr = 0.01 \n",
    "        optimizer = torch.optim.Adam(params_list, lr=lr)\n",
    "\n",
    "        T_MAX = 200\n",
    "        ETA_MIN = 1e-6\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=T_MAX, eta_min=ETA_MIN)\n",
    "\n",
    "        print(f\"Starting optimization run {i+1} on device {DEVICE} (Hamming, SpatialModel kernel)...\")\n",
    "        \n",
    "        # --- ðŸ’¥ REVISED: Unpack 5 return values ðŸ’¥ ---\n",
    "        nat_params_str, phi_params_str, raw_params_str, loss, epochs_run = run_full_tapered(\n",
    "            params_list=params_list,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            I_sample=I_sample,\n",
    "            n1=n1, n2=n2, p=p,\n",
    "            taper_autocorr_grid=taper_autocorr_grid, \n",
    "            epochs=EPOCHS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        \n",
    "        if loss is not None:\n",
    "            all_final_results.append((nat_params_str, phi_params_str, raw_params_str))\n",
    "            all_final_losses.append(loss)\n",
    "        else:\n",
    "            all_final_losses.append(float('inf'))\n",
    "\n",
    "    print(f\"\\n\\g{'='*25} Overall Result from Run {'='*25}\")\n",
    "    valid_losses = [l for l in all_final_losses if l is not None and l != float('inf')]\n",
    "\n",
    "    if not valid_losses:\n",
    "        print(f\"The run failed or resulted in an invalid loss for Day {DAY_TO_RUN}.\")\n",
    "    else:\n",
    "        best_loss = min(valid_losses)\n",
    "        best_run_index = all_final_losses.index(best_loss)\n",
    "        best_results = all_final_results[best_run_index]\n",
    "        \n",
    "        # --- ðŸ’¥ REVISED: Print all 3 param scales ðŸ’¥ ---\n",
    "        print(f\"Best Run Loss: {best_loss}\")\n",
    "        print(f\"Final Parameters (Natural Scale): {best_results[0]}\")\n",
    "        print(f\"Final Parameters (Phi Scale)    : {best_results[1]}\")\n",
    "        print(f\"Final Parameters (Raw Log Scale): {best_results[2]}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad9ad1f",
   "metadata": {},
   "source": [
    "# Compute NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56f26171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import cmath\n",
    "import pickle\n",
    "import time \n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import Parameter\n",
    "import torch.fft \n",
    "from typing import List, Dict, Any, Callable\n",
    "\n",
    "# =========================================================================\n",
    "# ALL HELPER FUNCTIONS FROM YOUR REFERENCE CODE\n",
    "# (Copied verbatim)\n",
    "# =========================================================================\n",
    "\n",
    "def cgn_hamming(u, n1, n2):\n",
    "    \"\"\"Computes a 2D Hamming window.\"\"\"\n",
    "    u1, u2 = u\n",
    "    device = u1.device if isinstance(u1, torch.Tensor) else (u2.device if isinstance(u2, torch.Tensor) else torch.device('cpu'))\n",
    "    u1_tensor = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_tensor = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "\n",
    "    n1_eff = float(n1) if n1 > 0 else 1.0\n",
    "    n2_eff = float(n2) if n2 > 0 else 1.0\n",
    "    hamming1 = 0.54 + 0.46 * torch.cos(2.0 * torch.pi * u1_tensor / n1_eff)\n",
    "    hamming2 = 0.54 + 0.46 * torch.cos(2.0 * torch.pi * u2_tensor / n2_eff)\n",
    "    return hamming1 * hamming2\n",
    "\n",
    "def cgn_2dbartlett(u, n1, n2): \n",
    "    \"\"\"Computes a 2D Bartlett window function.\"\"\"\n",
    "    u1, u2 = u\n",
    "    device = u1.device if isinstance(u1, torch.Tensor) else (u2.device if isinstance(u2, torch.Tensor) else torch.device('cpu'))\n",
    "    u1_tensor = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_tensor = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    n1_eff = float(n1) if n1 > 0 else 1.0\n",
    "    n2_eff = float(n2) if n2 > 0 else 1.0\n",
    "    kernel = (1.0 - torch.abs(u1_tensor) / n1_eff) * (1.0 - torch.abs(u2_tensor) / n2_eff)\n",
    "    return torch.clamp(kernel, min=0.0)\n",
    "\n",
    "def calculate_taper_autocorrelation_fft(taper_grid, n1, n2, device):\n",
    "    \"\"\"\n",
    "    Computes the normalized taper autocorrelation function c_gn(u) using FFT.\n",
    "    \"\"\"\n",
    "    taper_grid = taper_grid.to(device) \n",
    "    H = torch.sum(taper_grid**2)\n",
    "    if H < 1e-12:\n",
    "        print(\"Warning: Sum of squared taper weights (H) is near zero.\")\n",
    "        return torch.zeros((2*n1-1, 2*n2-1), device=device, dtype=taper_grid.dtype)\n",
    "\n",
    "    N1, N2 = 2 * n1 - 1, 2 * n2 - 1\n",
    "    taper_fft = torch.fft.fft2(taper_grid, s=(N1, N2))\n",
    "    power_spectrum = torch.abs(taper_fft)**2\n",
    "    autocorr_unnormalized = torch.fft.ifft2(power_spectrum).real\n",
    "    autocorr_shifted = torch.fft.fftshift(autocorr_unnormalized)\n",
    "\n",
    "    c_gn_grid = autocorr_shifted / (H + 1e-12)\n",
    "\n",
    "    return c_gn_grid \n",
    "\n",
    "def cov_x_spatial_model_kernel(u1, u2, t, params):\n",
    "    \"\"\"\n",
    "    Computes autocovariance of X using the SPATIAL-ONLY kernel.\n",
    "    u1, u2 are PHYSICAL lags (already scaled by deltas).\n",
    "    \"\"\"\n",
    "    device = params.device\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    if torch.isnan(params).any() or torch.isinf(params).any():\n",
    "         out_shape = torch.broadcast_shapes(u1_dev.shape, u2_dev.shape, t_dev.shape)\n",
    "         return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "\n",
    "    phi1   = torch.exp(params[0])\n",
    "    phi2   = torch.exp(params[1]) \n",
    "    phi3   = torch.exp(params[2]) \n",
    "    nugget = torch.exp(params[3])\n",
    "\n",
    "    epsilon = 1e-12\n",
    "    sigmasq = phi1 / (phi2 + epsilon)  \n",
    "    range_lon_inv = phi2\n",
    "    range_lat_inv = torch.sqrt(phi3 + epsilon) * phi2\n",
    "\n",
    "    dist_sq = (u1_dev * range_lat_inv).pow(2) + (u2_dev * range_lon_inv).pow(2)\n",
    "    distance = torch.sqrt(dist_sq + epsilon) \n",
    "\n",
    "    cov_smooth = sigmasq * torch.exp(-distance)\n",
    "\n",
    "    is_zero_lag = (torch.abs(u1_dev) < 1e-9) & (torch.abs(u2_dev) < 1e-9) & (torch.abs(t_dev) < 1e-9)\n",
    "    final_cov = torch.where(is_zero_lag, cov_smooth + nugget, cov_smooth)\n",
    "\n",
    "    if torch.isnan(final_cov).any(): print(\"Warning: NaN detected in cov_x_spatial_model_kernel output.\")\n",
    "    return final_cov\n",
    "\n",
    "\n",
    "def cov_spatial_difference(u1, u2, t, params, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Calculates covariance Cov(Y(s), Y(s+u)) for Y(s) = X(s+d1) + X(s+d2) - 2X(s)\n",
    "    Based on the underlying spatial kernel.\n",
    "    u1, u2 are PHYSICAL lags.\n",
    "    \"\"\"\n",
    "    weights = {(0, 0): -2.0, (1, 0): 1.0, (0, 1): 1.0}\n",
    "    device = params.device\n",
    "    out_shape = torch.broadcast_shapes(u1.shape if isinstance(u1, torch.Tensor) else (),\n",
    "                                     u2.shape if isinstance(u2, torch.Tensor) else (),\n",
    "                                     t.shape if isinstance(t, torch.Tensor) else ())\n",
    "    cov = torch.zeros(out_shape, device=device, dtype=torch.float32)\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    for (a_idx, b_idx), w_ab in weights.items():\n",
    "        offset_a1 = a_idx * delta1\n",
    "        offset_a2 = b_idx * delta2\n",
    "        for (c_idx, d_idx), w_cd in weights.items():\n",
    "            offset_c1 = c_idx * delta1\n",
    "            offset_c2 = d_idx * delta2\n",
    "            \n",
    "            lag_u1 = u1_dev + (offset_a1 - offset_c1)\n",
    "            lag_u2 = u2_dev + (offset_a2 - offset_c2)\n",
    "            \n",
    "            term_cov = cov_x_spatial_model_kernel(lag_u1, lag_u2, t_dev, params) \n",
    "            \n",
    "            if torch.isnan(term_cov).any():\n",
    "                 print(f\"Warning: NaN in term_cov within cov_spatial_difference.\")\n",
    "                 return torch.full_like(cov, float('nan'))\n",
    "            cov += w_ab * w_cd * term_cov\n",
    "\n",
    "    if torch.isnan(cov).any(): print(\"Warning: NaN in final cov_spatial_difference output.\")\n",
    "    return cov\n",
    "\n",
    "def cn_bar_tapered(u1, u2, t, params, n1, n2, taper_autocorr_grid, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Computes c_Y(u) * c_gn(u).\n",
    "    u1, u2 are GRID index lags (e.g., -n1..0..n1)\n",
    "    \"\"\"\n",
    "    device = params.device\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    # --- Convert GRID lags to PHYSICAL lags ---\n",
    "    lag_u1 = u1_dev * delta1\n",
    "    lag_u2 = u2_dev * delta2\n",
    "    \n",
    "    cov_X_value = cov_spatial_difference(lag_u1, lag_u2, t_dev, params, delta1, delta2)\n",
    "\n",
    "    # --- Get Taper Autocorrelation Value c_gn(u) from grid ---\n",
    "    u1_idx = u1_dev.long()\n",
    "    u2_idx = u2_dev.long()\n",
    "\n",
    "    idx1 = (n1 - 1 + u1_idx) # Centering index\n",
    "    idx2 = (n2 - 1 + u2_idx) # Centering index\n",
    "    \n",
    "    idx1 = torch.clamp(idx1, 0, 2 * n1 - 2)\n",
    "    idx2 = torch.clamp(idx2, 0, 2 * n2 - 2)\n",
    "\n",
    "    taper_autocorr_value = taper_autocorr_grid[idx1, idx2]\n",
    "\n",
    "    if torch.isnan(cov_X_value).any() or torch.isnan(taper_autocorr_value).any():\n",
    "        out_shape = torch.broadcast_shapes(cov_X_value.shape, taper_autocorr_value.shape)\n",
    "        return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "\n",
    "    result = cov_X_value * taper_autocorr_value\n",
    "    if torch.isnan(result).any(): print(\"Warning: NaN in cn_bar_tapered output.\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def expected_periodogram_fft_tapered(params, n1, n2, p, taper_autocorr_grid, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Calculates the expected periodogram I(omega_s) (a pxp matrix in time)\n",
    "    using the exact taper autocorrelation c_gn(u) and\n",
    "    CORRECTLY implementing the aliasing sum (Lemma 2).\n",
    "    \"\"\"\n",
    "    device = params.device if isinstance(params, torch.Tensor) else params[0].device\n",
    "    if isinstance(params, list):\n",
    "        params_tensor = torch.cat([p.to(device) for p in params])\n",
    "    else:\n",
    "        params_tensor = params.to(device)\n",
    "\n",
    "    # This function uses positive grid index lags [0, n-1] for the sum\n",
    "    u1_lags = torch.arange(n1, dtype=torch.float32, device=device)\n",
    "    u2_lags = torch.arange(n2, dtype=torch.float32, device=device)\n",
    "    u1_mesh, u2_mesh = torch.meshgrid(u1_lags, u2_lags, indexing='ij')\n",
    "\n",
    "    t_lags = torch.arange(p, dtype=torch.float32, device=device)\n",
    "    tilde_cn_tensor = torch.zeros((n1, n2, p, p), dtype=torch.complex64, device=device)\n",
    "\n",
    "    for q in range(p):\n",
    "        for r in range(p):\n",
    "            t_diff = t_lags[q] - t_lags[r]\n",
    "            \n",
    "            # These are the four terms for the aliasing sum\n",
    "            term1 = cn_bar_tapered(u1_mesh, u2_mesh, t_diff, \n",
    "                                   params_tensor, n1, n2, taper_autocorr_grid, delta1, delta2)\n",
    "            term2 = cn_bar_tapered(u1_mesh - n1, u2_mesh, t_diff, \n",
    "                                   params_tensor, n1, n2, taper_autocorr_grid, delta1, delta2)\n",
    "            term3 = cn_bar_tapered(u1_mesh, u2_mesh - n2, t_diff, \n",
    "                                   params_tensor, n1, n2, taper_autocorr_grid, delta1, delta2)\n",
    "            term4 = cn_bar_tapered(u1_mesh - n1, u2_mesh - n2, t_diff,\n",
    "                                   params_tensor, n1, n2, taper_autocorr_grid, delta1, delta2)\n",
    "            \n",
    "            tilde_cn_grid_qr = (term1 + term2 + term3 + term4)\n",
    "            \n",
    "            if torch.isnan(tilde_cn_grid_qr).any():\n",
    "                 tilde_cn_tensor[:, :, q, r] = float('nan')\n",
    "            else:\n",
    "                 tilde_cn_tensor[:, :, q, r] = tilde_cn_grid_qr.to(torch.complex64)\n",
    "\n",
    "    if torch.isnan(tilde_cn_tensor).any():\n",
    "        print(\"Warning: NaN detected in tilde_cn_tensor before FFT.\")\n",
    "        nan_shape = (n1, n2, p, p)\n",
    "        return torch.full(nan_shape, float('nan'), dtype=torch.complex64, device=device)\n",
    "\n",
    "    fft_result = torch.fft.fft2(tilde_cn_tensor, dim=(0, 1))\n",
    "    fft_result_real = fft_result.real \n",
    "    normalization_factor = 1.0 / (4.0 * cmath.pi**2)\n",
    "    result = fft_result_real * normalization_factor\n",
    "\n",
    "    if torch.isnan(result).any(): print(\"Warning: NaN in expected_periodogram_fft_tapered output.\")\n",
    "    return result\n",
    "\n",
    "def generate_Jvector_tapered(tensor_list, tapering_func, lat_col, lon_col, val_col, device):\n",
    "    \"\"\"\n",
    "    Generates J-vector for a single component using the specified taper,\n",
    "    placing result on device.\n",
    "    \"\"\"\n",
    "    p = len(tensor_list)\n",
    "    if p == 0: return torch.empty(0, 0, 0, device=device), 0, 0, 0, None \n",
    "\n",
    "    valid_tensors = [t for t in tensor_list if t.numel() > 0 and t.shape[1] > max(lat_col, lon_col, val_col)]\n",
    "    if not valid_tensors:\n",
    "         print(\"Warning: No valid tensors found in tensor_list.\")\n",
    "         return torch.empty(0, 0, 0, device=device), 0, 0, 0, None\n",
    "\n",
    "    try:\n",
    "        all_lats_cpu = torch.cat([t[:, lat_col] for t in valid_tensors])\n",
    "        all_lons_cpu = torch.cat([t[:, lon_col] for t in valid_tensors])\n",
    "    except IndexError:\n",
    "        print(f\"Error: Invalid column index. Check tensor shapes.\")\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0, None\n",
    "\n",
    "    all_lats_cpu = all_lats_cpu[~torch.isnan(all_lats_cpu) & ~torch.isinf(all_lats_cpu)]\n",
    "    all_lons_cpu = all_lons_cpu[~torch.isnan(all_lons_cpu) & ~torch.isinf(all_lons_cpu)]\n",
    "    if all_lats_cpu.numel() == 0 or all_lons_cpu.numel() == 0:\n",
    "        print(\"Warning: No valid coordinates after NaN/Inf filtering.\")\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0, None\n",
    "\n",
    "    unique_lats_cpu, unique_lons_cpu = torch.unique(all_lats_cpu), torch.unique(all_lons_cpu)\n",
    "    n1, n2 = len(unique_lats_cpu), len(unique_lons_cpu)\n",
    "    if n1 == 0 or n2 == 0:\n",
    "        print(\"Warning: Grid dimensions are zero.\")\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0, None\n",
    "\n",
    "    lat_map = {lat.item(): i for i, lat in enumerate(unique_lats_cpu)}\n",
    "    lon_map = {lon.item(): i for i, lon in enumerate(unique_lons_cpu)}\n",
    "\n",
    "    u1_mesh_cpu, u2_mesh_cpu = torch.meshgrid(\n",
    "        torch.arange(n1, dtype=torch.float32),\n",
    "        torch.arange(n2, dtype=torch.float32),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    taper_grid = tapering_func((u1_mesh_cpu, u2_mesh_cpu), n1, n2).to(device) # Taper on device\n",
    "\n",
    "    fft_results = []\n",
    "    for tensor in tensor_list:\n",
    "        data_grid = torch.zeros((n1, n2), dtype=torch.float32, device=device)\n",
    "        if tensor.numel() > 0 and tensor.shape[1] > max(lat_col, lon_col, val_col):\n",
    "            for row in tensor:\n",
    "                lat_item, lon_item = row[lat_col].item(), row[lon_col].item()\n",
    "                if not (np.isnan(lat_item) or np.isnan(lon_item)):\n",
    "                    i = lat_map.get(lat_item)\n",
    "                    j = lon_map.get(lon_item)\n",
    "                    if i is not None and j is not None:\n",
    "                        val = row[val_col]\n",
    "                        val_num = val.item() if isinstance(val, torch.Tensor) else val\n",
    "                        if not np.isnan(val_num) and not np.isinf(val_num):\n",
    "                           data_grid[i, j] = val_num\n",
    "\n",
    "        data_grid_tapered = data_grid * taper_grid \n",
    "\n",
    "        if torch.isnan(data_grid_tapered).any() or torch.isinf(data_grid_tapered).any():\n",
    "             print(\"Warning: NaN/Inf detected in data_grid_tapered before FFT. Replacing with zeros.\")\n",
    "             data_grid_tapered = torch.nan_to_num(data_grid_tapered, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        fft_results.append(torch.fft.fft2(data_grid_tapered))\n",
    "\n",
    "    if not fft_results:\n",
    "         print(\"Warning: No FFT results generated.\")\n",
    "         return torch.empty(0, 0, 0, device=device), n1, n2, 0, taper_grid\n",
    "\n",
    "    J_vector_tensor = torch.stack(fft_results, dim=2).to(device)\n",
    "\n",
    "    H = torch.sum(taper_grid**2)\n",
    "    if H < 1e-12:\n",
    "        print(\"Warning: Normalization factor H is near zero.\")\n",
    "        norm_factor = torch.tensor(0.0, device=device)\n",
    "    else:\n",
    "        norm_factor = (torch.sqrt(1.0 / H) / (2.0 * cmath.pi)).to(device)\n",
    "\n",
    "    result = J_vector_tensor * norm_factor\n",
    "    if torch.isnan(result).any(): print(\"Warning: NaN in J_vector output.\")\n",
    "    return result, n1, n2, p, taper_grid\n",
    "\n",
    "\n",
    "def calculate_sample_periodogram_vectorized(J_vector_tensor):\n",
    "    \"\"\"Calculates sample periodogram I_n = J J^H (pxp matrix for each spatial freq).\"\"\"\n",
    "    if torch.isnan(J_vector_tensor).any() or torch.isinf(J_vector_tensor).any():\n",
    "        print(\"Warning: NaN/Inf detected in J_vector_tensor input.\")\n",
    "        n1, n2, p = J_vector_tensor.shape\n",
    "        return torch.full((n1, n2, p, p), float('nan'), dtype=torch.complex64, device=J_vector_tensor.device)\n",
    "\n",
    "    J_col = J_vector_tensor.unsqueeze(-1)\n",
    "    J_row_conj = J_vector_tensor.unsqueeze(-2).conj()\n",
    "    result = J_col @ J_row_conj\n",
    "\n",
    "    if torch.isnan(result).any(): print(\"Warning: NaN in periodogram matrix output.\")\n",
    "    return result\n",
    "\n",
    "def whittle_likelihood_loss_tapered(params, I_sample, n1, n2, p, taper_autocorr_grid, delta1, delta2):\n",
    "    \"\"\"\n",
    "    âœ… Whittle Likelihood Loss using data tapering and exact taper autocorrelation c_gn.\n",
    "    \"\"\"\n",
    "    device = I_sample.device\n",
    "    params_tensor = params.to(device)\n",
    "\n",
    "    if torch.isnan(params_tensor).any() or torch.isinf(params_tensor).any():\n",
    "        print(\"Warning: NaN/Inf detected in input parameters to likelihood.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    I_expected = expected_periodogram_fft_tapered(\n",
    "        params_tensor, n1, n2, p, taper_autocorr_grid, \n",
    "        delta1, delta2\n",
    "    )\n",
    "\n",
    "    if torch.isnan(I_expected).any() or torch.isinf(I_expected).any():\n",
    "        print(\"Warning: NaN/Inf returned from expected_periodogram calculation.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    eye_matrix = torch.eye(p, dtype=torch.complex64, device=device)\n",
    "    diag_vals = torch.abs(I_expected.diagonal(dim1=-2, dim2=-1))\n",
    "    mean_diag_abs = diag_vals.mean().item() if diag_vals.numel() > 0 and not torch.isnan(diag_vals).all() else 1.0\n",
    "    diag_load = max(mean_diag_abs * 1e-8, 1e-9)\n",
    "    I_expected_stable = I_expected + eye_matrix * diag_load\n",
    "\n",
    "    sign, logabsdet = torch.linalg.slogdet(I_expected_stable)\n",
    "    if torch.any(sign.real <= 1e-9):\n",
    "        print(\"Warning: Non-positive determinant encountered. Applying penalty.\")\n",
    "        log_det_term = torch.where(sign.real > 1e-9, logabsdet, torch.tensor(1e10, device=device))\n",
    "    else:\n",
    "        log_det_term = logabsdet\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Warning: NaN/Inf detected in I_sample input to likelihood.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    try:\n",
    "        solved_term = torch.linalg.solve(I_expected_stable, I_sample)\n",
    "        trace_term = torch.einsum('...ii->...', solved_term).real\n",
    "    except torch.linalg.LinAlgError as e:\n",
    "        print(f\"Warning: LinAlgError during solve: {e}. Applying high loss penalty.\")\n",
    "        return torch.tensor(float('inf'), device=device)\n",
    "\n",
    "    if torch.isnan(trace_term).any() or torch.isinf(trace_term).any():\n",
    "        print(\"Warning: NaN/Inf detected in trace_term. Returning NaN loss.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    likelihood_terms = log_det_term + trace_term\n",
    "\n",
    "    if torch.isnan(likelihood_terms).any():\n",
    "        print(\"Warning: NaN detected in likelihood_terms before summation. Returning NaN loss.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    total_sum = torch.sum(likelihood_terms)\n",
    "    dc_term = likelihood_terms[0, 0] if n1 > 0 and n2 > 0 else torch.tensor(0.0, device=device)\n",
    "    if torch.isnan(dc_term).any() or torch.isinf(dc_term).any():\n",
    "        print(\"Warning: NaN/Inf detected in DC term. Setting to 0.\")\n",
    "        dc_term = torch.tensor(0.0, device=device)\n",
    "\n",
    "    loss = total_sum - dc_term if (n1 > 1 or n2 > 1) else total_sum\n",
    "\n",
    "    if torch.isnan(loss) or torch.isinf(loss):\n",
    "         print(\"Warning: NaN/Inf detected in final loss. Returning Inf penalty.\")\n",
    "         return torch.tensor(float('inf'), device=device)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# =========================================================================\n",
    "# 6. NEW NLL CALCULATOR CLASS\n",
    "# =========================================================================\n",
    "\n",
    "class WhittleNLLCalculator:\n",
    "    \"\"\"\n",
    "    A class to pre-load data and compute the Whittle NLL for given parameters.\n",
    "    \n",
    "    This class assumes the 4-parameter SPATIAL model from the reference code.\n",
    "    \"\"\"\n",
    "    def __init__(self, pickle_file_path, day_to_run, device='cpu'):\n",
    "        print(\"Initializing WhittleNLLCalculator...\")\n",
    "        self.device = torch.device(device)\n",
    "        self.DELTA_LAT, self.DELTA_LON = 0.044, 0.063\n",
    "        self.tapering_func = cgn_hamming\n",
    "        self.LAT_COL, self.LON_COL = 0, 1\n",
    "        self.VAL_COL = 2 \n",
    "        self.TIME_COL = 3\n",
    "        \n",
    "        # --- 1. Load and Process Data ---\n",
    "        print(f\"Loading data for Day {day_to_run} from {pickle_file_path}...\")\n",
    "        try:\n",
    "            with open(pickle_file_path, 'rb') as f:\n",
    "                processed_df = pickle.load(f)\n",
    "            processed_df = [\n",
    "                torch.tensor(arr, dtype=torch.float32).cpu() if not isinstance(arr, torch.Tensor)\n",
    "                else arr.cpu().to(torch.float32)\n",
    "                for arr in processed_df\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading pickle file: {e}\")\n",
    "            raise\n",
    "            \n",
    "        cur_df = processed_df[day_to_run - 1]\n",
    "        \n",
    "        # Filter to a single time point (p=1) as in the reference\n",
    "        cur_df = cur_df[cur_df[:,3] == cur_df[:,3].min()]\n",
    "        print(f\"Filtered data to single time point. Shape: {cur_df.shape}\")\n",
    "        \n",
    "        if cur_df.numel() == 0:\n",
    "            print(\"Error: No data found for this day and time point.\")\n",
    "            raise ValueError(\"No data found\")\n",
    "            \n",
    "        time_slices_list = [cur_df]\n",
    "        \n",
    "        # --- 2. Pre-compute J-vector and Taper Autocorrelation ---\n",
    "        print(\"Pre-computing J-vector, Periodogram, and Taper Autocorrelation...\")\n",
    "        J_vec, n1, n2, p, taper_grid = generate_Jvector_tapered( \n",
    "            time_slices_list,\n",
    "            tapering_func=self.tapering_func, \n",
    "            lat_col=self.LAT_COL, lon_col=self.LON_COL, \n",
    "            val_col=self.VAL_COL,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        if J_vec is None or J_vec.numel() == 0:\n",
    "           print(f\"Error: J-vector generation failed.\")\n",
    "           raise RuntimeError(\"J-vector generation failed\")\n",
    "\n",
    "        self.n1, self.n2, self.p = n1, n2, p\n",
    "        \n",
    "        # Store the pre-computed data-dependent objects\n",
    "        self.I_sample = calculate_sample_periodogram_vectorized(J_vec)\n",
    "        self.taper_autocorr_grid = calculate_taper_autocorrelation_fft(taper_grid, n1, n2, self.device)\n",
    "\n",
    "        if torch.isnan(self.I_sample).any():\n",
    "            raise ValueError(\"NaN detected in sample periodogram (I_sample).\")\n",
    "        if torch.isnan(self.taper_autocorr_grid).any():\n",
    "            raise ValueError(\"NaN detected in taper autocorrelation grid.\")\n",
    "\n",
    "        print(f\"Initialization complete. Grid: {n1}x{n2}, Time: {p}.\")\n",
    "\n",
    "    def calculate_nll(self, log_phi1: float, log_phi2: float, log_phi3: float, log_nugget: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the debiased Whittle NLL for a given set of parameters.\n",
    "        \n",
    "        Args:\n",
    "            log_phi1 (float): Raw log(sigmasq / range_lon)\n",
    "            log_phi2 (float): Raw log(1 / range_lon)\n",
    "            log_phi3 (float): Raw log((range_lon / range_lat)^2)\n",
    "            log_nugget (float): Raw log(nugget)\n",
    "            \n",
    "        Returns:\n",
    "            float: The computed Negative Log-Likelihood (NLL).\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Convert Python floats to a single Torch tensor\n",
    "        params_tensor = torch.tensor(\n",
    "            [log_phi1, log_phi2, log_phi3, log_nugget], \n",
    "            dtype=torch.float32, \n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # 2. Call the NLL loss function from your reference code\n",
    "        loss = whittle_likelihood_loss_tapered(\n",
    "            params=params_tensor, \n",
    "            I_sample=self.I_sample, \n",
    "            n1=self.n1, \n",
    "            n2=self.n2, \n",
    "            p=self.p, \n",
    "            taper_autocorr_grid=self.taper_autocorr_grid, \n",
    "            delta1=self.DELTA_LAT, \n",
    "            delta2=self.DELTA_LON\n",
    "        )\n",
    "        \n",
    "        # 3. Return the NLL as a simple Python float\n",
    "        return loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d49d644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing WhittleNLLCalculator...\n",
      "Loading data for Day 1 from spatial_first_difference_data.pkl...\n",
      "Filtered data to single time point. Shape: torch.Size([17854, 4])\n",
      "Pre-computing J-vector, Periodogram, and Taper Autocorrelation...\n",
      "Initialization complete. Grid: 113x158, Time: 1.\n",
      "\n",
      "--- Calculating NLL at test parameters ---\n",
      "\n",
      "FINAL NLL VALUE: 71326.2890625\n",
      "\n",
      "--- Calculating NLL at another point ---\n",
      "FINAL NLL VALUE (2): 19477.99609375\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# 7. EXAMPLE USAGE\n",
    "# =========================================================================\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # --- This is an example of how to use the new function ---\n",
    "    \n",
    "    # 1. SETTINGS\n",
    "    PICKLE_PATH = \"spatial_first_difference_data.pkl\"\n",
    "    DAY = 1\n",
    "    \n",
    "    # Set the 4 parameters you want to test\n",
    "    # (Using the initial values from your reference code as an example)\n",
    "    log_phi1_test = np.log(3.7371)\n",
    "    log_phi2_test = np.log(1.3191)\n",
    "    log_phi3_test = np.log(0.8742)\n",
    "    log_nugget_test = np.log(0.4129)\n",
    "\n",
    "    try:\n",
    "        # 2. Initialize the calculator ONCE. \n",
    "        # This loads data and does all pre-computation.\n",
    "        calculator = WhittleNLLCalculator(\n",
    "            pickle_file_path=PICKLE_PATH, \n",
    "            day_to_run=DAY,\n",
    "            device='cpu' # Use 'cuda' if available\n",
    "        )\n",
    "        \n",
    "        # 3. Call the .calculate_nll() method with your parameters.\n",
    "        # This part is fast and can be called many times.\n",
    "        print(\"\\n--- Calculating NLL at test parameters ---\")\n",
    "        nll_value = calculator.calculate_nll(\n",
    "            log_phi1=log_phi1_test,\n",
    "            log_phi2=log_phi2_test,\n",
    "            log_phi3=log_phi3_test,\n",
    "            log_nugget=log_nugget_test\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nFINAL NLL VALUE: {nll_value}\")\n",
    "\n",
    "        # You can now test another point quickly:\n",
    "        print(\"\\n--- Calculating NLL at another point ---\")\n",
    "        nll_value_2 = calculator.calculate_nll(\n",
    "            log_phi1=np.log(20.0), # Changed sigmasq\n",
    "            log_phi2=log_phi2_test,\n",
    "            log_phi3=log_phi3_test,\n",
    "            log_nugget=log_nugget_test\n",
    "        )\n",
    "        print(f\"FINAL NLL VALUE (2): {nll_value_2}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nERROR: Could not find the data file '{PICKLE_PATH}'.\")\n",
    "        print(\"Please make sure the file is in the same directory.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082c2542",
   "metadata": {},
   "source": [
    "# simulation study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2465a336",
   "metadata": {},
   "source": [
    "hamming tapering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9fb0a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Grid: 25x25 (625 points).\n",
      "\n",
      "--- True Parameters (Natural Scale) ---\n",
      "  sigmasq   : 30.0000\n",
      "  range_lat : 1.0000\n",
      "  range_lon : 1.5000\n",
      "  nugget    : 0.1000\n",
      "Generating spatial data (25 lats x 25 lons)...\n",
      "Building N x N covariance matrix (N=625)...\n",
      "Performing Cholesky decomposition...\n",
      "Data generated.\n",
      "\n",
      "--- Initial Parameters for Fitting ---\n",
      "{'sigmasq': 25.0, 'range_lat': 0.8, 'range_lon': 1.2, 'nugget': 0.2}\n",
      "\n",
      "--- Starting Full MLE Optimization (No Tapering) ---\n",
      "Epoch 1/200 | Loss: 2.3033 | LR: 0.009999\n",
      "  Params: sigmasq: 24.5050, range_lat: 0.7881, range_lon: 1.1881, nugget: 0.1980\n",
      "Epoch 21/200 | Loss: 2.3029 | LR: 0.009730\n",
      "Epoch 41/200 | Loss: 2.3028 | LR: 0.008999\n",
      "Epoch 61/200 | Loss: 2.3028 | LR: 0.007875\n",
      "Epoch 81/200 | Loss: 2.3028 | LR: 0.006471\n",
      "Epoch 101/200 | Loss: 2.3028 | LR: 0.004922\n",
      "  Params: sigmasq: 21.7264, range_lat: 0.7025, range_lon: 1.0993, nugget: 0.2689\n",
      "Epoch 121/200 | Loss: 2.3028 | LR: 0.003381\n",
      "Epoch 141/200 | Loss: 2.3028 | LR: 0.001999\n",
      "Epoch 161/200 | Loss: 2.3028 | LR: 0.000910\n",
      "Epoch 181/200 | Loss: 2.3028 | LR: 0.000222\n",
      "Epoch 200/200 | Loss: 2.3028 | LR: 0.000001\n",
      "  Params: sigmasq: 21.7293, range_lat: 0.7038, range_lon: 1.1024, nugget: 0.2772\n",
      "\n",
      "--- Optimization Complete ---\n",
      "\n",
      "\n",
      "========================= FINAL SIMULATION RESULTS =========================\n",
      "Grid Size: 25 lats x 25 lons (625 points)\n",
      "\n",
      "--- True Parameters (Natural Scale) ---\n",
      "  sigmasq   : 30.0000\n",
      "  range_lat : 1.0000\n",
      "  range_lon : 1.5000\n",
      "  nugget    : 0.1000\n",
      "\n",
      "--- Estimated Parameters (Natural Scale) ---\n",
      "  Final Loss: 2.3028\n",
      "  Params: sigmasq: 21.7288, range_lat: 0.7036, range_lon: 1.1018, nugget: 0.2757\n",
      "\n",
      "--- True Reparameterized Parameters (Phi Scale) ---\n",
      "  Params: phi1: 20.0000, phi2: 0.6667, phi3: 2.2500, nugget: 0.1000\n",
      "\n",
      "--- Estimated Reparameterized Parameters (Phi Scale) ---\n",
      "  Params: phi1: 19.7210, phi2: 0.9076, phi3: 2.4523, nugget: 0.2757\n",
      "\n",
      "Total execution time: 0.93 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import cmath\n",
    "import pickle\n",
    "import time \n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import Parameter\n",
    "import torch.fft \n",
    "from typing import List, Dict, Any, Callable\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# =========================================================================\n",
    "# 1. Data Generation and Model Functions\n",
    "# =========================================================================\n",
    "\n",
    "def convert_to_log_reparam(sigmasq, range_lat, range_lon, nugget):\n",
    "    \"\"\"\n",
    "    Converts physical parameters to the 4-parameter log-reparameterization.\n",
    "    \"\"\"\n",
    "    device = sigmasq.device\n",
    "    epsilon = 1e-12\n",
    "    \n",
    "    range_lon_inv = 1.0 / (range_lon + epsilon)\n",
    "    log_phi2 = torch.log(range_lon_inv)\n",
    "    log_phi1 = torch.log(sigmasq * range_lon_inv + epsilon)\n",
    "    log_phi3 = torch.log((range_lon / range_lat).pow(2) + epsilon)\n",
    "    log_nugget = torch.log(nugget + epsilon)\n",
    "    \n",
    "    log_params = torch.tensor([log_phi1, log_phi2, log_phi3, log_nugget], device=device)\n",
    "    return log_params\n",
    "\n",
    "\n",
    "def build_cov_matrix(locs, log_params):\n",
    "    \"\"\"\n",
    "    Builds the full N x N covariance matrix from the SpatialModel's\n",
    "    4-parameter reparameterization.\n",
    "    \n",
    "    locs: (N, 2) tensor of [lat, lon] coordinates\n",
    "    log_params: (4,) tensor of [log_phi1, log_phi2, log_phi3, log_nugget]\n",
    "    \"\"\"\n",
    "    device = locs.device\n",
    "    \n",
    "    # --- A. Recover all parameters ---\n",
    "    phi1   = torch.exp(log_params[0])\n",
    "    phi2   = torch.exp(log_params[1]) # This is range_lon_inv\n",
    "    phi3   = torch.exp(log_params[2]) # This is (range_lon / range_lat)^2\n",
    "    nugget = torch.exp(log_params[3])\n",
    "\n",
    "    # --- B. Derive Physical Parameters ---\n",
    "    epsilon = 1e-12\n",
    "    # Ensure phi2 is not zero to avoid division by zero\n",
    "    phi2_safe = phi2 + epsilon \n",
    "    sigmasq = phi1 / phi2_safe\n",
    "    range_lon_inv = phi2\n",
    "    range_lat_inv = torch.sqrt(phi3 + epsilon) * phi2\n",
    "\n",
    "    # --- C. Calculate Anisotropic Spatial Distance ---\n",
    "    x_lat, x_lon = locs[:, 0], locs[:, 1]\n",
    "    \n",
    "    delta_lat = x_lat.unsqueeze(1) - x_lat.unsqueeze(0)\n",
    "    delta_lon = x_lon.unsqueeze(1) - x_lon.unsqueeze(0)\n",
    "\n",
    "    dist_sq = (delta_lat * range_lat_inv).pow(2) + (delta_lon * range_lon_inv).pow(2)\n",
    "    distance = torch.sqrt(dist_sq + epsilon) \n",
    "\n",
    "    # --- D. Calculate Covariance (Matern 0.5 = Exponential) ---\n",
    "    cov = sigmasq * torch.exp(-distance)\n",
    "\n",
    "    # --- E. Add Nugget ---\n",
    "    cov.diagonal().add_(nugget + epsilon) # Add jitter to nugget\n",
    "    \n",
    "    return cov\n",
    "\n",
    "\n",
    "def generate_spatial_data(n1, n2, lat_range, lon_range, true_params, device):\n",
    "    \"\"\"\n",
    "    Generates a single n1 x n2 spatial field.\n",
    "    \"\"\"\n",
    "    print(f\"Generating spatial data ({n1} lats x {n2} lons)...\")\n",
    "    N = n1 * n2\n",
    "    \n",
    "    lats = torch.linspace(lat_range[0], lat_range[1], n1, device=device, dtype=torch.float64)\n",
    "    lons = torch.linspace(lon_range[0], lon_range[1], n2, device=device, dtype=torch.float64)\n",
    "    grid_lats, grid_lons = torch.meshgrid(lats, lons, indexing='ij')\n",
    "    \n",
    "    locs = torch.stack([grid_lats.ravel(), grid_lons.ravel()], dim=1)\n",
    "    \n",
    "    log_params_true = convert_to_log_reparam(\n",
    "        torch.tensor(true_params['sigmasq'], device=device, dtype=torch.float64), \n",
    "        torch.tensor(true_params['range_lat'], device=device, dtype=torch.float64),\n",
    "        torch.tensor(true_params['range_lon'], device=device, dtype=torch.float64),\n",
    "        torch.tensor(true_params['nugget'], device=device, dtype=torch.float64)\n",
    "    )\n",
    "    \n",
    "    print(f\"Building N x N covariance matrix (N={N})...\")\n",
    "    K = build_cov_matrix(locs, log_params_true)\n",
    "    \n",
    "    print(\"Performing Cholesky decomposition...\")\n",
    "    try:\n",
    "        L = torch.linalg.cholesky(K)\n",
    "    except torch.linalg.LinAlgError:\n",
    "        print(\"Warning: Cholesky failed. Adding jitter.\")\n",
    "        L = torch.linalg.cholesky(K + torch.eye(N, device=device, dtype=torch.float64) * 1e-5)\n",
    "        \n",
    "    z = torch.randn(N, 1, device=device, dtype=torch.float64)\n",
    "    # Return a flat (N, 1) vector and the (N, 2) locations\n",
    "    data_vec = (L @ z) \n",
    "    \n",
    "    print(\"Data generated.\")\n",
    "    return data_vec.float(), locs.float() # Return N,1 data and N,2 locs\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 2. Full Maximum Likelihood Loss (No Tapering)\n",
    "# =========================================================================\n",
    "\n",
    "def full_nll_loss(log_params, data_vec, locs):\n",
    "    \"\"\"\n",
    "    Computes the exact Negative Log-Likelihood (NLL) for a Gaussian Process.\n",
    "    \n",
    "    log_params: (4,) tensor of [log_phi1, log_phi2, log_phi3, log_nugget]\n",
    "    data_vec: (N, 1) tensor of observations\n",
    "    locs: (N, 2) tensor of [lat, lon] coordinates\n",
    "    \"\"\"\n",
    "    N = data_vec.shape[0]\n",
    "    device = data_vec.device\n",
    "    \n",
    "    # 1. Build the covariance matrix from the current parameters\n",
    "    # We must rebuild this every time for gradient calculation\n",
    "    try:\n",
    "        K = build_cov_matrix(locs, log_params)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error in build_cov_matrix: {e}\")\n",
    "        return torch.tensor(float('inf'), device=device)\n",
    "\n",
    "    # 2. Compute the Cholesky decomposition\n",
    "    try:\n",
    "        # Add small jitter for numerical stability during optimization\n",
    "        jitter = torch.eye(N, device=device, dtype=K.dtype) * 1e-6\n",
    "        L = torch.linalg.cholesky(K + jitter)\n",
    "    except torch.linalg.LinAlgError:\n",
    "        print(\"Warning: Cholesky decomposition failed.\")\n",
    "        # Return a very large loss to penalize these parameters\n",
    "        return torch.tensor(float('inf'), device=device)\n",
    "\n",
    "    # 3. Calculate the log-determinant term\n",
    "    # log(det(K)) = 2 * sum(log(diag(L)))\n",
    "    log_det_K = 2 * torch.sum(torch.log(torch.diag(L)))\n",
    "\n",
    "    # 4. Calculate the quadratic form term (y^T * K^{-1} * y)\n",
    "    # Solve L*v = y for v\n",
    "    v = torch.cholesky_solve(data_vec, L, upper=False)\n",
    "    # Quad form is y^T * v\n",
    "    quad_form = torch.matmul(data_vec.T, v)\n",
    "\n",
    "    # 5. Calculate the NLL\n",
    "    # NLL = 0.5 * (N*log(2*pi) + log(det(K)) + y^T * K^{-1} * y)\n",
    "    log_2pi = torch.log(torch.tensor(2 * np.pi, device=device, dtype=K.dtype))\n",
    "    nll = 0.5 * (N * log_2pi + log_det_K + quad_form)\n",
    "\n",
    "    # Return the average NLL per point for stable optimization\n",
    "    return nll / N\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 3. Training Loop (Adam)\n",
    "# =========================================================================\n",
    "\n",
    "\n",
    "\n",
    "def get_phi_params(log_params_list):\n",
    "    \"\"\"Helper to print reparameterized (phi-scale) params.\"\"\"\n",
    "    try:\n",
    "        # p_list is a list of 1-element tensors\n",
    "        p_cat = torch.cat([p.detach().clone().cpu() for p in log_params_list])\n",
    "        # Exponentiate the log-params\n",
    "        phi1 = torch.exp(p_cat[0])\n",
    "        phi2 = torch.exp(p_cat[1])\n",
    "        phi3 = torch.exp(p_cat[2])\n",
    "        nugget = torch.exp(p_cat[3]) # This is already what you were doing\n",
    "        \n",
    "        return (f\"phi1: {phi1.item():.4f}, phi2: {phi2.item():.4f}, \"\n",
    "                f\"phi3: {phi3.item():.4f}, nugget: {nugget.item():.4f}\")\n",
    "    except Exception:\n",
    "        return \"[Error in reparam conversion]\"\n",
    "    \n",
    "# =========================================================================\n",
    "# 4. Main Simulation Script\n",
    "# =========================================================================\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- 0. Configuration ---\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    N1 = 25  # Number of latitude points\n",
    "    N2 = 25  # Number of longitude points\n",
    "    \n",
    "    LAT_RANGE = (0.0, 5.0)\n",
    "    LON_RANGE = (0.0, 5.0) \n",
    "    \n",
    "    EPOCHS = 200\n",
    "    LEARNING_RATE = 0.01 \n",
    "\n",
    "    print(f\"Grid: {N1}x{N2} ({N1*N2} points).\")\n",
    "    \n",
    "    # --- 1. Define True Parameters ---\n",
    "    TRUE_PARAMS = {\n",
    "        'sigmasq': 30.0,\n",
    "        'range_lat': 1.0,\n",
    "        'range_lon': 1.5,\n",
    "        'nugget': 0.1 \n",
    "    }\n",
    "    print(f\"\\n--- True Parameters (Natural Scale) ---\")\n",
    "    for key, val in TRUE_PARAMS.items():\n",
    "        print(f\"  {key:<10}: {val:.4f}\")\n",
    "\n",
    "    # --- Calculate and print true log-params ---\n",
    "    true_log_params_tensor = convert_to_log_reparam(\n",
    "        torch.tensor(TRUE_PARAMS['sigmasq']),\n",
    "        torch.tensor(TRUE_PARAMS['range_lat']),\n",
    "        torch.tensor(TRUE_PARAMS['range_lon']),\n",
    "        torch.tensor(TRUE_PARAMS['nugget'])\n",
    "    )\n",
    "    # ðŸ’¥ Store the true log params as a list of tensors for the helper\n",
    "    true_log_params_list = [Parameter(p.unsqueeze(0)) for p in true_log_params_tensor]\n",
    "    \n",
    "    # --- 2. Generate Simulated Data ---\n",
    "    data_vec, locs = generate_spatial_data(\n",
    "        N1, N2, LAT_RANGE, LON_RANGE, TRUE_PARAMS, DEVICE\n",
    "    )\n",
    "    \n",
    "    data_vec_dev = data_vec.to(DEVICE)\n",
    "    locs_dev = locs.to(DEVICE)\n",
    "\n",
    "    # --- 3. Optimization (Using Adam) ---\n",
    "    INIT_PARAMS = {\n",
    "        'sigmasq': 25.0,\n",
    "        'range_lat': 0.8,\n",
    "        'range_lon': 1.2,\n",
    "        'nugget': 0.2 \n",
    "    }\n",
    "    print(f\"\\n--- Initial Parameters for Fitting ---\")\n",
    "    print(INIT_PARAMS)\n",
    "    \n",
    "    log_params_init = convert_to_log_reparam(\n",
    "        torch.tensor(INIT_PARAMS['sigmasq']),\n",
    "        torch.tensor(INIT_PARAMS['range_lat']),\n",
    "        torch.tensor(INIT_PARAMS['range_lon']),\n",
    "        torch.tensor(INIT_PARAMS['nugget'])\n",
    "    ).float()\n",
    "\n",
    "    params_list = [\n",
    "        Parameter(torch.tensor([val], dtype=torch.float32, device=DEVICE))\n",
    "        for val in log_params_init\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.Adam(params_list, lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_params_state = None\n",
    "\n",
    "    print(\"\\n--- Starting Full MLE Optimization (No Tapering) ---\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        optimizer.zero_grad()\n",
    "        params_tensor = torch.cat(params_list)\n",
    "        \n",
    "        loss = full_nll_loss(params_tensor, data_vec_dev, locs_dev)\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS}: Loss is NaN/Inf. Stopping optimization.\")\n",
    "            if best_params_state is None: \n",
    "                 best_params_state = [p.detach().clone() for p in params_list]\n",
    "            break\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(params_list, max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        current_loss_item = loss.item()\n",
    "        \n",
    "        if current_loss_item < best_loss:\n",
    "            params_valid = not any(torch.isnan(p.data).any() or torch.isinf(p.data).any() for p in params_list)\n",
    "            if params_valid:\n",
    "                best_loss = current_loss_item\n",
    "                best_params_state = [p.detach().clone() for p in params_list]\n",
    "\n",
    "        if epoch % 20 == 0 or epoch == EPOCHS - 1:\n",
    "            print(f'Epoch {epoch+1}/{EPOCHS} | Loss: {current_loss_item:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}')\n",
    "            if epoch % 50 == 0 or epoch == EPOCHS - 1:\n",
    "                 print(f'  Params: {get_printable_params(params_list)}')\n",
    "\n",
    "    print(\"\\n--- Optimization Complete ---\")\n",
    "\n",
    "    # --- 4. Report Results ---\n",
    "    print(f\"\\n\\n{'='*25} FINAL SIMULATION RESULTS {'='*25}\")\n",
    "    print(f\"Grid Size: {N1} lats x {N2} lons ({N1*N2} points)\")\n",
    "    \n",
    "    print(\"\\n--- True Parameters (Natural Scale) ---\")\n",
    "    for key, val in TRUE_PARAMS.items():\n",
    "        print(f\"  {key:<10}: {val:.4f}\")\n",
    "        \n",
    "    print(\"\\n--- Estimated Parameters (Natural Scale) ---\")\n",
    "    if best_params_state:\n",
    "        final_params_str = get_printable_params(best_params_state)\n",
    "        print(f\"  Final Loss: {best_loss:.4f}\")\n",
    "        print(f\"  Params: {final_params_str}\")\n",
    "    else:\n",
    "        print(\"Optimization failed to produce valid results.\")\n",
    "\n",
    "    # --- ðŸ’¥ REVISED: Print reparameterized results ðŸ’¥ ---\n",
    "    print(\"\\n--- True Reparameterized Parameters (Phi Scale) ---\")\n",
    "    # Use the new helper on the true log params list\n",
    "    print(f\"  Params: {get_phi_params(true_log_params_list)}\") \n",
    "    \n",
    "    print(\"\\n--- Estimated Reparameterized Parameters (Phi Scale) ---\")\n",
    "    if best_params_state:\n",
    "        # Use the new helper on the final estimated params\n",
    "        final_reparam_str = get_phi_params(best_params_state) \n",
    "        print(f\"  Params: {final_reparam_str}\")\n",
    "    else:\n",
    "        print(\"Optimization failed to produce valid results.\")\n",
    "    # --- END REVISED ---\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb271a",
   "metadata": {},
   "source": [
    "no tapering below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce6a3952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Grid: 115x160 (18400 points).\n",
      "Lat Range: (0.0, 5.016). Lon Range: (123.0, 132.997).\n",
      "Deltas (hard-coded in likelihood): 0.044, 0.063\n",
      "\n",
      "--- True Parameters (Natural Scale) ---\n",
      "  sigmasq   : 30.0000\n",
      "  range_lat : 1.0000\n",
      "  range_lon : 1.5000\n",
      "  nugget    : 0.1000\n",
      "\n",
      "--- True Reparameterized Parameters ---\n",
      "  log_phi1: 2.9957, log_phi2: -0.4055, log_phi3: 0.8109, log_nugget: -2.3026\n",
      "WARNING: Building an 18400 x 18400 covariance matrix.\n",
      "Generating spatial data (115 lats x 160 lons)...\n",
      "Building N x N covariance matrix (N=18400)... This may take a moment.\n",
      "Performing Cholesky decomposition...\n",
      "Data generated.\n",
      "\n",
      "Pre-computing Taper, Autocorrelation, and Periodogram...\n",
      "Using RECTANGULAR taper (i.e., no tapering).\n",
      "Pre-computation complete.\n",
      "\n",
      "--- Initial Parameters for Fitting ---\n",
      "{'sigmasq': 25.0, 'range_lat': 0.8, 'range_lon': 1.2, 'nugget': 0.2}\n",
      "\n",
      "--- Starting Adam Optimization (No Tapering) ---\n",
      "Epoch 1/1000 | Loss: -39207.4883 | LR: 0.005000\n",
      "  Params: sigmasq: 24.7512, range_lat: 0.7980, range_lon: 1.1940, nugget: 0.1990\n",
      "Epoch 51/1000 | Loss: -39314.7812 | LR: 0.004968\n",
      "Epoch 101/1000 | Loss: -39319.3359 | LR: 0.004875\n",
      "Epoch 151/1000 | Loss: -39319.3555 | LR: 0.004724\n",
      "Epoch 201/1000 | Loss: -39319.3594 | LR: 0.004518\n",
      "  Params: sigmasq: 25.1741, range_lat: 0.8201, range_lon: 1.2720, nugget: 0.1064\n",
      "Epoch 251/1000 | Loss: -39319.3594 | LR: 0.004262\n",
      "Epoch 301/1000 | Loss: -39319.3672 | LR: 0.003963\n",
      "Epoch 351/1000 | Loss: -39319.3438 | LR: 0.003628\n",
      "Epoch 401/1000 | Loss: -39319.3281 | LR: 0.003265\n",
      "  Params: sigmasq: 25.1451, range_lat: 0.8193, range_lon: 1.2707, nugget: 0.1064\n",
      "Epoch 451/1000 | Loss: -39319.3633 | LR: 0.002884\n",
      "Epoch 501/1000 | Loss: -39319.3516 | LR: 0.002493\n",
      "Epoch 551/1000 | Loss: -39319.3711 | LR: 0.002102\n",
      "Epoch 601/1000 | Loss: -39319.3672 | LR: 0.001721\n",
      "  Params: sigmasq: 25.1502, range_lat: 0.8193, range_lon: 1.2708, nugget: 0.1064\n",
      "Epoch 651/1000 | Loss: -39319.3750 | LR: 0.001359\n",
      "Epoch 701/1000 | Loss: -39319.3672 | LR: 0.001025\n",
      "Epoch 751/1000 | Loss: -39319.3516 | LR: 0.000728\n",
      "Epoch 801/1000 | Loss: -39319.3672 | LR: 0.000474\n",
      "  Params: sigmasq: 25.1602, range_lat: 0.8193, range_lon: 1.2710, nugget: 0.1064\n",
      "Epoch 851/1000 | Loss: -39319.3633 | LR: 0.000270\n",
      "Epoch 901/1000 | Loss: -39319.3711 | LR: 0.000121\n",
      "Epoch 951/1000 | Loss: -39319.3555 | LR: 0.000031\n",
      "Epoch 1000/1000 | Loss: -39319.3555 | LR: 0.000001\n",
      "  Params: sigmasq: 25.1609, range_lat: 0.8194, range_lon: 1.2711, nugget: 0.1064\n",
      "\n",
      "--- Adam Optimization Complete ---\n",
      "\n",
      "\n",
      "========================= FINAL SIMULATION RESULTS =========================\n",
      "Grid Size: 115 lats x 160 lons (18400 points)\n",
      "Method: Whittle Likelihood (No Tapering)\n",
      "\n",
      "--- True Parameters (Natural Scale) ---\n",
      "  sigmasq   : 30.0000\n",
      "  range_lat : 1.0000\n",
      "  range_lon : 1.5000\n",
      "  nugget    : 0.1000\n",
      "\n",
      "--- True Reparameterized Parameters ---\n",
      "  log_phi1: 2.9957, log_phi2: -0.4055, log_phi3: 0.8109, log_nugget: -2.3026\n",
      "\n",
      "--- Estimated Parameters (Natural Scale) ---\n",
      "  Final Loss: -39319.3867\n",
      "  Final Params: sigmasq: 25.1768, range_lat: 0.8196, range_lon: 1.2718, nugget: 0.1063\n",
      "\n",
      "--- Estimated Reparameterized Parameters ---\n",
      "  Final Reparam Params: log_phi1: 2.9855, log_phi2: -0.2405, log_phi3: 0.8787, log_nugget: -2.2410\n",
      "\n",
      "Total execution time: 15.17 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import cmath\n",
    "import pickle\n",
    "import time \n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import Parameter\n",
    "import torch.fft \n",
    "from typing import List, Dict, Any, Callable\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# =========================================================================\n",
    "# 1. Data Generation and Model Functions\n",
    "# =========================================================================\n",
    "\n",
    "def convert_to_log_reparam(sigmasq, range_lat, range_lon, nugget):\n",
    "    \"\"\"\n",
    "    Converts physical parameters to the 4-parameter log-reparameterization.\n",
    "    \"\"\"\n",
    "    device = sigmasq.device\n",
    "    epsilon = 1e-12\n",
    "    \n",
    "    range_lon_inv = 1.0 / (range_lon + epsilon)\n",
    "    log_phi2 = torch.log(range_lon_inv)\n",
    "    log_phi1 = torch.log(sigmasq * range_lon_inv + epsilon)\n",
    "    log_phi3 = torch.log((range_lon / range_lat).pow(2) + epsilon)\n",
    "    log_nugget = torch.log(nugget + epsilon)\n",
    "    \n",
    "    log_params = torch.tensor([log_phi1, log_phi2, log_phi3, log_nugget], device=device)\n",
    "    return log_params\n",
    "\n",
    "\n",
    "def build_cov_matrix(locs, log_params):\n",
    "    \"\"\"\n",
    "    Builds the full N x N covariance matrix from the SpatialModel's\n",
    "    4-parameter reparameterization.\n",
    "    \n",
    "    locs: (N, 2) tensor of [lat, lon] coordinates\n",
    "    log_params: (4,) tensor of [log_phi1, log_phi2, log_phi3, log_nugget]\n",
    "    \"\"\"\n",
    "    device = locs.device\n",
    "    \n",
    "    # --- A. Recover all parameters ---\n",
    "    phi1   = torch.exp(log_params[0])\n",
    "    phi2   = torch.exp(log_params[1]) # This is range_lon_inv\n",
    "    phi3   = torch.exp(log_params[2]) # This is (range_lon / range_lat)^2\n",
    "    nugget = torch.exp(log_params[3])\n",
    "\n",
    "    # --- B. Derive Physical Parameters ---\n",
    "    epsilon = 1e-12\n",
    "    phi2_safe = phi2 + epsilon \n",
    "    sigmasq = phi1 / phi2_safe\n",
    "    range_lon_inv = phi2\n",
    "    range_lat_inv = torch.sqrt(phi3 + epsilon) * phi2\n",
    "\n",
    "    # --- C. Calculate Anisotropic Spatial Distance ---\n",
    "    x_lat, x_lon = locs[:, 0], locs[:, 1]\n",
    "    \n",
    "    delta_lat = x_lat.unsqueeze(1) - x_lat.unsqueeze(0)\n",
    "    delta_lon = x_lon.unsqueeze(1) - x_lon.unsqueeze(0)\n",
    "\n",
    "    dist_sq = (delta_lat * range_lat_inv).pow(2) + (delta_lon * range_lon_inv).pow(2)\n",
    "    distance = torch.sqrt(dist_sq + epsilon) \n",
    "\n",
    "    # --- D. Calculate Covariance (Matern 0.5 = Exponential) ---\n",
    "    cov = sigmasq * torch.exp(-distance)\n",
    "\n",
    "    # --- E. Add Nugget ---\n",
    "    cov.diagonal().add_(nugget + epsilon) # Add jitter to nugget\n",
    "    \n",
    "    return cov\n",
    "\n",
    "\n",
    "def generate_spatial_data(n1, n2, lat_range, lon_range, true_params, device):\n",
    "    \"\"\"\n",
    "    Generates a single n1 x n2 spatial field.\n",
    "    \"\"\"\n",
    "    print(f\"Generating spatial data ({n1} lats x {n2} lons)...\")\n",
    "    N = n1 * n2\n",
    "    \n",
    "    lats = torch.linspace(lat_range[0], lat_range[1], n1, device=device, dtype=torch.float64)\n",
    "    lons = torch.linspace(lon_range[0], lon_range[1], n2, device=device, dtype=torch.float64)\n",
    "    grid_lats, grid_lons = torch.meshgrid(lats, lons, indexing='ij')\n",
    "    \n",
    "    locs = torch.stack([grid_lats.ravel(), grid_lons.ravel()], dim=1)\n",
    "    \n",
    "    log_params_true = convert_to_log_reparam(\n",
    "        torch.tensor(true_params['sigmasq'], device=device, dtype=torch.float64), \n",
    "        torch.tensor(true_params['range_lat'], device=device, dtype=torch.float64),\n",
    "        torch.tensor(true_params['range_lon'], device=device, dtype=torch.float64),\n",
    "        torch.tensor(true_params['nugget'], device=device, dtype=torch.float64)\n",
    "    )\n",
    "    \n",
    "    print(\"Building N x N covariance matrix (N={})... This may take a moment.\".format(N))\n",
    "    K = build_cov_matrix(locs, log_params_true)\n",
    "    \n",
    "    print(\"Performing Cholesky decomposition...\")\n",
    "    try:\n",
    "        L = torch.linalg.cholesky(K)\n",
    "    except torch.linalg.LinAlgError:\n",
    "        print(\"Warning: Cholesky failed. Adding jitter.\")\n",
    "        L = torch.linalg.cholesky(K + torch.eye(N, device=device, dtype=torch.float64) * 1e-5)\n",
    "        \n",
    "    z = torch.randn(N, 1, device=device, dtype=torch.float64)\n",
    "    data_vec = (L @ z).float() # Convert to float32 for FFT/Likelihood\n",
    "    \n",
    "    print(\"Data generated.\")\n",
    "    return data_vec.reshape(n1, n2)\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 2. Tapering, Autocorrelation, and Covariance Functions\n",
    "# =========================================================================\n",
    "\n",
    "def cgn_hamming(u, n1, n2):\n",
    "    \"\"\"Computes a 2D Hamming window.\"\"\"\n",
    "    u1, u2 = u\n",
    "    device = u1.device if isinstance(u1, torch.Tensor) else (u2.device if isinstance(u2, torch.Tensor) else torch.device('cpu'))\n",
    "    u1_tensor = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_tensor = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "\n",
    "    n1_eff = float(n1) if n1 > 0 else 1.0\n",
    "    n2_eff = float(n2) if n2 > 0 else 1.0\n",
    "    hamming1 = 0.54 + 0.46 * torch.cos(2.0 * torch.pi * u1_tensor / n1_eff)\n",
    "    hamming2 = 0.54 + 0.46 * torch.cos(2.0 * torch.pi * u2_tensor / n2_eff)\n",
    "    return hamming1 * hamming2\n",
    "\n",
    "# --- ðŸ’¥ NEW TAPERING FUNCTION ðŸ’¥ ---\n",
    "def cgn_rectangular(u, n1, n2):\n",
    "    \"\"\"Computes a 2D Rectangular window (i.e., no taper).\"\"\"\n",
    "    # u is a tuple of meshgrids (u1_mesh, u2_mesh)\n",
    "    u1_mesh, _ = u\n",
    "    return torch.ones_like(u1_mesh)\n",
    "# --- END NEW FUNCTION ---\n",
    "\n",
    "def calculate_taper_autocorrelation_fft(taper_grid, n1, n2, device):\n",
    "    \"\"\"\n",
    "    Computes the normalized taper autocorrelation function c_gn(u) using FFT.\n",
    "    This function is generic and works for any taper_grid, including rectangular.\n",
    "    \"\"\"\n",
    "    taper_grid = taper_grid.to(device) \n",
    "    H = torch.sum(taper_grid**2)\n",
    "    if H < 1e-12:\n",
    "        print(\"Warning: Sum of squared taper weights (H) is near zero.\")\n",
    "        return torch.zeros((2*n1-1, 2*n2-1), device=device, dtype=taper_grid.dtype)\n",
    "\n",
    "    N1, N2 = 2 * n1 - 1, 2 * n2 - 1\n",
    "    taper_fft = torch.fft.fft2(taper_grid, s=(N1, N2))\n",
    "    power_spectrum = torch.abs(taper_fft)**2\n",
    "    autocorr_unnormalized = torch.fft.ifft2(power_spectrum).real\n",
    "    autocorr_shifted = torch.fft.fftshift(autocorr_unnormalized)\n",
    "    c_gn_grid = autocorr_shifted / (H + 1e-12)\n",
    "    return c_gn_grid \n",
    "\n",
    "# =========================================================================\n",
    "# 3. Covariance & Spectral Density\n",
    "# =========================================================================\n",
    "\n",
    "# --- DEFINE DELTAS GLOBALLY (or within function) ---\n",
    "DELTA_LAT = 0.044\n",
    "DELTA_LON = 0.063\n",
    "\n",
    "\n",
    "def cov_x_spatial_model_kernel(u1, u2, t, params):\n",
    "    \"\"\"\n",
    "    Computes autocovariance of X using the SPATIAL-ONLY kernel.\n",
    "    u1, u2 are PHYSICAL lags (already scaled by deltas).\n",
    "    \"\"\"\n",
    "    device = params.device\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    if torch.isnan(params).any() or torch.isinf(params).any():\n",
    "         out_shape = torch.broadcast_shapes(u1_dev.shape, u2_dev.shape, t_dev.shape)\n",
    "         return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "\n",
    "    phi1   = torch.exp(params[0])\n",
    "    phi2   = torch.exp(params[1]) \n",
    "    phi3   = torch.exp(params[2])\n",
    "    nugget = torch.exp(params[3])\n",
    "\n",
    "    epsilon = 1e-12\n",
    "    sigmasq = phi1 / (phi2 + epsilon)  \n",
    "    range_lon_inv = phi2\n",
    "    range_lat_inv = torch.sqrt(phi3 + epsilon) * phi2\n",
    "\n",
    "    dist_sq = (u1_dev * range_lat_inv).pow(2) + (u2_dev * range_lon_inv).pow(2)\n",
    "    distance = torch.sqrt(dist_sq + epsilon) \n",
    "\n",
    "    cov_smooth = sigmasq * torch.exp(-distance)\n",
    "\n",
    "    is_zero_lag = (torch.abs(u1_dev) < 1e-9) & (torch.abs(u2_dev) < 1e-9) & (torch.abs(t_dev) < 1e-9)\n",
    "    final_cov = torch.where(is_zero_lag, cov_smooth + nugget, cov_smooth)\n",
    "\n",
    "    return final_cov\n",
    "\n",
    "\n",
    "def cn_bar_tapered(u1, u2, t, params, n1, n2, taper_autocorr_grid):\n",
    "    \"\"\"\n",
    "    Computes c_X(u) * c_gn(u).\n",
    "    u1, u2 are GRID index lags (e.g., -n1..0..n1)\n",
    "    \"\"\"\n",
    "    device = params.device\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    # --- KEY CORRECTION: Scale lags by deltas ---\n",
    "    lag_u1 = u1_dev * DELTA_LAT\n",
    "    lag_u2 = u2_dev * DELTA_LON\n",
    "\n",
    "    # Call kernel with true spatial lags\n",
    "    cov_X_value = cov_x_spatial_model_kernel(lag_u1, lag_u2, t_dev, params)\n",
    "\n",
    "    # --- Get Taper Autocorrelation Value c_gn(u) from grid ---\n",
    "    u1_idx = u1_dev.long()\n",
    "    u2_idx = u2_dev.long()\n",
    "    idx1 = (n1 - 1 + u1_idx)\n",
    "    idx2 = (n2 - 1 + u2_idx)\n",
    "    idx1 = torch.clamp(idx1, 0, 2 * n1 - 2)\n",
    "    idx2 = torch.clamp(idx2, 0, 2 * n2 - 2)\n",
    "\n",
    "    taper_autocorr_value = taper_autocorr_grid[idx1, idx2] \n",
    "\n",
    "    if torch.isnan(cov_X_value).any() or torch.isnan(taper_autocorr_value).any():\n",
    "        out_shape = torch.broadcast_shapes(cov_X_value.shape, taper_autocorr_value.shape)\n",
    "        return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "\n",
    "    result = cov_X_value * taper_autocorr_value\n",
    "    return result\n",
    "\n",
    "\n",
    "def expected_periodogram_fft_tapered(params, n1, n2, p, taper_autocorr_grid):\n",
    "    \"\"\"\n",
    "    Calculates the expected periodogram using the aliasing sum (Lemma 2).\n",
    "    \"\"\"\n",
    "    device = params.device if isinstance(params, torch.Tensor) else params[0].device\n",
    "    if isinstance(params, list):\n",
    "        params_tensor = torch.cat([p.to(device) for p in params])\n",
    "    else:\n",
    "        params_tensor = params.to(device)\n",
    "\n",
    "    # 1. Create the grid for u from [0, n-1]\n",
    "    u1_lags = torch.arange(n1, dtype=torch.float32, device=device)\n",
    "    u2_lags = torch.arange(n2, dtype=torch.float32, device=device)\n",
    "    u1_mesh, u2_mesh = torch.meshgrid(u1_lags, u2_lags, indexing='ij')\n",
    "\n",
    "    t_lags = torch.arange(p, dtype=torch.float32, device=device)\n",
    "    tilde_cn_tensor = torch.zeros((n1, n2, p, p), dtype=torch.complex64, device=device)\n",
    "\n",
    "    # 2. Loop over the p x p matrix (for this spatial code, p=1)\n",
    "    for q in range(p):\n",
    "        for r in range(p):\n",
    "            t_diff = t_lags[q] - t_lags[r]\n",
    "            \n",
    "            # --- Calculate 4 aliasing terms from Lemma 2 ---\n",
    "            term1 = cn_bar_tapered(u1_mesh, u2_mesh, t_diff, \n",
    "                                   params_tensor, n1, n2, taper_autocorr_grid)\n",
    "            term2 = cn_bar_tapered(u1_mesh - n1, u2_mesh, t_diff, \n",
    "                                   params_tensor, n1, n2, taper_autocorr_grid)\n",
    "            term3 = cn_bar_tapered(u1_mesh, u2_mesh - n2, t_diff, \n",
    "                                   params_tensor, n1, n2, taper_autocorr_grid)\n",
    "            term4 = cn_bar_tapered(u1_mesh - n1, u2_mesh - n2, t_diff,\n",
    "                                   params_tensor, n1, n2, taper_autocorr_grid)\n",
    "            \n",
    "            # 3. Sum them to create tilde_c_n(u, t_diff)\n",
    "            tilde_cn_grid_qr = (term1 + term2 + term3 + term4)\n",
    "            \n",
    "            if torch.isnan(tilde_cn_grid_qr).any():\n",
    "                 tilde_cn_tensor[:, :, q, r] = float('nan')\n",
    "            else:\n",
    "                 tilde_cn_tensor[:, :, q, r] = tilde_cn_grid_qr.to(torch.complex64)\n",
    "\n",
    "    if torch.isnan(tilde_cn_tensor).any():\n",
    "        print(\"Warning: NaN in product_tensor before FFT.\")\n",
    "        nan_shape = (n1, n2, p, p)\n",
    "        return torch.full(nan_shape, float('nan'), dtype=torch.complex64, device=device)\n",
    "\n",
    "    # 4. Compute FFT of the *aliased* covariance\n",
    "    fft_result = torch.fft.fft2(tilde_cn_tensor, dim=(0, 1))\n",
    "    fft_result_real = fft_result.real \n",
    "    normalization_factor = 1.0 / (4.0 * cmath.pi**2)\n",
    "    result = fft_result_real * normalization_factor\n",
    "\n",
    "    return result\n",
    "\n",
    "# =========================================================================\n",
    "# 4. Data Processing (Whittle)\n",
    "# =========================================================================\n",
    "def generate_Jvector_tapered(data_grid, taper_grid, device):\n",
    "    \"\"\"\n",
    "    Generates J-vector for a single component using the specified taper.\n",
    "    \"\"\"\n",
    "    p = 1 # This is a spatial-only model\n",
    "    n1, n2 = data_grid.shape\n",
    "    \n",
    "    data_grid_tapered = data_grid.to(device) * taper_grid.to(device)\n",
    "\n",
    "    if torch.isnan(data_grid_tapered).any() or torch.isinf(data_grid_tapered).any():\n",
    "         print(\"Warning: NaN/Inf detected in data_grid_tapered before FFT. Replacing with zeros.\")\n",
    "         data_grid_tapered = torch.nan_to_num(data_grid_tapered, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    J_vec_spatial = torch.fft.fft2(data_grid_tapered)\n",
    "    \n",
    "    # Add 'p' dimension\n",
    "    J_vector_tensor = J_vec_spatial.unsqueeze(-1)\n",
    "\n",
    "    H = torch.sum(taper_grid**2)\n",
    "    if H < 1e-12:\n",
    "        print(\"Warning: Normalization factor H is near zero.\")\n",
    "        norm_factor = torch.tensor(0.0, device=device)\n",
    "    else:\n",
    "        norm_factor = (torch.sqrt(1.0 / H) / (2.0 * cmath.pi)).to(device)\n",
    "\n",
    "    result = J_vector_tensor * norm_factor\n",
    "    if torch.isnan(result).any(): print(\"Warning: NaN in J_vector output.\")\n",
    "    return result, n1, n2, p\n",
    "\n",
    "\n",
    "def calculate_sample_periodogram_vectorized(J_vector_tensor):\n",
    "    \"\"\"Calculates sample periodogram I_n = J J^H (pxp matrix for each spatial freq).\"\"\"\n",
    "    if torch.isnan(J_vector_tensor).any() or torch.isinf(J_vector_tensor).any():\n",
    "        print(\"Warning: NaN/Inf detected in J_vector_tensor input.\")\n",
    "        n1, n2, p = J_vector_tensor.shape\n",
    "        return torch.full((n1, n2, p, p), float('nan'), dtype=torch.complex64, device=J_vector_tensor.device)\n",
    "\n",
    "    J_col = J_vector_tensor.unsqueeze(-1)\n",
    "    J_row_conj = J_vector_tensor.unsqueeze(-2).conj()\n",
    "    result = J_col @ J_row_conj\n",
    "\n",
    "    if torch.isnan(result).any(): print(\"Warning: NaN in periodogram matrix output.\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 5. Likelihood Calculation (Whittle)\n",
    "# =========================================================================\n",
    "\n",
    "def whittle_likelihood_loss_tapered(params, I_sample, n1, n2, p, taper_autocorr_grid):\n",
    "    \"\"\"\n",
    "    Whittle Likelihood Loss using data tapering and exact taper autocorrelation c_gn.\n",
    "    \"\"\"\n",
    "    device = I_sample.device\n",
    "    params_tensor = params.to(device)\n",
    "\n",
    "    if torch.isnan(params_tensor).any() or torch.isinf(params_tensor).any():\n",
    "        print(\"Warning: NaN/Inf detected in input parameters to likelihood.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    I_expected = expected_periodogram_fft_tapered(\n",
    "        params_tensor, n1, n2, p, taper_autocorr_grid\n",
    "    )\n",
    "\n",
    "    if torch.isnan(I_expected).any() or torch.isinf(I_expected).any():\n",
    "        print(\"Warning: NaN/Inf returned from expected_periodogram calculation.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    eye_matrix = torch.eye(p, dtype=torch.complex64, device=device)\n",
    "    diag_vals = torch.abs(I_expected.diagonal(dim1=-2, dim2=-1))\n",
    "    mean_diag_abs = diag_vals.mean().item() if diag_vals.numel() > 0 and not torch.isnan(diag_vals).all() else 1.0\n",
    "    diag_load = max(mean_diag_abs * 1e-8, 1e-9)\n",
    "    I_expected_stable = I_expected + eye_matrix * diag_load\n",
    "\n",
    "    sign, logabsdet = torch.linalg.slogdet(I_expected_stable)\n",
    "    if torch.any(sign.real <= 1e-9):\n",
    "        print(\"Warning: Non-positive determinant encountered. Applying penalty.\")\n",
    "        log_det_term = torch.where(sign.real > 1e-9, logabsdet, torch.tensor(1e10, device=device))\n",
    "    else:\n",
    "        log_det_term = logabsdet\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Warning: NaN/Inf detected in I_sample input to likelihood.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    try:\n",
    "        solved_term = torch.linalg.solve(I_expected_stable, I_sample)\n",
    "        trace_term = torch.einsum('...ii->...', solved_term).real\n",
    "    except torch.linalg.LinAlgError as e:\n",
    "        print(f\"Warning: LinAlgError during solve: {e}. Applying high loss penalty.\")\n",
    "        return torch.tensor(float('inf'), device=device)\n",
    "\n",
    "    if torch.isnan(trace_term).any() or torch.isinf(trace_term).any():\n",
    "        print(\"Warning: NaN/Inf detected in trace_term. Returning NaN loss.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    likelihood_terms = log_det_term + trace_term\n",
    "\n",
    "    if torch.isnan(likelihood_terms).any():\n",
    "        print(\"Warning: NaN detected in likelihood_terms before summation. Returning NaN loss.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    total_sum = torch.sum(likelihood_terms)\n",
    "    # Exclude the DC component (frequency 0,0)\n",
    "    dc_term = likelihood_terms[0, 0] if n1 > 0 and n2 > 0 else torch.tensor(0.0, device=device)\n",
    "    if torch.isnan(dc_term).any() or torch.isinf(dc_term).any():\n",
    "        print(\"Warning: NaN/Inf detected in DC term. Setting to 0.\")\n",
    "        dc_term = torch.tensor(0.0, device=device)\n",
    "\n",
    "    loss = total_sum - dc_term if (n1 > 1 or n2 > 1) else total_sum\n",
    "\n",
    "    if torch.isnan(loss) or torch.isinf(loss):\n",
    "         print(\"Warning: NaN/Inf detected in final loss. Returning Inf penalty.\")\n",
    "         return torch.tensor(float('inf'), device=device)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 6. Training Loop Helpers\n",
    "# =========================================================================\n",
    "\n",
    "def get_printable_params(p_list):\n",
    "    \"\"\"Helper to convert log-params to natural scale for printing.\"\"\"\n",
    "    p_cat = torch.cat([p.detach().clone().cpu() for p in p_list])\n",
    "    if torch.isnan(p_cat).any() or torch.isinf(p_cat).any():\n",
    "        return \"[NaN/Inf in log_params]\"\n",
    "    try:\n",
    "        phi1, phi2, phi3, nugget_log = [torch.exp(p) for p in p_cat]\n",
    "        epsilon = 1e-12\n",
    "        sigmasq = phi1 / (phi2 + epsilon)\n",
    "        range_lon = 1.0 / (phi2 + epsilon)\n",
    "        range_lat = 1.0 / (torch.sqrt(phi3 + epsilon) * phi2 + epsilon)\n",
    "        nugget = nugget_log # This is exp(log_nugget)\n",
    "        \n",
    "        return (f\"sigmasq: {sigmasq.item():.4f}, range_lat: {range_lat.item():.4f}, \"\n",
    "                f\"range_lon: {range_lon.item():.4f}, nugget: {nugget.item():.4f}\")\n",
    "    except Exception:\n",
    "        return \"[Error in param conversion]\"\n",
    "\n",
    "# --- ðŸ’¥ NEW HELPER ðŸ’¥ ---\n",
    "def get_reparam_params(log_params_list):\n",
    "    \"\"\"Helper to print reparameterized (log-scale) params.\"\"\"\n",
    "    try:\n",
    "        p_cat = torch.cat([p.detach().clone().cpu() for p in log_params_list])\n",
    "        return (f\"log_phi1: {p_cat[0].item():.4f}, log_phi2: {p_cat[1].item():.4f}, \"\n",
    "                f\"log_phi3: {p_cat[2].item():.4f}, log_nugget: {p_cat[3].item():.4f}\")\n",
    "    except Exception:\n",
    "        return \"[Error in reparam conversion]\"\n",
    "# --- END NEW HELPER ---\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 7. Main Simulation Script\n",
    "# =========================================================================\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- 0. Configuration ---\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    N1 = 115  # Number of latitude points\n",
    "    N2 = 160  # Number of longitude points\n",
    "    \n",
    "    LAT_RANGE = (0.0, 5.016)\n",
    "    LON_RANGE = (123.0, 132.997) \n",
    "    \n",
    "    P = 1 # Spatial-only\n",
    "    EPOCHS = 1000\n",
    "    LEARNING_RATE = 0.005 \n",
    "\n",
    "    print(f\"Grid: {N1}x{N2} ({N1*N2} points).\")\n",
    "    print(f\"Lat Range: {LAT_RANGE}. Lon Range: {LON_RANGE}.\")\n",
    "    print(f\"Deltas (hard-coded in likelihood): {DELTA_LAT}, {DELTA_LON}\")\n",
    "\n",
    "    \n",
    "    # --- 1. Define True Parameters ---\n",
    "    TRUE_PARAMS = {\n",
    "        'sigmasq': 30.0,\n",
    "        'range_lat': 1.0,\n",
    "        'range_lon': 1.5,\n",
    "        'nugget': 0.1 \n",
    "    }\n",
    "    print(f\"\\n--- True Parameters (Natural Scale) ---\")\n",
    "    for key, val in TRUE_PARAMS.items():\n",
    "        print(f\"  {key:<10}: {val:.4f}\")\n",
    "        \n",
    "    # --- ðŸ’¥ NEW: Calculate and print true log-params ðŸ’¥ ---\n",
    "    true_log_params_tensor = convert_to_log_reparam(\n",
    "        torch.tensor(TRUE_PARAMS['sigmasq']),\n",
    "        torch.tensor(TRUE_PARAMS['range_lat']),\n",
    "        torch.tensor(TRUE_PARAMS['range_lon']),\n",
    "        torch.tensor(TRUE_PARAMS['nugget'])\n",
    "    )\n",
    "    print(\"\\n--- True Reparameterized Parameters ---\")\n",
    "    true_log_params_list = [Parameter(p.unsqueeze(0)) for p in true_log_params_tensor]\n",
    "    print(f\"  {get_reparam_params(true_log_params_list)}\")\n",
    "    # --- END NEW ---\n",
    "\n",
    "    # --- 2. Generate Simulated Data ---\n",
    "    print(\"WARNING: Building an 18400 x 18400 covariance matrix.\")\n",
    "    data_grid = generate_spatial_data(\n",
    "        N1, N2, LAT_RANGE, LON_RANGE, TRUE_PARAMS, DEVICE\n",
    "    )\n",
    "    \n",
    "    # --- 3. Pre-compute for Whittle Likelihood ---\n",
    "    print(\"\\nPre-computing Taper, Autocorrelation, and Periodogram...\")\n",
    "    \n",
    "    # --- ðŸ’¥ MODIFICATION: Use rectangular taper ðŸ’¥ ---\n",
    "    TAPERING_FUNC = cgn_rectangular\n",
    "    print(\"Using RECTANGULAR taper (i.e., no tapering).\")\n",
    "    # --- END MODIFICATION ---\n",
    "    \n",
    "    u1_mesh_cpu, u2_mesh_cpu = torch.meshgrid(\n",
    "        torch.arange(N1, dtype=torch.float32),\n",
    "        torch.arange(N2, dtype=torch.float32),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    taper_grid = TAPERING_FUNC((u1_mesh_cpu, u2_mesh_cpu), N1, N2).to(DEVICE)\n",
    "    taper_autocorr_grid = calculate_taper_autocorrelation_fft(taper_grid, N1, N2, DEVICE)\n",
    "\n",
    "    # Compute J-vector (FFT of tapered data)\n",
    "    J_vec, n1_check, n2_check, p_check = generate_Jvector_tapered(data_grid, taper_grid, DEVICE)\n",
    "    \n",
    "    I_sample = calculate_sample_periodogram_vectorized(J_vec) # (N1, N2, 1, 1)\n",
    "    \n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Error: NaN/Inf in sample periodogram. Exiting.\")\n",
    "        exit()\n",
    "    if torch.isnan(taper_autocorr_grid).any() or torch.isinf(taper_autocorr_grid).any():\n",
    "        print(\"Error: NaN/Inf in taper autocorrelation. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Pre-computation complete.\")\n",
    "\n",
    "    # --- 4. Optimization (Using Adam) ---\n",
    "    INIT_PARAMS = {\n",
    "        'sigmasq': 25.0,\n",
    "        'range_lat': 0.8,\n",
    "        'range_lon': 1.2,\n",
    "        'nugget': 0.2 \n",
    "    }\n",
    "    print(f\"\\n--- Initial Parameters for Fitting ---\")\n",
    "    print(INIT_PARAMS)\n",
    "    \n",
    "    log_params_init = convert_to_log_reparam(\n",
    "        torch.tensor(INIT_PARAMS['sigmasq']),\n",
    "        torch.tensor(INIT_PARAMS['range_lat']),\n",
    "        torch.tensor(INIT_PARAMS['range_lon']),\n",
    "        torch.tensor(INIT_PARAMS['nugget'])\n",
    "    ).float()\n",
    "\n",
    "    params_list = [\n",
    "        Parameter(torch.tensor([val], dtype=torch.float32, device=DEVICE))\n",
    "        for val in log_params_init\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.Adam(params_list, lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_params_state = None\n",
    "\n",
    "    I_sample_dev = I_sample.to(DEVICE)\n",
    "    taper_autocorr_grid_dev = taper_autocorr_grid.to(DEVICE)\n",
    "\n",
    "    print(\"\\n--- Starting Adam Optimization (No Tapering) ---\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        optimizer.zero_grad()\n",
    "        params_tensor = torch.cat(params_list)\n",
    "        \n",
    "        loss = whittle_likelihood_loss_tapered(\n",
    "            params_tensor, I_sample_dev, N1, N2, P, \n",
    "            taper_autocorr_grid_dev\n",
    "        )\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS}: Loss is NaN/Inf. Stopping optimization.\")\n",
    "            if best_params_state is None:\n",
    "                best_params_state = [p.detach().clone() for p in params_list]\n",
    "            break\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(params_list, max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        current_loss_item = loss.item()\n",
    "        \n",
    "        if current_loss_item < best_loss:\n",
    "            params_valid = not any(torch.isnan(p.data).any() or torch.isinf(p.data).any() for p in params_list)\n",
    "            if params_valid:\n",
    "                best_loss = current_loss_item\n",
    "                best_params_state = [p.detach().clone() for p in params_list]\n",
    "\n",
    "        if epoch % 50 == 0 or epoch == EPOCHS - 1:\n",
    "            print(f'Epoch {epoch+1}/{EPOCHS} | Loss: {current_loss_item:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}')\n",
    "            if epoch % 200 == 0 or epoch == EPOCHS - 1:\n",
    "                 print(f'  Params: {get_printable_params(params_list)}')\n",
    "\n",
    "\n",
    "    print(\"\\n--- Adam Optimization Complete ---\")\n",
    "\n",
    "    # --- 5. Report Results ---\n",
    "    print(f\"\\n\\n{'='*25} FINAL SIMULATION RESULTS {'='*25}\")\n",
    "    print(f\"Grid Size: {N1} lats x {N2} lons ({N1*N2} points)\")\n",
    "    print(\"Method: Whittle Likelihood (No Tapering)\")\n",
    "    \n",
    "    print(\"\\n--- True Parameters (Natural Scale) ---\")\n",
    "    for key, val in TRUE_PARAMS.items():\n",
    "        print(f\"  {key:<10}: {val:.4f}\")\n",
    "        \n",
    "    print(\"\\n--- True Reparameterized Parameters ---\")\n",
    "    print(f\"  {get_reparam_params(true_log_params_list)}\")\n",
    "        \n",
    "    print(\"\\n--- Estimated Parameters (Natural Scale) ---\")\n",
    "    if best_params_state:\n",
    "        final_params_str = get_printable_params(best_params_state)\n",
    "        print(f\"  Final Loss: {best_loss:.4f}\")\n",
    "        print(f\"  Final Params: {final_params_str}\")\n",
    "    else:\n",
    "        print(\"Optimization failed to produce valid results.\")\n",
    "\n",
    "    # --- ðŸ’¥ NEW: Print estimated reparam params ðŸ’¥ ---\n",
    "    print(\"\\n--- Estimated Reparameterized Parameters ---\")\n",
    "    if best_params_state:\n",
    "        final_reparam_str = get_reparam_params(best_params_state)\n",
    "        print(f\"  Final Reparam Params: {final_reparam_str}\")\n",
    "    else:\n",
    "        print(\"Optimization failed to produce valid results.\")\n",
    "    # --- END NEW ---\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
