{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27600a5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (__init__.py, line 25)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[36m(most recent call last)\u001b[39m:\n",
      "  File \u001b[92m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3670\u001b[39m in \u001b[95mrun_code\u001b[39m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[36m  \u001b[39m\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mimport GEMS_TCO\u001b[39m\n",
      "  \u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/__init__.py:25\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m>>>>>>> 98fc71c474ddced6792e89e9ab27c07529da5b48\u001b[39m\n             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy                    # clone tensor\n",
    "import time\n",
    "\n",
    "# Custom imports\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels\n",
    "\n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "import typer\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO import configuration as config\n",
    "from GEMS_TCO import data_preprocess as dmbh\n",
    "\n",
    "import os\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a74c5d1",
   "metadata": {},
   "source": [
    "### Full Lieklihood using raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58409ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load data from: /Users/joonwonlee/Documents/GEMS_DATA/pickle_2024/coarse_cen_map_without_decrement_latitude24_07.pkl\n",
      "\n",
      "Data loaded successfully! ✅\n",
      "Type of loaded data: <class 'dict'>\n",
      "Number of entries (hours) in the map: 248\n",
      "Example keys: ['y24m07day01_hm00:53', 'y24m07day01_hm01:53', 'y24m07day01_hm02:53', 'y24m07day01_hm03:53', 'y24m07day01_hm04:49']\n",
      "270\n",
      "273\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "# Assume your 'config' object is available\n",
    "# import config\n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# Specify the year and month you want to load\n",
    "YEAR_TO_LOAD = 2024\n",
    "MONTH_TO_LOAD = 7\n",
    "\n",
    "# Use the same base path as your saving script\n",
    "BASE_PATH = config.mac_data_load_path\n",
    "\n",
    "# --- 2. Construct the File Path ---\n",
    "# This must exactly match the naming convention from your saving script\n",
    "month_str = f\"{MONTH_TO_LOAD:02d}\"\n",
    "pickle_path = os.path.join(BASE_PATH, f'pickle_{YEAR_TO_LOAD}')\n",
    "filename = f\"coarse_cen_map_without_decrement_latitude{str(YEAR_TO_LOAD)[2:]}_{month_str}.pkl\"\n",
    "filepath_to_load = os.path.join(pickle_path, filename)\n",
    "\n",
    "print(f\"Attempting to load data from: {filepath_to_load}\")\n",
    "\n",
    "# --- 3. Load the Data ---\n",
    "try:\n",
    "    with open(filepath_to_load, 'rb') as pickle_file:\n",
    "        # Use pickle.load() to read the data from the file\n",
    "        loaded_coarse_map = pickle.load(pickle_file)\n",
    "    \n",
    "    print(\"\\nData loaded successfully! ✅\")\n",
    "    \n",
    "    # --- 4. Verify the Loaded Data ---\n",
    "    # The loaded data is a dictionary. Let's inspect it.\n",
    "    print(f\"Type of loaded data: {type(loaded_coarse_map)}\")\n",
    "    if isinstance(loaded_coarse_map, dict):\n",
    "        print(f\"Number of entries (hours) in the map: {len(loaded_coarse_map)}\")\n",
    "        # Print the first 5 keys to see what they look like\n",
    "        first_five_keys = list(loaded_coarse_map.keys())[:5]\n",
    "        print(f\"Example keys: {first_five_keys}\")\n",
    "        \n",
    "        # You can now access the data for a specific hour, for example:\n",
    "        # first_hour_data = loaded_coarse_map[first_five_keys[0]]\n",
    "        # print(f\"\\nData for first hour is a tensor of shape: {first_hour_data.shape}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nError: File not found. Please check if the file exists at the specified path.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")\n",
    "\n",
    "\n",
    "print(loaded_coarse_map['y24m07day01_hm00:53']['Longitude'].nunique())\n",
    "print(loaded_coarse_map['y24m07day01_hm00:53']['Latitude'].nunique())\n",
    "\n",
    "import GEMS_TCO\n",
    "load_data_instance = GEMS_TCO.load_data('')\n",
    "\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "for i in range(31):\n",
    "    cur_map, cur_df =load_data_instance.load_working_data_byday_wo_mm(loaded_coarse_map,[i*8, (i+1)*8])\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c8a5cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will look at 2 hours of data \n",
      "Original data shape: torch.Size([147420, 4])\n",
      "Subset data shape:   torch.Size([36252, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from typing import Callable\n",
    "\n",
    "# =========================================================================\n",
    "# 1. Helper Function for Subsetting\n",
    "# =========================================================================\n",
    "\n",
    "def subset_by_area(input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Subsets a tensor to the specified lat/lon area.\n",
    "    Latitude between 0 and 5.\n",
    "    Longitude between 123 and 133.\n",
    "    \"\"\"\n",
    "    # Assumes columns are [lat, lon, value, time]\n",
    "    lat_col, lon_col = 0, 1\n",
    "    lat_mask = (input_tensor[:, lat_col] >= 0) & (input_tensor[:, lat_col] <= 5)\n",
    "    lon_mask = (input_tensor[:, lon_col] >= 123) & (input_tensor[:, lon_col] <= 133)\n",
    "    \n",
    "    df_sub = input_tensor[lat_mask & lon_mask].clone()\n",
    "    return df_sub\n",
    "\n",
    "a = df_day_aggregated_list[0].shape[0]/8\n",
    "hour_n = 2\n",
    "print(f'will look at {hour_n} hours of data ')\n",
    "a = int(a*hour_n)\n",
    "# Use the first tensor from your data list\n",
    "raw_data = df_day_aggregated_list[0][:a].to(torch.float64) # Ensure data is float64 for precision\n",
    "\n",
    "# --- 2. Subset the data to the desired area ---\n",
    "print(f\"Original data shape: {raw_data.shape}\")\n",
    "subset_data = subset_by_area(raw_data)\n",
    "print(f\"Subset data shape:   {subset_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4ae4a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# 2. Covariance and Likelihood Functions\n",
    "# =========================================================================\n",
    "\n",
    "def custom_distance_matrix(U: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Efficient distance computation with broadcasting.\"\"\"\n",
    "    spatial_diff = torch.norm(U[:, :2].unsqueeze(1) - V[:, :2].unsqueeze(0), dim=2)\n",
    "    temporal_diff = torch.abs(U[:, 2].unsqueeze(1) - V[:, 2].unsqueeze(0))\n",
    "    distance = (spatial_diff**2 + temporal_diff**2)\n",
    "    return distance\n",
    "\n",
    "def precompute_coords_anisotropy(params: torch.Tensor, y_data: torch.Tensor, x_data: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Pre-computes transformed coordinates and the distance matrix.\"\"\"\n",
    "    sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "\n",
    "    if y_data is None or x_data is None:\n",
    "        raise ValueError(\"Both y_data and x_data must be provided.\")\n",
    "\n",
    "    # Assumes columns are [lat, lon, value, time]\n",
    "    x1, y1, t1 = x_data[:, 0], x_data[:, 1], x_data[:, 3]\n",
    "    x2, y2, t2 = y_data[:, 0], y_data[:, 1], y_data[:, 3]\n",
    "\n",
    "    spat_coord1 = torch.stack(((x1 - advec_lat * t1) / range_lat, (y1 - advec_lon * t1) / range_lon), dim=-1)\n",
    "    spat_coord2 = torch.stack(((x2 - advec_lat * t2) / range_lat, (y2 - advec_lon * t2) / range_lon), dim=-1)\n",
    "\n",
    "    U = torch.cat((spat_coord1, (beta * t1).reshape(-1, 1)), dim=1)\n",
    "    V = torch.cat((spat_coord2, (beta * t2).reshape(-1, 1)), dim=1)\n",
    "\n",
    "    distance = custom_distance_matrix(U, V)\n",
    "    return distance\n",
    "\n",
    "def matern_cov_anisotropy_v05(params: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Computes the Matérn covariance with v=0.5 (exponential).\"\"\"\n",
    "    sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "    \n",
    "    distance = precompute_coords_anisotropy(params, x, y)\n",
    "    out = torch.zeros_like(distance)\n",
    "\n",
    "    non_zero_indices = distance != 0\n",
    "    if torch.any(non_zero_indices):\n",
    "        out[non_zero_indices] = sigmasq * torch.exp(-torch.sqrt(distance[non_zero_indices]))\n",
    "    out[~non_zero_indices] = sigmasq\n",
    "\n",
    "    # Add nugget/jitter only to the diagonal of the main covariance matrix\n",
    "    if torch.equal(x, y):\n",
    "        out += torch.eye(out.shape[0], dtype=out.dtype) * nugget \n",
    "    return out\n",
    "           \n",
    "def full_likelihood(params: torch.Tensor, input_data: torch.Tensor, response: torch.Tensor, covariance_function: Callable) -> torch.Tensor:\n",
    "    \"\"\"Calculates the full Gaussian negative log-likelihood.\"\"\"\n",
    "    cov_matrix = covariance_function(params=params, y=input_data, x=input_data)\n",
    "    sign, log_det = torch.slogdet(cov_matrix)\n",
    "\n",
    "    # The design matrix 'X' in GLM, here just the spatial locations\n",
    "    locs = input_data[:, :2]\n",
    "\n",
    "    # Compute beta (trend coefficients)\n",
    "    tmp1 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, locs))\n",
    "    tmp2 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, response))\n",
    "    beta_coeffs = torch.linalg.solve(tmp1, tmp2)\n",
    "\n",
    "    # Compute the mean and residuals\n",
    "    mu = torch.matmul(locs, beta_coeffs)\n",
    "    y_mu = response - mu\n",
    "\n",
    "    # Compute the quadratic form\n",
    "    quad_form = torch.matmul(y_mu, torch.linalg.solve(cov_matrix, y_mu))\n",
    "\n",
    "    # Compute the negative log likelihood\n",
    "    neg_log_lik = 0.5 * (log_det + quad_form)\n",
    "    return neg_log_lik \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a565a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: torch.Size([73710, 4])\n",
      "Subset data shape:   torch.Size([18126, 4])\n",
      "\n",
      "Calculated Negative Log Likelihood: 23583.9029\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# 3. Main Execution Block\n",
    "# =========================================================================\n",
    "if __name__ == '__main__':\n",
    "    # --- 1. Define your parameters and load your data ---\n",
    "    \n",
    "    # Example parameters (on their natural scale)\n",
    "    #a = [25.0, 3.0, 4.0, 0.02, -0.08, 0.02, 3.01] # 24125. 50580\n",
    "    #a = [20.89, 1.04, 1.337, 0.040, -0.178, 0.195, 4.498] # 47385\n",
    "    a = [28.6847, 0.9147, 5.0289, 0.1551, 0.6344, 0.0, 4.1061] # 24608. 62513\n",
    "    a = [12.9046, 6.2184, 4.3523, -0.0724, -0.2941, -0.262, 5.4445]\n",
    "    a = [5.3553, 2.9051, 3.4829, -0.4144, -0.2813, -0.2952, 4.2593]\n",
    "    \n",
    "    a = [4.4833, 3.1014, 3.7344, -0.6867, 0.08, 0.2891, 4.8089]\n",
    "    #a = [19.89, 1.04, 1.337, 0.04, -0.178, 0.195, 4.498]\n",
    "    a = [24.7512, 1.0101, 1.5151, -0.03, -0.06, 0.03, 2.0201] # large data\n",
    "    a = [24.7512, 1.0101, 1.5151, -0.03, -0.06, 0.03, 2.0201] # small data\n",
    "    a = [9.442, 2.5688, 3.8532, -0.0, -0.0012, 0.0, 0.7356] # inde 3 components\n",
    "    a = [48.9713, 0.4105, 0.6803, 0.0, -0.49, 0.0, 2.0]\n",
    "    a = [300.6555, 0.4139, 0.4649, -1.2799, -0.1826, 1.0906, 2.0] #v1.5\n",
    "    a = [34.1987, 0.6579, 1.9012, 0.002, 0.2631, 0.0003, 1.6811]\n",
    "    a = [23.9528, 1.0246, 1.73, 0.244, 0.1445, 0.0, 1.7276]\n",
    "    a = [30.2594, 0.665, 1.8981, 0.0, 0.1317, -0.0, 1.9785]\n",
    "    a = [45.1402, 0.6299, 0.7308, -0.0003, -0.0151, 0.0, 7.8922]\n",
    "    a = [24.214204843325692, 0.7605639325477755, 1.3559509025375993, 0.04455842873488032, -0.14632970126172773, 0.16969297486942467, 3.8711802967371067]\n",
    "    # above 23536\n",
    "    a = [33.5371, 0.5245, 2.3767, -0.0, 0.2104, -0.0, 1.638] # 24650 78378\n",
    "\n",
    "    a = [22.892229690601923, 0.798430619739265, 1.9505592698122576, 5.977196685598199e-06, 4.443502918276306e-06, 0.0, 1.8082124041763596]\n",
    "    # above 24129\n",
    "    a =[ 29.7507942474575, 0.9895617352853928, 1.0668105024738395, 0.03646879682204495, -0.15565418931819383, 0.17944566056749772, 1.8907510307884106]\n",
    "    # above 23051. from 5000 vecchia    # 2 45718\n",
    "\n",
    "    a = [29.89872140748479, 1.1529714369768411, 1.7862356661866714, 0.03927727761515986, -0.15656505052873793, 0.1320573870050866, 4.130349222670235]\n",
    "    # above 23583  from 1250 vecchia\n",
    "    \n",
    "    #a = [29.776, 0.6619, 0.7052, 0.0, 0.0, 0.0, 1.3839]. # 22965. \n",
    "    params = torch.tensor(a, dtype=torch.float64)\n",
    "\n",
    "    # ⚠️ ASSUMPTION: 'df_day_aggregated_list' is loaded and available here.\n",
    "    # For example:\n",
    "    # with open(\"path_to_your_data.pkl\", 'rb') as f:\n",
    "    #     df_day_aggregated_list = pickle.load(f)\n",
    "    \n",
    "    a = df_day_aggregated_list[0].shape[0]/8\n",
    "    hour_n = 1\n",
    "    a = int(a*hour_n)\n",
    "    # Use the first tensor from your data list\n",
    "    raw_data = df_day_aggregated_list[0][:a].to(torch.float64) # Ensure data is float64 for precision\n",
    "\n",
    "    # --- 2. Subset the data to the desired area ---\n",
    "    print(f\"Original data shape: {raw_data.shape}\")\n",
    "    subset_data = subset_by_area(raw_data)\n",
    "    print(f\"Subset data shape:   {subset_data.shape}\")\n",
    "\n",
    "    # --- 3. Calculate the full likelihood on the subset ---\n",
    "    if subset_data.shape[0] > 0:\n",
    "        # The 'response' is the ozone column (index 2) of the subsetted data\n",
    "        response_y = subset_data[:, 2]\n",
    "\n",
    "        neg_log_lik_result = full_likelihood(\n",
    "            params=params, \n",
    "            input_data=subset_data, \n",
    "            response=response_y, \n",
    "            covariance_function=matern_cov_anisotropy_v05\n",
    "        )\n",
    "\n",
    "        print(f\"\\nCalculated Negative Log Likelihood: {neg_log_lik_result.item():.4f}\")\n",
    "    else:\n",
    "        print(\"\\nNo data points found in the specified area.\")\n",
    "\n",
    "# 24125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f5affb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of data 31.390108108520508\n",
      "Estimated SQ + Nugget = 25.388\n",
      "\n",
      "\n",
      "East to West wind speed 7.3379 (m/s) \n",
      "North to South Wind speed  1.2827 (m/s)\n",
      "\n",
      "\n",
      "East to West wind speed  0.0335 (m/s)\n"
     ]
    }
   ],
   "source": [
    "print(f'Variance of data {torch.var(df_day_aggregated_list[0][:,2])}')\n",
    "print(f'Estimated SQ + Nugget = {20.89+4.498}')\n",
    "\n",
    "\n",
    "\n",
    "print(f'\\n\\nEast to West wind speed { round(1.337*0.178*111/3600 *1000,4)} (m/s) ')\n",
    "print(f'North to South Wind speed  { round(1.04*0.04*111/3600*1000,4)} (m/s)')\n",
    "\n",
    "print(f'\\n\\nEast to West wind speed  { round(0.072*0.0151*111/3600*1000,4)} (m/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7902257d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.6874009"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.43*-0.155988*111/3600*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec2a890",
   "metadata": {},
   "source": [
    "# Full Likelihood\n",
    "\n",
    "Vecchia Parameters Used (raw data without differencing, but demeaned, regulargrid by center matching) \n",
    "```[21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]``` \n",
    "### 1 hour of data 18126x1\n",
    "Calculated Negative Log Likelihood: ```23849.8334``\n",
    "if 21.303 --> 20.303:\n",
    "Calculated Negative Log Likelihood: ```23857.1187```\n",
    "\n",
    "### 2 hours of data 18126x2\n",
    "Calculated Negative Log Likelihood: ```47732.5073```.  ## 47732 ~= 2*238550, so it scales well\n",
    "\n",
    "\n",
    "Whittle Parameters Used (Once differenced in space [[-2 1][1 0 ]]) + hamming tapering \n",
    "```[31.2594, 0.665, 1.8981, 0.0, 0.1317, -0.0, 1.9785]```\n",
    "\n",
    "### 1 hour of data 18126x1\n",
    "Calculated Negative Log Likelihood: ```23642```\n",
    "if 31.2594 --> 30.2594:\n",
    "Calculated Negative Log Likelihood: ```23669```\n",
    "\n",
    "### 2 hours of data 18126x2\n",
    "Calculated Negative Log Likelihood: ```70664``` ## 70664> 23642*2, does not scale well\n",
    "\n",
    "\n",
    "Whittle Parameters Used (Once differenced in space [[-2 1][1 0 ]]) + once difference in time (two stage differencing)\n",
    "```[45.1402, 0.6299, 0.7308, -0.0003, -0.0151, 0.0, 7.8922]```\n",
    "\n",
    "### 1 hour of data 18126x1\n",
    "Calculated Negative Log Likelihood: ```26766```\n",
    "if 45.7499-->44.7499:\n",
    "Calculated Negative Log Likelihood: ```26726``` ## nll actually dereased? when the variance parameter is 1 unit away from local optimal\n",
    "\n",
    "### 2 hours of data 18126x2\n",
    "Calculated Negative Log Likelihood: ```56130``` ## difference in time seems to help capturing temporal structure\n",
    "\n",
    "\n",
    "\n",
    "# Whittle Likelihood on 8 hours of data\n",
    "\n",
    "-> Whittle Likelihood ('Vecchia Optimized' Params): \n",
    "```[21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]```, negative log likelihood:   ```58326.332```\n",
    "if 21.303 --> 20.303    ```57998.410```\n",
    "\n",
    "-> Whittle Likelihood ('Whittle Optimized' Params): \n",
    "```[31.2594, 0.665, 1.8981, 0.0, 0.1317, -0.0, 1.9785]```  , negative log likelihood:  ```41604.566```\n",
    "if 31.2594 --> 30.2594    ```41623.910```\n",
    "\n",
    "\n",
    "# Vecchia Likelihood on 8 hours of data\n",
    "\n",
    "-> Vecchia Likelihood ('Vecchia Optimized' Params): \n",
    "```[21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]```, negative log likelihood:   ```64351```\n",
    "if 21.303 --> 20.303    ```64384```\n",
    "\n",
    "-> Vecchia Likelihood ('Whittle Optimized' Params): \n",
    "```[31.2594, 0.665, 1.8981, 0.0, 0.1317, -0.0, 1.9785]```  , negative log likelihood:  ```71865```\n",
    "if 31.2594 --> 30.2594    ```72132```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b581b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0aabc5d",
   "metadata": {},
   "source": [
    "### Full likelihood using spatially differenced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d348f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/joonwonlee/Documents/GEMS_DATA/pickle_2024/coarse_cen_map_without_decrement_latitude24_07.pkl\n",
      "Loaded 31 days of raw data.\n",
      "\n",
      "--- Results ---\n",
      "Number of final spatially-differenced day tensors: 31\n",
      "Processed data saved to spatial_first_difference_data.pkl\n",
      "\n",
      "Shape of the first final tensor: torch.Size([142832, 4])\n",
      "First final tensor head:\n",
      "tensor([[ 4.0000e-03,  1.2303e+02,  2.9422e+00,  2.1000e+01],\n",
      "        [ 4.0000e-03,  1.2309e+02,  1.9636e+00,  2.1000e+01],\n",
      "        [ 4.0000e-03,  1.2316e+02, -1.3187e+00,  2.1000e+01],\n",
      "        [ 4.0000e-03,  1.2322e+02, -3.1683e+00,  2.1000e+01],\n",
      "        [ 4.0000e-03,  1.2328e+02, -5.4922e-01,  2.1000e+01]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Assume GEMS_TCO is a custom class/module you have available\n",
    "# from your_project import GEMS_TCO\n",
    "\n",
    "# =========================================================================\n",
    "# 1. Helper Functions\n",
    "# =========================================================================\n",
    "\n",
    "def subset_tensor(df_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Subsets a tensor to a specific lat/lon range.\"\"\"\n",
    "    #lat_mask = (df_tensor[:, 0] >= -5) & (df_tensor[:, 0] <= 6.3)\n",
    "    #lon_mask = (df_tensor[:, 1] >= 118) & (df_tensor[:, 1] <= 134.2)\n",
    "    lat_mask = (df_tensor[:, 0] >= 0) & (df_tensor[:, 0] <= 5)\n",
    "    lon_mask = (df_tensor[:, 1] >= 123) & (df_tensor[:, 1] <= 133)\n",
    "\n",
    "    df_sub = df_tensor[lat_mask & lon_mask].clone()\n",
    "    return df_sub\n",
    "\n",
    "def apply_first_difference_2d_tensor(df_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies a 2D first-order difference filter using convolution.\n",
    "    This approximates Z(s) = [X(s+d_lat) - X(s)] + [X(s+d_lon) - X(s)].\n",
    "    \"\"\"\n",
    "    if df_tensor.size(0) == 0:\n",
    "        return torch.empty(0, 4)\n",
    "\n",
    "    # 1. Get grid dimensions and validate\n",
    "    unique_lats = torch.unique(df_tensor[:, 0])\n",
    "    unique_lons = torch.unique(df_tensor[:, 1])\n",
    "    lat_count, lon_count = unique_lats.size(0), unique_lons.size(0)\n",
    "\n",
    "    if df_tensor.size(0) != lat_count * lon_count:\n",
    "        raise ValueError(\"Tensor size does not match grid dimensions. Must be a complete grid.\")\n",
    "    if lat_count < 2 or lon_count < 2:\n",
    "        return torch.empty(0, 4)\n",
    "\n",
    "    # 2. Reshape data and define the correct kernel\n",
    "    ozone_data = df_tensor[:, 2].reshape(1, 1, lat_count, lon_count)\n",
    "    \n",
    "    # ✅ CORRECT KERNEL: This kernel results in the standard first-order difference:\n",
    "    # Z(i,j) = X(i+1,j) + X(i,j+1) - 2*X(i,j)\n",
    "    # Note: F.conv2d in PyTorch actually performs cross-correlation. To get a true\n",
    "    # convolution result, the kernel would need to be flipped. However, for a \n",
    "    # forward difference operator, defining the kernel for cross-correlation is more direct.\n",
    "    # The kernel below is designed for cross-correlation to achieve the desired differencing.\n",
    "    diff_kernel = torch.tensor([[[[-2., 1.],\n",
    "                                  [ 1., 0.]]]], dtype=torch.float32)\n",
    "\n",
    "    # 3. Apply convolution (which acts as cross-correlation)\n",
    "    filtered_grid = F.conv2d(ozone_data, diff_kernel, padding='valid').squeeze()\n",
    "\n",
    "    # 4. Determine coordinates for the new, smaller grid\n",
    "    # The new grid corresponds to the anchor points of the kernel\n",
    "    new_lats = unique_lats[:-1]\n",
    "    new_lons = unique_lons[:-1]\n",
    "\n",
    "    # 5. Reconstruct the output tensor\n",
    "    new_lat_grid, new_lon_grid = torch.meshgrid(new_lats, new_lons, indexing='ij')\n",
    "    filtered_values = filtered_grid.flatten()\n",
    "    time_value = df_tensor[0, 3].repeat(filtered_values.size(0))\n",
    "\n",
    "    new_tensor = torch.stack([\n",
    "        new_lat_grid.flatten(),\n",
    "        new_lon_grid.flatten(),\n",
    "        filtered_values,\n",
    "        time_value\n",
    "    ], dim=1)\n",
    "    \n",
    "    return new_tensor\n",
    "\n",
    "# =========================================================================\n",
    "# 2. Data Loading (Unchanged)\n",
    "# =========================================================================\n",
    "# ⚠️ NOTE: You must define these variables\n",
    "# mac_data_path = \"...\"\n",
    "# year = 2022\n",
    "# month_str = \"01\"\n",
    "# class GEMS_TCO: # Placeholder\n",
    "#     def load_data(self, path): return self\n",
    "#     def load_working_data_byday_wo_mm(self, data, indices):\n",
    "#         return {'key': torch.randn(100, 4)}, torch.randn(100, 4)\n",
    "mac_data_path = config.mac_data_load_path\n",
    "year = 2024\n",
    "pickle_path = os.path.join(mac_data_path, f'pickle_{year}')\n",
    "output_filename = f\"coarse_cen_map_without_decrement_latitude{str(year)[2:]}_{month_str}.pkl\"\n",
    "output_filepath = os.path.join(pickle_path, output_filename)\n",
    "print(f\"Loading data from: {output_filepath}\")\n",
    "with open(output_filepath, 'rb') as pickle_file:\n",
    "    cbmap_ori = pickle.load(pickle_file)\n",
    "\n",
    "load_data_instance = GEMS_TCO.load_data('')\n",
    "df_day_map_list = []\n",
    "for i in range(31): # Adjust if necessary\n",
    "    cur_map, _ = load_data_instance.load_working_data_byday_wo_mm(cbmap_ori, [i*8, (i+1)*8])\n",
    "    df_day_map_list.append(cur_map)\n",
    "print(f\"Loaded {len(df_day_map_list)} days of raw data.\")\n",
    "\n",
    "# =========================================================================\n",
    "# 3. Main Processing Loop (Unchanged)\n",
    "# =========================================================================\n",
    "spatially_filtered_days = []\n",
    "for day_idx, day_map in enumerate(df_day_map_list):\n",
    "    tensors_to_aggregate = []\n",
    "    for key, tensor in day_map.items():\n",
    "        subsetted = subset_tensor(tensor)\n",
    "        if subsetted.size(0) > 0:\n",
    "            try:\n",
    "                diff_applied = apply_first_difference_2d_tensor(subsetted)\n",
    "                if diff_applied.size(0) > 0:\n",
    "                    tensors_to_aggregate.append(diff_applied)\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping data chunk on day {day_idx+1} due to error: {e}\")\n",
    "\n",
    "    if tensors_to_aggregate:\n",
    "        aggregated_day_tensor = torch.cat(tensors_to_aggregate, dim=0)\n",
    "        spatially_filtered_days.append(aggregated_day_tensor)\n",
    "\n",
    "# =========================================================================\n",
    "# 4. Verification (Unchanged)\n",
    "# =========================================================================\n",
    "print(\"\\n--- Results ---\")\n",
    "print(f\"Number of final spatially-differenced day tensors: {len(spatially_filtered_days)}\")\n",
    "if spatially_filtered_days:\n",
    "    # Save the processed data for the next script\n",
    "    processed_output_path = \"spatial_first_difference_data.pkl\"\n",
    "    with open(processed_output_path, 'wb') as f:\n",
    "        pickle.dump(spatially_filtered_days, f)\n",
    "    print(f\"Processed data saved to {processed_output_path}\")\n",
    "\n",
    "    print(f\"\\nShape of the first final tensor: {spatially_filtered_days[0].shape}\")\n",
    "    print(\"First final tensor head:\")\n",
    "    print(spatially_filtered_days[0][:5])\n",
    "else:\n",
    "    print(\"\\nNo final differenced tensors were created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bccbf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:70: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:70: SyntaxWarning: invalid escape sequence '\\m'\n",
      "/var/folders/9p/53hd4c7d2fl193h4jwp194wc0000gn/T/ipykernel_92958/1514199930.py:70: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  print(f\"\\n🛑 WARNING: The current data size ({N} points) will be extremely slow for NLL calculation ($\\mathcal{{O}}(N^3)$).\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data from: spatial_first_difference_data.pkl\n",
      "Total points after spatial subsetting: 142832\n",
      "Using time indices: [21.0]\n",
      "Final subset data shape for likelihood (time-limited): torch.Size([17854, 4])\n",
      "\n",
      "🛑 WARNING: The current data size (17854 points) will be extremely slow for NLL calculation ($\\mathcal{O}(N^3)$).\n",
      "         The previous limit of 1000 was for performance. Proceeding may take a long time or fail due to memory.\n",
      "\n",
      "Calculated Negative Log Likelihood: 64440.0133\n",
      "Parameters used: [11.0973, 3.6585, 4.6663, 0.3542, -0.7808, -0.0019, 3.0497]\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# 3. Main Execution Block (Adjusted to use only the first two hours)\n",
    "# =========================================================================\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # --- 1. Load the processed data ---\n",
    "    processed_output_path = \"spatial_first_difference_data.pkl\"\n",
    "    print(f\"Loading processed data from: {processed_output_path}\")\n",
    "\n",
    "    try:\n",
    "        with open(processed_output_path, 'rb') as f:\n",
    "            # 💡 Loading your spatially differenced data\n",
    "            spatially_filtered_days = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Processed data file not found at {processed_output_path}. Ensure the differencing script ran and saved the data.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading data: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # --- 2. Define parameters and select data subset ---\n",
    "    \n",
    "    # Parameters: [sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget]\n",
    "    # Using the new parameters provided in your prompt\n",
    "    a = [19.89, 1.04, 1.337, 0.040, -0.178, 0.195, 4.498] # 47385\n",
    "    #a = [28.6847, 0.9147, 5.0289, 0.1551, 0.6344, 0.0, 4.1061] \n",
    "    a = [12.9046, 6.2184, 4.3523, -0.0724, -0.2941, -0.262, 5.4445]\n",
    "    a = [20.7046, 1.952, 4.1366, -1.0769, -0.3244, 0.1074, 3.9057]\n",
    "    a = [11.0973, 3.6585, 4.6663, 0.3542, -0.7808, -0.0019, 3.0497]\n",
    "    params = torch.tensor(a, dtype=torch.float64)\n",
    "    \n",
    "    # Select the first day's differenced tensor\n",
    "    raw_data_day1 = spatially_filtered_days[0].to(torch.float64) \n",
    "    \n",
    "    # --- 3. Subset by Area and Time ---\n",
    "    \n",
    "    # Apply the spatial area subset first\n",
    "    subset_data_area = subset_by_area(raw_data_day1)\n",
    "    \n",
    "    print(f\"Total points after spatial subsetting: {subset_data_area.shape[0]}\")\n",
    "\n",
    "    # --- Time-based Filtering (First Two Hours) ---\n",
    "    # The 'time' column is the 4th column (index 3).\n",
    "    # Since the original data was loaded in 8-hour chunks per day, and the time indices \n",
    "    # typically represent the chunk number or the time within the day, let's examine \n",
    "    # the time indices in the subsetted data.\n",
    "    \n",
    "    # Find the unique time indices in the subsetted data\n",
    "    unique_times = torch.unique(subset_data_area[:, 3], sorted=True)\n",
    "    hour_n = 1\n",
    "    if len(unique_times) >= hour_n:\n",
    "        # Select the first two unique time indices\n",
    "        time_limit = unique_times[: hour_n]\n",
    "        \n",
    "        # Create a mask for data points where the time index matches one of the first two times\n",
    "        time_mask = torch.isin(subset_data_area[:, 3], time_limit)\n",
    "        subset_data = subset_data_area[time_mask]\n",
    "        \n",
    "        print(f\"Using time indices: {time_limit.tolist()}\")\n",
    "    else:\n",
    "        # Fallback if there aren't two unique time indices\n",
    "        print(\"Warning: Less than two unique time indices available. Using all data after spatial subset.\")\n",
    "        subset_data = subset_data_area\n",
    "        \n",
    "    print(f\"Final subset data shape for likelihood (time-limited): {subset_data.shape}\")\n",
    "\n",
    "    # --- 4. Calculate the full likelihood ---\n",
    "    N = subset_data.shape[0]\n",
    "    if N > 2000:\n",
    "        print(f\"\\n🛑 WARNING: The current data size ({N} points) will be extremely slow for NLL calculation ($\\mathcal{{O}}(N^3)$).\")\n",
    "        print(\"         The previous limit of 1000 was for performance. Proceeding may take a long time or fail due to memory.\")\n",
    "        # Proceed with caution or add a user confirmation/exit here\n",
    "        \n",
    "    if N > 3: \n",
    "        # The 'response' is the differenced ozone column (index 2)\n",
    "        response_y = subset_data[:, 2]\n",
    "\n",
    "        neg_log_lik_result = full_likelihood(\n",
    "            params=params, \n",
    "            input_data=subset_data, \n",
    "            response=response_y, \n",
    "            covariance_function=matern_cov_anisotropy_v05\n",
    "        )\n",
    "\n",
    "        print(f\"\\nCalculated Negative Log Likelihood: {neg_log_lik_result.item():.4f}\")\n",
    "        print(f\"Parameters used: {a}\")\n",
    "    else:\n",
    "        print(\"\\nNot enough data points found in the specified area after filtering/sampling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6468e8",
   "metadata": {},
   "source": [
    "Parameters used: [28.6847, 0.9147, 5.0289, 0.1551, 0.6344, 0.0, 4.1061]\n",
    "### 1\n",
    "Calculated Negative Log Likelihood: 51301.4845\n",
    "\n",
    "### 2\n",
    "Calculated Negative Log Likelihood: 100719.2866\n",
    "\n",
    "Parameters used: [27.6847, 0.9147, 5.0289, 0.1551, 0.6344, 0.0, 4.1061]\n",
    "### 1\n",
    "Calculated Negative Log Likelihood: 51382.0330\n",
    "\n",
    "\n",
    "###################################\n",
    "\n",
    "\n",
    "Parameters used: [20.89, 1.04, 1.337, 0.04, -0.178, 0.195, 4.498]\n",
    "### 1\n",
    "Calculated Negative Log Likelihood: 47928.9610\n",
    "\n",
    "Parameters used: [19.89, 1.04, 1.337, 0.04, -0.178, 0.195, 4.498]\n",
    "### 1\n",
    "Calculated Negative Log Likelihood: 48065.0109\n",
    "\n",
    "### 2\n",
    "Calculated Negative Log Likelihood: 93039.9350\n",
    "\n",
    "\n",
    "this is estimate from 3d once differencing filter using larger data set\n",
    "Parameters used: [12.9046, 6.2184, 4.3523, -0.0724, -0.2941, -0.262, 5.4445]\n",
    "### 1\n",
    "Calculated Negative Log Likelihood: 46520.4330\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca0628",
   "metadata": {},
   "source": [
    "# 3d first differenced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98a15e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================================================================\n",
    "# 1. Helper Function (UNCHANGED)\n",
    "# =========================================================================\n",
    "\n",
    "def subset_tensor(df_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Subsets a tensor to a specific lat/lon range.\n",
    "    Columns are assumed to be [lat, lon, ozone, time].\n",
    "    \"\"\"\n",
    "    lat_mask = (df_tensor[:, 0] >= 0) & (df_tensor[:, 0] <= 5)\n",
    "    lon_mask = (df_tensor[:, 1] >= 123) & (df_tensor[:, 1] <= 133)\n",
    "    return df_tensor[lat_mask & lon_mask].clone()\n",
    "\n",
    "# =========================================================================\n",
    "# 2. CORRECTED 3D Differencing Function\n",
    "# =========================================================================\n",
    "\n",
    "def apply_first_difference_3d(day_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies a 3D first-difference filter to calculate the gradient (rate of change)\n",
    "    simultaneously across time, latitude, and longitude.\n",
    "\n",
    "    Args:\n",
    "        day_tensor: A tensor for a single day with columns [lat, lon, ozone, time].\n",
    "\n",
    "    Returns:\n",
    "        A tensor with columns [lat, lon, time, grad_t, grad_lat, grad_lon].\n",
    "    \"\"\"\n",
    "    if day_tensor.numel() == 0:\n",
    "        return torch.empty(0, 6)\n",
    "\n",
    "    # 1. Map long-format data to a dense 3D grid\n",
    "    unique_lats = torch.unique(day_tensor[:, 0])\n",
    "    unique_lons = torch.unique(day_tensor[:, 1])\n",
    "    unique_times = torch.unique(day_tensor[:, 3])\n",
    "    \n",
    "    T, H, W = len(unique_times), len(unique_lats), len(unique_lons)\n",
    "    if T < 2 or H < 2 or W < 2:\n",
    "        return torch.empty(0, 6)\n",
    "\n",
    "    lat_map = {lat.item(): i for i, lat in enumerate(unique_lats)}\n",
    "    lon_map = {lon.item(): i for i, lon in enumerate(unique_lons)}\n",
    "    time_map = {time.item(): i for i, time in enumerate(unique_times)}\n",
    "\n",
    "    ozone_grid = torch.zeros(T, H, W, dtype=torch.float32)\n",
    "    for row in day_tensor:\n",
    "        lat, lon, ozone, time = row\n",
    "        t_idx, h_idx, w_idx = time_map[time.item()], lat_map[lat.item()], lon_map[lon.item()]\n",
    "        ozone_grid[t_idx, h_idx, w_idx] = ozone\n",
    "    \n",
    "    # Reshape for conv3d: (N, C_in, D, H, W) -> (1, 1, Time, Lat, Lon)\n",
    "    ozone_grid = ozone_grid.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # 2. Define 3D kernels for first difference Z(i) - Z(i-1) along each axis\n",
    "    kernel_t = torch.tensor([-1., 1.], dtype=torch.float32).reshape(1, 1, 2, 1, 1)   # D-axis (Time)\n",
    "    kernel_lat = torch.tensor([-1., 1.], dtype=torch.float32).reshape(1, 1, 1, 2, 1) # H-axis (Lat)\n",
    "    kernel_lon = torch.tensor([-1., 1.], dtype=torch.float32).reshape(1, 1, 1, 1, 2) # W-axis (Lon)\n",
    "\n",
    "    # 3. Apply 3D convolution to get gradient components\n",
    "    # Output shapes: grad_t (T-1, H, W), grad_lat (T, H-1, W), grad_lon (T, H, W-1)\n",
    "    grad_t = F.conv3d(ozone_grid, kernel_t, padding='valid').squeeze()\n",
    "    grad_lat = F.conv3d(ozone_grid, kernel_lat, padding='valid').squeeze()\n",
    "    grad_lon = F.conv3d(ozone_grid, kernel_lon, padding='valid').squeeze()\n",
    "\n",
    "    # 4. Align gradient grids to the common (T-1, H-1, W-1) shape\n",
    "    # This aligns the anchor point (Lat[i], Lon[j], Time[k]) for all three derivatives.\n",
    "    \n",
    "    # grad_t is (T-1, H, W). We slice H -> H-1 and W -> W-1\n",
    "    grad_t_common = grad_t[:, :-1, :-1] \n",
    "\n",
    "    # grad_lat is (T, H-1, W). We slice T -> T-1 and W -> W-1\n",
    "    grad_lat_common = grad_lat[:-1, :, :-1]\n",
    "\n",
    "    # grad_lon is (T, H, W-1). We slice T -> T-1 and H -> H-1\n",
    "    grad_lon_common = grad_lon[:-1, :-1, :]\n",
    "\n",
    "    # 5. Create new coordinate grids (T-1, H-1, W-1)\n",
    "    # The new coordinates correspond to the point *after* the difference (i.e., X(i)-X(i-1) is anchored at i)\n",
    "    new_times = unique_times[1:] # T-1 coordinates (e.g., hour 2 to 8)\n",
    "    new_lats = unique_lats[1:]   # H-1 coordinates\n",
    "    new_lons = unique_lons[1:]   # W-1 coordinates\n",
    "    \n",
    "    time_grid, lat_grid, lon_grid = torch.meshgrid(new_times, new_lats, new_lons, indexing='ij')\n",
    "\n",
    "    # 6. Flatten and stack\n",
    "    final_tensor = torch.stack([\n",
    "        lat_grid.flatten(),\n",
    "        lon_grid.flatten(),\n",
    "        time_grid.flatten(),\n",
    "        grad_t_common.flatten(),\n",
    "        grad_lat_common.flatten(),\n",
    "        grad_lon_common.flatten()\n",
    "    ], dim=1)\n",
    "    \n",
    "    return final_tensor\n",
    "\n",
    "# =========================================================================\n",
    "# 3. Main Processing Loop (UNCHANGED logic, requires df_day_map_list)\n",
    "# =========================================================================\n",
    "\n",
    "# Assuming df_day_map_list is pre-loaded and sorted by date\n",
    "all_gradient_tensors = []\n",
    "# NOTE: df_day_map_list must be defined and loaded for this loop to run correctly.\n",
    "# For demonstration purposes, we assume it is loaded as requested.\n",
    "# for day_map in df_day_map_list: \n",
    "#     # Aggregate all data for one day and apply the initial spatial subset\n",
    "#     tensors_for_day = [subset_tensor(t) for t in day_map.values() if t.numel() > 0]\n",
    "\n",
    "#     if not tensors_for_day:\n",
    "#         continue\n",
    "        \n",
    "#     full_day_tensor = torch.cat(tensors_for_day, dim=0)\n",
    "    \n",
    "#     # Apply the unified 3D differencing function to the day's data\n",
    "#     gradient_tensor = apply_first_difference_3d(full_day_tensor)\n",
    "    \n",
    "#     if gradient_tensor.size(0) > 0:\n",
    "#         all_gradient_tensors.append(gradient_tensor)\n",
    "\n",
    "\n",
    "# --- Verification ---\n",
    "# print(f\"Number of final gradient tensors (one per day): {len(all_gradient_tensors)}\")\n",
    "\n",
    "# if all_gradient_tensors:\n",
    "#     print(\"\\nShape of the first day's gradient tensor:\", all_gradient_tensors[0].shape)\n",
    "#     print(\"Columns: [lat, lon, time, grad_t, grad_lat, grad_lon]\")\n",
    "#     print(\"Head of the first gradient tensor:\")\n",
    "#     print(all_gradient_tensors[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b78daca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data from: all_gradient_tensors.pkl\n",
      "Error: Processed data file not found at all_gradient_tensors.pkl. Ensure the differencing script ran and saved the data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:71: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:71: SyntaxWarning: invalid escape sequence '\\m'\n",
      "/var/folders/9p/53hd4c7d2fl193h4jwp194wc0000gn/T/ipykernel_90986/3708915208.py:71: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  print(f\"\\n🛑 WARNING: The current data size ({N} points) will be extremely slow for NLL calculation ($\\mathcal{{O}}(N^3)$).\")\n",
      "/var/folders/9p/53hd4c7d2fl193h4jwp194wc0000gn/T/ipykernel_90986/3708915208.py:71: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  print(f\"\\n🛑 WARNING: The current data size ({N} points) will be extremely slow for NLL calculation ($\\mathcal{{O}}(N^3)$).\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'spatially_filtered_days' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m params = torch.tensor(a, dtype=torch.float64)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Select the first day's differenced tensor\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m raw_data_day1 = \u001b[43mspatially_filtered_days\u001b[49m[\u001b[32m0\u001b[39m].to(torch.float64) \n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# --- 3. Subset by Area and Time ---\u001b[39;00m\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Apply the spatial area subset first\u001b[39;00m\n\u001b[32m     39\u001b[39m subset_data_area = subset_by_area(raw_data_day1)\n",
      "\u001b[31mNameError\u001b[39m: name 'spatially_filtered_days' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# 3. Main Execution Block (Adjusted to use only the first two hours)\n",
    "# =========================================================================\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # --- 1. Load the processed data ---\n",
    "    # CHANGED: Using 'all_gradient_tensors.pkl' as requested.\n",
    "    processed_output_path = \"all_gradient_tensors.pkl\"\n",
    "    print(f\"Loading processed data from: {processed_output_path}\")\n",
    "\n",
    "    try:\n",
    "        with open(processed_output_path, 'rb') as f:\n",
    "            # 💡 Loading your spatially differenced data\n",
    "            spatially_filtered_days = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Processed data file not found at {processed_output_path}. Ensure the differencing script ran and saved the data.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading data: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # --- 2. Define parameters and select data subset ---\n",
    "    \n",
    "    # Parameters: [sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget]\n",
    "    # Using the new parameters provided in your prompt\n",
    "    a = [19.89, 1.04, 1.337, 0.040, -0.178, 0.195, 4.498] # 47385\n",
    "    #a = [28.6847, 0.9147, 5.0289, 0.1551, 0.6344, 0.0, 4.1061] \n",
    "    #a = [12.9046, 6.2184, 4.3523, -0.0724, -0.2941, -0.262, 5.4445]\n",
    "    #a = [20.7046, 1.952, 4.1366, -1.0769, -0.3244, 0.1074, 3.9057]\n",
    "    a = [5.2406, 1.4667, 1.3835, -0.5644, -0.6237, 0.5911, 0.0698]\n",
    "    params = torch.tensor(a, dtype=torch.float64)\n",
    "    \n",
    "    # Select the first day's differenced tensor\n",
    "    raw_data_day1 = spatially_filtered_days[0].to(torch.float64) \n",
    "    \n",
    "    # --- 3. Subset by Area and Time ---\n",
    "    \n",
    "    # Apply the spatial area subset first\n",
    "    subset_data_area = subset_by_area(raw_data_day1)\n",
    "    \n",
    "    print(f\"Total points after spatial subsetting: {subset_data_area.shape[0]}\")\n",
    "\n",
    "    # --- Time-based Filtering (First Two Hours) ---\n",
    "    # The 'time' column is the 4th column (index 3).\n",
    "    # Since the original data was loaded in 8-hour chunks per day, and the time indices \n",
    "    # typically represent the chunk number or the time within the day, let's examine \n",
    "    # the time indices in the subsetted data.\n",
    "    \n",
    "    # Find the unique time indices in the subsetted data\n",
    "    unique_times = torch.unique(subset_data_area[:, 3], sorted=True)\n",
    "    hour_n = 1\n",
    "    if len(unique_times) >= hour_n:\n",
    "        # Select the first two unique time indices\n",
    "        time_limit = unique_times[: hour_n]\n",
    "        \n",
    "        # Create a mask for data points where the time index matches one of the first two times\n",
    "        time_mask = torch.isin(subset_data_area[:, 3], time_limit)\n",
    "        subset_data = subset_data_area[time_mask]\n",
    "        \n",
    "        print(f\"Using time indices: {time_limit.tolist()}\")\n",
    "    else:\n",
    "        # Fallback if there aren't two unique time indices\n",
    "        print(\"Warning: Less than two unique time indices available. Using all data after spatial subset.\")\n",
    "        subset_data = subset_data_area\n",
    "        \n",
    "    print(f\"Final subset data shape for likelihood (time-limited): {subset_data.shape}\")\n",
    "\n",
    "    # --- 4. Calculate the full likelihood ---\n",
    "    N = subset_data.shape[0]\n",
    "    if N > 2000:\n",
    "        print(f\"\\n🛑 WARNING: The current data size ({N} points) will be extremely slow for NLL calculation ($\\mathcal{{O}}(N^3)$).\")\n",
    "        print(\"         The previous limit of 1000 was for performance. Proceeding may take a long time or fail due to memory.\")\n",
    "        # Proceed with caution or add a user confirmation/exit here\n",
    "        \n",
    "    if N > 3: \n",
    "        # The 'response' is the differenced ozone column (index 2)\n",
    "        response_y = subset_data[:, 2]\n",
    "\n",
    "        neg_log_lik_result = full_likelihood(\n",
    "            params=params, \n",
    "            input_data=subset_data, \n",
    "            response=response_y, \n",
    "            covariance_function=matern_cov_anisotropy_v05\n",
    "        )\n",
    "\n",
    "        print(f\"\\nCalculated Negative Log Likelihood: {neg_log_lik_result.item():.4f}\")\n",
    "        print(f\"Parameters used: {a}\")\n",
    "    else:\n",
    "        print(\"\\nNot enough data points found in the specified area after filtering/sampling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007aea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "Calculated Negative Log Likelihood: 53734.1218\n",
    "Parameters used: [20.7046, 1.952, 4.1366, -1.0769, -0.3244, 0.1074, 3.9057]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
