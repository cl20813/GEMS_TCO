{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27600a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy                    # clone tensor\n",
    "import time\n",
    "\n",
    "# Custom imports\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels\n",
    "\n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "import typer\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO import configuration as config\n",
    "from GEMS_TCO import data_preprocess as dmbh\n",
    "\n",
    "import os\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "from GEMS_TCO import kernels_nov25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a74c5d1",
   "metadata": {},
   "source": [
    "### Full Lieklihood using raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58409ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load data from: /Users/joonwonlee/Documents/GEMS_DATA/pickle_2024/coarse_cen_map_without_decrement_latitude24_07.pkl\n",
      "\n",
      "Data loaded successfully! âœ…\n",
      "Type of loaded data: <class 'dict'>\n",
      "Number of entries (hours) in the map: 248\n",
      "Example keys: ['y24m07day01_hm00:53', 'y24m07day01_hm01:53', 'y24m07day01_hm02:53', 'y24m07day01_hm03:53', 'y24m07day01_hm04:49']\n",
      "270\n",
      "273\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "# Assume your 'config' object is available\n",
    "# import config\n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# Specify the year and month you want to load\n",
    "YEAR_TO_LOAD = 2024\n",
    "MONTH_TO_LOAD = 7\n",
    "\n",
    "# Use the same base path as your saving script\n",
    "BASE_PATH = config.mac_data_load_path\n",
    "\n",
    "# --- 2. Construct the File Path ---\n",
    "# This must exactly match the naming convention from your saving script\n",
    "month_str = f\"{MONTH_TO_LOAD:02d}\"\n",
    "pickle_path = os.path.join(BASE_PATH, f'pickle_{YEAR_TO_LOAD}')\n",
    "filename = f\"coarse_cen_map_without_decrement_latitude{str(YEAR_TO_LOAD)[2:]}_{month_str}.pkl\"\n",
    "filepath_to_load = os.path.join(pickle_path, filename)\n",
    "\n",
    "print(f\"Attempting to load data from: {filepath_to_load}\")\n",
    "\n",
    "# --- 3. Load the Data ---\n",
    "try:\n",
    "    with open(filepath_to_load, 'rb') as pickle_file:\n",
    "        # Use pickle.load() to read the data from the file\n",
    "        loaded_coarse_map = pickle.load(pickle_file)\n",
    "    \n",
    "    print(\"\\nData loaded successfully! âœ…\")\n",
    "    \n",
    "    # --- 4. Verify the Loaded Data ---\n",
    "    # The loaded data is a dictionary. Let's inspect it.\n",
    "    print(f\"Type of loaded data: {type(loaded_coarse_map)}\")\n",
    "    if isinstance(loaded_coarse_map, dict):\n",
    "        print(f\"Number of entries (hours) in the map: {len(loaded_coarse_map)}\")\n",
    "        # Print the first 5 keys to see what they look like\n",
    "        first_five_keys = list(loaded_coarse_map.keys())[:5]\n",
    "        print(f\"Example keys: {first_five_keys}\")\n",
    "        \n",
    "        # You can now access the data for a specific hour, for example:\n",
    "        # first_hour_data = loaded_coarse_map[first_five_keys[0]]\n",
    "        # print(f\"\\nData for first hour is a tensor of shape: {first_hour_data.shape}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nError: File not found. Please check if the file exists at the specified path.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")\n",
    "\n",
    "\n",
    "print(loaded_coarse_map['y24m07day01_hm00:53']['Longitude'].nunique())\n",
    "print(loaded_coarse_map['y24m07day01_hm00:53']['Latitude'].nunique())\n",
    "\n",
    "import GEMS_TCO\n",
    "load_data_instance = GEMS_TCO.load_data('')\n",
    "\n",
    "df_day_aggregated_list = []\n",
    "df_day_map_list = []\n",
    "for i in range(31):\n",
    "    cur_map, cur_df =load_data_instance.load_working_data_byday_wo_mm(loaded_coarse_map,[i*8, (i+1)*8])\n",
    "    df_day_aggregated_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c8a5cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will look at 2 hours of data \n",
      "Original data shape: torch.Size([147420, 4])\n",
      "Subset data shape:   torch.Size([36252, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from typing import Callable\n",
    "\n",
    "# =========================================================================\n",
    "# 1. Helper Function for Subsetting\n",
    "# =========================================================================\n",
    "\n",
    "def subset_by_area(input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Subsets a tensor to the specified lat/lon area.\n",
    "    Latitude between 0 and 5.\n",
    "    Longitude between 123 and 133.\n",
    "    \"\"\"\n",
    "    # Assumes columns are [lat, lon, value, time]\n",
    "    lat_col, lon_col = 0, 1\n",
    "    lat_mask = (input_tensor[:, lat_col] >= 0) & (input_tensor[:, lat_col] <= 5)\n",
    "    lon_mask = (input_tensor[:, lon_col] >= 123) & (input_tensor[:, lon_col] <= 133)\n",
    "    \n",
    "    df_sub = input_tensor[lat_mask & lon_mask].clone()\n",
    "    return df_sub\n",
    "\n",
    "a = df_day_aggregated_list[0].shape[0]/8\n",
    "hour_n = 2\n",
    "print(f'will look at {hour_n} hours of data ')\n",
    "a = int(a*hour_n)\n",
    "# Use the first tensor from your data list\n",
    "raw_data = df_day_aggregated_list[0][:a].to(torch.float64) # Ensure data is float64 for precision\n",
    "\n",
    "# --- 2. Subset the data to the desired area ---\n",
    "print(f\"Original data shape: {raw_data.shape}\")\n",
    "subset_data = subset_by_area(raw_data)\n",
    "print(f\"Subset data shape:   {subset_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a565a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: torch.Size([73710, 4])\n",
      "Subset data shape:   torch.Size([18126, 4])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SpatioTemporalModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 92\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m subset_data.shape[\u001b[32m0\u001b[39m] > \u001b[32m0\u001b[39m:\n\u001b[32m     89\u001b[39m     \u001b[38;5;66;03m# The 'response' is the ozone column (index 2) of the subsetted data\u001b[39;00m\n\u001b[32m     90\u001b[39m     response_y = subset_data[:, \u001b[32m2\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     neg_log_lik_result = \u001b[43mSpatioTemporalModel\u001b[49m.full_likelihood(\n\u001b[32m     93\u001b[39m         params=params, \n\u001b[32m     94\u001b[39m         input_data=subset_data, \n\u001b[32m     95\u001b[39m         response=response_y, \n\u001b[32m     96\u001b[39m         covariance_function=matern_cov_anisotropy_v05\n\u001b[32m     97\u001b[39m     )\n\u001b[32m     99\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCalculated Negative Log Likelihood: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mneg_log_lik_result.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'SpatioTemporalModel' is not defined"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# 3. Main Execution Block\n",
    "# =========================================================================\n",
    "if __name__ == '__main__':\n",
    "    # --- 1. Define your parameters and load your data ---\n",
    "    \n",
    "    # Example parameters (on their natural scale)\n",
    "    #a = [25.0, 3.0, 4.0, 0.02, -0.08, 0.02, 3.01] # 24125. 50580\n",
    "    #a = [20.89, 1.04, 1.337, 0.040, -0.178, 0.195, 4.498] # 47385\n",
    "    a = [28.6847, 0.9147, 5.0289, 0.1551, 0.6344, 0.0, 4.1061] # 24608. 62513\n",
    "    a = [12.9046, 6.2184, 4.3523, -0.0724, -0.2941, -0.262, 5.4445]\n",
    "    a = [5.3553, 2.9051, 3.4829, -0.4144, -0.2813, -0.2952, 4.2593]\n",
    "    \n",
    "\n",
    "    a =[ 28.7507942474575, 0.9895617352853928, 1.0668105024738395, 0.03646879682204495, -0.15565418931819383, 0.17944566056749772, 1.8907510307884106]\n",
    "    # above 23051. from 5000 vecchia    # 2 45718\n",
    "    a = [30.785888081335365, 1.3235746443649596, 2.29756812249494, -4.966182354553938e-09, -1.4880259575893335e-08, 0.0, 3.1756934806176957]\n",
    "    #a = [29.89872140748479, 1.1529714369768411, 1.7862356661866714, 0.03927727761515986, -0.15656505052873793, 0.1320573870050866, 4.130349222670235]\n",
    "    # above 23583  from 1250 vecchia\n",
    "\n",
    "\n",
    "\n",
    "    a = [38.84195910498308, 2.49743267669403, 4.815691909532588, -6.631847938075385e-05, 2.4499641882277948e-05, 0.0, 3.405521300583113]\n",
    "    a = [32.970717648092105, 1.2238701512173191, 2.599741795413899, 1.0943618429021917e-05, 1.1398764403195979e-05, 0.0, 1.89]\n",
    "    a = [29.442378127761707, 1.0999898079353545, 2.334194369175332, 2.3734466610082268e-05, -5.715097473839803e-07, 0.0, 1.89]\n",
    "    a = [30.1311, 0.6599, 0.7021, 0.0, 0.0, 0.0, 1.3537]\n",
    "    a = [21.850765005142662, 2.1060712249817217, 3.284523770936945, -1.3326718837688786e-06, -8.340172696874311e-07, 0.0, 4.019367803314785]\n",
    "    #a = [29.776, 0.6619, 0.7052, 0.0, 0.0, 0.0, 1.3839]. # 22965. \n",
    "    \n",
    "    \n",
    "    \n",
    "    a = [31.78, 1.32, 2.29, -4.9639e-09, -1.4880e-08, 0.0, 3.175]      # fit 1 hour vecchia\n",
    "    a = [30.776, 0.6619, 0.7052, 0.0, 0.0, 0.0, 1.3839]               # fit 1 hour whittle \n",
    "\n",
    "    # ori. 23635.5148, center matched\n",
    "    a = [29.867643794098317, 1.8170445928338301, 3.452060053856289, 9.346808116428221e-06, 1.3480203304274802e-05, 0.0, 3.278045234831132]\n",
    "\n",
    "    #a = [29.937225805181328, 1.8701465350665567, 3.5104446735965977, 0.0815362732928427, -0.24146058733476328, 0.179, 3.338953474182954]\n",
    "    a = [29.921775534973218, 1.8597906958307489, 3.4954059102213106, 0.22512272633105687, -0.0648045117355326, 0.0, 3.3287024509849537]\n",
    "    \n",
    "    \n",
    "    a = [30.67103401,  0.5421034,   0.58002574,  0.03326813, -0.15298113,  0.27876268, 0.07482888]\n",
    "    a = [28.75, 0.98, 1.06, 0.036, -0.155, 0.179, 1.890]\n",
    "\n",
    "    a = [30.527, 1.3223e+00, 2.2772e+00, 1.3499e-05, 5.9777e-06, 0.0000e+00,\n",
    "        3.2343e+00] # 29.527 23362.7398 # 30.527 23348.1416    14.5982 difference\n",
    "    a =   [30.527, 1.3223e+00, 2.2772e+00, 0, 0, 0,\n",
    "        3.2343e+00] # 23348.1416\n",
    "    \n",
    "    ### fit 1 hour fix temporal. lr 0.02\n",
    "    a = [31.852697305756127, 1.3262400935301575, 2.3021025737222276, 0.0, 0.0, 0.0, 3.175507772196219]\n",
    "    # 23340\n",
    "\n",
    "    # lr 0.03.   32.62:23337. 33.62: 23324   \n",
    "    a = [33.62495453438428, 1.3991931103706319, 2.4235795624478227, 0.0, 0.0, 0.0, 3.1701018909601357]\n",
    "\n",
    "    a = [34.94996718914756, 1.4542571743382986, 2.5155905831082133, -5.559147004005255e-06, 4.5405397925270674e-06, 0.0, 3.167038512331945]\n",
    "    #.  33.94: 23335   34.94:  23322\n",
    "\n",
    "    a = [53.5307229, 2.25568639e+00, 3.86505395e+00, 5.23430145e-05,\n",
    " 3.49769959e-05, 0.00000000e+00, 3.16205496e+00] # 52.53: 23327.7921 53.53: 23319\n",
    "    \n",
    "    a = [29.07358259766055, 2.0814371586888822, 3.7483140035232414, -1.750823729127743e-05, -3.881040539593337e-06, 0.0, 3.6406220522441455]\n",
    "    \n",
    "    \n",
    "    a = [32.1336320, 4.190161, 5.099099, 0.0, -0.05, 0.0, 5.499893] # 24405 33: 24389\n",
    "\n",
    "    a = [29.776, 0.6619, 0.7052, 0.0, 0.0, 0.0, 1.3839]\n",
    "    a = [12.44, 0.39, 0.44, 0.0, 0,0, 1.0839] #14:23439 12.44 23777\n",
    "\n",
    "    params = torch.tensor(a, dtype=torch.float64)\n",
    "\n",
    "    \n",
    "\n",
    "    a = df_day_aggregated_list[0].shape[0]/8\n",
    "    hour_n = 1\n",
    "    a = int(a*hour_n)\n",
    "    # Use the first tensor from your data list\n",
    "    raw_data = df_day_aggregated_list[0][:a].to(torch.float64) # Ensure data is float64 for precision\n",
    "\n",
    "    # --- 2. Subset the data to the desired area ---\n",
    "    print(f\"Original data shape: {raw_data.shape}\")\n",
    "    subset_data = subset_by_area(raw_data)\n",
    "    print(f\"Subset data shape:   {subset_data.shape}\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # --- 3. Calculate the full likelihood on the subset ---\n",
    "    if subset_data.shape[0] > 0:\n",
    "        # The 'response' is the ozone column (index 2) of the subsetted data\n",
    "        response_y = subset_data[:, 2]\n",
    "\n",
    "        neg_log_lik_result = full_likelihood(\n",
    "            params=params, \n",
    "            input_data=subset_data, \n",
    "            response=response_y, \n",
    "            covariance_function=matern_cov_anisotropy_v05\n",
    "        )\n",
    "\n",
    "        print(f\"\\nCalculated Negative Log Likelihood: {neg_log_lik_result.item():.4f}\")\n",
    "    else:\n",
    "        print(\"\\nNo data points found in the specified area.\")\n",
    "\n",
    "\n",
    "\n",
    "# 24125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5affb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of data 31.390108108520508\n",
      "Estimated SQ + Nugget = 25.388\n",
      "\n",
      "\n",
      "East to West wind speed 7.3379 (m/s) \n",
      "North to South Wind speed  1.2827 (m/s)\n",
      "\n",
      "\n",
      "East to West wind speed  0.0335 (m/s)\n"
     ]
    }
   ],
   "source": [
    "print(f'Variance of data {torch.var(df_day_aggregated_list[0][:,2])}')\n",
    "print(f'Estimated SQ + Nugget = {20.89+4.498}')\n",
    "\n",
    "\n",
    "print(f'\\n\\nEast to West wind speed { round(1.337*0.178*111/3600 *1000,4)} (m/s) ')\n",
    "print(f'North to South Wind speed  { round(1.04*0.04*111/3600*1000,4)} (m/s)')\n",
    "\n",
    "print(f'\\n\\nEast to West wind speed  { round(0.072*0.0151*111/3600*1000,4)} (m/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e00ae25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.381054539960715"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "29.776/0.6619*1.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7902257d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.6874009"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.43*-0.155988*111/3600*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e3e1b3",
   "metadata": {},
   "source": [
    "Fit 1 hour \n",
    "\n",
    "Vecchia Parameters Used (raw data without differencing, but demeaned, regulargrid by center matching) \n",
    "\n",
    "``` [30.78, 1.32, 2.29, -4.9639e-09, -1.4880e-08, 0.0, 3.175]``` # more vulnerable to initial point, especially sigmasq, and nugget.\n",
    "Calculated Negative Log Likelihood: ```23341``\n",
    "30.78--> 31.78:                     ```23326```                 # 45 23269 50 23284 55 23319\n",
    "\n",
    "\n",
    "Whittle Parameters Used (Once differenced in space [[-2 1][1 0 ]]) \n",
    "```[29.776, 0.6619, 0.7052, 0.0, 0.0, 0.0, 1.3839]```\n",
    "Calculated Negative Log Likelihood: ```22965``\n",
    "29.776 --> 30.776                   ```22955 ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec2a890",
   "metadata": {},
   "source": [
    "# Full Likelihood\n",
    "\n",
    "Vecchia Parameters Used (raw data without differencing, but demeaned, regulargrid by center matching) \n",
    "\n",
    "```[29.75, 0.98, 1.06, 0.036, -0.155, 0.179, 1.890]```\n",
    "# 23045\n",
    "### 1 hour of data 18126x1\n",
    "Calculated Negative Log Likelihood: ```23051``\n",
    "if 29.75 --> 28.75:\n",
    "Calculated Negative Log Likelihood: ```23070```\n",
    "\n",
    "### 2 hours of data 18126x2\n",
    "Calculated Negative Log Likelihood: ```45718```.  ## 45718 ~= 2* 23051, so it scales well\n",
    "\n",
    "\n",
    "Whittle Parameters Used (Once differenced in space [[-2 1][1 0 ]]) + hamming tapering \n",
    "```[31.2594, 0.665, 1.8981, 0.0, 0.1317, -0.0, 1.9785]```\n",
    "\n",
    "### 1 hour of data 18126x1\n",
    "Calculated Negative Log Likelihood: ```23642```\n",
    "if 31.2594 --> 30.2594:\n",
    "Calculated Negative Log Likelihood: ```23669```\n",
    "\n",
    "### 2 hours of data 18126x2\n",
    "Calculated Negative Log Likelihood: ```70664``` ## 70664> 23642*2, does not scale well\n",
    "\n",
    "\n",
    "Whittle Parameters Used (Once differenced in space [[-2 1][1 0 ]]) + once difference in time (two stage differencing)\n",
    "```[45.1402, 0.6299, 0.7308, -0.0003, -0.0151, 0.0, 7.8922]```\n",
    "\n",
    "### 1 hour of data 18126x1\n",
    "Calculated Negative Log Likelihood: ```26766```\n",
    "if 45.7499-->44.7499:\n",
    "Calculated Negative Log Likelihood: ```26726``` ## nll actually dereased? when the variance parameter is 1 unit away from local optimal\n",
    "\n",
    "### 2 hours of data 18126x2\n",
    "Calculated Negative Log Likelihood: ```56130``` ## difference in time seems to help capturing temporal structure\n",
    "\n",
    "\n",
    "\n",
    "# Whittle Likelihood on 8 hours of data\n",
    "\n",
    "-> Whittle Likelihood ('Vecchia Optimized' Params): \n",
    "```[21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]```, negative log likelihood:   ```58326.332```\n",
    "if 21.303 --> 20.303                                                                  ```57998.410```\n",
    "\n",
    "-> Whittle Likelihood ('Whittle Optimized' Params): \n",
    "```[31.2594, 0.665, 1.8981, 0.0, 0.1317, -0.0, 1.9785]```  , negative log likelihood:  ```41604.566```\n",
    "if 31.2594 --> 30.2594                                                                 ```41623.910```\n",
    "\n",
    "\n",
    "# Vecchia Likelihood on 8 hours of data\n",
    "\n",
    "-> Vecchia Likelihood ('Vecchia Optimized' Params): \n",
    "```[21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]```, negative log likelihood:   ```64351```\n",
    "if 21.303 --> 20.303                                                                  ```64384```\n",
    "\n",
    "-> Vecchia Likelihood ('Whittle Optimized' Params): \n",
    "```[31.2594, 0.665, 1.8981, 0.0, 0.1317, -0.0, 1.9785]```  , negative log likelihood:  ```71865```\n",
    "if 31.2594 --> 30.2594                                                                 ```72132```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b581b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0aabc5d",
   "metadata": {},
   "source": [
    "### Full likelihood using spatially differenced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d348f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/joonwonlee/Documents/GEMS_DATA/pickle_2024/coarse_cen_map_without_decrement_latitude24_07.pkl\n",
      "Loaded 31 days of raw data.\n",
      "\n",
      "--- Results ---\n",
      "Number of final spatially-differenced day tensors: 31\n",
      "Processed data saved to spatial_first_difference_data.pkl\n",
      "\n",
      "Shape of the first final tensor: torch.Size([142832, 4])\n",
      "First final tensor head:\n",
      "tensor([[ 4.0000e-03,  1.2303e+02,  2.9422e+00,  2.1000e+01],\n",
      "        [ 4.0000e-03,  1.2309e+02,  1.9636e+00,  2.1000e+01],\n",
      "        [ 4.0000e-03,  1.2316e+02, -1.3187e+00,  2.1000e+01],\n",
      "        [ 4.0000e-03,  1.2322e+02, -3.1683e+00,  2.1000e+01],\n",
      "        [ 4.0000e-03,  1.2328e+02, -5.4922e-01,  2.1000e+01]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Assume GEMS_TCO is a custom class/module you have available\n",
    "# from your_project import GEMS_TCO\n",
    "\n",
    "# =========================================================================\n",
    "# 1. Helper Functions\n",
    "# =========================================================================\n",
    "\n",
    "def subset_tensor(df_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Subsets a tensor to a specific lat/lon range.\"\"\"\n",
    "    #lat_mask = (df_tensor[:, 0] >= -5) & (df_tensor[:, 0] <= 6.3)\n",
    "    #lon_mask = (df_tensor[:, 1] >= 118) & (df_tensor[:, 1] <= 134.2)\n",
    "    lat_mask = (df_tensor[:, 0] >= 0) & (df_tensor[:, 0] <= 5)\n",
    "    lon_mask = (df_tensor[:, 1] >= 123) & (df_tensor[:, 1] <= 133)\n",
    "\n",
    "    df_sub = df_tensor[lat_mask & lon_mask].clone()\n",
    "    return df_sub\n",
    "\n",
    "def apply_first_difference_2d_tensor(df_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies a 2D first-order difference filter using convolution.\n",
    "    This approximates Z(s) = [X(s+d_lat) - X(s)] + [X(s+d_lon) - X(s)].\n",
    "    \"\"\"\n",
    "    if df_tensor.size(0) == 0:\n",
    "        return torch.empty(0, 4)\n",
    "\n",
    "    # 1. Get grid dimensions and validate\n",
    "    unique_lats = torch.unique(df_tensor[:, 0])\n",
    "    unique_lons = torch.unique(df_tensor[:, 1])\n",
    "    lat_count, lon_count = unique_lats.size(0), unique_lons.size(0)\n",
    "\n",
    "    if df_tensor.size(0) != lat_count * lon_count:\n",
    "        raise ValueError(\"Tensor size does not match grid dimensions. Must be a complete grid.\")\n",
    "    if lat_count < 2 or lon_count < 2:\n",
    "        return torch.empty(0, 4)\n",
    "\n",
    "    # 2. Reshape data and define the correct kernel\n",
    "    ozone_data = df_tensor[:, 2].reshape(1, 1, lat_count, lon_count)\n",
    "    \n",
    "    # âœ… CORRECT KERNEL: This kernel results in the standard first-order difference:\n",
    "    # Z(i,j) = X(i+1,j) + X(i,j+1) - 2*X(i,j)\n",
    "    # Note: F.conv2d in PyTorch actually performs cross-correlation. To get a true\n",
    "    # convolution result, the kernel would need to be flipped. However, for a \n",
    "    # forward difference operator, defining the kernel for cross-correlation is more direct.\n",
    "    # The kernel below is designed for cross-correlation to achieve the desired differencing.\n",
    "    diff_kernel = torch.tensor([[[[-2., 1.],\n",
    "                                  [ 1., 0.]]]], dtype=torch.float32)\n",
    "\n",
    "    # 3. Apply convolution (which acts as cross-correlation)\n",
    "    filtered_grid = F.conv2d(ozone_data, diff_kernel, padding='valid').squeeze()\n",
    "\n",
    "    # 4. Determine coordinates for the new, smaller grid\n",
    "    # The new grid corresponds to the anchor points of the kernel\n",
    "    new_lats = unique_lats[:-1]\n",
    "    new_lons = unique_lons[:-1]\n",
    "\n",
    "    # 5. Reconstruct the output tensor\n",
    "    new_lat_grid, new_lon_grid = torch.meshgrid(new_lats, new_lons, indexing='ij')\n",
    "    filtered_values = filtered_grid.flatten()\n",
    "    time_value = df_tensor[0, 3].repeat(filtered_values.size(0))\n",
    "\n",
    "    new_tensor = torch.stack([\n",
    "        new_lat_grid.flatten(),\n",
    "        new_lon_grid.flatten(),\n",
    "        filtered_values,\n",
    "        time_value\n",
    "    ], dim=1)\n",
    "    \n",
    "    return new_tensor\n",
    "\n",
    "# =========================================================================\n",
    "# 2. Data Loading (Unchanged)\n",
    "# =========================================================================\n",
    "# âš ï¸ NOTE: You must define these variables\n",
    "# mac_data_path = \"...\"\n",
    "# year = 2022\n",
    "# month_str = \"01\"\n",
    "# class GEMS_TCO: # Placeholder\n",
    "#     def load_data(self, path): return self\n",
    "#     def load_working_data_byday_wo_mm(self, data, indices):\n",
    "#         return {'key': torch.randn(100, 4)}, torch.randn(100, 4)\n",
    "mac_data_path = config.mac_data_load_path\n",
    "year = 2024\n",
    "pickle_path = os.path.join(mac_data_path, f'pickle_{year}')\n",
    "output_filename = f\"coarse_cen_map_without_decrement_latitude{str(year)[2:]}_{month_str}.pkl\"\n",
    "output_filepath = os.path.join(pickle_path, output_filename)\n",
    "print(f\"Loading data from: {output_filepath}\")\n",
    "with open(output_filepath, 'rb') as pickle_file:\n",
    "    cbmap_ori = pickle.load(pickle_file)\n",
    "\n",
    "load_data_instance = GEMS_TCO.load_data('')\n",
    "df_day_map_list = []\n",
    "for i in range(31): # Adjust if necessary\n",
    "    cur_map, _ = load_data_instance.load_working_data_byday_wo_mm(cbmap_ori, [i*8, (i+1)*8])\n",
    "    df_day_map_list.append(cur_map)\n",
    "print(f\"Loaded {len(df_day_map_list)} days of raw data.\")\n",
    "\n",
    "# =========================================================================\n",
    "# 3. Main Processing Loop (Unchanged)\n",
    "# =========================================================================\n",
    "spatially_filtered_days = []\n",
    "for day_idx, day_map in enumerate(df_day_map_list):\n",
    "    tensors_to_aggregate = []\n",
    "    for key, tensor in day_map.items():\n",
    "        subsetted = subset_tensor(tensor)\n",
    "        if subsetted.size(0) > 0:\n",
    "            try:\n",
    "                diff_applied = apply_first_difference_2d_tensor(subsetted)\n",
    "                if diff_applied.size(0) > 0:\n",
    "                    tensors_to_aggregate.append(diff_applied)\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping data chunk on day {day_idx+1} due to error: {e}\")\n",
    "\n",
    "    if tensors_to_aggregate:\n",
    "        aggregated_day_tensor = torch.cat(tensors_to_aggregate, dim=0)\n",
    "        spatially_filtered_days.append(aggregated_day_tensor)\n",
    "\n",
    "# =========================================================================\n",
    "# 4. Verification (Unchanged)\n",
    "# =========================================================================\n",
    "print(\"\\n--- Results ---\")\n",
    "print(f\"Number of final spatially-differenced day tensors: {len(spatially_filtered_days)}\")\n",
    "if spatially_filtered_days:\n",
    "    # Save the processed data for the next script\n",
    "    processed_output_path = \"spatial_first_difference_data.pkl\"\n",
    "    with open(processed_output_path, 'wb') as f:\n",
    "        pickle.dump(spatially_filtered_days, f)\n",
    "    print(f\"Processed data saved to {processed_output_path}\")\n",
    "\n",
    "    print(f\"\\nShape of the first final tensor: {spatially_filtered_days[0].shape}\")\n",
    "    print(\"First final tensor head:\")\n",
    "    print(spatially_filtered_days[0][:5])\n",
    "else:\n",
    "    print(\"\\nNo final differenced tensors were created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bccbf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:70: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:70: SyntaxWarning: invalid escape sequence '\\m'\n",
      "/var/folders/9p/53hd4c7d2fl193h4jwp194wc0000gn/T/ipykernel_92958/1514199930.py:70: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  print(f\"\\nðŸ›‘ WARNING: The current data size ({N} points) will be extremely slow for NLL calculation ($\\mathcal{{O}}(N^3)$).\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data from: spatial_first_difference_data.pkl\n",
      "Total points after spatial subsetting: 142832\n",
      "Using time indices: [21.0]\n",
      "Final subset data shape for likelihood (time-limited): torch.Size([17854, 4])\n",
      "\n",
      "ðŸ›‘ WARNING: The current data size (17854 points) will be extremely slow for NLL calculation ($\\mathcal{O}(N^3)$).\n",
      "         The previous limit of 1000 was for performance. Proceeding may take a long time or fail due to memory.\n",
      "\n",
      "Calculated Negative Log Likelihood: 64440.0133\n",
      "Parameters used: [11.0973, 3.6585, 4.6663, 0.3542, -0.7808, -0.0019, 3.0497]\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# 3. Main Execution Block (Adjusted to use only the first two hours)\n",
    "# =========================================================================\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # --- 1. Load the processed data ---\n",
    "    processed_output_path = \"spatial_first_difference_data.pkl\"\n",
    "    print(f\"Loading processed data from: {processed_output_path}\")\n",
    "\n",
    "    try:\n",
    "        with open(processed_output_path, 'rb') as f:\n",
    "            # ðŸ’¡ Loading your spatially differenced data\n",
    "            spatially_filtered_days = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Processed data file not found at {processed_output_path}. Ensure the differencing script ran and saved the data.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading data: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # --- 2. Define parameters and select data subset ---\n",
    "    \n",
    "    # Parameters: [sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget]\n",
    "    # Using the new parameters provided in your prompt\n",
    "    a = [19.89, 1.04, 1.337, 0.040, -0.178, 0.195, 4.498] # 47385\n",
    "    #a = [28.6847, 0.9147, 5.0289, 0.1551, 0.6344, 0.0, 4.1061] \n",
    "    a = [12.9046, 6.2184, 4.3523, -0.0724, -0.2941, -0.262, 5.4445]\n",
    "    a = [20.7046, 1.952, 4.1366, -1.0769, -0.3244, 0.1074, 3.9057]\n",
    "    a = [11.0973, 3.6585, 4.6663, 0.3542, -0.7808, -0.0019, 3.0497]\n",
    "    params = torch.tensor(a, dtype=torch.float64)\n",
    "    \n",
    "    # Select the first day's differenced tensor\n",
    "    raw_data_day1 = spatially_filtered_days[0].to(torch.float64) \n",
    "    \n",
    "    # --- 3. Subset by Area and Time ---\n",
    "    \n",
    "    # Apply the spatial area subset first\n",
    "    subset_data_area = subset_by_area(raw_data_day1)\n",
    "    \n",
    "    print(f\"Total points after spatial subsetting: {subset_data_area.shape[0]}\")\n",
    "\n",
    "    # --- Time-based Filtering (First Two Hours) ---\n",
    "    # The 'time' column is the 4th column (index 3).\n",
    "    # Since the original data was loaded in 8-hour chunks per day, and the time indices \n",
    "    # typically represent the chunk number or the time within the day, let's examine \n",
    "    # the time indices in the subsetted data.\n",
    "    \n",
    "    # Find the unique time indices in the subsetted data\n",
    "    unique_times = torch.unique(subset_data_area[:, 3], sorted=True)\n",
    "    hour_n = 1\n",
    "    if len(unique_times) >= hour_n:\n",
    "        # Select the first two unique time indices\n",
    "        time_limit = unique_times[: hour_n]\n",
    "        \n",
    "        # Create a mask for data points where the time index matches one of the first two times\n",
    "        time_mask = torch.isin(subset_data_area[:, 3], time_limit)\n",
    "        subset_data = subset_data_area[time_mask]\n",
    "        \n",
    "        print(f\"Using time indices: {time_limit.tolist()}\")\n",
    "    else:\n",
    "        # Fallback if there aren't two unique time indices\n",
    "        print(\"Warning: Less than two unique time indices available. Using all data after spatial subset.\")\n",
    "        subset_data = subset_data_area\n",
    "        \n",
    "    print(f\"Final subset data shape for likelihood (time-limited): {subset_data.shape}\")\n",
    "\n",
    "    # --- 4. Calculate the full likelihood ---\n",
    "    N = subset_data.shape[0]\n",
    "    if N > 2000:\n",
    "        print(f\"\\nðŸ›‘ WARNING: The current data size ({N} points) will be extremely slow for NLL calculation ($\\mathcal{{O}}(N^3)$).\")\n",
    "        print(\"         The previous limit of 1000 was for performance. Proceeding may take a long time or fail due to memory.\")\n",
    "        # Proceed with caution or add a user confirmation/exit here\n",
    "        \n",
    "    if N > 3: \n",
    "        # The 'response' is the differenced ozone column (index 2)\n",
    "        response_y = subset_data[:, 2]\n",
    "\n",
    "        neg_log_lik_result = full_likelihood(\n",
    "            params=params, \n",
    "            input_data=subset_data, \n",
    "            response=response_y, \n",
    "            covariance_function=matern_cov_anisotropy_v05\n",
    "        )\n",
    "\n",
    "        print(f\"\\nCalculated Negative Log Likelihood: {neg_log_lik_result.item():.4f}\")\n",
    "        print(f\"Parameters used: {a}\")\n",
    "    else:\n",
    "        print(\"\\nNot enough data points found in the specified area after filtering/sampling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e35a28b",
   "metadata": {},
   "source": [
    "Fit 1 hour\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6468e8",
   "metadata": {},
   "source": [
    "Parameters used: [28.6847, 0.9147, 5.0289, 0.1551, 0.6344, 0.0, 4.1061]\n",
    "### 1\n",
    "Calculated Negative Log Likelihood: 51301.4845\n",
    "\n",
    "### 2\n",
    "Calculated Negative Log Likelihood: 100719.2866\n",
    "\n",
    "Parameters used: [27.6847, 0.9147, 5.0289, 0.1551, 0.6344, 0.0, 4.1061]\n",
    "### 1\n",
    "Calculated Negative Log Likelihood: 51382.0330\n",
    "\n",
    "\n",
    "###################################\n",
    "\n",
    "\n",
    "Parameters used: [20.89, 1.04, 1.337, 0.04, -0.178, 0.195, 4.498]\n",
    "### 1\n",
    "Calculated Negative Log Likelihood: 47928.9610\n",
    "\n",
    "Parameters used: [19.89, 1.04, 1.337, 0.04, -0.178, 0.195, 4.498]\n",
    "### 1\n",
    "Calculated Negative Log Likelihood: 48065.0109\n",
    "\n",
    "### 2\n",
    "Calculated Negative Log Likelihood: 93039.9350\n",
    "\n",
    "\n",
    "this is estimate from 3d once differencing filter using larger data set\n",
    "Parameters used: [12.9046, 6.2184, 4.3523, -0.0724, -0.2941, -0.262, 5.4445]\n",
    "### 1\n",
    "Calculated Negative Log Likelihood: 46520.4330\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca0628",
   "metadata": {},
   "source": [
    "# 3d first differenced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98a15e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =========================================================================\n",
    "# 1. Helper Function (UNCHANGED)\n",
    "# =========================================================================\n",
    "\n",
    "def subset_tensor(df_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Subsets a tensor to a specific lat/lon range.\n",
    "    Columns are assumed to be [lat, lon, ozone, time].\n",
    "    \"\"\"\n",
    "    lat_mask = (df_tensor[:, 0] >= 0) & (df_tensor[:, 0] <= 5)\n",
    "    lon_mask = (df_tensor[:, 1] >= 123) & (df_tensor[:, 1] <= 133)\n",
    "    return df_tensor[lat_mask & lon_mask].clone()\n",
    "\n",
    "# =========================================================================\n",
    "# 2. CORRECTED 3D Differencing Function\n",
    "# =========================================================================\n",
    "\n",
    "def apply_first_difference_3d(day_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies a 3D first-difference filter to calculate the gradient (rate of change)\n",
    "    simultaneously across time, latitude, and longitude.\n",
    "\n",
    "    Args:\n",
    "        day_tensor: A tensor for a single day with columns [lat, lon, ozone, time].\n",
    "\n",
    "    Returns:\n",
    "        A tensor with columns [lat, lon, time, grad_t, grad_lat, grad_lon].\n",
    "    \"\"\"\n",
    "    if day_tensor.numel() == 0:\n",
    "        return torch.empty(0, 6)\n",
    "\n",
    "    # 1. Map long-format data to a dense 3D grid\n",
    "    unique_lats = torch.unique(day_tensor[:, 0])\n",
    "    unique_lons = torch.unique(day_tensor[:, 1])\n",
    "    unique_times = torch.unique(day_tensor[:, 3])\n",
    "    \n",
    "    T, H, W = len(unique_times), len(unique_lats), len(unique_lons)\n",
    "    if T < 2 or H < 2 or W < 2:\n",
    "        return torch.empty(0, 6)\n",
    "\n",
    "    lat_map = {lat.item(): i for i, lat in enumerate(unique_lats)}\n",
    "    lon_map = {lon.item(): i for i, lon in enumerate(unique_lons)}\n",
    "    time_map = {time.item(): i for i, time in enumerate(unique_times)}\n",
    "\n",
    "    ozone_grid = torch.zeros(T, H, W, dtype=torch.float32)\n",
    "    for row in day_tensor:\n",
    "        lat, lon, ozone, time = row\n",
    "        t_idx, h_idx, w_idx = time_map[time.item()], lat_map[lat.item()], lon_map[lon.item()]\n",
    "        ozone_grid[t_idx, h_idx, w_idx] = ozone\n",
    "    \n",
    "    # Reshape for conv3d: (N, C_in, D, H, W) -> (1, 1, Time, Lat, Lon)\n",
    "    ozone_grid = ozone_grid.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # 2. Define 3D kernels for first difference Z(i) - Z(i-1) along each axis\n",
    "    kernel_t = torch.tensor([-1., 1.], dtype=torch.float32).reshape(1, 1, 2, 1, 1)   # D-axis (Time)\n",
    "    kernel_lat = torch.tensor([-1., 1.], dtype=torch.float32).reshape(1, 1, 1, 2, 1) # H-axis (Lat)\n",
    "    kernel_lon = torch.tensor([-1., 1.], dtype=torch.float32).reshape(1, 1, 1, 1, 2) # W-axis (Lon)\n",
    "\n",
    "    # 3. Apply 3D convolution to get gradient components\n",
    "    # Output shapes: grad_t (T-1, H, W), grad_lat (T, H-1, W), grad_lon (T, H, W-1)\n",
    "    grad_t = F.conv3d(ozone_grid, kernel_t, padding='valid').squeeze()\n",
    "    grad_lat = F.conv3d(ozone_grid, kernel_lat, padding='valid').squeeze()\n",
    "    grad_lon = F.conv3d(ozone_grid, kernel_lon, padding='valid').squeeze()\n",
    "\n",
    "    # 4. Align gradient grids to the common (T-1, H-1, W-1) shape\n",
    "    # This aligns the anchor point (Lat[i], Lon[j], Time[k]) for all three derivatives.\n",
    "    \n",
    "    # grad_t is (T-1, H, W). We slice H -> H-1 and W -> W-1\n",
    "    grad_t_common = grad_t[:, :-1, :-1] \n",
    "\n",
    "    # grad_lat is (T, H-1, W). We slice T -> T-1 and W -> W-1\n",
    "    grad_lat_common = grad_lat[:-1, :, :-1]\n",
    "\n",
    "    # grad_lon is (T, H, W-1). We slice T -> T-1 and H -> H-1\n",
    "    grad_lon_common = grad_lon[:-1, :-1, :]\n",
    "\n",
    "    # 5. Create new coordinate grids (T-1, H-1, W-1)\n",
    "    # The new coordinates correspond to the point *after* the difference (i.e., X(i)-X(i-1) is anchored at i)\n",
    "    new_times = unique_times[1:] # T-1 coordinates (e.g., hour 2 to 8)\n",
    "    new_lats = unique_lats[1:]   # H-1 coordinates\n",
    "    new_lons = unique_lons[1:]   # W-1 coordinates\n",
    "    \n",
    "    time_grid, lat_grid, lon_grid = torch.meshgrid(new_times, new_lats, new_lons, indexing='ij')\n",
    "\n",
    "    # 6. Flatten and stack\n",
    "    final_tensor = torch.stack([\n",
    "        lat_grid.flatten(),\n",
    "        lon_grid.flatten(),\n",
    "        time_grid.flatten(),\n",
    "        grad_t_common.flatten(),\n",
    "        grad_lat_common.flatten(),\n",
    "        grad_lon_common.flatten()\n",
    "    ], dim=1)\n",
    "    \n",
    "    return final_tensor\n",
    "\n",
    "# =========================================================================\n",
    "# 3. Main Processing Loop (UNCHANGED logic, requires df_day_map_list)\n",
    "# =========================================================================\n",
    "\n",
    "# Assuming df_day_map_list is pre-loaded and sorted by date\n",
    "all_gradient_tensors = []\n",
    "# NOTE: df_day_map_list must be defined and loaded for this loop to run correctly.\n",
    "# For demonstration purposes, we assume it is loaded as requested.\n",
    "# for day_map in df_day_map_list: \n",
    "#     # Aggregate all data for one day and apply the initial spatial subset\n",
    "#     tensors_for_day = [subset_tensor(t) for t in day_map.values() if t.numel() > 0]\n",
    "\n",
    "#     if not tensors_for_day:\n",
    "#         continue\n",
    "        \n",
    "#     full_day_tensor = torch.cat(tensors_for_day, dim=0)\n",
    "    \n",
    "#     # Apply the unified 3D differencing function to the day's data\n",
    "#     gradient_tensor = apply_first_difference_3d(full_day_tensor)\n",
    "    \n",
    "#     if gradient_tensor.size(0) > 0:\n",
    "#         all_gradient_tensors.append(gradient_tensor)\n",
    "\n",
    "\n",
    "# --- Verification ---\n",
    "# print(f\"Number of final gradient tensors (one per day): {len(all_gradient_tensors)}\")\n",
    "\n",
    "# if all_gradient_tensors:\n",
    "#     print(\"\\nShape of the first day's gradient tensor:\", all_gradient_tensors[0].shape)\n",
    "#     print(\"Columns: [lat, lon, time, grad_t, grad_lat, grad_lon]\")\n",
    "#     print(\"Head of the first gradient tensor:\")\n",
    "#     print(all_gradient_tensors[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b78daca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data from: all_gradient_tensors.pkl\n",
      "Error: Processed data file not found at all_gradient_tensors.pkl. Ensure the differencing script ran and saved the data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:71: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:71: SyntaxWarning: invalid escape sequence '\\m'\n",
      "/var/folders/9p/53hd4c7d2fl193h4jwp194wc0000gn/T/ipykernel_90986/3708915208.py:71: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  print(f\"\\nðŸ›‘ WARNING: The current data size ({N} points) will be extremely slow for NLL calculation ($\\mathcal{{O}}(N^3)$).\")\n",
      "/var/folders/9p/53hd4c7d2fl193h4jwp194wc0000gn/T/ipykernel_90986/3708915208.py:71: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  print(f\"\\nðŸ›‘ WARNING: The current data size ({N} points) will be extremely slow for NLL calculation ($\\mathcal{{O}}(N^3)$).\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'spatially_filtered_days' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m params = torch.tensor(a, dtype=torch.float64)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Select the first day's differenced tensor\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m raw_data_day1 = \u001b[43mspatially_filtered_days\u001b[49m[\u001b[32m0\u001b[39m].to(torch.float64) \n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# --- 3. Subset by Area and Time ---\u001b[39;00m\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Apply the spatial area subset first\u001b[39;00m\n\u001b[32m     39\u001b[39m subset_data_area = subset_by_area(raw_data_day1)\n",
      "\u001b[31mNameError\u001b[39m: name 'spatially_filtered_days' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# 3. Main Execution Block (Adjusted to use only the first two hours)\n",
    "# =========================================================================\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # --- 1. Load the processed data ---\n",
    "    # CHANGED: Using 'all_gradient_tensors.pkl' as requested.\n",
    "    processed_output_path = \"all_gradient_tensors.pkl\"\n",
    "    print(f\"Loading processed data from: {processed_output_path}\")\n",
    "\n",
    "    try:\n",
    "        with open(processed_output_path, 'rb') as f:\n",
    "            # ðŸ’¡ Loading your spatially differenced data\n",
    "            spatially_filtered_days = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Processed data file not found at {processed_output_path}. Ensure the differencing script ran and saved the data.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading data: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # --- 2. Define parameters and select data subset ---\n",
    "    \n",
    "    # Parameters: [sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget]\n",
    "    # Using the new parameters provided in your prompt\n",
    "    a = [19.89, 1.04, 1.337, 0.040, -0.178, 0.195, 4.498] # 47385\n",
    "    #a = [28.6847, 0.9147, 5.0289, 0.1551, 0.6344, 0.0, 4.1061] \n",
    "    #a = [12.9046, 6.2184, 4.3523, -0.0724, -0.2941, -0.262, 5.4445]\n",
    "    #a = [20.7046, 1.952, 4.1366, -1.0769, -0.3244, 0.1074, 3.9057]\n",
    "    a = [5.2406, 1.4667, 1.3835, -0.5644, -0.6237, 0.5911, 0.0698]\n",
    "    params = torch.tensor(a, dtype=torch.float64)\n",
    "    \n",
    "    # Select the first day's differenced tensor\n",
    "    raw_data_day1 = spatially_filtered_days[0].to(torch.float64) \n",
    "    \n",
    "    # --- 3. Subset by Area and Time ---\n",
    "    \n",
    "    # Apply the spatial area subset first\n",
    "    subset_data_area = subset_by_area(raw_data_day1)\n",
    "    \n",
    "    print(f\"Total points after spatial subsetting: {subset_data_area.shape[0]}\")\n",
    "\n",
    "    # --- Time-based Filtering (First Two Hours) ---\n",
    "    # The 'time' column is the 4th column (index 3).\n",
    "    # Since the original data was loaded in 8-hour chunks per day, and the time indices \n",
    "    # typically represent the chunk number or the time within the day, let's examine \n",
    "    # the time indices in the subsetted data.\n",
    "    \n",
    "    # Find the unique time indices in the subsetted data\n",
    "    unique_times = torch.unique(subset_data_area[:, 3], sorted=True)\n",
    "    hour_n = 1\n",
    "    if len(unique_times) >= hour_n:\n",
    "        # Select the first two unique time indices\n",
    "        time_limit = unique_times[: hour_n]\n",
    "        \n",
    "        # Create a mask for data points where the time index matches one of the first two times\n",
    "        time_mask = torch.isin(subset_data_area[:, 3], time_limit)\n",
    "        subset_data = subset_data_area[time_mask]\n",
    "        \n",
    "        print(f\"Using time indices: {time_limit.tolist()}\")\n",
    "    else:\n",
    "        # Fallback if there aren't two unique time indices\n",
    "        print(\"Warning: Less than two unique time indices available. Using all data after spatial subset.\")\n",
    "        subset_data = subset_data_area\n",
    "        \n",
    "    print(f\"Final subset data shape for likelihood (time-limited): {subset_data.shape}\")\n",
    "\n",
    "    # --- 4. Calculate the full likelihood ---\n",
    "    N = subset_data.shape[0]\n",
    "    if N > 2000:\n",
    "        print(f\"\\nðŸ›‘ WARNING: The current data size ({N} points) will be extremely slow for NLL calculation ($\\mathcal{{O}}(N^3)$).\")\n",
    "        print(\"         The previous limit of 1000 was for performance. Proceeding may take a long time or fail due to memory.\")\n",
    "        # Proceed with caution or add a user confirmation/exit here\n",
    "        \n",
    "    if N > 3: \n",
    "        # The 'response' is the differenced ozone column (index 2)\n",
    "        response_y = subset_data[:, 2]\n",
    "\n",
    "        neg_log_lik_result = full_likelihood(\n",
    "            params=params, \n",
    "            input_data=subset_data, \n",
    "            response=response_y, \n",
    "            covariance_function=matern_cov_anisotropy_v05\n",
    "        )\n",
    "\n",
    "        print(f\"\\nCalculated Negative Log Likelihood: {neg_log_lik_result.item():.4f}\")\n",
    "        print(f\"Parameters used: {a}\")\n",
    "    else:\n",
    "        print(\"\\nNot enough data points found in the specified area after filtering/sampling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007aea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "Calculated Negative Log Likelihood: 53734.1218\n",
    "Parameters used: [20.7046, 1.952, 4.1366, -1.0769, -0.3244, 0.1074, 3.9057]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
