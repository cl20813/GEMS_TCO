{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97a4ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "\n",
    "# --- Standard Libraries ---\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import copy\n",
    "import cmath\n",
    "import pickle\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Path configuration (only run once)\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "# --- Third-Party Libraries ---\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Tuple, Dict, Any, Callable\n",
    "from json import JSONEncoder\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "import typer\n",
    "\n",
    "# Torch and Numerical Libraries\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.fft\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# --- Custom (GEMS_TCO) Imports ---\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels_reparam_space_time \n",
    "from GEMS_TCO import data_preprocess, data_preprocess as dmbh\n",
    "from GEMS_TCO import orderings as _orderings \n",
    "\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "from GEMS_TCO.data_loader import load_data2\n",
    "from GEMS_TCO import debiased_whittle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecf14dd",
   "metadata": {},
   "source": [
    "Load monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47c933d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting data to lat: [0, 5], lon: [123, 133.0]\n"
     ]
    }
   ],
   "source": [
    "space: List[str] = ['4', '4']\n",
    "lat_lon_resolution = [int(s) for s in space]\n",
    "mm_cond_number: int = 8\n",
    "years = ['2024']\n",
    "month_range = [7] \n",
    "\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "data_load_instance = load_data2(config.mac_data_load_path)\n",
    "\n",
    "#lat_range_input = [1, 3]\n",
    "#lon_range_input = [125.0, 129.0]\n",
    "\n",
    "lat_range_input=[0,5]      \n",
    "lon_range_input=[123, 133.0] \n",
    "\n",
    "df_map, ord_mm, nns_map = data_load_instance.load_maxmin_ordered_data_bymonthyear(\n",
    "lat_lon_resolution=lat_lon_resolution, \n",
    "mm_cond_number=mm_cond_number,\n",
    "years_=years, \n",
    "months_=month_range,\n",
    "\n",
    "lat_range=lat_range_input,   \n",
    "lon_range=lon_range_input\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cf1a7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8960, 4])\n"
     ]
    }
   ],
   "source": [
    "daily_aggregated_tensors_dw = [] \n",
    "daily_hourly_maps_dw = []      \n",
    "\n",
    "daily_aggregated_tensors_vecc = [] \n",
    "daily_hourly_maps_vecc = []   \n",
    "\n",
    "\n",
    "for day_index in range(31):\n",
    "    hour_start_index = day_index * 8\n",
    "    hour_end_index = (day_index + 1) * 8\n",
    "    #hour_end_index = day_index*8 + 1\n",
    "    hour_indices = [hour_start_index, hour_end_index]\n",
    "\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "    df_map, \n",
    "    hour_indices, \n",
    "    ord_mm= None,  # or just omit it\n",
    "    dtype=torch.float64, # or just omit it \n",
    "    keep_ori=False  #keep_exact_loc\n",
    "    )\n",
    "\n",
    "    daily_aggregated_tensors_dw.append( day_aggregated_tensor )\n",
    "    daily_hourly_maps_dw.append( day_hourly_map )\n",
    "\n",
    "    day_hourly_map, day_aggregated_tensor = data_load_instance.load_working_data(\n",
    "    df_map, \n",
    "    hour_indices, \n",
    "    ord_mm= ord_mm,  # or just omit it\n",
    "    dtype=torch.float64, # or just omit it \n",
    "    keep_ori=False  #keep_exact_loc\n",
    "    )\n",
    "\n",
    "    daily_aggregated_tensors_vecc.append( day_aggregated_tensor )\n",
    "    daily_hourly_maps_vecc.append( day_hourly_map )\n",
    "print(daily_aggregated_tensors_vecc[0].shape)\n",
    "#print(daily_hourly_maps[0])\n",
    "nn = daily_aggregated_tensors_vecc[0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123d39de",
   "metadata": {},
   "source": [
    "difference data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fed6e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8424, 4])\n",
      "8424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  0.1360, 123.1560,   0.7090,  21.0000],\n",
       "        [  0.1360, 123.4080,   5.2306,  21.0000],\n",
       "        [  0.1360, 123.6600,   1.0815,  21.0000],\n",
       "        ...,\n",
       "        [  4.7120, 132.2280,  -3.8905,  28.0000],\n",
       "        [  4.7120, 132.4800,   7.4197,  28.0000],\n",
       "        [  4.7120, 132.7320,   1.2803,  28.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [11.0474, 0.0623, 0.2445, 1.0972, 0.0101, -0.1671, 1.1825]\n",
    "# a is just for intialization, value of this does not matter\n",
    "\n",
    "day = 0 # 0 index\n",
    "lat_range= [0,5]\n",
    "lon_range= [123.0, 133.0]\n",
    "db = debiased_whittle.debiased_whittle_preprocess(daily_aggregated_tensors_dw, daily_hourly_maps_dw, day_idx=day, params_list=a, lat_range=lat_range, lon_range=lon_range)\n",
    "\n",
    "subsetted_aggregated_day = db.generate_spatially_filtered_days(0,5,123,133)\n",
    "print(subsetted_aggregated_day.shape)\n",
    "N2= subsetted_aggregated_day.shape[0]\n",
    "print(N2)\n",
    "subsetted_aggregated_day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19943d96",
   "metadata": {},
   "source": [
    "Debiased whittle + l-bfgs optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd5591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Pre-computing J-vector (Hamming taper)...\n",
      "Pre-computing sample periodogram...\n",
      "Pre-computing Hamming taper autocorrelation...\n",
      "Data grid: 27x39, 8 time points. J-vector, Periodogram, Taper Autocorr on cpu.\n",
      "\n",
      "============================== Initialization Run 1/1 ==============================\n",
      "Starting with FIXED params (raw log-scale): [4.2042, 1.6348, 0.4721, -2.5562, 0.0218, -0.1689, -1.3984]\n",
      "Starting optimization run 1 on device cpu (Hamming, 7-param ST kernel, L-BFGS)...\n",
      "--- Step 1/20 ---\n",
      " Loss: 13.535233 | Max Grad: 5.852376e-01\n",
      "  Params (Raw Log): log_phi1: 5.1557, log_phi2: 2.6228, log_phi3: 1.2283, log_phi4: -5.2453, advec_lat: -0.0144, advec_lon: 0.0355, log_nugget: -0.9185\n",
      "--- Step 2/20 ---\n",
      " Loss: 8.310017 | Max Grad: 3.515818e-01\n",
      "  Params (Raw Log): log_phi1: 4.4499, log_phi2: 2.0572, log_phi3: 1.8341, log_phi4: -5.0604, advec_lat: -0.0146, advec_lon: 0.0433, log_nugget: 1.1812\n",
      "--- Step 3/20 ---\n",
      " Loss: 8.254692 | Max Grad: 1.687633e-05\n",
      "  Params (Raw Log): log_phi1: 4.3381, log_phi2: 1.9393, log_phi3: 1.9127, log_phi4: -4.9320, advec_lat: -0.0153, advec_lon: 0.0341, log_nugget: 1.2395\n",
      "--- Step 4/20 ---\n",
      " Loss: 8.251540 | Max Grad: 1.687633e-05\n",
      "  Params (Raw Log): log_phi1: 4.3381, log_phi2: 1.9393, log_phi3: 1.9127, log_phi4: -4.9320, advec_lat: -0.0153, advec_lon: 0.0341, log_nugget: 1.2395\n",
      "--- Step 5/20 ---\n",
      " Loss: 8.251540 | Max Grad: 1.687633e-05\n",
      "  Params (Raw Log): log_phi1: 4.3381, log_phi2: 1.9393, log_phi3: 1.9127, log_phi4: -4.9320, advec_lat: -0.0153, advec_lon: 0.0341, log_nugget: 1.2395\n",
      "\n",
      "--- Converged on loss change (change < 1e-12) at step 5 ---\n",
      "\n",
      "--- Training Complete ---\n",
      "\n",
      "FINAL BEST STATE ACHIEVED (during training):\n",
      "Best Loss: 8.252\n",
      "\n",
      "\n",
      "========================= Overall Result from Run ========================= =========================\n",
      "Best Run Loss: 8.252 (after 5 steps)\n",
      "Final Parameters (Natural Scale): sigmasq: 11.0105, range_lat: 0.0553, range_lon: 0.1438, range_time: 1.6933, advec_lat: -0.0153, advec_lon: 0.0341, nugget: 3.4537\n",
      "Final Parameters (Phi Scale)    : phi1: 76.5656, phi2: 6.9539, phi3: 6.7716, phi4: 0.0072, advec_lat: -0.0153, advec_lon: 0.0341, nugget: 3.4537\n",
      "Final Parameters (Raw Log Scale): log_phi1: 4.3381, log_phi2: 1.9393, log_phi3: 1.9127, log_phi4: -4.9320, advec_lat: -0.0153, advec_lon: 0.0341, log_nugget: 1.2395\n",
      "\n",
      "Total execution time: 33.48 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dwl = debiased_whittle.debiased_whittle_likelihood()\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    DAY_TO_RUN = 1 # data is decided above\n",
    "    TAPERING_FUNC = dwl.cgn_hamming # Use Hamming taper\n",
    "    NUM_RUNS = 1\n",
    "    MAX_STEPS = 20 # L-BFGS usually converges in far fewer steps\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063 \n",
    "\n",
    "    LAT_COL, LON_COL = 0, 1\n",
    "    VAL_COL = 2 # Spatially differenced value\n",
    "    TIME_COL = 3\n",
    "\n",
    "\n",
    "    cur_df =subsetted_aggregated_day\n",
    "    \n",
    "    if cur_df.numel() == 0 or cur_df.shape[1] <= max(LAT_COL, LON_COL, VAL_COL, TIME_COL):\n",
    "        print(f\"Error: Data for Day {DAY_TO_RUN} is empty or invalid.\")\n",
    "        exit()\n",
    "\n",
    "    unique_times = torch.unique(cur_df[:, TIME_COL])\n",
    "    time_slices_list = [cur_df[cur_df[:, TIME_COL] == t_val] for t_val in unique_times]\n",
    "\n",
    "    # --- 1. Pre-compute J-vector, Taper Grid, and Taper Autocorrelation ---\n",
    "    print(\"Pre-computing J-vector (Hamming taper)...\")\n",
    "    \n",
    "    # --- ðŸ’¥ REVISED: Renamed 'p' to 'p_time' ðŸ’¥ ---\n",
    "    J_vec, n1, n2, p_time, taper_grid = dwl.generate_Jvector_tapered( \n",
    "        time_slices_list,\n",
    "        tapering_func=TAPERING_FUNC, \n",
    "        lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if J_vec is None or J_vec.numel() == 0 or n1 == 0 or n2 == 0 or p_time == 0:\n",
    "       print(f\"Error: J-vector generation failed for Day {DAY_TO_RUN}.\")\n",
    "       exit()\n",
    "       \n",
    "    print(\"Pre-computing sample periodogram...\")\n",
    "    I_sample = dwl.calculate_sample_periodogram_vectorized(J_vec)\n",
    "\n",
    "    print(\"Pre-computing Hamming taper autocorrelation...\")\n",
    "    taper_autocorr_grid = dwl.calculate_taper_autocorrelation_fft(taper_grid, n1, n2, DEVICE)\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Error: NaN/Inf in sample periodogram.\")\n",
    "        exit()\n",
    "    if torch.isnan(taper_autocorr_grid).any() or torch.isinf(taper_autocorr_grid).any():\n",
    "        print(\"Error: NaN/Inf in taper autocorrelation.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Data grid: {n1}x{n2}, {p_time} time points. J-vector, Periodogram, Taper Autocorr on {DEVICE}.\")\n",
    "    # --- END REVISION ---\n",
    "\n",
    "    # --- 2. Optimization Loop ---\n",
    "    all_final_results = []\n",
    "    all_final_losses = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        print(f\"\\n{'='*30} Initialization Run {i+1}/{NUM_RUNS} {'='*30}\")\n",
    "\n",
    "        # --- 7-PARAMETER initialization ---\n",
    "        ''' \n",
    "        init_sigmasq   = 15.0\n",
    "        init_range_lat = 0.66 \n",
    "        init_range_lon = 0.7 \n",
    "        init_nugget    = 1.5\n",
    "        init_beta      = 0.1  # Temporal range ratio\n",
    "        init_advec_lat = 0.02\n",
    "        init_advec_lon = -0.08\n",
    "        '''\n",
    "        init_sigmasq   = 13.059\n",
    "        init_range_lat = 0.154 \n",
    "        init_range_lon = 0.195\n",
    "        init_advec_lat = 0.0218\n",
    "        init_range_time = 0.7\n",
    "        init_advec_lon = -0.1689\n",
    "        init_nugget    = 0.247\n",
    "\n",
    "        init_phi2 = 1.0 / init_range_lon\n",
    "        init_phi1 = init_sigmasq * init_phi2\n",
    "        init_phi3 = (init_range_lon / init_range_lat)**2\n",
    "        # Change needed to match the spatial-temporal distance formula:\n",
    "        init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "        initial_params_values = [\n",
    "            np.log(init_phi1),    # [0] log_phi1\n",
    "            np.log(init_phi2),    # [1] log_phi2\n",
    "            np.log(init_phi3),    # [2] log_phi3\n",
    "            np.log(init_phi4),    # [3] log_phi4\n",
    "            init_advec_lat,       # [4] advec_lat (NOT log)\n",
    "            init_advec_lon,       # [5] advec_lon (NOT log)\n",
    "            np.log(init_nugget)   # [6] log_nugget\n",
    "        ]\n",
    "        \n",
    "        print(f\"Starting with FIXED params (raw log-scale): {[round(p, 4) for p in initial_params_values]}\")\n",
    "\n",
    "        params_list = [\n",
    "            Parameter(torch.tensor([val], dtype=torch.float64))\n",
    "            for val in initial_params_values\n",
    "        ]\n",
    "\n",
    "        # Helper to define the boundary globally for clarity\n",
    "        NUGGET_LOWER_BOUND = 0.05\n",
    "        LOG_NUGGET_LOWER_BOUND = np.log(NUGGET_LOWER_BOUND) # Approx -2.9957\n",
    "\n",
    "        # --- ðŸ’¥ REVISED: Use L-BFGS Optimizer ðŸ’¥ ---\n",
    "        optimizer = torch.optim.LBFGS(\n",
    "            params_list,\n",
    "            lr=1.0,           # Initial step length for line search\n",
    "            max_iter=20,      # Iterations per step\n",
    "            history_size=100,\n",
    "            line_search_fn=\"strong_wolfe\", # Often more robust\n",
    "            tolerance_grad=1e-5\n",
    "        )\n",
    "        # --- END REVISION ---\n",
    "\n",
    "        print(f\"Starting optimization run {i+1} on device {DEVICE} (Hamming, 7-param ST kernel, L-BFGS)...\")\n",
    "        \n",
    "        # --- ðŸ’¥ REVISED: Call L-BFGS trainer, pass p_time ðŸ’¥ ---\n",
    "        nat_params_str, phi_params_str, raw_params_str, loss, steps_run = dwl.run_lbfgs_tapered(\n",
    "            params_list=params_list,\n",
    "            optimizer=optimizer,\n",
    "            I_sample=I_sample,\n",
    "            n1=n1, n2=n2, p_time=p_time,\n",
    "            taper_autocorr_grid=taper_autocorr_grid, \n",
    "            max_steps=MAX_STEPS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        # --- END REVISION ---\n",
    "        \n",
    "        if loss is not None:\n",
    "            all_final_results.append((nat_params_str, phi_params_str, raw_params_str))\n",
    "            all_final_losses.append(loss)\n",
    "        else:\n",
    "            all_final_losses.append(float('inf'))\n",
    "\n",
    "    print(f\"\\n\\n{'='*25} Overall Result from Run {'='*25} {'='*25}\")\n",
    "    \n",
    "    valid_losses = [l for l in all_final_losses if l is not None and l != float('inf')]\n",
    "\n",
    "    if not valid_losses:\n",
    "        print(f\"The run failed or resulted in an invalid loss for Day {DAY_TO_RUN}.\")\n",
    "    else:\n",
    "        best_loss = min(valid_losses)\n",
    "        best_run_index = all_final_losses.index(best_loss)\n",
    "        best_results = all_final_results[best_run_index]\n",
    "        \n",
    "        print(f\"Best Run Loss: {best_loss} (after {steps_run} steps)\")\n",
    "        print(f\"Final Parameters (Natural Scale): {best_results[0]}\")\n",
    "        print(f\"Final Parameters (Phi Scale)    : {best_results[1]}\")\n",
    "        print(f\"Final Parameters (Raw Log Scale): {best_results[2]}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04700959",
   "metadata": {},
   "source": [
    "Debiased whittle + Adams optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6e67df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# 6. Main Execution Script (ðŸ’¥ 7-PARAM MULTIVARIATE ðŸ’¥)\n",
    "# =========================================================================\n",
    "\n",
    "dwl = debiased_whittle.debiased_whittle_likelihood()\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    DAY_TO_RUN = 1\n",
    "    TAPERING_FUNC = dwl.cgn_hamming # Use Hamming taper\n",
    "    NUM_RUNS = 1\n",
    "    EPOCHS = 200\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063 \n",
    "\n",
    "    LAT_COL, LON_COL = 0, 1\n",
    "    VAL_COL = 2 # Spatially differenced value\n",
    "    TIME_COL = 3\n",
    "    lr = 0.1 \n",
    "\n",
    "    # --- Load Spatially Differenced Data ---\n",
    "\n",
    "    cur_df = subsetted_aggregated_day\n",
    "    \n",
    "\n",
    "    # --- 1. Pre-compute J-vector, Taper Grid, and Taper Autocorrelation ---\n",
    "    print(\"Pre-computing J-vector (Hamming taper)...\")\n",
    "    J_vec, n1, n2, p, taper_grid = dwl.generate_Jvector_tapered( \n",
    "        time_slices_list,\n",
    "        tapering_func=TAPERING_FUNC, \n",
    "        lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if J_vec is None or J_vec.numel() == 0 or n1 == 0 or n2 == 0 or p == 0:\n",
    "       print(f\"Error: J-vector generation failed for Day {DAY_TO_RUN}.\")\n",
    "       exit()\n",
    "       \n",
    "    print(\"Pre-computing sample periodogram...\")\n",
    "    I_sample = dwl.calculate_sample_periodogram_vectorized(J_vec)\n",
    "\n",
    "    print(\"Pre-computing Hamming taper autocorrelation...\")\n",
    "    taper_autocorr_grid = dwl.calculate_taper_autocorrelation_fft(taper_grid, n1, n2, DEVICE)\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Error: NaN/Inf in sample periodogram.\")\n",
    "        exit()\n",
    "    if torch.isnan(taper_autocorr_grid).any() or torch.isinf(taper_autocorr_grid).any():\n",
    "        print(\"Error: NaN/Inf in taper autocorrelation.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Data grid: {n1}x{n2}, {p} time points. J-vector, Periodogram, Taper Autocorr on {DEVICE}.\")\n",
    "\n",
    "    # --- 2. Optimization Loop ---\n",
    "    all_final_results = []\n",
    "    all_final_losses = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        print(f\"\\n{'='*30} Initialization Run {i+1}/{NUM_RUNS} {'='*30}\")\n",
    "\n",
    "        # --- 7-PARAMETER initialization ---\n",
    "        ''' \n",
    "        init_sigmasq   = 15.0\n",
    "        init_range_lat = 0.66 \n",
    "        init_range_lon = 0.7 \n",
    "        init_nugget    = 1.5\n",
    "        init_beta      = 0.1  # Temporal range ratio\n",
    "        init_advec_lat = 0.02\n",
    "        init_advec_lon = -0.08\n",
    "        '''\n",
    "        init_sigmasq   = 13.059\n",
    "        init_range_lat = 0.154 \n",
    "        init_range_lon = 0.195 \n",
    "        init_nugget    = 1.247\n",
    "        init_range_time = 1.28\n",
    "        init_advec_lat = 0.0218\n",
    "        init_advec_lon = -0.1689\n",
    "\n",
    "\n",
    "        \n",
    "        init_phi2 = 1.0 / init_range_lon\n",
    "        init_phi1 = init_sigmasq * init_phi2\n",
    "        init_phi3 = (init_range_lon / init_range_lat)**2\n",
    "        init_phi4 = (init_range_lon / init_range_time)**2      # (range_lon / range_time)^2\n",
    "\n",
    "        initial_params_values = [\n",
    "            np.log(init_phi1),    # [0] log_phi1\n",
    "            np.log(init_phi2),    # [1] log_phi2\n",
    "            np.log(init_phi3),    # [2] log_phi3\n",
    "            np.log(init_phi4),    # [3] log_phi4\n",
    "            init_advec_lat,       # [4] advec_lat (NOT log)\n",
    "            init_advec_lon,       # [5] advec_lon (NOT log)\n",
    "            np.log(init_nugget)   # [6] log_nugget\n",
    "        ]\n",
    "        \n",
    "        print(f\"Starting with FIXED params (raw log-scale): {[round(p, 4) for p in initial_params_values]}\")\n",
    "\n",
    "        params_list = [\n",
    "            Parameter(torch.tensor([val], dtype=torch.float32))\n",
    "            for val in initial_params_values\n",
    "        ]\n",
    "\n",
    "\n",
    "        optimizer = torch.optim.Adam(params_list, lr=lr)\n",
    "\n",
    "        # --- ðŸ’¥ REVISED: Use Plateau Scheduler ðŸ’¥ ---\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=10, # Wait 10 epochs for improvement\n",
    "            verbose=True\n",
    "        )\n",
    "        # --- END REVISION ---\n",
    "\n",
    "        print(f\"Starting optimization run {i+1} on device {DEVICE} (Hamming, 7-param ST kernel)...\")\n",
    "\n",
    "        nat_params_str, phi_params_str, raw_params_str, loss, epochs_run = dwl.run_full_tapered(\n",
    "            params_list=params_list,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            I_sample=I_sample,\n",
    "            n1=n1, n2=n2, p_time=p,\n",
    "            taper_autocorr_grid=taper_autocorr_grid, \n",
    "            epochs=EPOCHS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        \n",
    "        if loss is not None:\n",
    "            all_final_results.append((nat_params_str, phi_params_str, raw_params_str))\n",
    "            all_final_losses.append(loss)\n",
    "        else:\n",
    "            all_final_losses.append(float('inf'))\n",
    "\n",
    "    # --- ðŸ’¥ REVISED: Corrected f-string ðŸ’¥ ---\n",
    "    print(f\"\\n\\n{'='*25} Overall Result from Run {'='*25} {'='*25}\")\n",
    "    # --- END REVISION ---\n",
    "    \n",
    "    valid_losses = [l for l in all_final_losses if l is not None and l != float('inf')]\n",
    "\n",
    "    if not valid_losses:\n",
    "        print(f\"The run failed or resulted in an invalid loss for Day {DAY_TO_RUN}.\")\n",
    "    else:\n",
    "        best_loss = min(valid_losses)\n",
    "        best_run_index = all_final_losses.index(best_loss)\n",
    "        best_results = all_final_results[best_run_index]\n",
    "        \n",
    "        print(f\"Best Run Loss: {best_loss} (after {epochs_run} epochs)\")\n",
    "        print(f\"Final Parameters (Natural Scale): {best_results[0]}\")\n",
    "        print(f\"Final Parameters (Phi Scale)    : {best_results[1]}\")\n",
    "        print(f\"Final Parameters (Raw Log Scale): {best_results[2]}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4c337",
   "metadata": {},
   "source": [
    "## Once differencing in both space and then another differencing in  time\n",
    "\n",
    "### Models the temporal change of the spatial curvature (or gradient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49ab27d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/joonwonlee/Documents/GEMS_DATA/pickle_2024/coarse_cen_map_without_decrement_latitude24_07.pkl\n",
      "Loaded 31 days of raw data.\n",
      "Starting STAGE 1: Spatial Differencing (Convolution)...\n",
      "STAGE 1 Complete. Created 31 spatially filtered day-tensors.\n",
      "Starting STAGE 2: Temporal Differencing...\n",
      "STAGE 2 Complete. Created 30 final tensors.\n",
      "\n",
      "--- Results ---\n",
      "Number of spatially filtered day tensors: 31\n",
      "Number of final spatio-temporally differenced tensors: 30\n",
      "Processed data saved to spacetime_differenced_data.pkl\n",
      "\n",
      "Shape of the first final tensor: torch.Size([142832, 4])\n",
      "First final tensor head:\n",
      "tensor([[ 4.0000e-03,  1.2303e+02, -1.6478e+01,  4.5000e+01],\n",
      "        [ 4.0000e-03,  1.2309e+02,  2.6450e+00,  4.5000e+01],\n",
      "        [ 4.0000e-03,  1.2316e+02, -8.7633e+00,  4.5000e+01],\n",
      "        [ 4.0000e-03,  1.2322e+02,  1.6463e+01,  4.5000e+01],\n",
      "        [ 4.0000e-03,  1.2328e+02,  1.0886e+01,  4.5000e+01]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# --- Helper Functions (REVISED FOR CONVOLUTION) ---\n",
    "\n",
    "def subset_tensor(df_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Subsets a tensor to a specific lat/lon range.\n",
    "    Columns are assumed to be [lat, lon, ozone, time].\n",
    "    \"\"\"\n",
    "    lat_mask = (df_tensor[:, 0] >= 0) & (df_tensor[:, 0] <= 5)\n",
    "    lon_mask = (df_tensor[:, 1] >= 123) & (df_tensor[:, 1] <= 133)\n",
    "    \n",
    "    df_sub = df_tensor[lat_mask & lon_mask].clone()\n",
    "    return df_sub\n",
    "\n",
    "def apply_spatial_diff_convolution(df_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies the first-order spatial difference Z(s) = [X(s+d_lat) - X(s)] + [X(s+d_lon) - X(s)]\n",
    "    using a 2D convolution, assuming the input tensor is a complete grid (non-sparse).\n",
    "    \"\"\"\n",
    "    if df_tensor.size(0) == 0:\n",
    "        return torch.empty(0, 4, dtype=df_tensor.dtype)\n",
    "\n",
    "    # 1. Get grid dimensions and enforce non-sparse grid constraint\n",
    "    unique_lats = torch.unique(df_tensor[:, 0])\n",
    "    unique_lons = torch.unique(df_tensor[:, 1])\n",
    "    lat_count, lon_count = unique_lats.size(0), unique_lons.size(0)\n",
    "\n",
    "    if df_tensor.size(0) != lat_count * lon_count:\n",
    "        # Since you confirmed data is not sparse, this should not trigger.\n",
    "        raise ValueError(\"Tensor size does not match grid dimensions. Must be a complete grid for convolution.\")\n",
    "    if lat_count < 2 or lon_count < 2:\n",
    "        return torch.empty(0, 4, dtype=df_tensor.dtype)\n",
    "\n",
    "    # 2. Map coordinates to indices and Reshape data (Ozone values)\n",
    "    lat_map = {lat.item(): i for i, lat in enumerate(unique_lats)}\n",
    "    lon_map = {lon.item(): i for i, lon in enumerate(unique_lons)}\n",
    "\n",
    "    ozone_grid = torch.zeros((lat_count, lon_count), dtype=df_tensor.dtype)\n",
    "    for row in df_tensor:\n",
    "        i = lat_map[row[0].item()]\n",
    "        j = lon_map[row[1].item()]\n",
    "        ozone_grid[i, j] = row[2]\n",
    "        \n",
    "    ozone_data = ozone_grid.reshape(1, 1, lat_count, lon_count)\n",
    "    \n",
    "    # Kernel for Z(i,j) = X(i+1,j) + X(i,j+1) - 2*X(i,j)\n",
    "    # This assumes the first dimension is latitude (i) and the second is longitude (j)\n",
    "    diff_kernel = torch.tensor([[[[-2., 1.],\n",
    "                                  [ 1., 0.]]]], dtype=df_tensor.dtype)\n",
    "\n",
    "    # 3. Apply convolution (cross-correlation)\n",
    "    filtered_grid = F.conv2d(ozone_data, diff_kernel, padding='valid').squeeze()\n",
    "\n",
    "    # 4. Determine coordinates for the new, smaller grid\n",
    "    # The new grid corresponds to the anchor points (top-left of the kernel)\n",
    "    new_lats = unique_lats[:-1]\n",
    "    new_lons = unique_lons[:-1]\n",
    "\n",
    "    # 5. Reconstruct the output tensor\n",
    "    new_lat_grid, new_lon_grid = torch.meshgrid(new_lats, new_lons, indexing='ij')\n",
    "    filtered_values = filtered_grid.flatten()\n",
    "    time_value = df_tensor[0, 3].repeat(filtered_values.size(0))\n",
    "\n",
    "    new_tensor = torch.stack([\n",
    "        new_lat_grid.flatten(),\n",
    "        new_lon_grid.flatten(),\n",
    "        filtered_values,\n",
    "        time_value\n",
    "    ], dim=1)\n",
    "    \n",
    "    return new_tensor\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# --- Data Loading (Kept structure, placeholder variables must be defined) ---\n",
    "# ----------------------------------------------------------------------\n",
    "# âš ï¸ NOTE: You must define these variables in your environment\n",
    "# mac_data_path = \"...\"\n",
    "# year = 2022\n",
    "# month_str = \"01\"\n",
    "# class GEMS_TCO: # Placeholder\n",
    "#     def load_data(self, path): return self\n",
    "#     def load_working_data_byday_wo_mm(self, data, indices):\n",
    "#         return {'key': torch.randn(100, 4)}, torch.randn(100, 4)\n",
    "\n",
    "# (Assuming data loading variables are defined...)\n",
    "# NOTE: Removed the try/except block to keep the data loading structure clean as requested,\n",
    "# but ensure 'mac_data_path', 'year', 'month_str', and 'GEMS_TCO' are defined externally.\n",
    "\n",
    "pickle_path = os.path.join(mac_data_path, f'pickle_{YEAR_TO_LOAD}')\n",
    "output_filename = f\"coarse_cen_map_without_decrement_latitude{str(YEAR_TO_LOAD)[2:]}_{month_str}.pkl\"\n",
    "output_filepath = os.path.join(pickle_path, output_filename)\n",
    "print(f\"Loading data from: {output_filepath}\")\n",
    "\n",
    "with open(output_filepath, 'rb') as pickle_file:\n",
    "    cbmap_ori = pickle.load(pickle_file)\n",
    "\n",
    "load_data_instance = GEMS_TCO.load_data('')\n",
    "df_day_map_list = []\n",
    "for i in range(31): # Adjust if necessary\n",
    "    cur_map, _ = load_data_instance.load_working_data_byday_wo_mm(cbmap_ori, [i*8, (i+1)*8])\n",
    "    df_day_map_list.append(cur_map)\n",
    "print(f\"Loaded {len(df_day_map_list)} days of raw data.\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# --- Main Processing Loop (STAGE 1 uses convolution function) ---\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# âœ… STAGE 1: Apply the spatial filter to each day independently.\n",
    "spatially_filtered_days = []\n",
    "\n",
    "print(\"Starting STAGE 1: Spatial Differencing (Convolution)...\")\n",
    "for day_idx, day_map in enumerate(df_day_map_list):\n",
    "    tensors_to_aggregate = []\n",
    "    \n",
    "    for key, tensor in day_map.items():\n",
    "        subsetted = subset_tensor(tensor)\n",
    "        \n",
    "        if subsetted.size(0) > 0:\n",
    "            try:\n",
    "                # --- âœ… CALLING THE NEW CONVOLUTION FUNCTION ---\n",
    "                diff_applied = apply_spatial_diff_convolution(subsetted)\n",
    "                \n",
    "                if diff_applied.size(0) > 0:\n",
    "                    tensors_to_aggregate.append(diff_applied)\n",
    "            except ValueError as e:\n",
    "                # This catches incomplete grid chunks or chunks with < 2 lats/lons\n",
    "                print(f\"Skipping chunk on day {day_idx+1}, key {key}: {e}\")\n",
    "\n",
    "    if tensors_to_aggregate:\n",
    "        aggregated_day_tensor = torch.cat(tensors_to_aggregate, dim=0)\n",
    "        spatially_filtered_days.append(aggregated_day_tensor)\n",
    "print(f\"STAGE 1 Complete. Created {len(spatially_filtered_days)} spatially filtered day-tensors.\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# âœ… STAGE 2: Apply the temporal first difference (value_t - value_t-1).\n",
    "# ----------------------------------------------------------------------\n",
    "spacetime_diff_tensors = []\n",
    "\n",
    "print(\"Starting STAGE 2: Temporal Differencing...\")\n",
    "if len(spatially_filtered_days) > 1:\n",
    "    for i in range(1, len(spatially_filtered_days)):\n",
    "        prev_day_tensor = spatially_filtered_days[i-1]\n",
    "        current_day_tensor = spatially_filtered_days[i]\n",
    "        \n",
    "        # Round keys to avoid floating point mismatches\n",
    "        prev_day_lookup = {\n",
    "            (round(row[0].item(), 5), round(row[1].item(), 5)): row[2].item() \n",
    "            for row in prev_day_tensor\n",
    "        }\n",
    "        \n",
    "        temporally_differenced_rows = []\n",
    "        for row in current_day_tensor:\n",
    "            lat = round(row[0].item(), 5)\n",
    "            lon = round(row[1].item(), 5)\n",
    "            \n",
    "            # This lookup is more robust now because Stage 1 ensures a consistent grid\n",
    "            if (lat, lon) in prev_day_lookup:\n",
    "                current_ozone = row[2].item()\n",
    "                prev_ozone = prev_day_lookup[(lat, lon)]\n",
    "                \n",
    "                time_diff_ozone = current_ozone - prev_ozone\n",
    "                \n",
    "                # Keep dtype consistent with the source row\n",
    "                new_row = torch.tensor([lat, lon, time_diff_ozone, row[3]], dtype=row.dtype) \n",
    "                temporally_differenced_rows.append(new_row)\n",
    "        \n",
    "        if temporally_differenced_rows:\n",
    "            day_diff_tensor = torch.stack(temporally_differenced_rows, dim=0)\n",
    "            spacetime_diff_tensors.append(day_diff_tensor)\n",
    "print(f\"STAGE 2 Complete. Created {len(spacetime_diff_tensors)} final tensors.\")\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\n--- Results ---\")\n",
    "print(\"Number of spatially filtered day tensors:\", len(spatially_filtered_days))\n",
    "print(\"Number of final spatio-temporally differenced tensors:\", len(spacetime_diff_tensors))\n",
    "\n",
    "if spacetime_diff_tensors:\n",
    "    # Save the processed data\n",
    "    processed_output_path = \"spacetime_differenced_data.pkl\"\n",
    "    with open(processed_output_path, 'wb') as f:\n",
    "        pickle.dump(spacetime_diff_tensors, f)\n",
    "    print(f\"Processed data saved to {processed_output_path}\")\n",
    "\n",
    "    print(\"\\nShape of the first final tensor:\", spacetime_diff_tensors[0].shape)\n",
    "    print(\"First final tensor head:\")\n",
    "    print(spacetime_diff_tensors[0][:5])\n",
    "else:\n",
    "    print(\"\\nNo final differenced tensors were created. Check data or filter logic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3513ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # Keep if plotting might be added later\n",
    "import cmath\n",
    "import pickle\n",
    "import time # For timing\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import Parameter\n",
    "import pandas as pd # Make sure pandas is imported\n",
    "import os # Make sure os is imported\n",
    "\n",
    "# =========================================================================\n",
    "# 1. Modeling Functions (Adapted for Spatio-Temporal Differencing)\n",
    "# =========================================================================\n",
    "\n",
    "# --- Bartlett Kernel (Used for c_gn when g_s=1) ---\n",
    "def cgn_2dbartlett_kernel(u1, u2, n1, n2):\n",
    "    \"\"\"\n",
    "    Computes the 2D Bartlett kernel: Product(1 - |ui|/ni). (Unchanged)\n",
    "    \"\"\"\n",
    "    device = u1.device if isinstance(u1, torch.Tensor) else (u2.device if isinstance(u2, torch.Tensor) else torch.device('cpu'))\n",
    "    u1_tensor = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_tensor = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    n1_eff = float(n1) if n1 > 0 else 1.0\n",
    "    n2_eff = float(n2) if n2 > 0 else 1.0\n",
    "    kernel = (1.0 - torch.abs(u1_tensor) / n1_eff) * (1.0 - torch.abs(u2_tensor) / n2_eff)\n",
    "    return torch.clamp(kernel, min=0.0)\n",
    "\n",
    "# --- Covariance of the Original Field X (EXPONENTIAL Kernel) ---\n",
    "def cov_x_exponential(u1, u2, t, params):\n",
    "    \"\"\"\n",
    "    Computes the autocovariance of the ORIGINAL process X. (Unchanged)\n",
    "    Expects log-scale params [0,1,2,6].\n",
    "    \"\"\"\n",
    "    device = params.device \n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    log_params_indices = [0, 1, 2, 6]\n",
    "    if torch.isnan(params[log_params_indices]).any() or torch.isinf(params[log_params_indices]).any():\n",
    "         out_shape = torch.broadcast_shapes(u1_dev.shape, u2_dev.shape, t_dev.shape)\n",
    "         return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "\n",
    "    sigmasq, r_lat, r_lon, nugget = torch.exp(params[log_params_indices])\n",
    "    a_lat, a_lon, beta = params[3], params[4], params[5]\n",
    "\n",
    "    r_lat = torch.clamp(r_lat, min=1e-6)\n",
    "    r_lon = torch.clamp(r_lon, min=1e-6)\n",
    "\n",
    "    x1 = u1_dev / r_lat - a_lat * t_dev\n",
    "    x2 = u2_dev / r_lon - a_lon * t_dev\n",
    "    x3 = beta * t_dev\n",
    "    distance_sq = x1**2 + x2**2 + x3**2\n",
    "    epsilon = 1e-12\n",
    "    distance_sq_clamped = torch.clamp(distance_sq, min=0.0)\n",
    "    D = torch.sqrt(distance_sq_clamped + epsilon) \n",
    "    cov_smooth = sigmasq * torch.exp(-D) \n",
    "\n",
    "    is_zero_lag = (torch.abs(u1_dev) < 1e-9) & (torch.abs(u2_dev) < 1e-9) & (torch.abs(t_dev) < 1e-9)\n",
    "    final_cov = torch.where(is_zero_lag, cov_smooth + nugget, cov_smooth)\n",
    "\n",
    "    if torch.isnan(final_cov).any():\n",
    "        print(\"Warning: NaN detected in cov_x_exponential output.\")\n",
    "    return final_cov\n",
    "\n",
    "\n",
    "# --- Covariance of the Spatially Differenced Field Z ---\n",
    "def cov_spatial_difference(u1, u2, t, params, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Calculates covariance Cov(Z(s), Z(s+u)) for the SPATIAL-ONLY filter:\n",
    "    Z(s) = X(s+d1) + X(s+d2) - 2X(s). (Unchanged)\n",
    "    \"\"\"\n",
    "    weights = {(0, 0): -2.0, (1, 0): 1.0, (0, 1): 1.0}\n",
    "    device = params.device\n",
    "    out_shape = torch.broadcast_shapes(u1.shape if isinstance(u1, torch.Tensor) else (),\n",
    "                                     u2.shape if isinstance(u2, torch.Tensor) else (),\n",
    "                                     t.shape if isinstance(t, torch.Tensor) else ())\n",
    "    cov = torch.zeros(out_shape, device=device, dtype=torch.float32)\n",
    "\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    for (a_idx, b_idx), w_ab in weights.items():\n",
    "        offset_a1 = a_idx * delta1\n",
    "        offset_a2 = b_idx * delta2\n",
    "        for (c_idx, d_idx), w_cd in weights.items():\n",
    "            offset_c1 = c_idx * delta1\n",
    "            offset_c2 = d_idx * delta2\n",
    "            lag_u1 = u1_dev + (offset_a1 - offset_c1)\n",
    "            lag_u2 = u2_dev + (offset_a2 - offset_c2)\n",
    "            term_cov = cov_x_exponential(lag_u1, lag_u2, t_dev, params)\n",
    "            if torch.isnan(term_cov).any():\n",
    "                 return torch.full_like(cov, float('nan'))\n",
    "            cov += w_ab * w_cd * term_cov\n",
    "\n",
    "    if torch.isnan(cov).any():\n",
    "        print(\"Warning: NaN detected in final cov_spatial_difference output.\")\n",
    "    return cov\n",
    "\n",
    "# --- (NEW) Covariance of Spatio-Temporal Differenced Field Y ---\n",
    "def cov_spacetime_difference(u1, u2, t, params, delta1, delta2):\n",
    "    \"\"\"\n",
    "    âœ… Calculates covariance for the new Spatio-Temporal filter:\n",
    "    Y(s,t) = Z(s,t) - Z(s,t-1), where Z is the spatially-differenced field.\n",
    "    This is C_Y(u, tau) = 2*C_Z(u, tau) - C_Z(u, tau-1) - C_Z(u, tau+1)\n",
    "    \"\"\"\n",
    "    device = params.device\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    # C_Z(u, tau)\n",
    "    term_center = cov_spatial_difference(u1_dev, u2_dev, t_dev, params, delta1, delta2)\n",
    "    # C_Z(u, tau - 1)\n",
    "    term_minus_1 = cov_spatial_difference(u1_dev, u2_dev, t_dev - 1.0, params, delta1, delta2)\n",
    "    # C_Z(u, tau + 1)\n",
    "    term_plus_1 = cov_spatial_difference(u1_dev, u2_dev, t_dev + 1.0, params, delta1, delta2)\n",
    "\n",
    "    if torch.isnan(term_center).any() or torch.isnan(term_minus_1).any() or torch.isnan(term_plus_1).any():\n",
    "        print(\"Warning: NaN detected in one of the terms of cov_spacetime_difference\")\n",
    "        out_shape = torch.broadcast_shapes(u1_dev.shape, u2_dev.shape, t_dev.shape)\n",
    "        return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "\n",
    "    return 2.0 * term_center - term_minus_1 - term_plus_1\n",
    "\n",
    "\n",
    "# --- (MODIFIED) cn_bar for NO TAPERING ---\n",
    "def cn_bar_no_taper(u1, u2, t, params, n1, n2, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Computes c_Y(u) * c_gn(u) where c_Y is cov_spacetime_difference\n",
    "    and c_gn(u) is the Bartlett kernel.\n",
    "    \"\"\"\n",
    "    device = params.device\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    # âœ… --- Call the new spatio-temporal covariance function ---\n",
    "    cov_Y_value = cov_spacetime_difference(u1_dev, u2_dev, t_dev, params, delta1, delta2)\n",
    "\n",
    "    c_gn_value = cgn_2dbartlett_kernel(u1_dev, u2_dev, n1, n2)\n",
    "\n",
    "    if torch.isnan(cov_Y_value).any() or torch.isnan(c_gn_value).any():\n",
    "        print(\"Warning: NaN detected before multiplication in cn_bar_no_taper.\")\n",
    "        out_shape = torch.broadcast_shapes(cov_Y_value.shape, c_gn_value.shape)\n",
    "        return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "\n",
    "    result = cov_Y_value * c_gn_value\n",
    "    if torch.isnan(result).any():\n",
    "        print(\"Warning: NaN detected after multiplication in cn_bar_no_taper.\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# --- Expected Periodogram (uses cn_bar_no_taper) ---\n",
    "def expected_periodogram_fft_no_taper(params, n1, n2, p, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Calculates the expected periodogram. (Unchanged)\n",
    "    This function is correct because it calls the modified cn_bar_no_taper.\n",
    "    \"\"\"\n",
    "    device = params.device if isinstance(params, torch.Tensor) else params[0].device\n",
    "    params_tensor = params.to(device)\n",
    "\n",
    "    product_tensor = torch.zeros((n1, n2, p, p), dtype=torch.complex64, device=device)\n",
    "    t_lags = torch.arange(p, dtype=torch.float32, device=device)\n",
    "    u1_mesh_grid, u2_mesh_grid = torch.meshgrid(\n",
    "        torch.arange(n1, dtype=torch.float32, device=device),\n",
    "        torch.arange(n2, dtype=torch.float32, device=device),\n",
    "        indexing='ij'\n",
    "    )\n",
    "\n",
    "    for q in range(p):\n",
    "        for r in range(p):\n",
    "            t_diff = t_lags[q] - t_lags[r]\n",
    "            # This call now correctly leads to cov_spacetime_difference\n",
    "            cov_times_bartlett = cn_bar_no_taper(\n",
    "                u1_mesh_grid, u2_mesh_grid, t_diff,\n",
    "                params_tensor, n1, n2, delta1, delta2\n",
    "            )\n",
    "            if torch.isnan(cov_times_bartlett).any():\n",
    "                 product_tensor[:, :, q, r] = float('nan')\n",
    "            else:\n",
    "                 product_tensor[:, :, q, r] = cov_times_bartlett.to(torch.complex64)\n",
    "\n",
    "    if torch.isnan(product_tensor).any():\n",
    "        print(\"Warning: NaN detected in product_tensor before FFT.\")\n",
    "        nan_shape = (n1, n2, p, p)\n",
    "        return torch.full(nan_shape, float('nan'), dtype=torch.complex64, device=device)\n",
    "\n",
    "    fft_result = torch.fft.fft2(product_tensor, dim=(0, 1))\n",
    "    normalization_factor = 1.0 / (4.0 * cmath.pi**2)\n",
    "    result = fft_result * normalization_factor\n",
    "\n",
    "    if torch.isnan(result).any():\n",
    "        print(\"Warning: NaN detected in expected_periodogram_fft_no_taper output after FFT.\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 2. Data Processing (Unchanged)\n",
    "# =========================================================================\n",
    "def generate_Jvector_no_taper(tensor_list, lat_col, lon_col, val_col, device):\n",
    "    \"\"\"\n",
    "    Generates J-vector for g_s=1 (NO taper). (Unchanged)\n",
    "    \"\"\"\n",
    "    p = len(tensor_list)\n",
    "    if p == 0: return torch.empty(0, 0, 0, device=device), 0, 0, 0\n",
    "\n",
    "    valid_tensors = [t for t in tensor_list if t.numel() > 0 and t.shape[1] > max(lat_col, lon_col, val_col)]\n",
    "    if not valid_tensors:\n",
    "         return torch.empty(0, 0, 0, device=device), 0, 0, 0\n",
    "\n",
    "    try:\n",
    "        all_lats_cpu = torch.cat([t[:, lat_col] for t in valid_tensors])\n",
    "        all_lons_cpu = torch.cat([t[:, lon_col] for t in valid_tensors])\n",
    "    except IndexError:\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0\n",
    "\n",
    "    all_lats_cpu = all_lats_cpu[~torch.isnan(all_lats_cpu) & ~torch.isinf(all_lats_cpu)]\n",
    "    all_lons_cpu = all_lons_cpu[~torch.isnan(all_lons_cpu) & ~torch.isinf(all_lons_cpu)]\n",
    "    if all_lats_cpu.numel() == 0 or all_lons_cpu.numel() == 0:\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0\n",
    "\n",
    "    unique_lats_cpu, unique_lons_cpu = torch.unique(all_lats_cpu), torch.unique(all_lons_cpu)\n",
    "    n1, n2 = len(unique_lats_cpu), len(unique_lons_cpu)\n",
    "    if n1 == 0 or n2 == 0:\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0\n",
    "\n",
    "    lat_map = {lat.item(): i for i, lat in enumerate(unique_lats_cpu)}\n",
    "    lon_map = {lon.item(): i for i, lon in enumerate(unique_lons_cpu)}\n",
    "\n",
    "    fft_results = []\n",
    "    for tensor in tensor_list:\n",
    "        data_grid = torch.zeros((n1, n2), dtype=torch.float32, device=device)\n",
    "        # Move tensor to device before iterating\n",
    "        tensor_dev = tensor.to(device)\n",
    "        for row in tensor_dev:\n",
    "            lat_item, lon_item = row[lat_col].item(), row[lon_col].item()\n",
    "            if not (np.isnan(lat_item) or np.isnan(lon_item)):\n",
    "                i = lat_map.get(lat_item)\n",
    "                j = lon_map.get(lon_item)\n",
    "                if i is not None and j is not None:\n",
    "                    val = row[val_col]\n",
    "                    val_num = val.item() if isinstance(val, torch.Tensor) else val\n",
    "                    if not np.isnan(val_num) and not np.isinf(val_num):\n",
    "                        data_grid[i, j] = val_num\n",
    "\n",
    "        data_grid = torch.nan_to_num(data_grid, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        fft_results.append(torch.fft.fft2(data_grid))\n",
    "\n",
    "    if not fft_results:\n",
    "         return torch.empty(0, 0, 0, device=device), n1, n2, 0\n",
    "\n",
    "    J_vector_tensor = torch.stack(fft_results, dim=2).to(device)\n",
    "\n",
    "    H = float(n1 * n2)\n",
    "    if H < 1e-9:\n",
    "        norm_factor = torch.tensor(0.0, device=device)\n",
    "    else:\n",
    "        norm_factor = (torch.sqrt(torch.tensor(1.0 / H, device=device)) / (2.0 * cmath.pi))\n",
    "\n",
    "    result = J_vector_tensor * norm_factor\n",
    "    return result, n1, n2, p\n",
    "\n",
    "\n",
    "def calculate_sample_periodogram_vectorized(J_vector_tensor):\n",
    "    \"\"\"Calculates sample periodogram I_n = J J^H. (Unchanged)\"\"\"\n",
    "    if torch.isnan(J_vector_tensor).any() or torch.isinf(J_vector_tensor).any():\n",
    "        n1, n2, p = J_vector_tensor.shape\n",
    "        return torch.full((n1, n2, p, p), float('nan'), dtype=torch.complex64, device=J_vector_tensor.device)\n",
    "\n",
    "    J_col = J_vector_tensor.unsqueeze(-1)\n",
    "    J_row_conj = J_vector_tensor.unsqueeze(-2).conj()\n",
    "    result = J_col @ J_row_conj\n",
    "    return result\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 4. Likelihood Calculation (Unchanged)\n",
    "# =========================================================================\n",
    "\n",
    "def whittle_likelihood_loss_no_taper(params, I_sample, n1, n2, p, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Whittle Likelihood Loss. (Unchanged)\n",
    "    \"\"\"\n",
    "    device = I_sample.device\n",
    "    params_tensor = params.to(device)\n",
    "\n",
    "    if torch.isnan(params_tensor).any() or torch.isinf(params_tensor).any():\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    # This call now correctly leads to cov_spacetime_difference\n",
    "    I_expected = expected_periodogram_fft_no_taper(\n",
    "        params_tensor, n1, n2, p, delta1, delta2\n",
    "    )\n",
    "\n",
    "    if torch.isnan(I_expected).any() or torch.isinf(I_expected).any():\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    eye_matrix = torch.eye(p, dtype=torch.complex64, device=device)\n",
    "    diag_vals = torch.abs(I_expected.diagonal(dim1=-2, dim2=-1))\n",
    "    mean_diag_abs = diag_vals.mean().item() if diag_vals.numel() > 0 and not torch.isnan(diag_vals).all() else 1.0\n",
    "    diag_load = max(mean_diag_abs * 1e-8, 1e-9) \n",
    "    \n",
    "    I_expected_stable = I_expected + eye_matrix * diag_load\n",
    "\n",
    "    sign, logabsdet = torch.linalg.slogdet(I_expected_stable)\n",
    "    if torch.any(sign.real <= 1e-9):\n",
    "        log_det_term = torch.where(sign.real > 1e-9, logabsdet, torch.tensor(1e10, device=device))\n",
    "    else:\n",
    "        log_det_term = logabsdet\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    try:\n",
    "        solved_term = torch.linalg.solve(I_expected_stable, I_sample)\n",
    "        trace_term = torch.einsum('...ii->...', solved_term).real\n",
    "    except torch.linalg.LinAlgError as e:\n",
    "        return torch.tensor(float('inf'), device=device)\n",
    "\n",
    "    if torch.isnan(trace_term).any() or torch.isinf(trace_term).any():\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    likelihood_terms = log_det_term + trace_term\n",
    "\n",
    "    if torch.isnan(likelihood_terms).any():\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    total_sum = torch.sum(likelihood_terms)\n",
    "    dc_term = likelihood_terms[0, 0] if n1 > 0 and n2 > 0 else torch.tensor(0.0, device=device)\n",
    "    if torch.isnan(dc_term).any() or torch.isinf(dc_term).any():\n",
    "        dc_term = torch.tensor(0.0, device=device)\n",
    "\n",
    "    loss = total_sum - dc_term if (n1 > 1 or n2 > 1) else total_sum\n",
    "\n",
    "    if torch.isnan(loss) or torch.isinf(loss):\n",
    "         return torch.tensor(float('inf'), device=device) \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 5. Training Loop (Unchanged)\n",
    "# =========================================================================\n",
    "def run_full(params_list, optimizer, scheduler, I_sample, n1, n2, p, epochs=600, device='cpu'):\n",
    "    \"\"\"Corrected training loop. (Unchanged)\"\"\"\n",
    "    best_loss = float('inf')\n",
    "    params_list = [p.to(device) for p in params_list]\n",
    "    best_params_state = [p.detach().clone() for p in params_list]\n",
    "    epochs_completed = 0\n",
    "    # DELTA_LAT, DELTA_LON are needed for the spatial part of the filter\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063\n",
    "\n",
    "    def get_printable_params(p_list):\n",
    "        valid_tensors = [p for p in p_list if isinstance(p, torch.Tensor)]\n",
    "        if not valid_tensors: return \"Invalid params_list\"\n",
    "        p_cat = torch.cat([p.detach().clone().cpu() for p in valid_tensors])\n",
    "        log_indices = [0, 1, 2, 6]\n",
    "        if all(idx < len(p_cat) for idx in log_indices):\n",
    "            log_vals = p_cat[log_indices]\n",
    "            if not (torch.isnan(log_vals).any() or torch.isinf(log_vals).any()):\n",
    "                 p_cat[log_indices] = torch.exp(log_vals)\n",
    "            else:\n",
    "                 p_cat[log_indices] = float('nan')\n",
    "        return p_cat.numpy().round(4)\n",
    "\n",
    "    I_sample_dev = I_sample.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epochs_completed = epoch + 1\n",
    "        optimizer.zero_grad()\n",
    "        params_tensor = torch.cat(params_list)\n",
    "\n",
    "        loss = whittle_likelihood_loss_no_taper(\n",
    "            params_tensor, I_sample_dev, n1, n2, p, DELTA_LAT, DELTA_LON\n",
    "        )\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"Loss became NaN or Inf at epoch {epoch+1}. Stopping.\")\n",
    "            if epoch == 0: best_params_state = None\n",
    "            epochs_completed = epoch\n",
    "            break \n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        nan_grad = False\n",
    "        for param in params_list:\n",
    "            if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
    "                nan_grad = True\n",
    "                break\n",
    "        if nan_grad:\n",
    "             optimizer.zero_grad() \n",
    "             continue \n",
    "\n",
    "        all_params_on_device = params_list\n",
    "        if all_params_on_device:\n",
    "            torch.nn.utils.clip_grad_norm_(all_params_on_device, max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step() \n",
    "\n",
    "        current_loss_item = loss.item()\n",
    "        if current_loss_item < best_loss:\n",
    "            params_valid = not any(torch.isnan(p.data).any() or torch.isinf(p.data).any() for p in params_list)\n",
    "            if params_valid:\n",
    "                best_loss = current_loss_item\n",
    "                best_params_state = [p.detach().clone() for p in params_list]\n",
    "\n",
    "        if epoch % 50 == 0 or epoch == epochs - 1:\n",
    "            current_lr = optimizer.param_groups[0]['lr'] if optimizer.param_groups else 0.0\n",
    "            print(f'--- Epoch {epoch+1}/{epochs} (LR: {current_lr:.6f}) ---')\n",
    "            print(f' Loss: {current_loss_item:.4f}')\n",
    "            print(f' Parameters (Natural Scale): {get_printable_params(params_list)}')\n",
    "\n",
    "    if best_params_state is None:\n",
    "        return None, epochs_completed\n",
    "\n",
    "    final_params_log_scale = torch.cat([p.cpu() for p in best_params_state])\n",
    "    final_params_natural_scale = final_params_log_scale.detach().clone()\n",
    "    log_indices = [0, 1, 2, 6]\n",
    "    if all(idx < len(final_params_natural_scale) for idx in log_indices):\n",
    "        log_vals = final_params_natural_scale[log_indices]\n",
    "        if not (torch.isnan(log_vals).any() or torch.isinf(log_vals).any()):\n",
    "            final_params_natural_scale[log_indices] = torch.exp(log_vals)\n",
    "        else:\n",
    "            final_params_natural_scale[log_indices] = float('nan')\n",
    "\n",
    "    final_params_rounded = [round(p.item(), 4) if not np.isnan(p.item()) else float('nan') for p in final_params_natural_scale]\n",
    "    final_loss_rounded = round(best_loss, 3) if best_loss != float('inf') else float('inf')\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(f'\\nFINAL BEST STATE ACHIEVED (during training):')\n",
    "    print(f'Best Loss: {final_loss_rounded}')\n",
    "    print(f'Parameters Corresponding to Best Loss (Natural Scale): {final_params_rounded}')\n",
    "\n",
    "    return final_params_rounded + [final_loss_rounded], epochs_completed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1897debc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded 30 days from spacetime_differenced_data.pkl.\n",
      "Pre-computing sample periodogram (NO data taper)...\n",
      "Data grid: 113x158 spatial points, 8 time points. Sample Periodogram on cpu.\n",
      "\n",
      "============================== Initialization Run 1/1 ==============================\n",
      "Starting with fixed params (log-scale for [0,1,2,6]): [3.0588, 0.2677, 0.4466, 0.022, -0.144, 0.198, 1.5621]\n",
      "Starting optimization run 1 on device cpu (NO data taper, Spatio-Temporal Diff)...\n",
      "--- Epoch 1/700 (LR: 0.005000) ---\n",
      " Loss: 837379.0000\n",
      " Parameters (Natural Scale): [21.4098  1.3005  1.5552  0.042  -0.124   0.178   4.7929]\n",
      "--- Epoch 51/700 (LR: 0.004240) ---\n",
      " Loss: 636619.0000\n",
      " Parameters (Natural Scale): [ 2.57281e+01  1.08040e+00  1.28980e+00 -4.00000e-03 -1.11000e-02\n",
      "  1.90000e-03  5.56710e+00]\n",
      "--- Epoch 101/700 (LR: 0.002461) ---\n",
      " Loss: 587499.3750\n",
      " Parameters (Natural Scale): [ 2.80687e+01  1.00900e+00  1.17890e+00 -1.00000e-03 -9.40000e-03\n",
      " -1.00000e-03  5.79440e+00]\n",
      "--- Epoch 151/700 (LR: 0.000706) ---\n",
      " Loss: 563940.6875\n",
      " Parameters (Natural Scale): [ 2.98396e+01  9.63200e-01  1.10880e+00 -7.00000e-04 -1.02000e-02\n",
      "  4.00000e-04  5.94280e+00]\n",
      "--- Epoch 201/700 (LR: 0.000001) ---\n",
      " Loss: 555401.3750\n",
      " Parameters (Natural Scale): [ 3.04117e+01  9.47900e-01  1.08810e+00 -6.00000e-04 -1.03000e-02\n",
      " -0.00000e+00  6.00000e+00]\n",
      "--- Epoch 251/700 (LR: 0.000761) ---\n",
      " Loss: 545689.0625\n",
      " Parameters (Natural Scale): [ 3.10602e+01  9.28400e-01  1.06540e+00 -4.00000e-04 -1.06000e-02\n",
      " -0.00000e+00  6.11390e+00]\n",
      "--- Epoch 301/700 (LR: 0.002540) ---\n",
      " Loss: 529286.0625\n",
      " Parameters (Natural Scale): [ 3.22827e+01  8.93300e-01  1.02500e+00 -1.00000e-04 -1.09000e-02\n",
      " -0.00000e+00  6.32360e+00]\n",
      "--- Epoch 351/700 (LR: 0.004295) ---\n",
      " Loss: 516570.7188\n",
      " Parameters (Natural Scale): [ 3.36112e+01  8.56600e-01  9.83700e-01 -5.70000e-03 -1.35000e-02\n",
      "  1.40000e-03  6.53950e+00]\n",
      "--- Epoch 401/700 (LR: 0.005000) ---\n",
      " Loss: 519863.6562\n",
      " Parameters (Natural Scale): [ 3.55089e+01  8.04800e-01  9.30400e-01  1.50000e-03 -1.08000e-02\n",
      "  3.00000e-04  6.83070e+00]\n",
      "--- Epoch 451/700 (LR: 0.004240) ---\n",
      " Loss: 468132.2812\n",
      " Parameters (Natural Scale): [ 3.7802e+01  7.5230e-01  8.7330e-01 -1.3000e-03 -1.3200e-02 -1.2000e-03\n",
      "  7.1316e+00]\n",
      "--- Epoch 501/700 (LR: 0.002461) ---\n",
      " Loss: 449581.6250\n",
      " Parameters (Natural Scale): [ 3.99729e+01  7.10900e-01  8.25400e-01  2.00000e-04 -1.51000e-02\n",
      "  5.00000e-04  7.36630e+00]\n",
      "--- Epoch 551/700 (LR: 0.000706) ---\n",
      " Loss: 437659.7500\n",
      " Parameters (Natural Scale): [ 4.14782e+01  6.85100e-01  7.95300e-01 -4.00000e-04 -1.36000e-02\n",
      " -1.00000e-04  7.50630e+00]\n",
      "--- Epoch 601/700 (LR: 0.000001) ---\n",
      " Loss: 433484.9375\n",
      " Parameters (Natural Scale): [ 4.20516e+01  6.75900e-01  7.84400e-01 -4.00000e-04 -1.36000e-02\n",
      " -0.00000e+00  7.55660e+00]\n",
      "--- Epoch 651/700 (LR: 0.000761) ---\n",
      " Loss: 427154.5312\n",
      " Parameters (Natural Scale): [ 4.30213e+01  6.60800e-01  7.66800e-01 -6.00000e-04 -1.38000e-02\n",
      " -0.00000e+00  7.66790e+00]\n",
      "--- Epoch 700/700 (LR: 0.002500) ---\n",
      " Loss: 414330.0625\n",
      " Parameters (Natural Scale): [ 4.51402e+01  6.29900e-01  7.30800e-01 -3.00000e-04 -1.51000e-02\n",
      "  0.00000e+00  7.89220e+00]\n",
      "\n",
      "--- Training Complete ---\n",
      "\n",
      "FINAL BEST STATE ACHIEVED (during training):\n",
      "Best Loss: 414330.062\n",
      "Parameters Corresponding to Best Loss (Natural Scale): [45.1402, 0.6299, 0.7308, -0.0003, -0.0151, 0.0, 7.8922]\n",
      "\n",
      "\n",
      "========================= Overall Result from Run =========================\n",
      "Run Loss: 414330.062\n",
      "Final Parameters (Natural Scale): [45.1402, 0.6299, 0.7308, -0.0003, -0.0151, 0.0, 7.8922]\n",
      "\n",
      "Total execution time: 556.02 seconds\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# 6. Main Execution Script (REVISED with Fixed Initial Parameters)\n",
    "# =========================================================================\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    DAY_TO_RUN = 1\n",
    "    NUM_RUNS = 1 \n",
    "    EPOCHS = 700 \n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    # --- Grid Spacing (Still needed for the spatial part of the filter) ---\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063 \n",
    "\n",
    "    # --- Column Indices ---\n",
    "    LAT_COL, LON_COL = 0, 1\n",
    "    VAL_COL = 2\n",
    "    TIME_COL = 3\n",
    "\n",
    "    # --- Load Spatio-Temporal Differenced Data ---\n",
    "    try:\n",
    "        with open(\"spacetime_differenced_data.pkl\", 'rb') as f:\n",
    "            processed_df = pickle.load(f)\n",
    "        print(f\"Loaded {len(processed_df)} days from spacetime_differenced_data.pkl.\")\n",
    "        \n",
    "        processed_df = [\n",
    "            torch.tensor(arr, dtype=torch.float32).cpu() if not isinstance(arr, torch.Tensor)\n",
    "            else arr.cpu().to(torch.float32)\n",
    "            for arr in processed_df\n",
    "        ]\n",
    "        if not processed_df: raise ValueError(\"'processed_df' is empty.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: `spacetime_differenced_data.pkl` not found.\")\n",
    "        print(\"Please run the data preparation script first.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing 'processed_df': {e}\")\n",
    "        exit()\n",
    "\n",
    "    if DAY_TO_RUN > len(processed_df) or DAY_TO_RUN <= 0:\n",
    "        print(f\"Error: DAY_TO_RUN ({DAY_TO_RUN}) out of bounds.\")\n",
    "        exit()\n",
    "\n",
    "    cur_df = processed_df[DAY_TO_RUN - 1]\n",
    "    if cur_df.numel() == 0 or cur_df.shape[1] <= max(LAT_COL, LON_COL, VAL_COL, TIME_COL):\n",
    "        print(f\"Error: Data for Day {DAY_TO_RUN} is empty or invalid.\")\n",
    "        exit()\n",
    "\n",
    "    unique_times = torch.unique(cur_df[:, TIME_COL])\n",
    "    time_slices_list = [cur_df[cur_df[:, TIME_COL] == t_val] for t_val in unique_times]\n",
    "\n",
    "    # --- 1. Pre-compute Sample Periodogram (NO Tapering) ---\n",
    "    print(\"Pre-computing sample periodogram (NO data taper)...\")\n",
    "    J_vec, n1, n2, p = generate_Jvector_no_taper(\n",
    "        time_slices_list,\n",
    "        lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if J_vec.numel() == 0 or n1 == 0 or n2 == 0 or p == 0:\n",
    "       print(f\"Error: J-vector generation failed for Day {DAY_TO_RUN}.\")\n",
    "       exit()\n",
    "\n",
    "    I_sample = calculate_sample_periodogram_vectorized(J_vec)\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Error: NaN or Inf detected in the sample periodogram. Cannot proceed.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Data grid: {n1}x{n2} spatial points, {p} time points. Sample Periodogram on {DEVICE}.\")\n",
    "    \n",
    "    # --- 2. Optimization Loop ---\n",
    "    all_final_results = []\n",
    "    all_final_losses = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        print(f\"\\n{'='*30} Initialization Run {i+1}/{NUM_RUNS} {'='*30}\")\n",
    "\n",
    "        # âœ… --- Use Fixed Initial Parameters ---\n",
    "        # User specified natural scale: [21.303, 1.307, 1.563, 0.022, -0.144, 0.198, 4.769]\n",
    "        # Convert indices 0, 1, 2, 6 to log-scale for the model\n",
    "        initial_params_values = [\n",
    "            np.log(21.303), # log(sigmasq)\n",
    "            np.log(1.307), # log(r_lat)\n",
    "            np.log(1.563), # log(r_lon)\n",
    "            0.022,         # a_lat\n",
    "            -0.144,        # a_lon\n",
    "            0.198,         # beta\n",
    "            np.log(4.769)  # log(nugget)\n",
    "        ]\n",
    "        \n",
    "        print(f\"Starting with fixed params (log-scale for [0,1,2,6]): {[round(p, 4) for p in initial_params_values]}\")\n",
    "\n",
    "        params_list = [\n",
    "            Parameter(torch.tensor([val], dtype=torch.float32))\n",
    "            for val in initial_params_values\n",
    "        ]\n",
    "\n",
    "        lr_slow, lr_fast = 0.005, 0.02\n",
    "        slow_indices = [0, 1, 2, 6]\n",
    "        fast_indices = [3, 4, 5]\n",
    "\n",
    "        valid_slow_indices = [idx for idx in slow_indices if idx < len(params_list)]\n",
    "        valid_fast_indices = [idx for idx in fast_indices if idx < len(params_list)]\n",
    "\n",
    "        param_groups = [\n",
    "            {'params': [params_list[idx] for idx in valid_slow_indices], 'lr': lr_slow, 'name': 'slow_group'},\n",
    "            {'params': [params_list[idx] for idx in valid_fast_indices], 'lr': lr_fast, 'name': 'fast_group'}\n",
    "        ]\n",
    "\n",
    "        optimizer = torch.optim.Adam(param_groups)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=200, eta_min=1e-6)\n",
    "\n",
    "        print(f\"Starting optimization run {i+1} on device {DEVICE} (NO data taper, Spatio-Temporal Diff)...\")\n",
    "        final_results, epochs_run = run_full(\n",
    "            params_list=params_list,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            I_sample=I_sample,\n",
    "            n1=n1, n2=n2, p=p,\n",
    "            epochs=EPOCHS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "        if final_results:\n",
    "            all_final_results.append(final_results)\n",
    "            all_final_losses.append(final_results[-1])\n",
    "        else:\n",
    "            all_final_results.append(None)\n",
    "            all_final_losses.append(float('inf'))\n",
    "\n",
    "    print(f\"\\n\\n{'='*25} Overall Result from Run {'='*25}\")\n",
    "    valid_losses = [l for l in all_final_losses if l is not None and l != float('inf')]\n",
    "\n",
    "    if not valid_losses:\n",
    "        print(f\"The run failed or resulted in an invalid loss for Day {DAY_TO_RUN}.\")\n",
    "    else:\n",
    "        best_loss = valid_losses[0]\n",
    "        best_run_index = 0\n",
    "        best_results = all_final_results[best_run_index]\n",
    "        print(f\"Run Loss: {best_results[-1]}\")\n",
    "        print(f\"Final Parameters (Natural Scale): {best_results[:-1]}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ba988a",
   "metadata": {},
   "source": [
    "# 3d once difference not two stage\n",
    "\n",
    "ey Change: From Sparse Lookup to Grid Convolution\n",
    "\n",
    "The logic now:\n",
    "\n",
    "Reshapes the day-long tensor into a 3D grid: [1, 1, N_lat, N_lon, N_time].\n",
    "\n",
    "Applies the 3D first-order difference kernel K: Y(s,t)=X(s+Î” \n",
    "lat\n",
    "â€‹\t\n",
    " ,t)+X(s+Î” \n",
    "lon\n",
    "â€‹\t\n",
    " ,t)+X(s,t+1)âˆ’3X(s,t).\n",
    "\n",
    "The kernel K has weights {âˆ’3,1,1,1} at (0,0,0),(Î” \n",
    "lat\n",
    "â€‹\t\n",
    " ,0,0),(0,Î” \n",
    "lon\n",
    "â€‹\t\n",
    " ,0),(0,0,Î” \n",
    "time\n",
    "â€‹\t\n",
    " )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7fa3a3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/joonwonlee/Documents/GEMS_DATA/pickle_2024/coarse_cen_map_without_decrement_latitude24_07.pkl\n",
      "\n",
      "Loading and SUBSETTING aggregated data for 31 days...\n",
      "  Aggregated & Subset tensor shape for day 1: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 2: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 3: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 4: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 5: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 6: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 7: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 8: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 9: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 10: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 11: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 12: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 13: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 14: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 15: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 16: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 17: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 18: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 19: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 20: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 21: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 22: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 23: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 24: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 25: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 26: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 27: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 28: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 29: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 30: torch.Size([145008, 4])\n",
      "  Aggregated & Subset tensor shape for day 31: torch.Size([145008, 4])\n",
      "\n",
      "Finished loading. Created 31 aggregated day tensors.\n",
      "\n",
      "Applying ONE-STAGE 3D Convolution filter...\n",
      "Filtering Day 1...\n",
      "  Successfully filtered day 1. New shape: torch.Size([124978, 4])\n",
      "Filtering Day 2...\n",
      "  Successfully filtered day 2. New shape: torch.Size([124978, 4])\n",
      "Filtering Day 3...\n",
      "  Successfully filtered day 3. New shape: torch.Size([124978, 4])\n",
      "Filtering Day 4...\n",
      "  Successfully filtered day 4. New shape: torch.Size([124978, 4])\n",
      "Filtering Day 5...\n",
      "  Successfully filtered day 5. New shape: torch.Size([124978, 4])\n",
      "Filtering Day 6...\n",
      "\n",
      "Filtering complete. Generated 5 final filtered day-tensors.\n",
      "Processed data for 5 days saved to filtered_3d_convolution_data_2024_07.pkl\n",
      "\n",
      "Shape of the first filtered day tensor: torch.Size([124978, 4])\n",
      "Head of the first filtered day tensor:\n",
      "tensor([[ 4.0000e-03,  1.2303e+02,  6.5868e+00,  2.1000e+01],\n",
      "        [ 4.0000e-03,  1.2303e+02, -1.9318e-02,  2.2000e+01],\n",
      "        [ 4.0000e-03,  1.2303e+02, -6.3661e-01,  2.3000e+01],\n",
      "        [ 4.0000e-03,  1.2303e+02,  4.7857e+00,  2.4000e+01],\n",
      "        [ 4.0000e-03,  1.2303e+02, -4.8611e+00,  2.5000e+01]])\n",
      "Unique time values in first tensor: tensor([21., 22., 23., 24., 25., 26., 27.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch.nn.functional as F # <-- Added F\n",
    "\n",
    "# --- Constants from your reference code ---\n",
    "STEP_LAT = 0.044\n",
    "STEP_LON = 0.063\n",
    "\n",
    "# =========================================================================\n",
    "# 1. Helper Functions\n",
    "# =========================================================================\n",
    "\n",
    "def subset_tensor(df_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Subsets a tensor to lat [0, 5] and lon [123, 133].\"\"\"\n",
    "    lat_mask = (df_tensor[:, 0] >= 0) & (df_tensor[:, 0] <= 5)\n",
    "    lon_mask = (df_tensor[:, 1] >= 123) & (df_tensor[:, 1] <= 133)\n",
    "    \n",
    "    df_sub = df_tensor[lat_mask & lon_mask].clone()\n",
    "    return df_sub\n",
    "\n",
    "def reshape_day_tensor_to_3d_grid(day_tensor: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Utility function to reshape a 1D spatio-temporal tensor into a 3D grid [N_lat, N_lon, N_time].\n",
    "    Requires a non-sparse grid for each time point.\n",
    "    \"\"\"\n",
    "    if day_tensor.size(0) == 0:\n",
    "        raise ValueError(\"Input tensor is empty.\")\n",
    "    \n",
    "    unique_lats = torch.unique(day_tensor[:, 0])\n",
    "    unique_lons = torch.unique(day_tensor[:, 1])\n",
    "    unique_times = torch.unique(day_tensor[:, 3])\n",
    "    \n",
    "    n_lat, n_lon, n_time = len(unique_lats), len(unique_lons), len(unique_times)\n",
    "    \n",
    "    # Check for non-sparse grid across all time points\n",
    "    if day_tensor.size(0) != n_lat * n_lon * n_time:\n",
    "         # This check is crucial for the convolution approach\n",
    "         raise ValueError(f\"Input tensor size ({day_tensor.size(0)}) does not match expected 3D grid size ({n_lat*n_lon*n_time}). Data must be a complete, non-sparse grid.\")\n",
    "\n",
    "    lat_map = {lat.item(): i for i, lat in enumerate(unique_lats)}\n",
    "    lon_map = {lon.item(): j for j, lon in enumerate(unique_lons)}\n",
    "    time_map = {t.item(): k for k, t in enumerate(unique_times)}\n",
    "    \n",
    "    # Create the 3D grid\n",
    "    grid_data = torch.zeros((n_lat, n_lon, n_time), dtype=day_tensor.dtype, device=day_tensor.device)\n",
    "    \n",
    "    for row in day_tensor:\n",
    "        lat, lon, ozone, t = row[0].item(), row[1].item(), row[2], row[3].item()\n",
    "        i, j, k = lat_map[lat], lon_map[lon], time_map[t]\n",
    "        grid_data[i, j, k] = ozone\n",
    "\n",
    "    return grid_data, unique_lats, unique_lons, unique_times\n",
    "\n",
    "def apply_3d_filter_convolution(day_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    âœ… Applies the ONE-STAGE 3D filter Y(s,t) = X(s+d_lat) + X(s+d_lon) + X(s,t+1) - 3X(s,t)\n",
    "    using 3D convolution, requiring a complete, non-sparse 3D grid input.\n",
    "    \"\"\"\n",
    "    if day_tensor.size(0) == 0:\n",
    "        return torch.empty(0, 4, dtype=day_tensor.dtype, device=day_tensor.device)\n",
    "\n",
    "    # 1. Reshape data into a 3D grid [N_lat, N_lon, N_time]\n",
    "    grid_data, unique_lats, unique_lons, unique_times = reshape_day_tensor_to_3d_grid(day_tensor)\n",
    "    \n",
    "    n_lat, n_lon, n_time = grid_data.shape\n",
    "    if n_lat < 2 or n_lon < 2 or n_time < 2:\n",
    "        # Not enough dimensions for 3D differencing\n",
    "        return torch.empty(0, 4, dtype=day_tensor.dtype, device=day_tensor.device)\n",
    "\n",
    "    # 2. Define the 3D Kernel (for cross-correlation)\n",
    "    # Filter: X(i+1, j, k) + X(i, j+1, k) + X(i, j, k+1) - 3*X(i, j, k)\n",
    "    kernel_weights = torch.zeros((1, 1, 2, 2, 2), dtype=day_tensor.dtype, device=day_tensor.device)\n",
    "    kernel_weights[0, 0, 0, 0, 0] = -3.0 # X(i, j, k)\n",
    "    kernel_weights[0, 0, 1, 0, 0] = 1.0  # X(i+1, j, k) - Lat Diff\n",
    "    kernel_weights[0, 0, 0, 1, 0] = 1.0  # X(i, j+1, k) - Lon Diff\n",
    "    kernel_weights[0, 0, 0, 0, 1] = 1.0  # X(i, j, k+1) - Time Diff\n",
    "\n",
    "    # 3. Apply 3D convolution\n",
    "    # Input shape: [1, 1, N_lat, N_lon, N_time]\n",
    "    input_conv = grid_data.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    # Output shape: [1, 1, N_lat-1, N_lon-1, N_time-1]\n",
    "    filtered_grid = F.conv3d(input_conv, kernel_weights, padding='valid').squeeze()\n",
    "    \n",
    "    # 4. Determine coordinates for the new, smaller grid\n",
    "    new_lats = unique_lats[:-1]\n",
    "    new_lons = unique_lons[:-1]\n",
    "    new_times = unique_times[:-1]\n",
    "\n",
    "    # 5. Reconstruct the output tensor\n",
    "    new_lat_grid, new_lon_grid, new_time_grid = torch.meshgrid(\n",
    "        new_lats, new_lons, new_times, indexing='ij'\n",
    "    )\n",
    "    \n",
    "    filtered_values = filtered_grid.flatten()\n",
    "    \n",
    "    new_tensor = torch.stack([\n",
    "        new_lat_grid.flatten(),\n",
    "        new_lon_grid.flatten(),\n",
    "        filtered_values,\n",
    "        new_time_grid.flatten() # Anchored at time t\n",
    "    ], dim=1)\n",
    "    \n",
    "    return new_tensor\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 2. Data Loading (Structure retained)\n",
    "# =========================================================================\n",
    "# âš ï¸ NOTE: You must define 'mac_data_path', 'year', 'month', and the GEMS_TCO class\n",
    "# (Assuming data loading variables are defined...)\n",
    "# NOTE: The data loading assumes 'load_working_data_byday_wo_mm' returns a\n",
    "# dictionary of chunks/hours (cur_map) AND an aggregated tensor (aggregated_day_tensor).\n",
    "# The original code used the aggregated tensor, so we will use it directly below.\n",
    "\n",
    "year = 2024\n",
    "month = 7\n",
    "month_str = f\"{month:02d}\"\n",
    "\n",
    "# Placeholder definitions for running:\n",
    "# class GEMS_TCO:\n",
    "#     def load_data(self, path): return self\n",
    "#     def load_working_data_byday_wo_mm(self, data, indices):\n",
    "#         # Placeholder for a complete 3D grid (N_lat x N_lon x N_hour)\n",
    "#         lats = torch.linspace(0.0, 5.0, 5) # N_lat=5\n",
    "#         lons = torch.linspace(123.0, 133.0, 5) # N_lon=5\n",
    "#         times = torch.arange(8) + indices[0] # N_hour=8\n",
    "#         grid_lats, grid_lons, grid_times = torch.meshgrid(lats, lons, times, indexing='ij')\n",
    "#         ozone = torch.randn_like(grid_lats) * 10\n",
    "#         aggregated_tensor = torch.stack([grid_lats.flatten(), grid_lons.flatten(), ozone.flatten(), grid_times.flatten()], dim=1)\n",
    "#         return {'chunk': aggregated_tensor}, aggregated_tensor # cur_map (dict), aggregated_day_tensor\n",
    "\n",
    "# (Need actual data loading setup here)\n",
    "\n",
    "pickle_path = os.path.join(mac_data_path, f'pickle_{year}')\n",
    "output_filename = f\"coarse_cen_map_without_decrement_latitude{str(year)[2:]}_{month_str}.pkl\"\n",
    "output_filepath = os.path.join(pickle_path, output_filename)\n",
    "print(f\"Loading data from: {output_filepath}\")\n",
    "\n",
    "try:\n",
    "    with open(output_filepath, 'rb') as pickle_file:\n",
    "        cbmap_ori = pickle.load(pickle_file)\n",
    "except:\n",
    "     # Placeholder data loading in case actual file is missing\n",
    "     class GEMS_TCO: # Placeholder\n",
    "        def load_data(self, path): return self\n",
    "        def load_working_data_byday_wo_mm(self, data, indices):\n",
    "             lats = torch.linspace(0.0, 5.0, 5) # N_lat=5\n",
    "             lons = torch.linspace(123.0, 133.0, 5) # N_lon=5\n",
    "             times = torch.arange(8) + indices[0] # N_hour=8\n",
    "             grid_lats, grid_lons, grid_times = torch.meshgrid(lats, lons, times, indexing='ij')\n",
    "             ozone = torch.randn_like(grid_lats) * 10\n",
    "             aggregated_tensor = torch.stack([grid_lats.flatten(), grid_lons.flatten(), ozone.flatten(), grid_times.flatten()], dim=1)\n",
    "             return {'chunk': aggregated_tensor}, aggregated_tensor\n",
    "     cbmap_ori = {}\n",
    "\n",
    "\n",
    "load_data_instance = GEMS_TCO.load_data('')\n",
    "\n",
    "# =========================================================================\n",
    "# 3. Build df_day_aggregated_list (MODIFIED to use aggregated_day_tensor)\n",
    "# =========================================================================\n",
    "df_day_aggregated_list = []\n",
    "num_days_to_process = 31 # For July\n",
    "\n",
    "print(f\"\\nLoading and SUBSETTING aggregated data for {num_days_to_process} days...\")\n",
    "for i in range(num_days_to_process): \n",
    "    # Use the entire aggregated_day_tensor for 3D processing\n",
    "    cur_map, aggregated_day_tensor = load_data_instance.load_working_data_byday_wo_mm(\n",
    "        cbmap_ori, [i*8, (i+1)*8]\n",
    "    )\n",
    "    \n",
    "    if aggregated_day_tensor is not None and aggregated_day_tensor.numel() > 0:\n",
    "        \n",
    "        subsetted_tensor = subset_tensor(aggregated_day_tensor)\n",
    "        \n",
    "        if subsetted_tensor.size(0) > 0:\n",
    "            df_day_aggregated_list.append(subsetted_tensor)\n",
    "            print(f\"  Aggregated & Subset tensor shape for day {i+1}: {subsetted_tensor.shape}\")\n",
    "        else:\n",
    "            print(f\"  No valid data found after SUBSETTING for day {i+1}.\")\n",
    "            \n",
    "    else:\n",
    "         print(f\"  No valid aggregated data (cur_df) found for day {i+1}.\")\n",
    "\n",
    "print(f\"\\nFinished loading. Created {len(df_day_aggregated_list)} aggregated day tensors.\")\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 4. Main 3D Filtering Loop (MODIFIED to use convolution)\n",
    "# =========================================================================\n",
    "all_filtered_days = [] \n",
    "\n",
    "print(\"\\nApplying ONE-STAGE 3D Convolution filter...\")\n",
    "if not df_day_aggregated_list:\n",
    "     print(\"Error: `df_day_aggregated_list` is empty after loading/subsetting.\")\n",
    "else:\n",
    "    for day_idx, aggregated_day_tensor in enumerate(df_day_aggregated_list):\n",
    "        print(f\"Filtering Day {day_idx+1}...\")\n",
    "        if day_idx==5:\n",
    "            break\n",
    "        \n",
    "        # Move to GPU if available for convolution speed\n",
    "        tensor_dev = aggregated_day_tensor.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")) \n",
    "\n",
    "        try:\n",
    "            # Apply the 3D convolution filter\n",
    "            filtered_day_tensor = apply_3d_filter_convolution(tensor_dev)\n",
    "            \n",
    "            if filtered_day_tensor.numel() > 0:\n",
    "                # Move back to CPU for consistency before saving\n",
    "                all_filtered_days.append(filtered_day_tensor.cpu()) \n",
    "                print(f\"  Successfully filtered day {day_idx+1}. New shape: {filtered_day_tensor.shape}\")\n",
    "            else:\n",
    "                print(f\"  Skipping Day {day_idx+1}: filter resulted in an empty tensor (no valid points found).\")\n",
    "                 \n",
    "        except ValueError as e:\n",
    "            # Catch the non-sparse grid error here\n",
    "            print(f\" Skipping Day {day_idx+1}: Data structure error: {e}\")\n",
    "        except Exception as e: \n",
    "            print(f\" An unexpected error occurred filtering Day {day_idx+1}: {e}\")\n",
    "\n",
    "\n",
    "print(f\"\\nFiltering complete. Generated {len(all_filtered_days)} final filtered day-tensors.\")\n",
    "\n",
    "# =========================================================================\n",
    "# 5. Verification and Saving (Unchanged)\n",
    "# =========================================================================\n",
    "if all_filtered_days:\n",
    "    # Filename now reflects the correct year and month\n",
    "    processed_output_path = f\"filtered_3d_convolution_data_{year}_{month_str}.pkl\" \n",
    "    with open(processed_output_path, 'wb') as f:\n",
    "        pickle.dump(all_filtered_days, f)\n",
    "    print(f\"Processed data for {len(all_filtered_days)} days saved to {processed_output_path}\")\n",
    "\n",
    "    first_day_tensor = all_filtered_days[0] \n",
    "    print(\"\\nShape of the first filtered day tensor:\", first_day_tensor.shape)\n",
    "    print(\"Head of the first filtered day tensor:\")\n",
    "    print(first_day_tensor[:5])\n",
    "    \n",
    "    if first_day_tensor.numel() > 0:\n",
    "        print(\"Unique time values in first tensor:\", torch.unique(first_day_tensor[:, 3]))\n",
    "\n",
    "else:\n",
    "    print(f\"\\nNo final filtered tensors were created for {year}-{month_str}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "15cd52fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded 5 days from filtered_3d_sparse_data_2024_07.pkl.\n",
      "Pre-computing sample periodogram for Day Index 0 (NO data taper)...\n",
      "Data grid: 113x110 spatial points, 7 time points (hours). Sample Periodogram on cpu.\n",
      "\n",
      "============================== Initialization Run 1/1 ==============================\n",
      "Starting with fixed params (log-scale for [0,1,2,6]): [3.0588, 0.2677, 0.4466, 0.022, -0.144, 0.198, 1.5621]\n",
      "Starting optimization run 1 on device cpu (NO data taper, 3D Diff)...\n",
      "--- Epoch 1/700 (LR: 0.005000) ---\n",
      " Loss: 111587.6562\n",
      " Parameters (Natural Scale): [21.1967  1.3136  1.5708  0.042  -0.124   0.178   4.7452]\n",
      "--- Epoch 51/700 (LR: 0.004240) ---\n",
      " Loss: 107623.2734\n",
      " Parameters (Natural Scale): [ 1.84694e+01  1.52260e+00  1.78040e+00  1.54000e-02  2.62000e-02\n",
      " -7.90000e-03  4.00510e+00]\n",
      "--- Epoch 101/700 (LR: 0.002461) ---\n",
      " Loss: 107617.1875\n",
      " Parameters (Natural Scale): [ 1.88965e+01  1.59290e+00  1.61780e+00  5.80000e-03  2.71000e-02\n",
      " -2.00000e-03  4.00710e+00]\n",
      "--- Epoch 151/700 (LR: 0.000706) ---\n",
      " Loss: 107613.0859\n",
      " Parameters (Natural Scale): [ 1.94704e+01  1.73960e+00  1.46200e+00  6.50000e-03  2.72000e-02\n",
      " -0.00000e+00  3.98390e+00]\n",
      "--- Epoch 201/700 (LR: 0.000001) ---\n",
      " Loss: 107612.2188\n",
      " Parameters (Natural Scale): [1.97397e+01 1.78010e+00 1.42740e+00 6.70000e-03 2.75000e-02 0.00000e+00\n",
      " 3.97590e+00]\n",
      "--- Epoch 251/700 (LR: 0.000761) ---\n",
      " Loss: 107611.5781\n",
      " Parameters (Natural Scale): [2.00802e+01 1.81240e+00 1.40170e+00 6.90000e-03 2.80000e-02 0.00000e+00\n",
      " 3.96680e+00]\n",
      "--- Epoch 301/700 (LR: 0.002540) ---\n",
      " Loss: 107610.5000\n",
      " Parameters (Natural Scale): [ 2.06479e+01  1.86860e+00  1.35970e+00  7.50000e-03  2.88000e-02\n",
      " -0.00000e+00  3.95170e+00]\n",
      "--- Epoch 351/700 (LR: 0.004295) ---\n",
      " Loss: 107609.4531\n",
      " Parameters (Natural Scale): [2.11907e+01 1.92820e+00 1.31910e+00 7.70000e-03 2.96000e-02 0.00000e+00\n",
      " 3.93700e+00]\n",
      "--- Epoch 401/700 (LR: 0.005000) ---\n",
      " Loss: 107608.1484\n",
      " Parameters (Natural Scale): [ 2.17697e+01  2.00070e+00  1.27560e+00  1.00000e-02  3.03000e-02\n",
      " -0.00000e+00  3.92080e+00]\n",
      "--- Epoch 451/700 (LR: 0.004240) ---\n",
      " Loss: 107605.9219\n",
      " Parameters (Natural Scale): [ 2.25644e+01  2.10850e+00  1.22000e+00  1.30000e-02  3.36000e-02\n",
      " -0.00000e+00  3.90060e+00]\n",
      "--- Epoch 501/700 (LR: 0.002461) ---\n",
      " Loss: 107605.0469\n",
      " Parameters (Natural Scale): [2.30317e+01 2.17860e+00 1.18810e+00 9.20000e-03 3.08000e-02 0.00000e+00\n",
      " 3.88490e+00]\n",
      "--- Epoch 551/700 (LR: 0.000706) ---\n",
      " Loss: 107602.6328\n",
      " Parameters (Natural Scale): [2.34597e+01 2.24480e+00 1.16220e+00 1.19000e-02 3.19000e-02 0.00000e+00\n",
      " 3.87100e+00]\n",
      "--- Epoch 601/700 (LR: 0.000001) ---\n",
      " Loss: 107601.7500\n",
      " Parameters (Natural Scale): [2.36556e+01 2.27430e+00 1.15120e+00 1.24000e-02 3.18000e-02 0.00000e+00\n",
      " 3.86490e+00]\n",
      "--- Epoch 651/700 (LR: 0.000761) ---\n",
      " Loss: 107599.7656\n",
      " Parameters (Natural Scale): [2.42497e+01 2.34360e+00 1.12160e+00 1.33000e-02 3.26000e-02 0.00000e+00\n",
      " 3.84550e+00]\n",
      "--- Epoch 700/700 (LR: 0.002500) ---\n",
      " Loss: 107591.3281\n",
      " Parameters (Natural Scale): [2.59905e+01 2.55560e+00 1.04340e+00 1.56000e-02 3.92000e-02 2.08000e-02\n",
      " 3.74260e+00]\n",
      "\n",
      "--- Training Complete ---\n",
      "\n",
      "FINAL BEST STATE ACHIEVED (during training):\n",
      "Best Loss: 107591.328\n",
      "Parameters Corresponding to Best Loss (Natural Scale): [25.9905, 2.5556, 1.0434, 0.0156, 0.0392, 0.0208, 3.7426]\n",
      "\n",
      "\n",
      "========================= Overall Result from Run =========================\n",
      "Run Loss: 107591.328\n",
      "Final Parameters (Natural Scale): [25.9905, 2.5556, 1.0434, 0.0156, 0.0392, 0.0208, 3.7426]\n",
      "\n",
      "Total execution time: 163.11 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # Keep if plotting might be added later\n",
    "import cmath\n",
    "import pickle\n",
    "import time # For timing\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import Parameter\n",
    "import pandas as pd # Make sure pandas is imported\n",
    "import os # Make sure os is imported\n",
    "\n",
    "# =========================================================================\n",
    "# 1. Modeling Functions (Adapted for 3D Differencing)\n",
    "# =========================================================================\n",
    "\n",
    "# --- Bartlett Kernel (Used for c_gn when g_s=1) ---\n",
    "def cgn_2dbartlett_kernel(u1, u2, n1, n2):\n",
    "    \"\"\"Computes the 2D Bartlett kernel. (Unchanged)\"\"\"\n",
    "    device = u1.device if isinstance(u1, torch.Tensor) else (u2.device if isinstance(u2, torch.Tensor) else torch.device('cpu'))\n",
    "    u1_tensor = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_tensor = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    n1_eff = float(n1) if n1 > 0 else 1.0\n",
    "    n2_eff = float(n2) if n2 > 0 else 1.0\n",
    "    kernel = (1.0 - torch.abs(u1_tensor) / n1_eff) * (1.0 - torch.abs(u2_tensor) / n2_eff)\n",
    "    return torch.clamp(kernel, min=0.0)\n",
    "\n",
    "# --- Covariance of the Original Field X (EXPONENTIAL Kernel) ---\n",
    "def cov_x_exponential(u1, u2, t, params):\n",
    "    \"\"\"Computes the autocovariance of the ORIGINAL process X. (Unchanged)\"\"\"\n",
    "    device = params.device\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    log_params_indices = [0, 1, 2, 6]\n",
    "    if torch.isnan(params[log_params_indices]).any() or torch.isinf(params[log_params_indices]).any():\n",
    "         out_shape = torch.broadcast_shapes(u1_dev.shape, u2_dev.shape, t_dev.shape)\n",
    "         return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "\n",
    "    sigmasq, r_lat, r_lon, nugget = torch.exp(params[log_params_indices])\n",
    "    a_lat, a_lon, beta = params[3], params[4], params[5]\n",
    "\n",
    "    r_lat = torch.clamp(r_lat, min=1e-6)\n",
    "    r_lon = torch.clamp(r_lon, min=1e-6)\n",
    "\n",
    "    x1 = u1_dev / r_lat - a_lat * t_dev\n",
    "    x2 = u2_dev / r_lon - a_lon * t_dev\n",
    "    x3 = beta * t_dev\n",
    "    distance_sq = x1**2 + x2**2 + x3**2\n",
    "    epsilon = 1e-12\n",
    "    distance_sq_clamped = torch.clamp(distance_sq, min=0.0)\n",
    "    D = torch.sqrt(distance_sq_clamped + epsilon)\n",
    "    cov_smooth = sigmasq * torch.exp(-D)\n",
    "\n",
    "    is_zero_lag = (torch.abs(u1_dev) < 1e-9) & (torch.abs(u2_dev) < 1e-9) & (torch.abs(t_dev) < 1e-9)\n",
    "    final_cov = torch.where(is_zero_lag, cov_smooth + nugget, cov_smooth)\n",
    "\n",
    "    if torch.isnan(final_cov).any():\n",
    "        print(\"Warning: NaN detected in cov_x_exponential output.\")\n",
    "    return final_cov\n",
    "\n",
    "# --- (NEW) Covariance of the 3D Differenced Field Y ---\n",
    "def cov_3d_difference(u1, u2, t, params, delta1, delta2):\n",
    "    \"\"\"\n",
    "    âœ… Calculates covariance for the 3D filter:\n",
    "    Y(s,t) = X(s+d1,t) + X(s+d2,t) + X(s,t+1) - 3X(s,t)\n",
    "    \"\"\"\n",
    "    device = params.device\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    d1 = delta1 # spatial lag lat\n",
    "    d2 = delta2 # spatial lag lon\n",
    "    \n",
    "    # Pre-calculate terms C_X(u+h_j-h_k, tau)\n",
    "    term_00 = cov_x_exponential(u1_dev, u2_dev, t_dev, params) # u, tau\n",
    "    \n",
    "    term_p10 = cov_x_exponential(u1_dev + d1, u2_dev, t_dev, params) # u+d1, tau\n",
    "    term_m10 = cov_x_exponential(u1_dev - d1, u2_dev, t_dev, params) # u-d1, tau\n",
    "    \n",
    "    term_0p1 = cov_x_exponential(u1_dev, u2_dev + d2, t_dev, params) # u+d2, tau\n",
    "    term_0m1 = cov_x_exponential(u1_dev, u2_dev - d2, t_dev, params) # u-d2, tau\n",
    "    \n",
    "    term_m1p1 = cov_x_exponential(u1_dev - d1, u2_dev + d2, t_dev, params) # u-d1+d2, tau\n",
    "    term_p1m1 = cov_x_exponential(u1_dev + d1, u2_dev - d2, t_dev, params) # u+d1-d2, tau\n",
    "    \n",
    "    term_00_tp1 = cov_x_exponential(u1_dev, u2_dev, t_dev + 1.0, params) # u, tau+1\n",
    "    term_00_tm1 = cov_x_exponential(u1_dev, u2_dev, t_dev - 1.0, params) # u, tau-1\n",
    "    \n",
    "    term_m10_tp1 = cov_x_exponential(u1_dev - d1, u2_dev, t_dev + 1.0, params) # u-d1, tau+1\n",
    "    term_p10_tm1 = cov_x_exponential(u1_dev + d1, u2_dev, t_dev - 1.0, params) # u+d1, tau-1\n",
    "    \n",
    "    term_0m1_tp1 = cov_x_exponential(u1_dev, u2_dev - d2, t_dev + 1.0, params) # u-d2, tau+1\n",
    "    term_0p1_tm1 = cov_x_exponential(u1_dev, u2_dev + d2, t_dev - 1.0, params) # u+d2, tau-1\n",
    "\n",
    "    # Check for NaNs in any term\n",
    "    all_terms = [term_00, term_p10, term_m10, term_0p1, term_0m1, term_m1p1, term_p1m1,\n",
    "                 term_00_tp1, term_00_tm1, term_m10_tp1, term_p10_tm1, term_0m1_tp1, term_0p1_tm1]\n",
    "    if any(torch.isnan(term).any() for term in all_terms):\n",
    "        print(\"Warning: NaN detected in one of the terms of cov_3d_difference\")\n",
    "        out_shape = torch.broadcast_shapes(u1_dev.shape, u2_dev.shape, t_dev.shape)\n",
    "        return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "\n",
    "    # Combine terms according to the derived formula\n",
    "    cov_Y = (12 * term_00\n",
    "             - 3 * (term_p10 + term_m10)\n",
    "             - 3 * (term_0p1 + term_0m1)\n",
    "             + (term_m1p1 + term_p1m1)\n",
    "             - 3 * (term_00_tp1 + term_00_tm1)\n",
    "             + (term_m10_tp1 + term_p10_tm1)\n",
    "             + (term_0m1_tp1 + term_0p1_tm1)\n",
    "            )\n",
    "\n",
    "    return cov_Y\n",
    "\n",
    "\n",
    "# --- (MODIFIED) cn_bar for NO TAPERING ---\n",
    "def cn_bar_no_taper(u1, u2, t, params, n1, n2, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Computes c_Y(u) * c_gn(u) where c_Y is cov_3d_difference\n",
    "    and c_gn(u) is the Bartlett kernel.\n",
    "    \"\"\"\n",
    "    device = params.device\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    # âœ… --- Call the NEW 3D difference covariance function ---\n",
    "    cov_Y_value = cov_3d_difference(u1_dev, u2_dev, t_dev, params, delta1, delta2)\n",
    "\n",
    "    c_gn_value = cgn_2dbartlett_kernel(u1_dev, u2_dev, n1, n2)\n",
    "\n",
    "    if torch.isnan(cov_Y_value).any() or torch.isnan(c_gn_value).any():\n",
    "        print(\"Warning: NaN detected before multiplication in cn_bar_no_taper.\")\n",
    "        out_shape = torch.broadcast_shapes(cov_Y_value.shape, c_gn_value.shape)\n",
    "        return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "\n",
    "    result = cov_Y_value * c_gn_value\n",
    "    if torch.isnan(result).any():\n",
    "        print(\"Warning: NaN detected after multiplication in cn_bar_no_taper.\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# --- Expected Periodogram (uses cn_bar_no_taper) ---\n",
    "def expected_periodogram_fft_no_taper(params, n1, n2, p, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Calculates the expected periodogram. (Unchanged structure)\n",
    "    This function is correct because it calls the modified cn_bar_no_taper.\n",
    "    \"\"\"\n",
    "    device = params.device if isinstance(params, torch.Tensor) else params[0].device\n",
    "    params_tensor = params.to(device)\n",
    "\n",
    "    product_tensor = torch.zeros((n1, n2, p, p), dtype=torch.complex64, device=device)\n",
    "    t_lags = torch.arange(p, dtype=torch.float32, device=device)\n",
    "    u1_mesh_grid, u2_mesh_grid = torch.meshgrid(\n",
    "        torch.arange(n1, dtype=torch.float32, device=device),\n",
    "        torch.arange(n2, dtype=torch.float32, device=device),\n",
    "        indexing='ij'\n",
    "    )\n",
    "\n",
    "    for q in range(p):\n",
    "        for r in range(p):\n",
    "            t_diff = t_lags[q] - t_lags[r] # This is tau\n",
    "            # This call now correctly leads to cov_3d_difference\n",
    "            cov_times_bartlett = cn_bar_no_taper(\n",
    "                u1_mesh_grid, u2_mesh_grid, t_diff,\n",
    "                params_tensor, n1, n2, delta1, delta2\n",
    "            )\n",
    "            if torch.isnan(cov_times_bartlett).any():\n",
    "                 product_tensor[:, :, q, r] = float('nan')\n",
    "            else:\n",
    "                 product_tensor[:, :, q, r] = cov_times_bartlett.to(torch.complex64)\n",
    "\n",
    "    if torch.isnan(product_tensor).any():\n",
    "        print(\"Warning: NaN detected in product_tensor before FFT.\")\n",
    "        nan_shape = (n1, n2, p, p)\n",
    "        return torch.full(nan_shape, float('nan'), dtype=torch.complex64, device=device)\n",
    "\n",
    "    fft_result = torch.fft.fft2(product_tensor, dim=(0, 1))\n",
    "    normalization_factor = 1.0 / (4.0 * cmath.pi**2)\n",
    "    result = fft_result * normalization_factor\n",
    "\n",
    "    if torch.isnan(result).any():\n",
    "        print(\"Warning: NaN detected in expected_periodogram_fft_no_taper output after FFT.\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 2. Data Processing (Unchanged)\n",
    "# =========================================================================\n",
    "def generate_Jvector_no_taper(tensor_list, lat_col, lon_col, val_col, device):\n",
    "    \"\"\"Generates J-vector for g_s=1 (NO taper). (Unchanged)\"\"\"\n",
    "    p = len(tensor_list) # p is now the number of hours (e.g., 7)\n",
    "    if p == 0: return torch.empty(0, 0, 0, device=device), 0, 0, 0\n",
    "\n",
    "    valid_tensors = [t for t in tensor_list if t is not None and t.numel() > 0 and t.shape[1] > max(lat_col, lon_col, val_col)]\n",
    "    if not valid_tensors:\n",
    "         print(\"Warning: No valid tensors found in tensor_list.\")\n",
    "         return torch.empty(0, 0, 0, device=device), 0, 0, 0\n",
    "\n",
    "    try:\n",
    "        # Collect coords only from valid tensors\n",
    "        all_lats_cpu = torch.cat([t[:, lat_col] for t in valid_tensors])\n",
    "        all_lons_cpu = torch.cat([t[:, lon_col] for t in valid_tensors])\n",
    "    except IndexError:\n",
    "        print(f\"Error: Invalid column index. Check tensor shapes.\")\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error concatenating coordinates: {e}\")\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0\n",
    "\n",
    "    # Ensure coordinates are finite\n",
    "    all_lats_cpu = all_lats_cpu[~torch.isnan(all_lats_cpu) & ~torch.isinf(all_lats_cpu)]\n",
    "    all_lons_cpu = all_lons_cpu[~torch.isnan(all_lons_cpu) & ~torch.isinf(all_lons_cpu)]\n",
    "    if all_lats_cpu.numel() == 0 or all_lons_cpu.numel() == 0:\n",
    "        print(\"Warning: No valid coordinates found after filtering.\")\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0\n",
    "\n",
    "    unique_lats_cpu, unique_lons_cpu = torch.unique(all_lats_cpu), torch.unique(all_lons_cpu)\n",
    "    n1, n2 = len(unique_lats_cpu), len(unique_lons_cpu)\n",
    "    if n1 == 0 or n2 == 0:\n",
    "        print(\"Warning: Grid dimensions are zero.\")\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0\n",
    "\n",
    "    lat_map = {round(lat.item(), 5): i for i, lat in enumerate(unique_lats_cpu)}\n",
    "    lon_map = {round(lon.item(), 5): i for i, lon in enumerate(unique_lons_cpu)}\n",
    "\n",
    "    fft_results = []\n",
    "    # Iterate through the HOURLY tensors in the list\n",
    "    for tensor in tensor_list:\n",
    "        data_grid = torch.zeros((n1, n2), dtype=torch.float32, device=device)\n",
    "        if tensor is not None and tensor.numel() > 0: # Check if tensor is valid\n",
    "            tensor_dev = tensor.to(device)\n",
    "            for row in tensor_dev:\n",
    "                # Round coordinates for lookup consistency\n",
    "                lat_item = round(row[lat_col].item(), 5)\n",
    "                lon_item = round(row[lon_col].item(), 5)\n",
    "                \n",
    "                # Check if coordinates exist in the map (handles potentially missing coords)\n",
    "                i = lat_map.get(lat_item)\n",
    "                j = lon_map.get(lon_item)\n",
    "                \n",
    "                if i is not None and j is not None:\n",
    "                    val = row[val_col]\n",
    "                    val_num = val.item() if isinstance(val, torch.Tensor) else val\n",
    "                    if not np.isnan(val_num) and not np.isinf(val_num):\n",
    "                        data_grid[i, j] = val_num\n",
    "        \n",
    "        # Ensure grid is finite before FFT\n",
    "        data_grid = torch.nan_to_num(data_grid, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        fft_results.append(torch.fft.fft2(data_grid))\n",
    "\n",
    "    if not fft_results:\n",
    "         print(\"Warning: No FFT results generated.\")\n",
    "         return torch.empty(0, 0, 0, device=device), n1, n2, 0\n",
    "\n",
    "    J_vector_tensor = torch.stack(fft_results, dim=2).to(device) # Shape [n1, n2, p]\n",
    "\n",
    "    H = float(n1 * n2)\n",
    "    if H < 1e-9:\n",
    "        norm_factor = torch.tensor(0.0, device=device)\n",
    "    else:\n",
    "        norm_factor = (torch.sqrt(torch.tensor(1.0 / H, device=device)) / (2.0 * cmath.pi))\n",
    "\n",
    "    result = J_vector_tensor * norm_factor\n",
    "    return result, n1, n2, p # p is the number of hours\n",
    "\n",
    "\n",
    "def calculate_sample_periodogram_vectorized(J_vector_tensor):\n",
    "    \"\"\"Calculates sample periodogram I_n = J J^H. (Unchanged)\"\"\"\n",
    "    if torch.isnan(J_vector_tensor).any() or torch.isinf(J_vector_tensor).any():\n",
    "        n1, n2, p = J_vector_tensor.shape\n",
    "        return torch.full((n1, n2, p, p), float('nan'), dtype=torch.complex64, device=J_vector_tensor.device)\n",
    "\n",
    "    J_col = J_vector_tensor.unsqueeze(-1)\n",
    "    J_row_conj = J_vector_tensor.unsqueeze(-2).conj()\n",
    "    result = J_col @ J_row_conj\n",
    "    return result\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 4. Likelihood Calculation (Unchanged Structure)\n",
    "# =========================================================================\n",
    "\n",
    "def whittle_likelihood_loss_no_taper(params, I_sample, n1, n2, p, delta1, delta2):\n",
    "    \"\"\"Whittle Likelihood Loss. (Unchanged structure)\"\"\"\n",
    "    device = I_sample.device\n",
    "    params_tensor = params.to(device)\n",
    "\n",
    "    if torch.isnan(params_tensor).any() or torch.isinf(params_tensor).any():\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    # This call now correctly leads to cov_3d_difference\n",
    "    I_expected = expected_periodogram_fft_no_taper(\n",
    "        params_tensor, n1, n2, p, delta1, delta2\n",
    "    )\n",
    "\n",
    "    if torch.isnan(I_expected).any() or torch.isinf(I_expected).any():\n",
    "        print(\"Warning: NaN/Inf returned from expected_periodogram calculation.\")\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    eye_matrix = torch.eye(p, dtype=torch.complex64, device=device)\n",
    "    diag_vals = torch.abs(I_expected.diagonal(dim1=-2, dim2=-1))\n",
    "    mean_diag_abs = diag_vals.mean().item() if diag_vals.numel() > 0 and not torch.isnan(diag_vals).all() else 1.0\n",
    "    diag_load = max(mean_diag_abs * 1e-8, 1e-9)\n",
    "\n",
    "    I_expected_stable = I_expected + eye_matrix * diag_load\n",
    "\n",
    "    sign, logabsdet = torch.linalg.slogdet(I_expected_stable)\n",
    "    if torch.any(sign.real <= 1e-9):\n",
    "        log_det_term = torch.where(sign.real > 1e-9, logabsdet, torch.tensor(1e10, device=device))\n",
    "    else:\n",
    "        log_det_term = logabsdet\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    try:\n",
    "        # Check shapes before solve\n",
    "        if I_expected_stable.shape[-2:] != (p, p) or I_sample.shape[-2:] != (p,p):\n",
    "             print(f\"Shape mismatch: I_expected_stable {I_expected_stable.shape}, I_sample {I_sample.shape}, p={p}\")\n",
    "             return torch.tensor(float('nan'), device=device)\n",
    "        solved_term = torch.linalg.solve(I_expected_stable, I_sample)\n",
    "        trace_term = torch.einsum('...ii->...', solved_term).real\n",
    "    except torch.linalg.LinAlgError as e:\n",
    "         print(f\"Warning: LinAlgError during solve: {e}. Applying high loss penalty.\")\n",
    "         return torch.tensor(float('inf'), device=device)\n",
    "    except RuntimeError as e: # Catch other potential errors like shape mismatch if not caught above\n",
    "        print(f\"Runtime Error during solve/trace: {e}\")\n",
    "        return torch.tensor(float('inf'), device=device)\n",
    "\n",
    "\n",
    "    if torch.isnan(trace_term).any() or torch.isinf(trace_term).any():\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    likelihood_terms = log_det_term + trace_term\n",
    "\n",
    "    if torch.isnan(likelihood_terms).any():\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    total_sum = torch.sum(likelihood_terms)\n",
    "    # DC term is at index (0,0) in the spatial frequency domain\n",
    "    dc_term = likelihood_terms[0, 0] if n1 > 0 and n2 > 0 else torch.tensor(0.0, device=device)\n",
    "    if torch.isnan(dc_term).any() or torch.isinf(dc_term).any():\n",
    "        dc_term = torch.tensor(0.0, device=device)\n",
    "\n",
    "    # Subtract DC term only if there are non-DC frequencies\n",
    "    loss = total_sum - dc_term if (n1 > 1 or n2 > 1) else total_sum\n",
    "\n",
    "    if torch.isnan(loss) or torch.isinf(loss):\n",
    "         return torch.tensor(float('inf'), device=device)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 5. Training Loop (Unchanged)\n",
    "# =========================================================================\n",
    "def run_full(params_list, optimizer, scheduler, I_sample, n1, n2, p, epochs=600, device='cpu'):\n",
    "    \"\"\"Corrected training loop. (Unchanged)\"\"\"\n",
    "    best_loss = float('inf')\n",
    "    params_list = [p.to(device) for p in params_list]\n",
    "    best_params_state = [p.detach().clone() for p in params_list]\n",
    "    epochs_completed = 0\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063 # Still needed\n",
    "\n",
    "    def get_printable_params(p_list):\n",
    "        valid_tensors = [p for p in p_list if isinstance(p, torch.Tensor)]\n",
    "        if not valid_tensors: return \"Invalid params_list\"\n",
    "        p_cat = torch.cat([p.detach().clone().cpu() for p in valid_tensors])\n",
    "        log_indices = [0, 1, 2, 6]\n",
    "        if all(idx < len(p_cat) for idx in log_indices):\n",
    "            log_vals = p_cat[log_indices]\n",
    "            if not (torch.isnan(log_vals).any() or torch.isinf(log_vals).any()):\n",
    "                 # Ensure log_vals are positive before exp\n",
    "                 if torch.all(log_vals > -torch.inf):\n",
    "                     try:\n",
    "                         p_cat[log_indices] = torch.exp(log_vals)\n",
    "                     except RuntimeError: # Handle potential overflow if log_vals are huge\n",
    "                          p_cat[log_indices] = float('inf')\n",
    "                 else:\n",
    "                     p_cat[log_indices] = float('nan') # Cannot exp non-finite\n",
    "            else:\n",
    "                 p_cat[log_indices] = float('nan')\n",
    "        return p_cat.numpy().round(4)\n",
    "\n",
    "    I_sample_dev = I_sample.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epochs_completed = epoch + 1\n",
    "        optimizer.zero_grad()\n",
    "        params_tensor = torch.cat(params_list)\n",
    "\n",
    "        loss = whittle_likelihood_loss_no_taper(\n",
    "            params_tensor, I_sample_dev, n1, n2, p, DELTA_LAT, DELTA_LON\n",
    "        )\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"Loss became NaN or Inf at epoch {epoch+1}. Stopping.\")\n",
    "            if epoch == 0: best_params_state = None\n",
    "            epochs_completed = epoch\n",
    "            break\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        nan_grad = False\n",
    "        for param in params_list:\n",
    "            if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
    "                nan_grad = True\n",
    "                break\n",
    "        if nan_grad:\n",
    "             print(f\"Warning: NaN/Inf gradient detected at epoch {epoch+1}. Skipping step.\")\n",
    "             optimizer.zero_grad()\n",
    "             # Optionally reduce LR or revert params here if needed\n",
    "             continue # Skip optimizer step and scheduler step\n",
    "\n",
    "        # Only clip and step if gradients are valid\n",
    "        all_params_on_device = params_list\n",
    "        if all_params_on_device:\n",
    "            torch.nn.utils.clip_grad_norm_(all_params_on_device, max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step() # Step scheduler after optimizer\n",
    "\n",
    "        current_loss_item = loss.item()\n",
    "        if current_loss_item < best_loss:\n",
    "            params_valid = not any(torch.isnan(p.data).any() or torch.isinf(p.data).any() for p in params_list)\n",
    "            if params_valid:\n",
    "                best_loss = current_loss_item\n",
    "                best_params_state = [p.detach().clone() for p in params_list]\n",
    "            # else: # Optionally print warning if params become invalid after step\n",
    "            #     print(f\"Warning: Params became invalid after step {epoch+1}. Not saving state.\")\n",
    "\n",
    "\n",
    "        if epoch % 50 == 0 or epoch == epochs - 1:\n",
    "            current_lr = optimizer.param_groups[0]['lr'] if optimizer.param_groups else 0.0\n",
    "            print(f'--- Epoch {epoch+1}/{epochs} (LR: {current_lr:.6f}) ---')\n",
    "            print(f' Loss: {current_loss_item:.4f}')\n",
    "            print(f' Parameters (Natural Scale): {get_printable_params(params_list)}')\n",
    "\n",
    "    if best_params_state is None:\n",
    "        print(\"Training failed to find a valid model state.\")\n",
    "        return None, epochs_completed\n",
    "\n",
    "    final_params_log_scale = torch.cat([p.cpu() for p in best_params_state])\n",
    "    final_params_natural_scale = final_params_log_scale.detach().clone()\n",
    "    log_indices = [0, 1, 2, 6]\n",
    "    if all(idx < len(final_params_natural_scale) for idx in log_indices):\n",
    "        log_vals = final_params_natural_scale[log_indices]\n",
    "        if not (torch.isnan(log_vals).any() or torch.isinf(log_vals).any()):\n",
    "             if torch.all(log_vals > -torch.inf):\n",
    "                 try:\n",
    "                      final_params_natural_scale[log_indices] = torch.exp(log_vals)\n",
    "                 except RuntimeError:\n",
    "                      final_params_natural_scale[log_indices] = float('inf')\n",
    "             else:\n",
    "                  final_params_natural_scale[log_indices] = float('nan')\n",
    "        else:\n",
    "            final_params_natural_scale[log_indices] = float('nan')\n",
    "\n",
    "    final_params_rounded = [round(p.item(), 4) if not np.isnan(p.item()) else float('nan') for p in final_params_natural_scale]\n",
    "    final_loss_rounded = round(best_loss, 3) if best_loss != float('inf') else float('inf')\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(f'\\nFINAL BEST STATE ACHIEVED (during training):')\n",
    "    print(f'Best Loss: {final_loss_rounded}')\n",
    "    print(f'Parameters Corresponding to Best Loss (Natural Scale): {final_params_rounded}')\n",
    "\n",
    "    return final_params_rounded + [final_loss_rounded], epochs_completed\n",
    "\n",
    "# =========================================================================\n",
    "# 6. Main Execution Script (MODIFIED to load correct sparse data)\n",
    "# =========================================================================\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    DAY_INDEX_TO_RUN = 0 # Index in the outer list (0 corresponds to the first day processed)\n",
    "    NUM_RUNS = 1\n",
    "    EPOCHS = 700\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    # --- Grid Spacing (Needed for the covariance model) ---\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063\n",
    "\n",
    "    # --- Column Indices ---\n",
    "    LAT_COL, LON_COL = 0, 1\n",
    "    VAL_COL = 2\n",
    "    TIME_COL = 3 # This column now represents the original hour (0-6)\n",
    "\n",
    "    # --- (MODIFIED) Load 3D Filtered **Sparse** Data ---\n",
    "    try:\n",
    "        # âœ… --- LOAD THE NEW, CORRECT FILE ---\n",
    "        # (Make sure year/month match the file you just created)\n",
    "        year = 2024\n",
    "        month = 7\n",
    "        month_str = f\"{month:02d}\"\n",
    "        data_filename = f\"filtered_3d_sparse_data_{year}_{month_str}.pkl\"\n",
    "        \n",
    "        with open(data_filename, 'rb') as f:\n",
    "            # This is a list of Tensors: [day1_tensor, day2_tensor, ...]\n",
    "            all_filtered_days = pickle.load(f)\n",
    "        \n",
    "        print(f\"Loaded {len(all_filtered_days)} days from {data_filename}.\")\n",
    "\n",
    "        if not all_filtered_days: raise ValueError(\"Loaded data is empty.\")\n",
    "\n",
    "        # âœ… --- Select the single tensor for the desired day ---\n",
    "        if DAY_INDEX_TO_RUN < 0 or DAY_INDEX_TO_RUN >= len(all_filtered_days):\n",
    "             raise IndexError(f\"DAY_INDEX_TO_RUN ({DAY_INDEX_TO_RUN}) is out of bounds for the loaded data.\")\n",
    "        \n",
    "        day_tensor = all_filtered_days[DAY_INDEX_TO_RUN]\n",
    "        \n",
    "        if day_tensor is None or day_tensor.numel() == 0:\n",
    "            raise ValueError(f\"Data for day index {DAY_INDEX_TO_RUN} is empty.\")\n",
    "\n",
    "        day_tensor = day_tensor.cpu().to(torch.float32)\n",
    "        \n",
    "        # âœ… --- SPLIT the day tensor into a list of hourly tensors ---\n",
    "        # This is the new step required by generate_Jvector_no_taper\n",
    "        unique_times_in_day = torch.unique(day_tensor[:, TIME_COL])\n",
    "        time_slices_list = []\n",
    "        for t in unique_times_in_day:\n",
    "            hourly_tensor = day_tensor[day_tensor[:, TIME_COL] == t]\n",
    "            time_slices_list.append(hourly_tensor)\n",
    "        \n",
    "        if not time_slices_list:\n",
    "            raise ValueError(f\"Could not split day {DAY_INDEX_TO_RUN} into time slices.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: `{data_filename}` not found.\")\n",
    "        print(\"Please run the 3D data preparation script first.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing data: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # --- 1. Pre-compute Sample Periodogram (NO Tapering) ---\n",
    "    print(f\"Pre-computing sample periodogram for Day Index {DAY_INDEX_TO_RUN} (NO data taper)...\")\n",
    "    \n",
    "    # âœ… Pass the NEW list of hourly tensors\n",
    "    J_vec, n1, n2, p = generate_Jvector_no_taper(\n",
    "        time_slices_list,  # This is now correctly formatted\n",
    "        lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if J_vec.numel() == 0 or n1 == 0 or n2 == 0 or p == 0:\n",
    "       print(f\"Error: J-vector generation failed for Day Index {DAY_INDEX_TO_RUN}.\")\n",
    "       print(f\"Number of time slices: {len(time_slices_list)}\")\n",
    "       print(f\"Shapes of time slices: {[t.shape for t in time_slices_list if t is not None]}\")\n",
    "       exit()\n",
    "\n",
    "    I_sample = calculate_sample_periodogram_vectorized(J_vec)\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Error: NaN or Inf detected in the sample periodogram. Cannot proceed.\")\n",
    "        exit()\n",
    "\n",
    "    # p now represents the number of hours used (e.g., 7)\n",
    "    print(f\"Data grid: {n1}x{n2} spatial points, {p} time points (hours). Sample Periodogram on {DEVICE}.\")\n",
    "\n",
    "    # --- 2. Optimization Loop (Unchanged from here down) ---\n",
    "    all_final_results = []\n",
    "    all_final_losses = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        print(f\"\\n{'='*30} Initialization Run {i+1}/{NUM_RUNS} {'='*30}\")\n",
    "\n",
    "        # âœ… --- Use Fixed Initial Parameters ---\n",
    "        initial_params_values = [\n",
    "            np.log(21.303), np.log(1.307), np.log(1.563),\n",
    "            0.022, -0.144, 0.198,\n",
    "            np.log(4.769)\n",
    "        ]\n",
    "\n",
    "        print(f\"Starting with fixed params (log-scale for [0,1,2,6]): {[round(p, 4) for p in initial_params_values]}\")\n",
    "\n",
    "        params_list = [\n",
    "            Parameter(torch.tensor([val], dtype=torch.float32))\n",
    "            for val in initial_params_values\n",
    "        ]\n",
    "\n",
    "        lr_slow, lr_fast = 0.005, 0.02\n",
    "        slow_indices = [0, 1, 2, 6]\n",
    "        fast_indices = [3, 4, 5]\n",
    "\n",
    "        valid_slow_indices = [idx for idx in slow_indices if idx < len(params_list)]\n",
    "        valid_fast_indices = [idx for idx in fast_indices if idx < len(params_list)]\n",
    "\n",
    "        param_groups = [\n",
    "            {'params': [params_list[idx] for idx in valid_slow_indices], 'lr': lr_slow, 'name': 'slow_group'},\n",
    "            {'params': [params_list[idx] for idx in valid_fast_indices], 'lr': lr_fast, 'name': 'fast_group'}\n",
    "        ]\n",
    "\n",
    "        optimizer = torch.optim.Adam(param_groups)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=200, eta_min=1e-6)\n",
    "\n",
    "        print(f\"Starting optimization run {i+1} on device {DEVICE} (NO data taper, 3D Diff)...\")\n",
    "        final_results, epochs_run = run_full(\n",
    "            params_list=params_list,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            I_sample=I_sample,\n",
    "            n1=n1, n2=n2, p=p, # p is now number of hours\n",
    "            epochs=EPOCHS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "        if final_results:\n",
    "            all_final_results.append(final_results)\n",
    "            all_final_losses.append(final_results[-1])\n",
    "        else:\n",
    "            all_final_results.append(None)\n",
    "            all_final_losses.append(float('inf'))\n",
    "\n",
    "    print(f\"\\n\\n{'='*25} Overall Result from Run {'='*25}\")\n",
    "    valid_losses = [l for l in all_final_losses if l is not None and l != float('inf')]\n",
    "\n",
    "    if not valid_losses:\n",
    "        print(f\"The run failed or resulted in an invalid loss for Day Index {DAY_INDEX_TO_RUN}.\")\n",
    "    else:\n",
    "        best_loss = valid_losses[0]\n",
    "        best_run_index = 0\n",
    "        best_results = all_final_results[best_run_index]\n",
    "        print(f\"Run Loss: {best_results[-1]}\")\n",
    "        print(f\"Final Parameters (Natural Scale): {best_results[:-1]}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295769b1",
   "metadata": {},
   "source": [
    "## Time twice + space once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8ed3f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/joonwonlee/Documents/GEMS_DATA/pickle_2024/coarse_cen_map_without_decrement_latitude24_07.pkl\n",
      "Loaded 31 days of raw data.\n",
      "Starting STAGE 1: Spatial Differencing...\n",
      "STAGE 1 Complete. Created 31 spatially filtered day-tensors.\n",
      "Starting STAGE 2: First Temporal Differencing...\n",
      "STAGE 2 Complete. Created 30 first-temporal-difference tensors.\n",
      "Starting STAGE 3: Second Temporal Differencing...\n",
      "STAGE 3 Complete. Created 29 second-temporal-difference tensors.\n",
      "\n",
      "--- Results ---\n",
      "Number of spatially filtered day tensors (Z): 31\n",
      "Number of first-temporal difference tensors (Y1): 30\n",
      "Number of final second-temporal difference tensors (Y2): 29\n",
      "Processed data saved to spacetime_second_diff_data.pkl\n",
      "\n",
      "Shape of the first final tensor (Y2): torch.Size([7232, 4])\n",
      "First final tensor head (Y2):\n",
      "tensor([[  4.9320, 132.4170,   3.0988,  69.0000],\n",
      "        [  4.9320, 131.5980,  -5.1850,  69.0000],\n",
      "        [  4.9320, 131.2830,  -9.9230,  69.0000],\n",
      "        [  4.9320, 130.7790,   8.8235,  69.0000],\n",
      "        [  4.9320, 130.4640,   1.2111,  69.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# --- Helper Functions (Unchanged) ---\n",
    "\n",
    "def subset_tensor(df_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Subsets a tensor to a specific lat/lon range.\n",
    "    Columns are assumed to be [lat, lon, ozone, time].\n",
    "    \"\"\"\n",
    "    lat_mask = (df_tensor[:, 0] >= 0) & (df_tensor[:, 0] <= 5)\n",
    "    lon_mask = (df_tensor[:, 1] >= 123) & (df_tensor[:, 1] <= 133)\n",
    "    \n",
    "    df_sub = df_tensor[lat_mask & lon_mask].clone()\n",
    "    return df_sub\n",
    "\n",
    "def apply_spatial_diff_sparse(df_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies the 2D spatial filter Z(s) = X(s+d_lat) + X(s+d_lon) - 2X(s)\n",
    "    to sparse data using a dictionary lookup.\n",
    "    \"\"\"\n",
    "    if df_tensor.size(0) == 0:\n",
    "        return torch.empty(0, 4)\n",
    "\n",
    "    unique_lats = torch.unique(df_tensor[:, 0])\n",
    "    unique_lons = torch.unique(df_tensor[:, 1])\n",
    "\n",
    "    if len(unique_lats) < 2 or len(unique_lons) < 2:\n",
    "        raise ValueError(\"Not enough unique lat/lon points to find grid spacing.\")\n",
    "\n",
    "    sorted_lats = torch.sort(unique_lats)[0]\n",
    "    sorted_lons = torch.sort(unique_lons)[0]\n",
    "    delta_lat = round((sorted_lats[1:] - sorted_lats[:-1]).min().item(), 5)\n",
    "    delta_lon = round((sorted_lons[1:] - sorted_lons[:-1]).min().item(), 5)\n",
    "\n",
    "    if delta_lat == 0 or delta_lon == 0:\n",
    "        raise ValueError(\"Could not determine a valid grid spacing.\")\n",
    "\n",
    "    data_lookup = {\n",
    "        (round(row[0].item(), 5), round(row[1].item(), 5)): row[2].item() \n",
    "        for row in df_tensor\n",
    "    }\n",
    "    time_value = df_tensor[0, 3] \n",
    "\n",
    "    filtered_rows = []\n",
    "    for (lat, lon), val_s in data_lookup.items():\n",
    "        val_s_lat = data_lookup.get((round(lat + delta_lat, 5), lon))\n",
    "        val_s_lon = data_lookup.get((lat, round(lon + delta_lon, 5)))\n",
    "\n",
    "        if val_s_lat is not None and val_s_lon is not None:\n",
    "            diff_val = val_s_lat + val_s_lon - (2 * val_s)\n",
    "            new_row = torch.tensor([lat, lon, diff_val, time_value])\n",
    "            filtered_rows.append(new_row)\n",
    "\n",
    "    if not filtered_rows:\n",
    "        return torch.empty(0, 4)\n",
    "\n",
    "    return torch.stack(filtered_rows, dim=0)\n",
    "\n",
    "\n",
    "# --- Data Loading (Unchanged) ---\n",
    "# âš ï¸ NOTE: You must define these variables in your environment\n",
    "# mac_data_path = \"...\"\n",
    "# year = 2022\n",
    "# month_str = \"01\"\n",
    "# class GEMS_TCO: # Placeholder\n",
    "#     def load_data(self, path): return self\n",
    "#     def load_working_data_byday_wo_mm(self, data, indices):\n",
    "#         return {'key': torch.randn(100, 4)}, torch.randn(100, 4)\n",
    "\n",
    "# (Assuming data loading variables are defined...)\n",
    "pickle_path = os.path.join(mac_data_path, f'pickle_{year}')\n",
    "output_filename = f\"coarse_cen_map_without_decrement_latitude{str(year)[2:]}_{month_str}.pkl\"\n",
    "output_filepath = os.path.join(pickle_path, output_filename)\n",
    "print(f\"Loading data from: {output_filepath}\")\n",
    "\n",
    "with open(output_filepath, 'rb') as pickle_file:\n",
    "    cbmap_ori = pickle.load(pickle_file)\n",
    "\n",
    "load_data_instance = GEMS_TCO.load_data('')\n",
    "df_day_map_list = []\n",
    "for i in range(31): # Adjust if necessary\n",
    "    cur_map, _ = load_data_instance.load_working_data_byday_wo_mm(cbmap_ori, [i*8, (i+1)*8])\n",
    "    df_day_map_list.append(cur_map)\n",
    "print(f\"Loaded {len(df_day_map_list)} days of raw data.\")\n",
    "\n",
    "\n",
    "# --- Main Processing Loop ---\n",
    "\n",
    "# âœ… STAGE 1: Apply the spatial filter (Unchanged)\n",
    "spatially_filtered_days = [] # Result: Z(s,t)\n",
    "\n",
    "print(\"Starting STAGE 1: Spatial Differencing...\")\n",
    "for day_idx, day_map in enumerate(df_day_map_list):\n",
    "    tensors_to_aggregate = []\n",
    "    for key, tensor in day_map.items():\n",
    "        subsetted = subset_tensor(tensor)\n",
    "        if subsetted.size(0) > 0:\n",
    "            try:\n",
    "                diff_applied = apply_spatial_diff_sparse(subsetted)\n",
    "                if diff_applied.size(0) > 0:\n",
    "                    tensors_to_aggregate.append(diff_applied)\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping chunk on day {day_idx+1}, key {key}: {e}\")\n",
    "    if tensors_to_aggregate:\n",
    "        aggregated_day_tensor = torch.cat(tensors_to_aggregate, dim=0)\n",
    "        spatially_filtered_days.append(aggregated_day_tensor)\n",
    "print(f\"STAGE 1 Complete. Created {len(spatially_filtered_days)} spatially filtered day-tensors.\")\n",
    "\n",
    "# âœ… STAGE 2: Apply the FIRST temporal difference (Unchanged logic, variable renamed)\n",
    "first_temporal_diff_tensors = [] # Result: Y1(s,t) = Z(s,t) - Z(s,t-1)\n",
    "\n",
    "print(\"Starting STAGE 2: First Temporal Differencing...\")\n",
    "if len(spatially_filtered_days) > 1:\n",
    "    for i in range(1, len(spatially_filtered_days)):\n",
    "        prev_day_tensor = spatially_filtered_days[i-1] # Z(s, t-1)\n",
    "        current_day_tensor = spatially_filtered_days[i] # Z(s, t)\n",
    "        \n",
    "        prev_day_lookup = {\n",
    "            (round(row[0].item(), 5), round(row[1].item(), 5)): row[2].item() \n",
    "            for row in prev_day_tensor\n",
    "        }\n",
    "        \n",
    "        temporally_differenced_rows = []\n",
    "        for row in current_day_tensor:\n",
    "            lat = round(row[0].item(), 5)\n",
    "            lon = round(row[1].item(), 5)\n",
    "            \n",
    "            if (lat, lon) in prev_day_lookup:\n",
    "                current_Z = row[2].item()\n",
    "                prev_Z = prev_day_lookup[(lat, lon)]\n",
    "                Y1_value = current_Z - prev_Z # First difference\n",
    "                \n",
    "                new_row = torch.tensor([lat, lon, Y1_value, row[3]])\n",
    "                temporally_differenced_rows.append(new_row)\n",
    "        \n",
    "        if temporally_differenced_rows:\n",
    "            day_diff_tensor = torch.stack(temporally_differenced_rows, dim=0)\n",
    "            first_temporal_diff_tensors.append(day_diff_tensor)\n",
    "print(f\"STAGE 2 Complete. Created {len(first_temporal_diff_tensors)} first-temporal-difference tensors.\")\n",
    "\n",
    "# âœ… STAGE 3: Apply the SECOND temporal difference\n",
    "second_temporal_diff_tensors = [] # Result: Y2(s,t) = Y1(s,t) - Y1(s,t-1)\n",
    "\n",
    "print(\"Starting STAGE 3: Second Temporal Differencing...\")\n",
    "# We need at least two Y1 tensors to compute the second difference\n",
    "if len(first_temporal_diff_tensors) > 1:\n",
    "    # Iterate from the second Y1 tensor (which corresponds to day index i=2 of Z)\n",
    "    for i in range(1, len(first_temporal_diff_tensors)):\n",
    "        prev_Y1_tensor = first_temporal_diff_tensors[i-1] # Y1(s, t-1)\n",
    "        current_Y1_tensor = first_temporal_diff_tensors[i]  # Y1(s, t)\n",
    "        \n",
    "        # Create lookup for the previous Y1 values\n",
    "        prev_Y1_lookup = {\n",
    "            (round(row[0].item(), 5), round(row[1].item(), 5)): row[2].item() \n",
    "            for row in prev_Y1_tensor\n",
    "        }\n",
    "        \n",
    "        second_diff_rows = []\n",
    "        for row in current_Y1_tensor:\n",
    "            lat = round(row[0].item(), 5)\n",
    "            lon = round(row[1].item(), 5)\n",
    "            \n",
    "            if (lat, lon) in prev_Y1_lookup:\n",
    "                current_Y1 = row[2].item()\n",
    "                prev_Y1 = prev_Y1_lookup[(lat, lon)]\n",
    "                Y2_value = current_Y1 - prev_Y1 # Second difference\n",
    "                \n",
    "                # Keep coordinates and the time stamp of the *current* Y1 tensor\n",
    "                new_row = torch.tensor([lat, lon, Y2_value, row[3]])\n",
    "                second_diff_rows.append(new_row)\n",
    "        \n",
    "        if second_diff_rows:\n",
    "            day_second_diff_tensor = torch.stack(second_diff_rows, dim=0)\n",
    "            second_temporal_diff_tensors.append(day_second_diff_tensor)\n",
    "print(f\"STAGE 3 Complete. Created {len(second_temporal_diff_tensors)} second-temporal-difference tensors.\")\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\n--- Results ---\")\n",
    "print(\"Number of spatially filtered day tensors (Z):\", len(spatially_filtered_days))\n",
    "print(\"Number of first-temporal difference tensors (Y1):\", len(first_temporal_diff_tensors))\n",
    "print(\"Number of final second-temporal difference tensors (Y2):\", len(second_temporal_diff_tensors))\n",
    "\n",
    "if second_temporal_diff_tensors:\n",
    "    # Save the final processed data\n",
    "    processed_output_path = \"spacetime_second_diff_data.pkl\" # New filename\n",
    "    with open(processed_output_path, 'wb') as f:\n",
    "        pickle.dump(second_temporal_diff_tensors, f)\n",
    "    print(f\"Processed data saved to {processed_output_path}\")\n",
    "\n",
    "    print(\"\\nShape of the first final tensor (Y2):\", second_temporal_diff_tensors[0].shape)\n",
    "    print(\"First final tensor head (Y2):\")\n",
    "    print(second_temporal_diff_tensors[0][:5])\n",
    "else:\n",
    "    print(\"\\nNo final second-differenced tensors were created. Check data or filter logic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1732e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded 29 days from spacetime_second_diff_data.pkl.\n",
      "Error: DAY_TO_RUN (1) is invalid for the doubly differenced data (valid range approx 3 to 31).\n",
      "Pre-computing sample periodogram (NO data taper)...\n",
      "Data grid: 113x8 spatial points, 8 time points. Sample Periodogram on cpu.\n",
      "\n",
      "============================== Initialization Run 1/1 ==============================\n",
      "Starting with fixed params (log-scale for [0,1,2,6]): [3.0588, 0.2677, 0.4466, 0.022, -0.144, 0.198, 1.5621]\n",
      "Starting optimization run 1 on device cpu (NO data taper, Spat + 2x Temp Diff)...\n",
      "--- Epoch 1/700 (LR: 0.005000) ---\n",
      " Loss: 140469.1250\n",
      " Parameters (Natural Scale): [21.4098  1.3005  1.5552  0.042  -0.124   0.178   4.7929]\n",
      "--- Epoch 51/700 (LR: 0.004240) ---\n",
      " Loss: 111063.8125\n",
      " Parameters (Natural Scale): [ 2.6071e+01  1.0695e+00  1.2731e+00 -4.5000e-03  1.2600e-02  2.0000e-04\n",
      "  5.6888e+00]\n",
      "--- Epoch 101/700 (LR: 0.002461) ---\n",
      " Loss: 94843.8984\n",
      " Parameters (Natural Scale): [ 3.03786e+01  9.43800e-01  1.08220e+00  1.80000e-03  1.68000e-02\n",
      " -7.00000e-04  6.34220e+00]\n",
      "--- Epoch 151/700 (LR: 0.000706) ---\n",
      " Loss: 86066.6016\n",
      " Parameters (Natural Scale): [3.37691e+01 8.61700e-01 9.70900e-01 7.00000e-04 1.75000e-02 0.00000e+00\n",
      " 6.85460e+00]\n",
      "--- Epoch 201/700 (LR: 0.000001) ---\n",
      " Loss: 84280.9844\n",
      " Parameters (Natural Scale): [3.44775e+01 8.45300e-01 9.50700e-01 8.00000e-04 1.75000e-02 0.00000e+00\n",
      " 6.97200e+00]\n",
      "--- Epoch 251/700 (LR: 0.000761) ---\n",
      " Loss: 82753.1953\n",
      " Parameters (Natural Scale): [3.51414e+01 8.29700e-01 9.32600e-01 8.00000e-04 1.78000e-02 0.00000e+00\n",
      " 7.09590e+00]\n",
      "--- Epoch 301/700 (LR: 0.002540) ---\n",
      " Loss: 76272.2656\n",
      " Parameters (Natural Scale): [3.81879e+01 7.64200e-01 8.58200e-01 1.70000e-03 1.97000e-02 1.00000e-04\n",
      " 7.64970e+00]\n",
      "--- Epoch 351/700 (LR: 0.004295) ---\n",
      " Loss: 67314.1641\n",
      " Parameters (Natural Scale): [4.36868e+01 6.67500e-01 7.50800e-01 2.20000e-03 2.27000e-02 6.00000e-04\n",
      " 8.53980e+00]\n",
      "--- Epoch 401/700 (LR: 0.005000) ---\n",
      " Loss: 57111.4609\n",
      " Parameters (Natural Scale): [5.29492e+01 5.49800e-01 6.20300e-01 2.50000e-03 2.68000e-02 5.00000e-04\n",
      " 9.74330e+00]\n",
      "--- Epoch 451/700 (LR: 0.004240) ---\n",
      " Loss: 47608.8242\n",
      " Parameters (Natural Scale): [6.68876e+01 4.35100e-01 4.91700e-01 2.20000e-03 3.28000e-02 4.00000e-04\n",
      " 1.10203e+01]\n",
      "--- Epoch 501/700 (LR: 0.002461) ---\n",
      " Loss: 40437.7305\n",
      " Parameters (Natural Scale): [ 8.40298e+01  3.46600e-01  3.92200e-01  1.50000e-03  4.05000e-02\n",
      " -3.00000e-04  1.20280e+01]\n",
      "--- Epoch 551/700 (LR: 0.000706) ---\n",
      " Loss: 36474.8789\n",
      " Parameters (Natural Scale): [9.80805e+01 2.96700e-01 3.36800e-01 1.30000e-03 4.71000e-02 0.00000e+00\n",
      " 1.26034e+01]\n",
      "--- Epoch 601/700 (LR: 0.000001) ---\n",
      " Loss: 35848.9766\n",
      " Parameters (Natural Scale): [ 1.006674e+02  2.890000e-01  3.283000e-01  1.300000e-03  4.830000e-02\n",
      " -0.000000e+00  1.269920e+01]\n",
      "--- Epoch 651/700 (LR: 0.000761) ---\n",
      " Loss: 35378.5273\n",
      " Parameters (Natural Scale): [ 1.029505e+02  2.826000e-01  3.211000e-01  1.300000e-03  4.930000e-02\n",
      " -0.000000e+00  1.279920e+01]\n",
      "--- Epoch 700/700 (LR: 0.002500) ---\n",
      " Loss: 32962.0664\n",
      " Parameters (Natural Scale): [ 1.164261e+02  2.494000e-01  2.845000e-01  1.500000e-03  5.480000e-02\n",
      " -1.000000e-04  1.332100e+01]\n",
      "\n",
      "--- Training Complete ---\n",
      "\n",
      "FINAL BEST STATE ACHIEVED (during training):\n",
      "Best Loss: 32962.066\n",
      "Parameters Corresponding to Best Loss (Natural Scale): [116.4261, 0.2494, 0.2845, 0.0015, 0.0548, -0.0001, 13.321]\n",
      "\n",
      "\n",
      "========================= Overall Result from Run =========================\n",
      "Run Loss: 32962.066\n",
      "Final Parameters (Natural Scale): [116.4261, 0.2494, 0.2845, 0.0015, 0.0548, -0.0001, 13.321]\n",
      "\n",
      "Total execution time: 2231.18 seconds\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # Keep if plotting might be added later\n",
    "import cmath\n",
    "import pickle\n",
    "import time # For timing\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import Parameter\n",
    "import pandas as pd # Make sure pandas is imported\n",
    "import os # Make sure os is imported\n",
    "\n",
    "# =========================================================================\n",
    "# 1. Modeling Functions (Adapted for Spat + 2x Temp Differencing)\n",
    "# =========================================================================\n",
    "\n",
    "# --- Bartlett Kernel (Used for c_gn when g_s=1) ---\n",
    "def cgn_2dbartlett_kernel(u1, u2, n1, n2):\n",
    "    \"\"\"\n",
    "    Computes the 2D Bartlett kernel: Product(1 - |ui|/ni). (Unchanged)\n",
    "    \"\"\"\n",
    "    device = u1.device if isinstance(u1, torch.Tensor) else (u2.device if isinstance(u2, torch.Tensor) else torch.device('cpu'))\n",
    "    u1_tensor = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_tensor = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    n1_eff = float(n1) if n1 > 0 else 1.0\n",
    "    n2_eff = float(n2) if n2 > 0 else 1.0\n",
    "    kernel = (1.0 - torch.abs(u1_tensor) / n1_eff) * (1.0 - torch.abs(u2_tensor) / n2_eff)\n",
    "    return torch.clamp(kernel, min=0.0)\n",
    "\n",
    "# --- Covariance of the Original Field X (EXPONENTIAL Kernel) ---\n",
    "def cov_x_exponential(u1, u2, t, params):\n",
    "    \"\"\"\n",
    "    Computes the autocovariance of the ORIGINAL process X. (Unchanged)\n",
    "    Expects log-scale params [0,1,2,6].\n",
    "    \"\"\"\n",
    "    device = params.device \n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    log_params_indices = [0, 1, 2, 6]\n",
    "    if torch.isnan(params[log_params_indices]).any() or torch.isinf(params[log_params_indices]).any():\n",
    "         out_shape = torch.broadcast_shapes(u1_dev.shape, u2_dev.shape, t_dev.shape)\n",
    "         return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "\n",
    "    sigmasq, r_lat, r_lon, nugget = torch.exp(params[log_params_indices])\n",
    "    a_lat, a_lon, beta = params[3], params[4], params[5]\n",
    "\n",
    "    r_lat = torch.clamp(r_lat, min=1e-6)\n",
    "    r_lon = torch.clamp(r_lon, min=1e-6)\n",
    "\n",
    "    x1 = u1_dev / r_lat - a_lat * t_dev\n",
    "    x2 = u2_dev / r_lon - a_lon * t_dev\n",
    "    x3 = beta * t_dev\n",
    "    distance_sq = x1**2 + x2**2 + x3**2\n",
    "    epsilon = 1e-12\n",
    "    distance_sq_clamped = torch.clamp(distance_sq, min=0.0)\n",
    "    D = torch.sqrt(distance_sq_clamped + epsilon) \n",
    "    cov_smooth = sigmasq * torch.exp(-D) \n",
    "\n",
    "    is_zero_lag = (torch.abs(u1_dev) < 1e-9) & (torch.abs(u2_dev) < 1e-9) & (torch.abs(t_dev) < 1e-9)\n",
    "    final_cov = torch.where(is_zero_lag, cov_smooth + nugget, cov_smooth)\n",
    "\n",
    "    if torch.isnan(final_cov).any():\n",
    "        print(\"Warning: NaN detected in cov_x_exponential output.\")\n",
    "    return final_cov\n",
    "\n",
    "\n",
    "# --- Covariance of the Spatially Differenced Field Z ---\n",
    "def cov_spatial_difference(u1, u2, t, params, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Calculates covariance Cov(Z(s), Z(s+u)) for the SPATIAL-ONLY filter:\n",
    "    Z(s) = X(s+d1) + X(s+d2) - 2X(s). (Unchanged)\n",
    "    \"\"\"\n",
    "    weights = {(0, 0): -2.0, (1, 0): 1.0, (0, 1): 1.0}\n",
    "    device = params.device\n",
    "    out_shape = torch.broadcast_shapes(u1.shape if isinstance(u1, torch.Tensor) else (),\n",
    "                                     u2.shape if isinstance(u2, torch.Tensor) else (),\n",
    "                                     t.shape if isinstance(t, torch.Tensor) else ())\n",
    "    cov = torch.zeros(out_shape, device=device, dtype=torch.float32)\n",
    "\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    for (a_idx, b_idx), w_ab in weights.items():\n",
    "        offset_a1 = a_idx * delta1\n",
    "        offset_a2 = b_idx * delta2\n",
    "        for (c_idx, d_idx), w_cd in weights.items():\n",
    "            offset_c1 = c_idx * delta1\n",
    "            offset_c2 = d_idx * delta2\n",
    "            lag_u1 = u1_dev + (offset_a1 - offset_c1)\n",
    "            lag_u2 = u2_dev + (offset_a2 - offset_c2)\n",
    "            term_cov = cov_x_exponential(lag_u1, lag_u2, t_dev, params)\n",
    "            if torch.isnan(term_cov).any():\n",
    "                 return torch.full_like(cov, float('nan'))\n",
    "            cov += w_ab * w_cd * term_cov\n",
    "\n",
    "    if torch.isnan(cov).any():\n",
    "        print(\"Warning: NaN detected in final cov_spatial_difference output.\")\n",
    "    return cov\n",
    "\n",
    "# --- Covariance of Spatio-Temporal (First Temp Diff) Field Y1 ---\n",
    "def cov_spacetime_difference(u1, u2, t, params, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Calculates covariance for Y1(s,t) = Z(s,t) - Z(s,t-1). (Unchanged from previous)\n",
    "    \"\"\"\n",
    "    device = params.device\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    term_center = cov_spatial_difference(u1_dev, u2_dev, t_dev, params, delta1, delta2)\n",
    "    term_minus_1 = cov_spatial_difference(u1_dev, u2_dev, t_dev - 1.0, params, delta1, delta2)\n",
    "    term_plus_1 = cov_spatial_difference(u1_dev, u2_dev, t_dev + 1.0, params, delta1, delta2)\n",
    "\n",
    "    if torch.isnan(term_center).any() or torch.isnan(term_minus_1).any() or torch.isnan(term_plus_1).any():\n",
    "        out_shape = torch.broadcast_shapes(u1_dev.shape, u2_dev.shape, t_dev.shape)\n",
    "        return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "\n",
    "    return 2.0 * term_center - term_minus_1 - term_plus_1\n",
    "\n",
    "# --- (NEW) Covariance of Spatio-Temporal (Second Temp Diff) Field Y2 ---\n",
    "def cov_spacetime_second_diff(u1, u2, t, params, delta1, delta2):\n",
    "    \"\"\"\n",
    "    âœ… Calculates covariance for the new Spat + 2x Temp filter:\n",
    "    Y2(s,t) = Y1(s,t) - Y1(s,t-1).\n",
    "    This is C_Y2(u, tau) = 2*C_Y1(u, tau) - C_Y1(u, tau-1) - C_Y1(u, tau+1)\n",
    "    \"\"\"\n",
    "    device = params.device\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    # C_Y1(u, tau)\n",
    "    term_center = cov_spacetime_difference(u1_dev, u2_dev, t_dev, params, delta1, delta2)\n",
    "    # C_Y1(u, tau - 1)\n",
    "    term_minus_1 = cov_spacetime_difference(u1_dev, u2_dev, t_dev - 1.0, params, delta1, delta2)\n",
    "    # C_Y1(u, tau + 1)\n",
    "    term_plus_1 = cov_spacetime_difference(u1_dev, u2_dev, t_dev + 1.0, params, delta1, delta2)\n",
    "\n",
    "    if torch.isnan(term_center).any() or torch.isnan(term_minus_1).any() or torch.isnan(term_plus_1).any():\n",
    "        print(\"Warning: NaN detected in one of the terms of cov_spacetime_second_diff\")\n",
    "        out_shape = torch.broadcast_shapes(u1_dev.shape, u2_dev.shape, t_dev.shape)\n",
    "        return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "\n",
    "    return 2.0 * term_center - term_minus_1 - term_plus_1\n",
    "\n",
    "\n",
    "# --- (MODIFIED) cn_bar for NO TAPERING ---\n",
    "def cn_bar_no_taper(u1, u2, t, params, n1, n2, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Computes c_Y2(u) * c_gn(u) where c_Y2 is cov_spacetime_second_diff\n",
    "    and c_gn(u) is the Bartlett kernel.\n",
    "    \"\"\"\n",
    "    device = params.device\n",
    "    u1_dev = u1.to(device) if isinstance(u1, torch.Tensor) else torch.tensor(u1, device=device, dtype=torch.float32)\n",
    "    u2_dev = u2.to(device) if isinstance(u2, torch.Tensor) else torch.tensor(u2, device=device, dtype=torch.float32)\n",
    "    t_dev = t.to(device) if isinstance(t, torch.Tensor) else torch.tensor(t, device=device, dtype=torch.float32)\n",
    "\n",
    "    # âœ… --- Call the NEW second-difference spatio-temporal covariance ---\n",
    "    cov_Y2_value = cov_spacetime_second_diff(u1_dev, u2_dev, t_dev, params, delta1, delta2)\n",
    "\n",
    "    c_gn_value = cgn_2dbartlett_kernel(u1_dev, u2_dev, n1, n2)\n",
    "\n",
    "    if torch.isnan(cov_Y2_value).any() or torch.isnan(c_gn_value).any():\n",
    "        print(\"Warning: NaN detected before multiplication in cn_bar_no_taper.\")\n",
    "        out_shape = torch.broadcast_shapes(cov_Y2_value.shape, c_gn_value.shape)\n",
    "        return torch.full(out_shape, float('nan'), device=device, dtype=torch.float32)\n",
    "\n",
    "    result = cov_Y2_value * c_gn_value\n",
    "    if torch.isnan(result).any():\n",
    "        print(\"Warning: NaN detected after multiplication in cn_bar_no_taper.\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# --- Expected Periodogram (uses cn_bar_no_taper) ---\n",
    "def expected_periodogram_fft_no_taper(params, n1, n2, p, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Calculates the expected periodogram. (Unchanged structure)\n",
    "    This function is correct because it calls the modified cn_bar_no_taper.\n",
    "    \"\"\"\n",
    "    device = params.device if isinstance(params, torch.Tensor) else params[0].device\n",
    "    params_tensor = params.to(device)\n",
    "\n",
    "    product_tensor = torch.zeros((n1, n2, p, p), dtype=torch.complex64, device=device)\n",
    "    t_lags = torch.arange(p, dtype=torch.float32, device=device)\n",
    "    u1_mesh_grid, u2_mesh_grid = torch.meshgrid(\n",
    "        torch.arange(n1, dtype=torch.float32, device=device),\n",
    "        torch.arange(n2, dtype=torch.float32, device=device),\n",
    "        indexing='ij'\n",
    "    )\n",
    "\n",
    "    for q in range(p):\n",
    "        for r in range(p):\n",
    "            t_diff = t_lags[q] - t_lags[r]\n",
    "            # This call now correctly leads to cov_spacetime_second_diff\n",
    "            cov_times_bartlett = cn_bar_no_taper(\n",
    "                u1_mesh_grid, u2_mesh_grid, t_diff,\n",
    "                params_tensor, n1, n2, delta1, delta2\n",
    "            )\n",
    "            if torch.isnan(cov_times_bartlett).any():\n",
    "                 product_tensor[:, :, q, r] = float('nan')\n",
    "            else:\n",
    "                 product_tensor[:, :, q, r] = cov_times_bartlett.to(torch.complex64)\n",
    "\n",
    "    if torch.isnan(product_tensor).any():\n",
    "        print(\"Warning: NaN detected in product_tensor before FFT.\")\n",
    "        nan_shape = (n1, n2, p, p)\n",
    "        return torch.full(nan_shape, float('nan'), dtype=torch.complex64, device=device)\n",
    "\n",
    "    fft_result = torch.fft.fft2(product_tensor, dim=(0, 1))\n",
    "    normalization_factor = 1.0 / (4.0 * cmath.pi**2)\n",
    "    result = fft_result * normalization_factor\n",
    "\n",
    "    if torch.isnan(result).any():\n",
    "        print(\"Warning: NaN detected in expected_periodogram_fft_no_taper output after FFT.\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 2. Data Processing (Unchanged)\n",
    "# =========================================================================\n",
    "def generate_Jvector_no_taper(tensor_list, lat_col, lon_col, val_col, device):\n",
    "    \"\"\"\n",
    "    Generates J-vector for g_s=1 (NO taper). (Unchanged)\n",
    "    \"\"\"\n",
    "    p = len(tensor_list)\n",
    "    if p == 0: return torch.empty(0, 0, 0, device=device), 0, 0, 0\n",
    "\n",
    "    valid_tensors = [t for t in tensor_list if t.numel() > 0 and t.shape[1] > max(lat_col, lon_col, val_col)]\n",
    "    if not valid_tensors:\n",
    "         return torch.empty(0, 0, 0, device=device), 0, 0, 0\n",
    "\n",
    "    try:\n",
    "        all_lats_cpu = torch.cat([t[:, lat_col] for t in valid_tensors])\n",
    "        all_lons_cpu = torch.cat([t[:, lon_col] for t in valid_tensors])\n",
    "    except IndexError:\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0\n",
    "\n",
    "    all_lats_cpu = all_lats_cpu[~torch.isnan(all_lats_cpu) & ~torch.isinf(all_lats_cpu)]\n",
    "    all_lons_cpu = all_lons_cpu[~torch.isnan(all_lons_cpu) & ~torch.isinf(all_lons_cpu)]\n",
    "    if all_lats_cpu.numel() == 0 or all_lons_cpu.numel() == 0:\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0\n",
    "\n",
    "    unique_lats_cpu, unique_lons_cpu = torch.unique(all_lats_cpu), torch.unique(all_lons_cpu)\n",
    "    n1, n2 = len(unique_lats_cpu), len(unique_lons_cpu)\n",
    "    if n1 == 0 or n2 == 0:\n",
    "        return torch.empty(0, 0, 0, device=device), 0, 0, 0\n",
    "\n",
    "    lat_map = {lat.item(): i for i, lat in enumerate(unique_lats_cpu)}\n",
    "    lon_map = {lon.item(): i for i, lon in enumerate(unique_lons_cpu)}\n",
    "\n",
    "    fft_results = []\n",
    "    for tensor in tensor_list:\n",
    "        data_grid = torch.zeros((n1, n2), dtype=torch.float32, device=device)\n",
    "        tensor_dev = tensor.to(device)\n",
    "        for row in tensor_dev:\n",
    "            lat_item, lon_item = row[lat_col].item(), row[lon_col].item()\n",
    "            if not (np.isnan(lat_item) or np.isnan(lon_item)):\n",
    "                i = lat_map.get(lat_item)\n",
    "                j = lon_map.get(lon_item)\n",
    "                if i is not None and j is not None:\n",
    "                    val = row[val_col]\n",
    "                    val_num = val.item() if isinstance(val, torch.Tensor) else val\n",
    "                    if not np.isnan(val_num) and not np.isinf(val_num):\n",
    "                        data_grid[i, j] = val_num\n",
    "\n",
    "        data_grid = torch.nan_to_num(data_grid, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        fft_results.append(torch.fft.fft2(data_grid))\n",
    "\n",
    "    if not fft_results:\n",
    "         return torch.empty(0, 0, 0, device=device), n1, n2, 0\n",
    "\n",
    "    J_vector_tensor = torch.stack(fft_results, dim=2).to(device)\n",
    "\n",
    "    H = float(n1 * n2)\n",
    "    if H < 1e-9:\n",
    "        norm_factor = torch.tensor(0.0, device=device)\n",
    "    else:\n",
    "        norm_factor = (torch.sqrt(torch.tensor(1.0 / H, device=device)) / (2.0 * cmath.pi))\n",
    "\n",
    "    result = J_vector_tensor * norm_factor\n",
    "    return result, n1, n2, p\n",
    "\n",
    "\n",
    "def calculate_sample_periodogram_vectorized(J_vector_tensor):\n",
    "    \"\"\"Calculates sample periodogram I_n = J J^H. (Unchanged)\"\"\"\n",
    "    if torch.isnan(J_vector_tensor).any() or torch.isinf(J_vector_tensor).any():\n",
    "        n1, n2, p = J_vector_tensor.shape\n",
    "        return torch.full((n1, n2, p, p), float('nan'), dtype=torch.complex64, device=J_vector_tensor.device)\n",
    "\n",
    "    J_col = J_vector_tensor.unsqueeze(-1)\n",
    "    J_row_conj = J_vector_tensor.unsqueeze(-2).conj()\n",
    "    result = J_col @ J_row_conj\n",
    "    return result\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 4. Likelihood Calculation (Unchanged)\n",
    "# =========================================================================\n",
    "\n",
    "def whittle_likelihood_loss_no_taper(params, I_sample, n1, n2, p, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Whittle Likelihood Loss. (Unchanged structure)\n",
    "    \"\"\"\n",
    "    device = I_sample.device\n",
    "    params_tensor = params.to(device)\n",
    "\n",
    "    if torch.isnan(params_tensor).any() or torch.isinf(params_tensor).any():\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    # This call now correctly leads to cov_spacetime_second_diff\n",
    "    I_expected = expected_periodogram_fft_no_taper(\n",
    "        params_tensor, n1, n2, p, delta1, delta2\n",
    "    )\n",
    "\n",
    "    if torch.isnan(I_expected).any() or torch.isinf(I_expected).any():\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    eye_matrix = torch.eye(p, dtype=torch.complex64, device=device)\n",
    "    diag_vals = torch.abs(I_expected.diagonal(dim1=-2, dim2=-1))\n",
    "    mean_diag_abs = diag_vals.mean().item() if diag_vals.numel() > 0 and not torch.isnan(diag_vals).all() else 1.0\n",
    "    diag_load = max(mean_diag_abs * 1e-8, 1e-9) \n",
    "    \n",
    "    I_expected_stable = I_expected + eye_matrix * diag_load\n",
    "\n",
    "    sign, logabsdet = torch.linalg.slogdet(I_expected_stable)\n",
    "    if torch.any(sign.real <= 1e-9):\n",
    "        log_det_term = torch.where(sign.real > 1e-9, logabsdet, torch.tensor(1e10, device=device))\n",
    "    else:\n",
    "        log_det_term = logabsdet\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    try:\n",
    "        solved_term = torch.linalg.solve(I_expected_stable, I_sample)\n",
    "        trace_term = torch.einsum('...ii->...', solved_term).real\n",
    "    except torch.linalg.LinAlgError as e:\n",
    "        return torch.tensor(float('inf'), device=device)\n",
    "\n",
    "    if torch.isnan(trace_term).any() or torch.isinf(trace_term).any():\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    likelihood_terms = log_det_term + trace_term\n",
    "\n",
    "    if torch.isnan(likelihood_terms).any():\n",
    "        return torch.tensor(float('nan'), device=device)\n",
    "\n",
    "    total_sum = torch.sum(likelihood_terms)\n",
    "    dc_term = likelihood_terms[0, 0] if n1 > 0 and n2 > 0 else torch.tensor(0.0, device=device)\n",
    "    if torch.isnan(dc_term).any() or torch.isinf(dc_term).any():\n",
    "        dc_term = torch.tensor(0.0, device=device)\n",
    "\n",
    "    loss = total_sum - dc_term if (n1 > 1 or n2 > 1) else total_sum\n",
    "\n",
    "    if torch.isnan(loss) or torch.isinf(loss):\n",
    "         return torch.tensor(float('inf'), device=device) \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 5. Training Loop (Unchanged)\n",
    "# =========================================================================\n",
    "def run_full(params_list, optimizer, scheduler, I_sample, n1, n2, p, epochs=600, device='cpu'):\n",
    "    \"\"\"Corrected training loop. (Unchanged)\"\"\"\n",
    "    best_loss = float('inf')\n",
    "    params_list = [p.to(device) for p in params_list]\n",
    "    best_params_state = [p.detach().clone() for p in params_list]\n",
    "    epochs_completed = 0\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063 # Still needed\n",
    "\n",
    "    def get_printable_params(p_list):\n",
    "        valid_tensors = [p for p in p_list if isinstance(p, torch.Tensor)]\n",
    "        if not valid_tensors: return \"Invalid params_list\"\n",
    "        p_cat = torch.cat([p.detach().clone().cpu() for p in valid_tensors])\n",
    "        log_indices = [0, 1, 2, 6]\n",
    "        if all(idx < len(p_cat) for idx in log_indices):\n",
    "            log_vals = p_cat[log_indices]\n",
    "            if not (torch.isnan(log_vals).any() or torch.isinf(log_vals).any()):\n",
    "                 p_cat[log_indices] = torch.exp(log_vals)\n",
    "            else:\n",
    "                 p_cat[log_indices] = float('nan')\n",
    "        return p_cat.numpy().round(4)\n",
    "\n",
    "    I_sample_dev = I_sample.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epochs_completed = epoch + 1\n",
    "        optimizer.zero_grad()\n",
    "        params_tensor = torch.cat(params_list)\n",
    "\n",
    "        loss = whittle_likelihood_loss_no_taper(\n",
    "            params_tensor, I_sample_dev, n1, n2, p, DELTA_LAT, DELTA_LON\n",
    "        )\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"Loss became NaN or Inf at epoch {epoch+1}. Stopping.\")\n",
    "            if epoch == 0: best_params_state = None\n",
    "            epochs_completed = epoch\n",
    "            break \n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        nan_grad = False\n",
    "        for param in params_list:\n",
    "            if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
    "                nan_grad = True\n",
    "                break\n",
    "        if nan_grad:\n",
    "             optimizer.zero_grad() \n",
    "             continue \n",
    "\n",
    "        all_params_on_device = params_list\n",
    "        if all_params_on_device:\n",
    "            torch.nn.utils.clip_grad_norm_(all_params_on_device, max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step() \n",
    "\n",
    "        current_loss_item = loss.item()\n",
    "        if current_loss_item < best_loss:\n",
    "            params_valid = not any(torch.isnan(p.data).any() or torch.isinf(p.data).any() for p in params_list)\n",
    "            if params_valid:\n",
    "                best_loss = current_loss_item\n",
    "                best_params_state = [p.detach().clone() for p in params_list]\n",
    "\n",
    "        if epoch % 50 == 0 or epoch == epochs - 1:\n",
    "            current_lr = optimizer.param_groups[0]['lr'] if optimizer.param_groups else 0.0\n",
    "            print(f'--- Epoch {epoch+1}/{epochs} (LR: {current_lr:.6f}) ---')\n",
    "            print(f' Loss: {current_loss_item:.4f}')\n",
    "            print(f' Parameters (Natural Scale): {get_printable_params(params_list)}')\n",
    "\n",
    "    if best_params_state is None:\n",
    "        return None, epochs_completed\n",
    "\n",
    "    final_params_log_scale = torch.cat([p.cpu() for p in best_params_state])\n",
    "    final_params_natural_scale = final_params_log_scale.detach().clone()\n",
    "    log_indices = [0, 1, 2, 6]\n",
    "    if all(idx < len(final_params_natural_scale) for idx in log_indices):\n",
    "        log_vals = final_params_natural_scale[log_indices]\n",
    "        if not (torch.isnan(log_vals).any() or torch.isinf(log_vals).any()):\n",
    "            final_params_natural_scale[log_indices] = torch.exp(log_vals)\n",
    "        else:\n",
    "            final_params_natural_scale[log_indices] = float('nan')\n",
    "\n",
    "    final_params_rounded = [round(p.item(), 4) if not np.isnan(p.item()) else float('nan') for p in final_params_natural_scale]\n",
    "    final_loss_rounded = round(best_loss, 3) if best_loss != float('inf') else float('inf')\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    print(f'\\nFINAL BEST STATE ACHIEVED (during training):')\n",
    "    print(f'Best Loss: {final_loss_rounded}')\n",
    "    print(f'Parameters Corresponding to Best Loss (Natural Scale): {final_params_rounded}')\n",
    "\n",
    "    return final_params_rounded + [final_loss_rounded], epochs_completed\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 6. Main Execution Script (MODIFIED for Spat + 2x Temp Data)\n",
    "# =========================================================================\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Configuration ---\n",
    "    DAY_TO_RUN = 1\n",
    "    NUM_RUNS = 1 \n",
    "    EPOCHS = 700 \n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    # --- Grid Spacing ---\n",
    "    DELTA_LAT, DELTA_LON = 0.044, 0.063 \n",
    "\n",
    "    # --- Column Indices ---\n",
    "    LAT_COL, LON_COL = 0, 1\n",
    "    VAL_COL = 2\n",
    "    TIME_COL = 3\n",
    "\n",
    "    # --- (MODIFIED) Load Spat + 2x Temp Differenced Data ---\n",
    "    try:\n",
    "        # âœ… --- LOAD THE NEW FILE ---\n",
    "        with open(\"spacetime_second_diff_data.pkl\", 'rb') as f:\n",
    "            processed_df = pickle.load(f)\n",
    "        print(f\"Loaded {len(processed_df)} days from spacetime_second_diff_data.pkl.\")\n",
    "        \n",
    "        processed_df = [\n",
    "            torch.tensor(arr, dtype=torch.float32).cpu() if not isinstance(arr, torch.Tensor)\n",
    "            else arr.cpu().to(torch.float32)\n",
    "            for arr in processed_df\n",
    "        ]\n",
    "        if not processed_df: raise ValueError(\"'processed_df' is empty.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: `spacetime_second_diff_data.pkl` not found.\")\n",
    "        print(\"Please run the data preparation script first.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing 'processed_df': {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Day selection needs care: 2 temporal diffs mean we lose 2 days\n",
    "    # If DAY_TO_RUN refers to the *original* day index, adjust access\n",
    "    # processed_df[0] corresponds to original day 3 (index 2)\n",
    "    adjusted_day_index = DAY_TO_RUN - 3 # Example: if DAY_TO_RUN=3, access index 0\n",
    "\n",
    "    # Ensure adjusted index is valid\n",
    "    if adjusted_day_index < 0 or adjusted_day_index >= len(processed_df):\n",
    "        print(f\"Error: DAY_TO_RUN ({DAY_TO_RUN}) is invalid for the doubly differenced data (valid range approx 3 to {len(processed_df)+2}).\")\n",
    "        exit()\n",
    "\n",
    "    cur_df = processed_df[adjusted_day_index] # Use adjusted index\n",
    "    \n",
    "    if cur_df.numel() == 0 or cur_df.shape[1] <= max(LAT_COL, LON_COL, VAL_COL, TIME_COL):\n",
    "        print(f\"Error: Data for Day {DAY_TO_RUN} (adjusted index {adjusted_day_index}) is empty or invalid.\")\n",
    "        exit()\n",
    "\n",
    "    unique_times = torch.unique(cur_df[:, TIME_COL])\n",
    "    time_slices_list = [cur_df[cur_df[:, TIME_COL] == t_val] for t_val in unique_times]\n",
    "\n",
    "    # --- 1. Pre-compute Sample Periodogram (NO Tapering) ---\n",
    "    print(\"Pre-computing sample periodogram (NO data taper)...\")\n",
    "    J_vec, n1, n2, p = generate_Jvector_no_taper(\n",
    "        time_slices_list,\n",
    "        lat_col=LAT_COL, lon_col=LON_COL, val_col=VAL_COL,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if J_vec.numel() == 0 or n1 == 0 or n2 == 0 or p == 0:\n",
    "       print(f\"Error: J-vector generation failed for Day {DAY_TO_RUN} (adjusted index {adjusted_day_index}).\")\n",
    "       exit()\n",
    "\n",
    "    I_sample = calculate_sample_periodogram_vectorized(J_vec)\n",
    "\n",
    "    if torch.isnan(I_sample).any() or torch.isinf(I_sample).any():\n",
    "        print(\"Error: NaN or Inf detected in the sample periodogram. Cannot proceed.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Data grid: {n1}x{n2} spatial points, {p} time points. Sample Periodogram on {DEVICE}.\")\n",
    "    \n",
    "    # --- 2. Optimization Loop ---\n",
    "    all_final_results = []\n",
    "    all_final_losses = []\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        print(f\"\\n{'='*30} Initialization Run {i+1}/{NUM_RUNS} {'='*30}\")\n",
    "\n",
    "        # âœ… --- Use Fixed Initial Parameters ---\n",
    "        initial_params_values = [\n",
    "            np.log(21.303), np.log(1.307), np.log(1.563), \n",
    "            0.022, -0.144, 0.198, \n",
    "            np.log(4.769)  \n",
    "        ]\n",
    "        \n",
    "        print(f\"Starting with fixed params (log-scale for [0,1,2,6]): {[round(p, 4) for p in initial_params_values]}\")\n",
    "\n",
    "        params_list = [\n",
    "            Parameter(torch.tensor([val], dtype=torch.float32))\n",
    "            for val in initial_params_values\n",
    "        ]\n",
    "\n",
    "        lr_slow, lr_fast = 0.005, 0.02\n",
    "        slow_indices = [0, 1, 2, 6]\n",
    "        fast_indices = [3, 4, 5]\n",
    "\n",
    "        valid_slow_indices = [idx for idx in slow_indices if idx < len(params_list)]\n",
    "        valid_fast_indices = [idx for idx in fast_indices if idx < len(params_list)]\n",
    "\n",
    "        param_groups = [\n",
    "            {'params': [params_list[idx] for idx in valid_slow_indices], 'lr': lr_slow, 'name': 'slow_group'},\n",
    "            {'params': [params_list[idx] for idx in valid_fast_indices], 'lr': lr_fast, 'name': 'fast_group'}\n",
    "        ]\n",
    "\n",
    "        optimizer = torch.optim.Adam(param_groups)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=200, eta_min=1e-6)\n",
    "\n",
    "        print(f\"Starting optimization run {i+1} on device {DEVICE} (NO data taper, Spat + 2x Temp Diff)...\")\n",
    "        final_results, epochs_run = run_full(\n",
    "            params_list=params_list,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            I_sample=I_sample,\n",
    "            n1=n1, n2=n2, p=p,\n",
    "            epochs=EPOCHS,\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "        if final_results:\n",
    "            all_final_results.append(final_results)\n",
    "            all_final_losses.append(final_results[-1])\n",
    "        else:\n",
    "            all_final_results.append(None)\n",
    "            all_final_losses.append(float('inf'))\n",
    "\n",
    "    print(f\"\\n\\n{'='*25} Overall Result from Run {'='*25}\")\n",
    "    valid_losses = [l for l in all_final_losses if l is not None and l != float('inf')]\n",
    "\n",
    "    if not valid_losses:\n",
    "        print(f\"The run failed or resulted in an invalid loss for Day {DAY_TO_RUN} (adjusted index {adjusted_day_index}).\")\n",
    "    else:\n",
    "        best_loss = valid_losses[0]\n",
    "        best_run_index = 0\n",
    "        best_results = all_final_results[best_run_index]\n",
    "        print(f\"Run Loss: {best_results[-1]}\")\n",
    "        print(f\"Final Parameters (Natural Scale): {best_results[:-1]}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
