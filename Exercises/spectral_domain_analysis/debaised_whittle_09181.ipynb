{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5149bad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy                    # clone tensor\n",
    "import time\n",
    "\n",
    "# Custom imports\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels\n",
    "\n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "import typer\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO import configuration as config\n",
    "from GEMS_TCO import data_preprocess as dmbh\n",
    "\n",
    "import os\n",
    "from sklearn.neighbors import BallTree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2322c7",
   "metadata": {},
   "source": [
    "## Load data (but this version has latitude calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d130de8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 31]\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "space = ['1,1']  # lat, lon resolution\n",
    "days = ['0,31']\n",
    "mm_cond_number = 20\n",
    "lat_lon_resolution = [int(s) for s in space[0].split(',')]\n",
    "days_s_e = list(map(int, days[0].split(',')))\n",
    "print(days_s_e)\n",
    "\n",
    "days_list = list(range(days_s_e[0], days_s_e[1]))\n",
    "years = ['2024']\n",
    "month_range =[7,8]\n",
    "output_path = input_path = Path(config.mac_estimates_day_path)\n",
    "\n",
    "## load ozone data from amarel\n",
    "data_load_instance = load_data(config.mac_data_load_path)\n",
    "df_map, ord_mm, nns_map= data_load_instance.load_mm20k_data_bymonthyear(lat_lon_resolution = lat_lon_resolution, mm_cond_number=mm_cond_number,years_=years, months_=month_range)  \n",
    "\n",
    "# only fit spline once because space are all same\n",
    "# load first data of analysis_data_map and aggregated_data to initialize spline_instance\n",
    "first_day_idx_for_datamap= [0,8]\n",
    "first_day_analysis_data_map, first_day_aggregated_data = data_load_instance.load_working_data_byday(df_map, ord_mm, nns_map, idx_for_datamap= first_day_idx_for_datamap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a3a359",
   "metadata": {},
   "source": [
    "## Load data and undo latitude calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d01e849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joonwonlee/Documents/GEMS_DATA/pickle_2024/orbit_map24_07.pkl\n"
     ]
    }
   ],
   "source": [
    "#keys = list(df_map.keys())\n",
    "#df = df_map[keys[0]]\n",
    "\n",
    "lon_s = 123\n",
    "lon_e = 133\n",
    "step_lat = 0.044\n",
    "step_lon = 0.063\n",
    "\n",
    "lat_coords = np.arange( 5 -0.044- 0.0002, 0 -0.044, -0.044)\n",
    "lon_coords = np.arange( lon_e-step_lon- 0.0002, lon_s-step_lon, -step_lon)\n",
    "\n",
    "# Apply the shift as in the original code\n",
    "# These are the unique lat/lon values for the \"center_points\" grid\n",
    "final_lat_values = lat_coords + step_lat \n",
    "final_lon_values = lon_coords + step_lon \n",
    "\n",
    "# Create 2D grid with broadcasting\n",
    "#decrement = 0.00012\n",
    "decrement = 0 \n",
    "lat_grid = final_lat_values[:, None] + np.arange(len(final_lon_values)) * decrement  # shape: (228, 152)\n",
    "\n",
    "\n",
    "mac_data_path = config.mac_data_load_path\n",
    "years = [2024]  # years = [2023,2024]\n",
    "months = list( range(7,8))\n",
    "year = years[0]\n",
    "month = months[0]\n",
    "month_str = f\"{month:02d}\"  \n",
    "filename = f\"pickle_2024/orbit_map{str(year)[2:]}_{month_str}.pkl\"\n",
    "picklefile_path = Path(mac_data_path) / filename\n",
    "print(picklefile_path)\n",
    "\n",
    "with open(picklefile_path, 'rb') as pickle_file:\n",
    "    data_map_hour = pickle.load(pickle_file)\n",
    "\n",
    "# Base file path and settings\n",
    "# base_path = \"C:\\\\Users\\\\joonw\\\\TCO\\\\GEMS_data\"    MSI notebook\n",
    "\n",
    "mac_data_path = config.mac_data_load_path\n",
    "lat_start, lat_end, lon_start, lon_end = 0, 5, 123, 133\n",
    "step_lat, step_lon = 0.044, 0.063\n",
    "\n",
    "# df = pd.read_csv(\"C:\\\\Users\\\\joonw\\\\TCO\\\\GEMS_data\\\\data_2024\\\\data_24_07_0131_N510_E110120.csv\")  MSI notebook\n",
    "df = pd.read_csv(\"/Users/joonwonlee/Documents/GEMS_DATA/data_2024/data_24_07_0131_N05_E123133.csv\")  # MAC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d6513bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class center_matching_hour():\n",
    "    \"\"\"\n",
    "    Processes orbit data by averaging over specified spatial regions and resolutions.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame containing the data.\n",
    "        lat_s (int): Start latitude for spatial averaging.\n",
    "        lat_e (int): End latitude for spatial averaging.\n",
    "        lon_s (int): Start longitude for spatial averaging.\n",
    "        lon_e (int): End longitude for spatial averaging.\n",
    "        lat_resolution (Optional[float]): Latitude resolution for spatial bins. Default is None.\n",
    "        lon_resolution (Optional[float]): Longitude resolution for spatial bins. Default is None.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        df:pd.DataFrame=None, \n",
    "        lat_s:float =0,\n",
    "        lat_e:float =5, \n",
    "        lon_s:float =123,\n",
    "        lon_e:float =133, \n",
    "        lat_resolution:float=None, \n",
    "        lon_resolution:float =None\n",
    "    ):\n",
    "        # Input validation\n",
    "        if df is not None:\n",
    "            assert isinstance(df, pd.DataFrame), \"df must be a pandas DataFrame\"\n",
    "\n",
    "        if lat_resolution is not None:\n",
    "            assert isinstance(lat_resolution, float), \"lat_resolution must be a float\"\n",
    "        if lon_resolution is not None:\n",
    "            assert isinstance(lon_resolution, float), \"lon_resolution must be a float\"\n",
    "        \n",
    "        self.df = df\n",
    "        self.lat_resolution = lat_resolution\n",
    "        self.lon_resolution = lon_resolution\n",
    "        self.lat_s = lat_s\n",
    "        self.lat_e = lat_e\n",
    "        self.lon_s = lon_s\n",
    "        self.lon_e = lon_e\n",
    "\n",
    "    def group_data_by_orbits(self):\n",
    "        \"\"\"\n",
    "        Groups data into a dictionary based on unique orbit timestamps.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary where keys represent formatted orbit identifiers \n",
    "                and values are DataFrames corresponding to each orbit.\n",
    "        \"\"\"\n",
    "        orbit_map = {}  \n",
    "        self.df['Orbit'] = self.df['Time'].str[0:16]\n",
    "        orbits = self.df['Orbit'].unique()\n",
    "        for orbit in orbits:\n",
    "            orbit_key = f'y{orbit[2:4]}m{int(orbit[5:7]):02d}day{ int(orbit[8:10]):02d}_hm{(orbit[11:16])}'\n",
    "            orbit_map[orbit_key] = self.df.loc[self.df['Orbit'] == orbit].reset_index(drop=True)\n",
    "        return orbit_map\n",
    "    \n",
    "    def make_center_points(self, step_lat:float=0.042, step_lon:float=0.062) -> pd.DataFrame:\n",
    "        lat_coords = np.arange( self.lat_e-step_lat- 0.0002, self.lat_s -step_lat, -step_lat)\n",
    "        lon_coords = np.arange( self.lon_e-step_lon- 0.0002, self.lon_s-step_lon, -step_lon)\n",
    "\n",
    "        # Apply the shift as in the original code\n",
    "        # These are the unique lat/lon values for the \"center_points\" grid\n",
    "        final_lat_values = lat_coords + step_lat \n",
    "        final_lon_values = lon_coords + step_lon \n",
    "        \n",
    "        # Create 2D grid with broadcasting\n",
    "        #decrement = 0.00012\n",
    "        decrement = 0\n",
    "        lat_grid = final_lat_values[:, None] + np.arange(len(final_lon_values)) * decrement  # shape: (228, 152)\n",
    "\n",
    "        # Flatten row-wise (C order)\n",
    "        center_lats = lat_grid.flatten()\n",
    "\n",
    "        # Create matching longitude grid\n",
    "        center_lons = np.tile(final_lon_values, len(final_lat_values))\n",
    "\n",
    "        # Now you can build your DataFrame\n",
    "        center_points_df = pd.DataFrame({'lat': center_lats, 'lon': center_lons})\n",
    "        return center_points_df\n",
    "    \n",
    "\n",
    "    '''  \n",
    "    coarse_by_center   allows duplicates while coarse_by_center_unique doesnt.\n",
    "    '''\n",
    "\n",
    "    def coarse_by_center(self, orbit_map: dict, center_points: pd.DataFrame) -> dict:\n",
    "        assert isinstance(orbit_map, dict), \"orbit_map must be a dict\"\n",
    "        assert isinstance(center_points, pd.DataFrame), \"center_points must be a pd.DataFrame\"\n",
    "\n",
    "        coarse_map = {}\n",
    "        key_list = sorted(orbit_map)\n",
    "\n",
    "        # Convert query points (lat, lon) to NumPy array\n",
    "        query_points = center_points[['lat', 'lon']].to_numpy()\n",
    "        query_points_rad = np.radians(query_points)  # if using haversine\n",
    "\n",
    "        num_center_points = len(center_points)\n",
    "\n",
    "        for key in key_list:\n",
    "            cur_data = orbit_map[key].reset_index(drop=True)\n",
    "            locs = cur_data[['Latitude', 'Longitude']].to_numpy()\n",
    "\n",
    "            if locs.shape[0] == 0:\n",
    "                coarse_map[key] = pd.DataFrame({\n",
    "                    'Latitude': center_points['lat'],\n",
    "                    'Longitude': center_points['lon'],\n",
    "                    'ColumnAmountO3': [np.nan] * num_center_points,\n",
    "                    'Hours_elapsed': [np.nan] * num_center_points,\n",
    "                    'Time': [pd.NaT] * num_center_points,\n",
    "                    'Source_Latitude': [np.nan] * num_center_points,\n",
    "                    'Source_Longitude': [np.nan] * num_center_points\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            # Use haversine\n",
    "            locs_rad = np.radians(locs)\n",
    "            tree = BallTree(locs_rad, metric='haversine')\n",
    "            dist, ind = tree.query(query_points_rad, k=1)\n",
    "\n",
    "            nearest_indices = ind.flatten()\n",
    "\n",
    "            # Extract values from the nearest source points\n",
    "            res_o3_values = cur_data.loc[nearest_indices, 'ColumnAmountO3'].values\n",
    "            source_lat = cur_data.loc[nearest_indices, 'Latitude'].values\n",
    "            source_lon = cur_data.loc[nearest_indices, 'Longitude'].values\n",
    "\n",
    "            hours_elapsed_val = cur_data['Hours_elapsed'].iloc[0] if not cur_data.empty else np.nan\n",
    "            time_val = cur_data['Time'].iloc[0] if not cur_data.empty else pd.NaT\n",
    "\n",
    "            coarse_map[key] = pd.DataFrame({\n",
    "                'Latitude': center_points['lat'].values,\n",
    "                'Longitude': center_points['lon'].values,\n",
    "                'ColumnAmountO3': res_o3_values,\n",
    "                'Hours_elapsed': [hours_elapsed_val] * num_center_points,\n",
    "                'Time': [time_val] * num_center_points,\n",
    "                'Source_Latitude': source_lat,\n",
    "                'Source_Longitude': source_lon\n",
    "            })\n",
    "        return coarse_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "858a01c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved data for year 24 month 07.\n"
     ]
    }
   ],
   "source": [
    "instance = center_matching_hour(df, lat_start, lat_end, lon_start, lon_end)  \n",
    "\n",
    "for year in years:        # years = [2023,2024]\n",
    "    for month in months:  \n",
    "        try:\n",
    "            # load pickle (dense ORI data)\n",
    "            pickle_path = os.path.join(mac_data_path, f'pickle_{year}')\n",
    "            input_filename = f\"orbit_map{str(year)[2:]}_{month_str}.pkl\"\n",
    "            input_filepath = os.path.join(pickle_path, input_filename)\n",
    "            with open(input_filepath, 'rb') as pickle_file:\n",
    "                loaded_map = pickle.load(pickle_file)\n",
    "            center_points = instance.make_center_points(step_lat = step_lat, step_lon= step_lon)\n",
    "            coarse_cen_map = instance.coarse_by_center(loaded_map, center_points)\n",
    "\n",
    "            # Save pickle (coarse data)\n",
    "            output_filename = f\"coarse_cen_map_without_decrement_latitude{str(year)[2:]}_{month_str}.pkl\"\n",
    "            output_filepath = os.path.join(pickle_path, output_filename)\n",
    "            with open(output_filepath, 'wb') as pickle_file:\n",
    "                pickle.dump(coarse_cen_map, pickle_file)\n",
    "            \n",
    "            print(f\"Successfully processed and saved data for year {str(year)[2:]} month {month_str}.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File {input_filename} not found. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {input_filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "213294a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = os.path.join(mac_data_path, f'pickle_{year}')\n",
    "output_filename = f\"coarse_cen_map_without_decrement_latitude{str(year)[2:]}_{month_str}.pkl\"\n",
    "output_filepath = os.path.join(pickle_path, output_filename)\n",
    "\n",
    "with open(output_filepath, 'rb') as pickle_file:\n",
    "    cbmap_ori = pickle.load(pickle_file)\n",
    "\n",
    "day1_df_ori = pd.concat(cbmap_ori.values(), axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d521132e",
   "metadata": {},
   "source": [
    "## Compute C_{g,n}(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73a1a5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing key: y24m07day01_hm00:52\n",
      "  Processing key: y24m07day01_hm01:52\n",
      "  Processing key: y24m07day01_hm02:52\n",
      "  Processing key: y24m07day01_hm03:52\n",
      "  Processing key: y24m07day01_hm04:48\n",
      "  Processing key: y24m07day01_hm05:48\n",
      "  Processing key: y24m07day01_hm06:48\n",
      "  Processing key: y24m07day01_hm07:48\n",
      "\n",
      "--- Processing Complete ---\n",
      "Final DataFrame shape: (65536, 5)\n",
      "Columns: Index(['Latitude', 'Longitude', 'laplacian', 'Time', 'Hours_elapsed'], dtype='object')\n",
      "\n",
      "Head of the final DataFrame:\n",
      "   Latitude  Longitude  laplacian                 Time  Hours_elapsed\n",
      "0    2.8438   131.6138   -6.56543  2024-07-01 00:52:00  477720.866667\n",
      "1    2.8438   131.5508   -3.03646  2024-07-01 00:52:00  477720.866667\n",
      "2    2.8438   131.4878   20.61612  2024-07-01 00:52:00  477720.866667\n",
      "3    2.8438   131.4248   -4.31279  2024-07-01 00:52:00  477720.866667\n",
      "4    2.8438   131.3618   -1.02134  2024-07-01 00:52:00  477720.866667\n",
      "\n",
      "Tail of the final DataFrame:\n",
      "       Latitude  Longitude  laplacian                 Time  Hours_elapsed\n",
      "65531    0.0718   123.8648    2.81470  2024-07-01 07:48:00       477727.8\n",
      "65532    0.0718   123.8018    1.03326  2024-07-01 07:48:00       477727.8\n",
      "65533    0.0718   123.7388   -3.56801  2024-07-01 07:48:00       477727.8\n",
      "65534    0.0718   123.6758   -3.82671  2024-07-01 07:48:00       477727.8\n",
      "65535    0.0718   123.6128    9.82056  2024-07-01 07:48:00       477727.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "def apply_laplacian_2d_valid(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies a 2D discrete Laplacian filter using 'mode=valid', returning a new,\n",
    "    smaller DataFrame with the cropped results and corresponding coordinates.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): A DataFrame for a single time slice, containing\n",
    "                           'ColumnAmountO3', 'Latitude', and 'Longitude' columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new, smaller DataFrame containing the 'laplacian' values\n",
    "                      and the corresponding 'Latitude', 'Longitude', and 'Time'.\n",
    "    \"\"\"\n",
    "    # Dynamically determine the grid shape and unique coordinates\n",
    "    unique_lats = df['Latitude'].unique()\n",
    "    unique_lons = df['Longitude'].unique()\n",
    "    lat_count = len(unique_lats)\n",
    "    lon_count = len(unique_lons)\n",
    "    \n",
    "    if df.shape[0] != lat_count * lon_count:\n",
    "        raise ValueError(\"DataFrame size does not match the grid dimensions (lat * lon).\")\n",
    "\n",
    "    # Reshape the data based on its flattening order (lat slowest, lon fastest)\n",
    "    grid = df['ColumnAmountO3'].values.reshape((lat_count, lon_count))\n",
    "\n",
    "    # Define the standard 3x3 discrete Laplacian kernel\n",
    "    laplacian_kernel = np.array([[0, 1, 0],\n",
    "                                 [1, -4, 1],\n",
    "                                 [0, 1, 0]])\n",
    "\n",
    "    # Apply convolution with 'valid' mode, which crops the boundaries.\n",
    "    laplacian_grid_cropped = convolve2d(grid, laplacian_kernel, mode='valid')\n",
    "    \n",
    "    # Determine the new, cropped coordinates. A 3x3 kernel removes one\n",
    "    # element from each of the four sides.\n",
    "    cropped_lats = unique_lats[1:-1]\n",
    "    cropped_lons = unique_lons[1:-1]\n",
    "\n",
    "    # If cropping results in an empty grid, return an empty DataFrame\n",
    "    if len(cropped_lats) == 0 or len(cropped_lons) == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Create a new meshgrid for the cropped coordinates\n",
    "    new_lon_grid, new_lat_grid = np.meshgrid(cropped_lons, cropped_lats)\n",
    "    \n",
    "    # Create the new DataFrame with the filtered data and new coordinates\n",
    "    new_df = pd.DataFrame({\n",
    "        'Latitude': new_lat_grid.flatten(),\n",
    "        'Longitude': new_lon_grid.flatten(),\n",
    "        'laplacian': laplacian_grid_cropped.flatten()\n",
    "    })\n",
    "    \n",
    "    # Preserve the timestamp from the original slice if it exists\n",
    "    if 'Time' in df.columns:\n",
    "        new_df['Time'] = df['Time'].iloc[0]\n",
    "        \n",
    "    new_df['Hours_elapsed'] = df['Hours_elapsed'].iloc[0]\n",
    "\n",
    "    return new_df\n",
    "\n",
    "def subset(df):\n",
    "    \"\"\"Subsets the DataFrame to a specific lat/lon range, as provided.\"\"\"\n",
    "    tmp = df['Longitude'].unique().copy()\n",
    "    tmp2 = tmp[(tmp >= 123.5) & (tmp <= 132)]\n",
    "    lon_cut = tmp2[5:]\n",
    "    t = df['Latitude'].unique().copy()\n",
    "    t2 = t[(t >= 0) & (t <= 3)]\n",
    "    lat_cut = t2[2:]\n",
    "    # Ensure that lat_cut and lon_cut are not empty to avoid errors\n",
    "    if len(lon_cut) == 0 or len(lat_cut) == 0:\n",
    "        return pd.DataFrame(columns=df.columns) # Return empty DF if no data\n",
    "    df_sub = df[(df['Longitude'].isin(lon_cut)) & (df['Latitude'].isin(lat_cut))].reset_index(drop=True)\n",
    "    return df_sub\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    a = list(cbmap_ori.keys())\n",
    "    \n",
    "    processed_dfs = []\n",
    "    \n",
    "    for k in a[:8]:\n",
    "        print(f\"  Processing key: {k}\")\n",
    "        \n",
    "        cur = subset(cbmap_ori[k])\n",
    "        \n",
    "        if not cur.empty:\n",
    "            # The function now returns a new, smaller DataFrame with the filtered data\n",
    "            filtered_df = apply_laplacian_2d_valid(cur)\n",
    "            if not filtered_df.empty:\n",
    "                processed_dfs.append(filtered_df)\n",
    "\n",
    "    # 3. Concatenate all processed (and now smaller) DataFrames\n",
    "    if processed_dfs:\n",
    "        final_df = pd.concat(processed_dfs, axis=0, ignore_index=True)\n",
    "        \n",
    "        print(\"\\n--- Processing Complete ---\")\n",
    "        print(\"Final DataFrame shape:\", final_df.shape)\n",
    "        print(\"Columns:\", final_df.columns)\n",
    "        print(\"\\nHead of the final DataFrame:\")\n",
    "        print(final_df.head())\n",
    "        print(\"\\nTail of the final DataFrame:\")\n",
    "        print(final_df.tail())\n",
    "    else:\n",
    "        print(\"\\nProcessing resulted in an empty DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3707a6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>laplacian</th>\n",
       "      <th>Time</th>\n",
       "      <th>Hours_elapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.8438</td>\n",
       "      <td>131.6138</td>\n",
       "      <td>2.77747</td>\n",
       "      <td>2024-07-01 07:48:00</td>\n",
       "      <td>477727.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.8438</td>\n",
       "      <td>131.5508</td>\n",
       "      <td>-15.47635</td>\n",
       "      <td>2024-07-01 07:48:00</td>\n",
       "      <td>477727.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.8438</td>\n",
       "      <td>131.4878</td>\n",
       "      <td>-1.04339</td>\n",
       "      <td>2024-07-01 07:48:00</td>\n",
       "      <td>477727.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.8438</td>\n",
       "      <td>131.4248</td>\n",
       "      <td>0.72936</td>\n",
       "      <td>2024-07-01 07:48:00</td>\n",
       "      <td>477727.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.8438</td>\n",
       "      <td>131.3618</td>\n",
       "      <td>-5.38342</td>\n",
       "      <td>2024-07-01 07:48:00</td>\n",
       "      <td>477727.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8187</th>\n",
       "      <td>0.0718</td>\n",
       "      <td>123.8648</td>\n",
       "      <td>2.81470</td>\n",
       "      <td>2024-07-01 07:48:00</td>\n",
       "      <td>477727.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8188</th>\n",
       "      <td>0.0718</td>\n",
       "      <td>123.8018</td>\n",
       "      <td>1.03326</td>\n",
       "      <td>2024-07-01 07:48:00</td>\n",
       "      <td>477727.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8189</th>\n",
       "      <td>0.0718</td>\n",
       "      <td>123.7388</td>\n",
       "      <td>-3.56801</td>\n",
       "      <td>2024-07-01 07:48:00</td>\n",
       "      <td>477727.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8190</th>\n",
       "      <td>0.0718</td>\n",
       "      <td>123.6758</td>\n",
       "      <td>-3.82671</td>\n",
       "      <td>2024-07-01 07:48:00</td>\n",
       "      <td>477727.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8191</th>\n",
       "      <td>0.0718</td>\n",
       "      <td>123.6128</td>\n",
       "      <td>9.82056</td>\n",
       "      <td>2024-07-01 07:48:00</td>\n",
       "      <td>477727.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8192 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Latitude  Longitude  laplacian                 Time  Hours_elapsed\n",
       "0       2.8438   131.6138    2.77747  2024-07-01 07:48:00       477727.8\n",
       "1       2.8438   131.5508  -15.47635  2024-07-01 07:48:00       477727.8\n",
       "2       2.8438   131.4878   -1.04339  2024-07-01 07:48:00       477727.8\n",
       "3       2.8438   131.4248    0.72936  2024-07-01 07:48:00       477727.8\n",
       "4       2.8438   131.3618   -5.38342  2024-07-01 07:48:00       477727.8\n",
       "...        ...        ...        ...                  ...            ...\n",
       "8187    0.0718   123.8648    2.81470  2024-07-01 07:48:00       477727.8\n",
       "8188    0.0718   123.8018    1.03326  2024-07-01 07:48:00       477727.8\n",
       "8189    0.0718   123.7388   -3.56801  2024-07-01 07:48:00       477727.8\n",
       "8190    0.0718   123.6758   -3.82671  2024-07-01 07:48:00       477727.8\n",
       "8191    0.0718   123.6128    9.82056  2024-07-01 07:48:00       477727.8\n",
       "\n",
       "[8192 rows x 5 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times = final_df['Hours_elapsed'].unique()\n",
    "cbmap = {}\n",
    "for t in range(len(times)):\n",
    "    cbmap[t] = final_df[final_df['Hours_elapsed'] == times[t]].reset_index(drop=True)\n",
    "\n",
    "cbmap[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edac057",
   "metadata": {},
   "source": [
    "# 2D SPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0529d101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55636.26023825511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.28387959, 0.09838602, 0.22251661, ..., 0.33407337, 0.22251661,\n",
       "        0.09838602],\n",
       "       [0.12220644, 0.27996254, 0.6458887 , ..., 0.49251526, 1.67686008,\n",
       "        0.01284934],\n",
       "       [0.24987992, 0.12154554, 0.19677225, ..., 1.39454568, 0.57255071,\n",
       "        0.15569056],\n",
       "       ...,\n",
       "       [1.08069075, 0.26650067, 0.78651477, ..., 2.68740763, 0.47185816,\n",
       "        4.24604453],\n",
       "       [0.24987992, 0.15569056, 0.57255071, ..., 0.32074295, 0.19677225,\n",
       "        0.12154554],\n",
       "       [0.12220644, 0.01284934, 1.67686008, ..., 0.79192762, 0.6458887 ,\n",
       "        0.27996254]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cmath\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def cgn(u):\n",
    "    \"\"\"\n",
    "    Computes a 2D Bartlett window function (triangular window).\n",
    "    \n",
    "    Args:\n",
    "        u (tuple): A tuple of lag indices (u1, u2).\n",
    "        \n",
    "    Returns:\n",
    "        float: The window value.\n",
    "    \"\"\"\n",
    "    u1, u2 = u\n",
    "    return (1 - np.abs(u1) / 64) * (1 - np.abs(u2) / 128) \n",
    "\n",
    "def cov_x(u1, u2, params):\n",
    "    \"\"\"\n",
    "    Computes the autocovariance of the original process.\n",
    "    \n",
    "    Args:\n",
    "        u1 (int): The first lag index.\n",
    "        u2 (int): The second lag index.\n",
    "        params (list): A list of parameters for the covariance function.\n",
    "                       Example: [sigma2, alpha1, alpha2].\n",
    "        \n",
    "    Returns:\n",
    "        float: The autocovariance value.\n",
    "    \"\"\"\n",
    "    sigma2, alpha1, alpha2 = params\n",
    "    return sigma2 * np.exp(-np.sqrt((u1 / alpha1)**2 + (u2 / alpha2)**2))\n",
    "\n",
    "def cov_laplacian(u1, u2, params):\n",
    "    \"\"\"\n",
    "    Computes the autocovariance of the Laplacian-filtered process.\n",
    "    \n",
    "    Args:\n",
    "        u1 (int): The first lag index.\n",
    "        u2 (int): The second lag index.\n",
    "        params (list): A list of parameters for the covariance function.\n",
    "        \n",
    "    Returns:\n",
    "        float: The autocovariance value of the filtered process.\n",
    "    \"\"\"\n",
    "    delta1, delta2 = 0.044, 0.063\n",
    "    \n",
    "    # Define the 5-point stencil of the discrete Laplacian\n",
    "    stencil_weights = {(0, 0): -4, (0, 1): 1, (0, -1): 1, (1, 0): 1, (-1, 0): 1}\n",
    "    \n",
    "    cov = 0\n",
    "    # Iterate through all pairs of points in the stencil\n",
    "    for (a, b), w_ab in stencil_weights.items():\n",
    "        for (c, d), w_cd in stencil_weights.items():\n",
    "            # Calculate the effective lag vector\n",
    "            lag_x = (u1 + a - c) * delta1\n",
    "            lag_y = (u2 + b - d) * delta2\n",
    "            \n",
    "            # Add the weighted covariance term\n",
    "            cov += w_ab * w_cd * cov_x(lag_x, lag_y, params)\n",
    "            \n",
    "    return cov\n",
    "\n",
    "def cn_bar(u1, u2, params):\n",
    "    \"\"\"\n",
    "    Computes the periodicized autocovariance by multiplying the\n",
    "    Laplacian covariance with a 2D Bartlett window.\n",
    "    \n",
    "    Args:\n",
    "        u1 (int): The first lag index.\n",
    "        u2 (int): The second lag index.\n",
    "        params (list): Model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        float: The periodicized and windowed autocovariance value.\n",
    "    \"\"\"\n",
    "    u = (u1, u2)\n",
    "    return cov_laplacian(u1, u2, params) * cgn(u)\n",
    "\n",
    "\n",
    "def expected_periodogram_fft(params, n1, n2, delta1, delta2):\n",
    "    \"\"\"\n",
    "    Computes the expected periodogram for ALL frequencies using a 2D FFT.\n",
    "    This method is much faster for a full grid of frequencies.\n",
    "    \n",
    "    Args:\n",
    "        params (list): Model parameters.\n",
    "        n1 (int): The number of samples in the first dimension.\n",
    "        n2 (int): The number of samples in the second dimension.\n",
    "        delta1 (float): The sampling interval in the first dimension.\n",
    "        delta2 (float): The sampling interval in the second dimension.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: A 2D array of expected periodogram values for all frequencies.\n",
    "    \"\"\"\n",
    "    cn_tilde_matrix = np.zeros((n1, n2), dtype=complex)\n",
    "    \n",
    "    for u1 in range(n1):\n",
    "        for u2 in range(n2):\n",
    "            cn_tilde_matrix[u1, u2] = cn_bar(u1, u2, params) + \\\n",
    "                                      cn_bar(u1 - n1, u2 - n2, params) + \\\n",
    "                                      cn_bar(u1, u2 - n2, params) + \\\n",
    "                                      cn_bar(u1 - n1, u2, params)\n",
    "    \n",
    "    fft_result = np.fft.fft2(cn_tilde_matrix)\n",
    "    \n",
    "    normalization_factor = (delta1 * delta2) / (2 * cmath.pi)**2\n",
    "    expected_periodogram = fft_result * normalization_factor\n",
    "    \n",
    "    return expected_periodogram\n",
    "\n",
    "def compute_2d_periodogram_from_df(df, value_column='laplacian', lat_column='Latitude', lon_column='Longitude'):\n",
    "    \"\"\"\n",
    "    Computes the 2D periodogram from a pandas DataFrame containing spatial data.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        value_column (str): The name of the column containing the data values (e.g., 'laplacian').\n",
    "        lat_column (str): The name of the column for the row index (e.g., 'Latitude').\n",
    "        lon_column (str): The name of the column for the column headers (e.g., 'Longitude').\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 2D NumPy array of the periodogram values.\n",
    "    \"\"\"\n",
    "    # 1. Pivot the DataFrame to reshape the 1D series into a 2D grid.\n",
    "    # The `lat_column` is used as the index and `lon_column` as the columns\n",
    "    # because Latitude typically represents the y-axis (rows) and Longitude the x-axis (columns).\n",
    "    # Since Latitude changes more slowly, it makes sense to use it as the index.\n",
    "    data_grid = df.pivot_table(index=lat_column, columns=lon_column, values=value_column)\n",
    "    \n",
    "    # 2. Convert the 2D pandas DataFrame to a 2D NumPy array.\n",
    "    data_array = data_grid.values\n",
    "    \n",
    "    # 3. Compute the 2D FFT.\n",
    "    # The number of rows and columns in the array.\n",
    "    n1, n2 = data_array.shape\n",
    "    \n",
    "    # The `np.fft.fft2` function is used for a 2D FFT.\n",
    "    fft_result = np.fft.fft2(data_array)\n",
    "    \n",
    "    # 4. Calculate the periodogram.\n",
    "    # The periodogram is the squared magnitude of the FFT result, normalized by the number of samples.\n",
    "    periodogram = (np.abs(fft_result)**2) / (n1 * n2)\n",
    "    \n",
    "    return periodogram\n",
    "\n",
    "\n",
    "\n",
    "def likelihood(params, df):\n",
    "    periodogram_values = compute_2d_periodogram_from_df(df)\n",
    "    n1, n2 = periodogram_values.shape\n",
    "    delta1, delta2 = 0.044, 0.063\n",
    "    n = n1*n2\n",
    "    # Ensure the expected periodogram's frequency order matches the data's periodogram\n",
    "    expected_periodogram_values = expected_periodogram_fft(params, n1, n2, delta1, delta2)\n",
    "    \n",
    "    # Flatten both periodograms for easier computation\n",
    "    periodogram_flat = periodogram_values.flatten()\n",
    "    expected_flat = expected_periodogram_values.flatten()\n",
    "    \n",
    "    # Use the real part and ensure it's non-negative for the log-likelihood\n",
    "    expected_flat_real = np.maximum(expected_flat.real, 1e-10)\n",
    "    \n",
    "    # Compute the negative log-likelihood using the real-valued expected periodogram\n",
    "    nll = np.sum(np.log(expected_flat_real) + periodogram_flat / expected_flat_real)\n",
    "    return nll/n\n",
    "\n",
    "params = [20, 0.5, 0.5]  # Example parameters: [sigma2, alpha1, alpha2]\n",
    "a = likelihood(params, df1)\n",
    "print(a)\n",
    "\n",
    "periodogram_values = compute_2d_periodogram_from_df(df1)\n",
    "periodogram_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d818c111",
   "metadata": {},
   "source": [
    "# 3 D SPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "92dd76d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for i in range(8):\n",
    "    df_list.append(cbmap[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9287d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cmath\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "\n",
    "\n",
    "def cgn(u):\n",
    "    \"\"\"\n",
    "    Computes a 2D Bartlett window function (triangular window).\n",
    "    \n",
    "    Args:\n",
    "        u (tuple): A tuple of lag indices (u1, u2).\n",
    "        \n",
    "    Returns:\n",
    "        float: The window value.\n",
    "    \"\"\"\n",
    "    u1, u2 = u\n",
    "    return (1 - np.abs(u1) / 64) * (1 - np.abs(u2) / 128) \n",
    "\n",
    "def cov_x(u1, u2, t, params):\n",
    "    sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "    \n",
    "    distance = (u1/range_lat - advec_lat * t)**2 + (u2/range_lon - advec_lon * t)**2 + (beta * t)**2\n",
    "\n",
    "    if distance != 0:\n",
    "        return sigmasq * torch.exp(- torch.sqrt(distance))\n",
    "    else:\n",
    "        return sigmasq + nugget\n",
    "    \n",
    "\n",
    "def cov_laplacian(u1, u2, t, params):\n",
    "    \"\"\"\n",
    "    Computes the autocovariance of the Laplacian-filtered process.\n",
    "    \n",
    "    Args:\n",
    "        u1 (int): The first lag index.\n",
    "        u2 (int): The second lag index.\n",
    "        params (list): A list of parameters for the covariance function.\n",
    "        \n",
    "    Returns:\n",
    "        float: The autocovariance value of the filtered process.\n",
    "    \"\"\"\n",
    "    delta1, delta2 = 0.044, 0.063\n",
    "    \n",
    "    # Define the 5-point stencil of the discrete Laplacian\n",
    "    stencil_weights = {(0, 0): -4, (0, 1): 1, (0, -1): 1, (1, 0): 1, (-1, 0): 1}\n",
    "    \n",
    "    cov = 0\n",
    "    # Iterate through all pairs of points in the stencil\n",
    "    for (a, b), w_ab in stencil_weights.items():\n",
    "        for (c, d), w_cd in stencil_weights.items():\n",
    "            # Calculate the effective lag vector\n",
    "            lag_x = (u1 + a - c) * delta1\n",
    "            lag_y = (u2 + b - d) * delta2\n",
    "            \n",
    "            # Add the weighted covariance term\n",
    "            cov += w_ab * w_cd * cov_x(lag_x, lag_y, t, params)\n",
    "            \n",
    "    return cov\n",
    "\n",
    "def cn_bar(u1, u2, t,params):\n",
    "    \"\"\"\n",
    "    Computes the periodicized autocovariance by multiplying the\n",
    "    Laplacian covariance with a 2D Bartlett window.\n",
    "    \n",
    "    Args:\n",
    "        u1 (int): The first lag index.\n",
    "        u2 (int): The second lag index.\n",
    "        params (list): Model parameters.\n",
    "        \n",
    "    Returns:\n",
    "        float: The periodicized and windowed autocovariance value.\n",
    "    \"\"\"\n",
    "    u = (u1, u2)\n",
    "    return cov_laplacian(u1, u2, t, params) * cgn(u)\n",
    "\n",
    "def expected_periodogram_fft_matrix(params, n1, n2, delta1, delta2):\n",
    "    # Create a 4D tensor to hold the term c_g,n * c_X\n",
    "    # Shape will be (n1, n2, p, p)\n",
    "\n",
    "    p=8\n",
    "    product_tensor = np.zeros((n1, n2, p, p), dtype=complex)\n",
    "\n",
    "    # Define time lags based on the number of components\n",
    "    t_lags = np.arange(p)  # e.g., [0, 1, 2, ..., 7]\n",
    "    \n",
    "    # Assuming g_q and g_r are the same for all components, this simplifies to cgn(u)\n",
    "    # The normalization constant for c_g,n is tricky, but let's assume it's included in cgn\n",
    "    \n",
    "    for u1 in range(n1):\n",
    "        for u2 in range(n2):\n",
    "            for q in range(p):\n",
    "                for r in range(p):\n",
    "                    # Temporal lag\n",
    "                    t = t_lags[q] - t_lags[r]\n",
    "                    \n",
    "                    # Compute the windowed autocovariance product c_g,n * c_X\n",
    "                    # The formula in the paper is a sum over u, so we're building the term for each u\n",
    "                    product_tensor[u1, u2, q, r] = cgn((u1, u2), n1, n2) * cov_laplacian(u1, u2, t, params)\n",
    "                    \n",
    "    # Perform the 2D FFT on the spatial dimensions for each component pair\n",
    "    # The FFT is applied to each p x p matrix for each u1, u2 combination\n",
    "    # The result will be (n1, n2, p, p)\n",
    "    fft_result = np.fft.fft2(product_tensor, axes=(0, 1))\n",
    "\n",
    "    # Normalization factor from the paper\n",
    "    normalization_factor = (delta1 * delta2) / (2 * cmath.pi)**2\n",
    "    \n",
    "    expected_periodogram_tensor = fft_result * normalization_factor\n",
    "\n",
    "    return np.fft.fftshift(expected_periodogram_tensor, axes=(0, 1))\n",
    "\n",
    "\n",
    "def compute_2d_multivariate_periodogram_from_df_list(df_list, value_column='laplacian', lat_column='Latitude', lon_column='Longitude'):\n",
    "    \"\"\"\n",
    "    Computes the 2D multivariate periodogram tensor from a list of pandas DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        df_list (list): A list of DataFrames, where each DataFrame represents one\n",
    "                        multivariate component.\n",
    "        value_column (str): The name of the column containing the data values (e.g., 'laplacian').\n",
    "        lat_column (str): The name of the column for the row index (e.g., 'Latitude').\n",
    "        lon_column (str): The name of the column for the column headers (e.g., 'Longitude').\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: A 4D NumPy array (n1, n2, p, p) of cross-periodogram matrices.\n",
    "    \"\"\"\n",
    "    p = len(df_list)\n",
    "    \n",
    "    # 1. Pivot each DataFrame and compute its 2D FFT\n",
    "    fft_results = []\n",
    "    n1, n2 = 0, 0\n",
    "    for df in df_list:\n",
    "        data_grid = df.pivot_table(index=lat_column, columns=lon_column, values=value_column)\n",
    "        data_array = data_grid.values\n",
    "        n1, n2 = data_array.shape\n",
    "        fft_results.append(np.fft.fft2(data_array))\n",
    "        \n",
    "    # 2. Create a 4D tensor for the cross-periodograms\n",
    "    cross_periodogram_tensor = np.zeros((n1, n2, p, p), dtype=complex)\n",
    "    \n",
    "    # 3. Compute the cross-periodogram for each pair of components\n",
    "    for q in range(p):\n",
    "        for r in range(p):\n",
    "            # I(qr)n(ω) = J(q)(ω) * J(r)∗(ω)\n",
    "            cross_periodogram_tensor[:, :, q, r] = (fft_results[q] * np.conj(fft_results[r])) / (n1 * n2)\n",
    "            \n",
    "    # 4. Apply fftshift to match the frequency order of the expected periodogram\n",
    "    return np.fft.fftshift(cross_periodogram_tensor, axes=(0, 1))\n",
    "\n",
    "\n",
    "\n",
    "def generate_Jvector(df_list, value_column='laplacian', lat_column='Latitude', lon_column='Longitude'):\n",
    "    \"\"\"\n",
    "    Generates a 3D tensor of DFT vectors from a list of DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        df_list (list): A list of DataFrames, where each DataFrame represents one\n",
    "                        multivariate component.\n",
    "        value_column (str): The name of the column containing the data values (e.g., 'laplacian').\n",
    "        lat_column (str): The name of the column for the row index (e.g., 'Latitude').\n",
    "        lon_column (str): The name of the column for the column headers (e.g., 'Longitude').\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: A 3D NumPy array (n1, n2, p) of DFT vectors for all frequencies.\n",
    "    \"\"\"\n",
    "    p = len(df_list)\n",
    "    n1, n2 = 0, 0\n",
    "    fft_results = []\n",
    "    \n",
    "    for df in df_list:\n",
    "        data_grid = df.pivot_table(index=lat_column, columns=lon_column, values=value_column)\n",
    "        data_array = data_grid.values\n",
    "        n1, n2 = data_array.shape\n",
    "        # Perform 2D FFT for each component\n",
    "        fft_results.append(np.fft.fft2(data_array))\n",
    "    \n",
    "    # Stack the 2D FFT results into a 3D tensor (n1, n2, p)\n",
    "    J_vector_tensor = np.stack(fft_results, axis=2)\n",
    "    \n",
    "    # Return the shifted tensor to match the frequency order\n",
    "    return np.fft.fftshift(J_vector_tensor, axes=(0, 1))\n",
    "\n",
    "\n",
    "def likelihood(params, df_list):\n",
    "    periodogram_values = compute_2d_multivariate_periodogram_from_df_list(df_list)\n",
    "    n1, n2 = periodogram_values.shape\n",
    "    delta1, delta2 = 0.044, 0.063\n",
    "    n = n1*n2\n",
    "    # Ensure the expected periodogram's frequency order matches the data's periodogram\n",
    "    expected_periodogram_values = expected_periodogram_fft_matrix(params, n1, n2, delta1, delta2)\n",
    "    j_vector = generate_Jvector(df_list)\n",
    "    tmp = np.logdet(expected_periodogram_values) +   J^H @ np.linalg.inv(expected_periodogram_values) @ J\n",
    "\n",
    "    # Compute the negative log-likelihood using the real-valued expected periodogram\n",
    "    nll = np.sum(tmp)\n",
    "    return nll/n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c387090",
   "metadata": {},
   "source": [
    "# 3d from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "720758cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cmath\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "\n",
    "def cgn(u, n1, n2):\n",
    "    \"\"\"\n",
    "    Computes a 2D Bartlett window function (triangular window).\n",
    "    \n",
    "    Args:\n",
    "        u (tuple): A tuple of lag indices (u1, u2) as torch.Tensors.\n",
    "        n1 (int): The number of samples in the first dimension.\n",
    "        n2 (int): The number of samples in the second dimension.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The window value.\n",
    "    \"\"\"\n",
    "    u1, u2 = u\n",
    "    # Use torch operations for element-wise calculation\n",
    "    return (1 - torch.abs(u1) / n1) * (1 - torch.abs(u2) / n2) \n",
    "\n",
    "def cov_x(u1, u2, t, params):\n",
    "    \"\"\"\n",
    "    Computes the spatio-temporal autocovariance of the original process.\n",
    "    \n",
    "    Args:\n",
    "        u1 (torch.Tensor): The first spatial lag.\n",
    "        u2 (torch.Tensor): The second spatial lag.\n",
    "        t (torch.Tensor): The temporal lag.\n",
    "        params (list): A list of parameters for the covariance function.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The autocovariance value.\n",
    "    \"\"\"\n",
    "    sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "    \n",
    "    distance = (u1 / range_lat - advec_lat * t)**2 + (u2 / range_lon - advec_lon * t)**2 + (beta * t)**2\n",
    "\n",
    "    # Using torch.where for conditional logic on tensors\n",
    "    # This avoids issues with distance == 0\n",
    "    return torch.where(distance != 0, sigmasq * torch.exp(-torch.sqrt(distance)), sigmasq + nugget)\n",
    "\n",
    "def cov_laplacian(u1, u2, t, params):\n",
    "    \"\"\"\n",
    "    Computes the autocovariance of the Laplacian-filtered process.\n",
    "    \n",
    "    Args:\n",
    "        u1 (torch.Tensor): The first lag index.\n",
    "        u2 (torch.Tensor): The second lag index.\n",
    "        t (torch.Tensor): The temporal lag.\n",
    "        params (list): A list of parameters for the covariance function.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The autocovariance value of the filtered process.\n",
    "    \"\"\"\n",
    "    delta1, delta2 = 0.044, 0.063\n",
    "    \n",
    "    # Define the 5-point stencil of the discrete Laplacian\n",
    "    stencil_weights = {(0, 0): -4, (0, 1): 1, (0, -1): 1, (1, 0): 1, (-1, 0): 1}\n",
    "    \n",
    "    # Initialize cov as a tensor of the correct shape to handle broadcasting\n",
    "    cov = torch.zeros_like(u1)\n",
    "    # Iterate through all pairs of points in the stencil\n",
    "    for (a, b), w_ab in stencil_weights.items():\n",
    "        for (c, d), w_cd in stencil_weights.items():\n",
    "            # Calculate the effective lag vector\n",
    "            lag_x = (u1 + a - c) * delta1\n",
    "            lag_y = (u2 + b - d) * delta2\n",
    "            \n",
    "            # Add the weighted covariance term\n",
    "            cov += w_ab * w_cd * cov_x(lag_x, lag_y, t, params)\n",
    "            \n",
    "    return cov\n",
    "\n",
    "def cn_bar(u1, u2, t, params, n1, n2):\n",
    "    \"\"\"\n",
    "    Computes the periodicized autocovariance by multiplying the\n",
    "    Laplacian covariance with a 2D Bartlett window.\n",
    "    \n",
    "    Args:\n",
    "        u1 (torch.Tensor): The first lag index.\n",
    "        u2 (torch.Tensor): The second lag index.\n",
    "        t (torch.Tensor): The temporal lag.\n",
    "        params (list): Model parameters.\n",
    "        n1 (int): The number of samples in the first dimension.\n",
    "        n2 (int): The number of samples in the second dimension.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The periodicized and windowed autocovariance value.\n",
    "    \"\"\"\n",
    "    # u1 and u2 are now tensors\n",
    "    u = (u1, u2)\n",
    "    return cov_laplacian(u1, u2, t, params) * cgn(u, n1, n2)\n",
    "\n",
    "def expected_periodogram_fft_multivariate(params, n1, n2, p):\n",
    "    \"\"\"\n",
    "    Computes the multivariate expected periodogram for ALL frequencies using a 2D FFT.\n",
    "    This method is much faster than the direct summation.\n",
    "    \n",
    "    Args:\n",
    "        params (list): Model parameters.\n",
    "        n1 (int): The number of samples in the first spatial dimension.\n",
    "        n2 (int): The number of samples in the second dimension.\n",
    "        p (int): The number of multivariate components.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: A 4D tensor (n1, n2, p, p) of expected periodogram matrices.\n",
    "    \"\"\"\n",
    "    delta1, delta2 = 0.044, 0.063\n",
    "    \n",
    "    # Create a 4D tensor to hold the term c_g,n * c_X\n",
    "    # Shape will be (n1, n2, p, p)\n",
    "    product_tensor = torch.zeros((n1, n2, p, p), dtype=torch.complex64)\n",
    "    \n",
    "    # Define time lags based on the number of components\n",
    "    t_lags = torch.arange(p, dtype=torch.float32)\n",
    "    \n",
    "    # Using torch.meshgrid to create tensors for u1 and u2 to enable vectorized operations\n",
    "    u1_mesh, u2_mesh = torch.meshgrid(torch.arange(n1, dtype=torch.float32), torch.arange(n2, dtype=torch.float32), indexing='ij')\n",
    "    \n",
    "    for q in range(p):\n",
    "        for r in range(p):\n",
    "            # Temporal lag\n",
    "            t = t_lags[q] - t_lags[r]\n",
    "            \n",
    "            # Compute the windowed autocovariance product c_g,n * c_X\n",
    "            # The formula in the paper is a sum over u, so we're building the term for each u\n",
    "            product_tensor[:, :, q, r] = cn_bar(u1_mesh, u2_mesh, t, params, n1, n2)\n",
    "            \n",
    "    # Perform the 2D FFT on the spatial dimensions for each component pair\n",
    "    fft_result = torch.fft.fft2(product_tensor, dim=(-4, -3))\n",
    "    # fft_result shape is (n1, n2, p, p)\n",
    "\n",
    "    # Normalization factor from the paper\n",
    "    normalization_factor = (delta1 * delta2) / (2 * cmath.pi)**2\n",
    "\n",
    "    # delta 1 and delta 2 are dx1 dx2 in continuous integral. \n",
    "    \n",
    "    expected_periodogram_tensor = fft_result * normalization_factor\n",
    "\n",
    "    # Use torch.fft.fftshift\n",
    "    return expected_periodogram_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be02e6b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 91\u001b[39m\n\u001b[32m     88\u001b[39m     \u001b[38;5;66;03m# Normalize the total negative log-likelihood\u001b[39;00m\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m nll / n\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m params = \u001b[43mtorch\u001b[49m.tensor([\u001b[32m20.0\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.1\u001b[39m, \u001b[32m0.1\u001b[39m, \u001b[32m0.1\u001b[39m, \u001b[32m0.01\u001b[39m], requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Example parameters\u001b[39;00m\n\u001b[32m     93\u001b[39m a = likelihood(params, df_list)\n\u001b[32m     94\u001b[39m \u001b[38;5;28mprint\u001b[39m(a)\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_Jvector(df_list, value_column='laplacian', lat_column='Latitude', lon_column='Longitude'):\n",
    "    \"\"\"\n",
    "    Generates a 3D tensor of DFT vectors from a list of DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        df_list (list): A list of DataFrames, where each DataFrame represents one\n",
    "                        multivariate component.\n",
    "        value_column (str): The name of the column containing the data values (e.g., 'laplacian').\n",
    "        lat_column (str): The name of the column for the row index (e.g., 'Latitude').\n",
    "        lon_column (str): The name of the column for the column headers (e.g., 'Longitude').\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: A 3D tensor (n1, n2, p) of DFT vectors for all frequencies.\n",
    "    \"\"\"\n",
    "    p = len(df_list)\n",
    "    fft_results = []\n",
    "    \n",
    "    for df in df_list:\n",
    "        data_grid = df.pivot_table(index=lat_column, columns=lon_column, values=value_column)\n",
    "        data_array = data_grid.values\n",
    "        n1, n2 = data_array.shape\n",
    "        # Convert numpy array to torch tensor\n",
    "        data_tensor = torch.tensor(data_array, dtype=torch.float32)\n",
    "        # Perform 2D FFT for each component\n",
    "        fft_results.append(torch.fft.fft2(data_tensor))\n",
    "    \n",
    "    # Stack the 2D FFT results into a 3D tensor (n1, n2, p)\n",
    "    J_vector_tensor = torch.stack(fft_results, dim=2)\n",
    "    # Apply the normalization factor from the paper (2*pi)^(-d/2), where d=2\n",
    "    # The normalization factor for the DFT vector is (2 * pi)^(-1)\n",
    "    normalization_factor = 1.0 / (2 * cmath.pi)\n",
    "    \n",
    "    return J_vector_tensor * normalization_factor\n",
    "def likelihood_vectorized(params, df_list):\n",
    "    \"\"\"\n",
    "    Calculates the negative log-likelihood for the multivariate model\n",
    "    using a vectorized approach for improved performance.\n",
    "    \n",
    "    Args:\n",
    "        params (list): Model parameters.\n",
    "        df_list (list): A list of DataFrames representing the multivariate data.\n",
    "        \n",
    "    Returns:\n",
    "        float: The negative log-likelihood value.\n",
    "    \"\"\"\n",
    "    p = len(df_list)\n",
    "    if p == 0:\n",
    "        return torch.tensor(0.0)\n",
    "    \n",
    "    # Determine dimensions from the first DataFrame\n",
    "    n1, n2 = df_list[0].pivot_table(index='Latitude', columns='Longitude', values='laplacian').shape\n",
    "    n = n1 * n2\n",
    "\n",
    "    # Compute the expected periodogram tensor from the model\n",
    "    expected_periodogram = expected_periodogram_fft_multivariate(params, n1, n2, p)\n",
    "    \n",
    "    # Generate the J-vector tensor from the data\n",
    "    j_vector = generate_Jvector(df_list)\n",
    "    \n",
    "    # Reshape tensors for batch processing\n",
    "    I_omega_batch = expected_periodogram.reshape(-1, p, p)\n",
    "    J_omega_batch = j_vector.reshape(-1, p, 1)\n",
    "\n",
    "    # Add a small value to the real part of the diagonal for stability\n",
    "    # `torch.linalg.norm(I_omega_batch)` is used to ensure the regularization is relative to the matrix scale.\n",
    "    I_omega_stable = torch.real(I_omega_batch) + torch.linalg.norm(I_omega_batch, dim=(1, 2), keepdim=True) * 1e-10 * torch.eye(p, dtype=I_omega_batch.dtype)\n",
    "\n",
    "    # Compute the log-determinant of each matrix in the batch\n",
    "    # The determinant of a complex matrix can be complex, so we take the log-abs-determinant.\n",
    "    log_det = torch.log(torch.det(I_omega_stable)).real\n",
    "\n",
    "    # Compute the inverse for each matrix in the batch\n",
    "    inv_I_batch = torch.linalg.inv(I_omega_stable)\n",
    "    \n",
    "    # Compute the quadratic form: J^H * I^-1 * J\n",
    "    # First, J^H @ inv_I (batch of 1 x p matrices @ batch of p x p matrices)\n",
    "    term2_temp = torch.bmm(torch.conj(J_omega_batch.transpose(1, 2)), inv_I_batch)\n",
    "\n",
    "    # Then, (J^H @ inv_I) @ J (batch of 1 x p matrices @ batch of p x 1 matrices)\n",
    "    term2_batch = torch.bmm(term2_temp, J_omega_batch).reshape(-1) # reshape to get a 1D tensor\n",
    "\n",
    "    # The sum of log(det(I)) and the trace term over all frequencies\n",
    "    nll_batch = log_det + term2_batch.real\n",
    "\n",
    "    # Sum up all the log-likelihood terms\n",
    "    nll = torch.sum(nll_batch)\n",
    "\n",
    "    # Normalize the total negative log-likelihood\n",
    "    return nll / n\n",
    "\n",
    "params = torch.tensor([20.0, 0.5, 0.5, 0.1, 0.1, 0.1, 0.01], requires_grad=True)  # Example parameters\n",
    "\n",
    "a = likelihood(params, df_list)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c753e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_Jvector(df_list, value_column='laplacian', lat_column='Latitude', lon_column='Longitude'):\n",
    "    \"\"\"\n",
    "    Generates a 3D tensor of DFT vectors from a list of DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        df_list (list): A list of DataFrames, where each DataFrame represents one\n",
    "                        multivariate component.\n",
    "        value_column (str): The name of the column containing the data values (e.g., 'laplacian').\n",
    "        lat_column (str): The name of the column for the row index (e.g., 'Latitude').\n",
    "        lon_column (str): The name of the column for the column headers (e.g., 'Longitude').\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: A 3D tensor (n1, n2, p) of DFT vectors for all frequencies.\n",
    "    \"\"\"\n",
    "    p = len(df_list)\n",
    "    fft_results = []\n",
    "    \n",
    "    for df in df_list:\n",
    "        data_grid = df.pivot_table(index=lat_column, columns=lon_column, values=value_column)\n",
    "        data_array = data_grid.values\n",
    "        n1, n2 = data_array.shape\n",
    "        # Convert numpy array to torch tensor\n",
    "        data_tensor = torch.tensor(data_array, dtype=torch.float32)\n",
    "        # Perform 2D FFT for each component\n",
    "        fft_results.append(torch.fft.fft2(data_tensor))\n",
    "    \n",
    "    # Stack the 2D FFT results into a 3D tensor (n1, n2, p)\n",
    "    J_vector_tensor = torch.stack(fft_results, dim=2)\n",
    "    \n",
    "    # Return the shifted tensor to match the frequency order\n",
    "    return torch.fft.fftshift(J_vector_tensor, dim=(-3, -2))\n",
    "\n",
    "def likelihood(params, df_list):\n",
    "    \"\"\"\n",
    "    Calculates the negative log-likelihood for the multivariate model.\n",
    "    \n",
    "    Args:\n",
    "        params (list): Model parameters.\n",
    "        df_list (list): A list of DataFrames representing the multivariate data.\n",
    "        \n",
    "    Returns:\n",
    "        float: The negative log-likelihood value.\n",
    "    \"\"\"\n",
    "    p = len(df_list)\n",
    "    if p == 0:\n",
    "        return torch.tensor(0.0)\n",
    "    \n",
    "    # Determine dimensions from the first DataFrame\n",
    "    n1, n2 = df_list[0].pivot_table(index='Latitude', columns='Longitude', values='laplacian').shape\n",
    "    n = n1 * n2\n",
    "\n",
    "    delta1, delta2 = 0.044, 0.063\n",
    "    \n",
    "    # Convert parameters to tensors with requires_grad=True for optimization\n",
    "    params_tensor = [torch.tensor(p, requires_grad=True) for p in params]\n",
    "    \n",
    "    # Compute the expected periodogram tensor from the model\n",
    "    expected_periodogram = expected_periodogram_fft_multivariate(params_tensor, n1, n2, p)\n",
    "    \n",
    "    # Generate the J-vector tensor from the data\n",
    "    j_vector = generate_Jvector(df_list)\n",
    "    \n",
    "    nll = torch.tensor(0.0, dtype=torch.complex64)\n",
    "    \n",
    "    # Loop over all spatial frequencies (u1, u2)\n",
    "    for u1 in range(n1):\n",
    "        for u2 in range(n2):\n",
    "            # Extract the p x p matrix for the expected periodogram at this frequency\n",
    "            I_omega = expected_periodogram[u1, u2, :, :]\n",
    "            \n",
    "            # Extract the p-dimensional J vector at this frequency\n",
    "            J_omega = j_vector[u1, u2, :]\n",
    "            \n",
    "            # Ensure matrices are numerically stable before inversion\n",
    "            I_omega_real = torch.real(I_omega)\n",
    "            I_omega_stable = I_omega_real + 1e-10 * torch.eye(p)\n",
    "            \n",
    "            # The log-likelihood term for this frequency is:\n",
    "            # log(det(I)) + J^H * I^-1 * J\n",
    "            # where J^H is the conjugate transpose of J\n",
    "            try:\n",
    "                # Compute the term log(det(I))\n",
    "                term1 = torch.log(torch.det(I_omega_stable))\n",
    "\n",
    "                # Compute the quadratic form J^H * I^-1 * J\n",
    "                # torch.conj(J_omega).T is J^H (Hermitian conjugate)\n",
    "                # @ is the matrix multiplication operator\n",
    "                inv_I = torch.inverse(I_omega_stable)\n",
    "                term2 = torch.conj(J_omega).T @ inv_I @ J_omega\n",
    "                \n",
    "                # Add the terms to the total negative log-likelihood\n",
    "                nll += (term1 + term2)\n",
    "            \n",
    "            except torch.linalg.LinAlgError:\n",
    "                # Handle cases where the matrix is singular or not invertible\n",
    "                return torch.tensor(float('inf'))\n",
    "\n",
    "    # Normalize the total negative log-likelihood\n",
    "    return torch.real(nll) / n\n",
    "\n",
    "params = [20, 0.5, 0.5, 0.1, 0.1, 0.1, 0.1]  # Example parameters: [sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget]\n",
    "params = torch.tensor(params, requires_grad=True)\n",
    "likelihood(params, df_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9396b33",
   "metadata": {},
   "source": [
    "import covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79f3778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "v05_base_path = Path(\"/Users/joonwonlee/Documents/GEMS_TCO-1/outputs/day/estimates/df_cv_smooth_05/\")\n",
    "\n",
    "\n",
    "#full_day_r2s10_v045_spline1250 = pd.read_csv( base_path / \"full_day_r2s10_v045_spline1250.0.csv\")\n",
    "#full_day_r2s10_v055_spline1250 = pd.read_csv( base_path / \"full_day_r2s10_v055_spline1250.0.csv\")\n",
    "\n",
    "full_day_v05_r2s10_1127 = pd.read_csv(v05_base_path / \"full_day_v05_r2s10_1127.csv\")\n",
    "vecchia_v05_r2s10_1127 = pd.read_csv( v05_base_path / \"vecchia_v05_r2s10_1127.csv\")\n",
    "vecchia_v05_r2s10_4508 = pd.read_csv( v05_base_path / \"vecchia_v05_r2s10_4508.csv\")\n",
    "vecchia_v05_r2s10_18033 = pd.read_csv( v05_base_path / \"vecchia_v05_r2s10_18033.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0bbe3cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sigma            21.197\n",
       "range_lat         1.267\n",
       "range_lon         1.635\n",
       "advec_lat         0.026\n",
       "advec_lon        -0.162\n",
       "beta              0.172\n",
       "nugget            4.814\n",
       "loss         198194.371\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimates = vecchia_v05_r2s10_18033.iloc[:,5:-2]\n",
    "day1 = estimates.iloc[0]\n",
    "day1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4fb2af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Callable\n",
    "\n",
    "class spatio_temporal_kernels:               #sigmasq range advec beta  nugget\n",
    "    def __init__(self, smooth:float, aggregated_data: torch.Tensor):\n",
    "  \n",
    "        self.smooth = smooth\n",
    "      \n",
    "        self.aggregated_data = aggregated_data[:,:4]\n",
    "        self.aggregated_response = aggregated_data[:,2]\n",
    "        self.aggregated_locs = aggregated_data[:,:2]\n",
    "\n",
    "\n",
    "\n",
    "    ## The torch.sqrt() is moved to the covariance function to track gradients of beta and avec\n",
    "    def custom_distance_matrix(self, U:torch.Tensor, V:torch.Tensor):\n",
    "        # Efficient distance computation with broadcasting\n",
    "        spatial_diff = torch.norm(U[:, :2].unsqueeze(1) - V[:, :2].unsqueeze(0), dim=2)\n",
    "        temporal_diff = torch.abs(U[:, 2].unsqueeze(1) - V[:, 2].unsqueeze(0))\n",
    "        distance = (spatial_diff**2 + temporal_diff**2)  # move torch.sqrt to covariance function to track gradients of beta and avec\n",
    "        return distance\n",
    "    \n",
    "    def precompute_coords_anisotropy(self, params:torch.Tensor, y: torch.Tensor, x: torch.Tensor)-> torch.Tensor:\n",
    "        sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "\n",
    "        if y is None or x is None:\n",
    "            raise ValueError(\"Both y and x_df must be provided.\")\n",
    "\n",
    "        x1, y1, t1 = x[:, 0], x[:, 1], x[:, 3]\n",
    "        x2, y2, t2 = y[:, 0], y[:, 1], y[:, 3]\n",
    "\n",
    "        # spat_coord1 = torch.stack((self.x1 , self.y1 - advec * self.t1), dim=-1)\n",
    "        spat_coord1 = torch.stack(( (x1 - advec_lat * t1)/range_lat, (y1 - advec_lon * t1)/range_lon ), dim=-1)\n",
    "        spat_coord2 = torch.stack(( (x2 - advec_lat * t2)/range_lat, (y2 - advec_lon * t2)/range_lon ), dim=-1)\n",
    "\n",
    "        U = torch.cat((spat_coord1, (beta * t1).reshape(-1, 1)), dim=1)\n",
    "        V = torch.cat((spat_coord2, (beta * t2).reshape(-1, 1)), dim=1)\n",
    "\n",
    "        distance = self.custom_distance_matrix(U,V)\n",
    "        non_zero_indices = distance != 0\n",
    "        return distance, non_zero_indices\n",
    " \n",
    "    def matern_cov_anisotropy_v05(self,params: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "        \n",
    "        distance, non_zero_indices = self.precompute_coords_anisotropy(params, x,y)\n",
    "        out = torch.zeros_like(distance)\n",
    "\n",
    "        non_zero_indices = distance != 0\n",
    "        if torch.any(non_zero_indices):\n",
    "            out[non_zero_indices] = sigmasq * torch.exp(- torch.sqrt(distance[non_zero_indices]))\n",
    "        out[~non_zero_indices] = sigmasq\n",
    "\n",
    "        # Add a small jitter term to the diagonal for numerical stability\n",
    "        out += torch.eye(out.shape[0], dtype=torch.float64) * nugget \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427fce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_df_clean = final_df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Step 2: Fill or drop NaNs\n",
    "final_df_clean = final_df_clean.fillna(0)  # or use .dropna()\n",
    "\n",
    "# Step 3: Convert to tensor\n",
    "tensor_data = torch.tensor(final_df_clean.values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "instance = spatio_temporal_kernels(smooth=0.5, aggregated_data= tensor_data)\n",
    "params = list(day1[:-1])\n",
    "cov_matrix = instance.matern_cov_anisotropy_v05(torch.tensor(params, dtype=torch.float64), tensor_data, tensor_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6917de83",
   "metadata": {},
   "source": [
    "Fortunately, there's a much more efficient method based on the Wiener-Khinchin theorem. Under the same circulant/periodic assumption that the FFT relies on, the power spectrum (which is exactly the diagonal elements you want) is simply the Fourier transform of the process's autocorrelation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c2e07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.78672437e+07    -0.j        , -6.25720019e+03+32639.77112216j,\n",
       "       -7.10920442e+03+21337.61938j   , ...,\n",
       "        6.82686846e+01 +1973.56554526j,  1.03793847e+03  +893.84617913j,\n",
       "       -1.67759824e+03 +8482.39478284j])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import fft\n",
    "\n",
    "def get_3d_fft_coefficients(data: np.ndarray, grid_shape: tuple = (66, 130, 8)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the 3D Discrete Fourier Transform (DFT) of a 1D spatio-temporal data array.\n",
    "\n",
    "    This function assumes the input data is flattened from a 3D grid where\n",
    "    the time dimension changes slowest, then longitude, then latitude. It reshapes\n",
    "    the data, transposes it to a (latitude, longitude, time) order, and then\n",
    "    computes the 3D FFT.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): A 1D NumPy array containing the flattened data.\n",
    "                           The flattening order is assumed to be (time, longitude, latitude).\n",
    "        grid_shape (tuple): The desired conceptual dimensions of the 3D grid\n",
    "                            in (latitude, longitude, time) order.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 1D NumPy array of size (66*130*8) containing the complex\n",
    "                    3D DFT coefficients.\n",
    "    \"\"\"\n",
    "    # 1. Verify that the data size matches the expected grid dimensions\n",
    "    expected_size = np.prod(grid_shape)\n",
    "    if data.size != expected_size:\n",
    "        raise ValueError(f\"Input data size is {data.size}, but expected {expected_size}.\")\n",
    "    \n",
    "    # 2. Determine the reshape order from the described flattening (time, lon, lat)\n",
    "    # The conceptual grid_shape is (lat, lon, time) -> (66, 130, 8)\n",
    "    # So the physical reshape order must be (time, lon, lat) -> (8, 130, 66)\n",
    "    reshape_order = (grid_shape[2], grid_shape[1], grid_shape[0])\n",
    "    \n",
    "    # 3. Reshape the 1D data into a 3D grid based on its physical storage layout\n",
    "    grid_physical_order = data.reshape(reshape_order) # Current shape: (8, 130, 66)\n",
    "\n",
    "    # 4. Transpose the grid to match the conceptual order for analysis (lat, lon, time)\n",
    "    # Current axes: 0=time, 1=lon, 2=lat\n",
    "    # Target axes:  0=lat, 1=lon, 2=time\n",
    "    # We need to map the old axes (2, 1, 0) to the new axes (0, 1, 2)\n",
    "    grid_conceptual_order = np.transpose(grid_physical_order, (2, 1, 0))\n",
    "\n",
    "    # 5. Compute the n-dimensional (3D in this case) Fast Fourier Transform\n",
    "    grid_fft = fft.fftn(grid_conceptual_order)\n",
    "\n",
    "    # 6. Flatten the 3D grid of coefficients into a 1D array to be added to a DataFrame\n",
    "    return grid_fft.flatten()\n",
    "\n",
    "def get_3d_fft_from_dataframe(df: pd.DataFrame, grid_shape: tuple = (66, 130, 8)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the 3D DFT for the 'ColumnAmountO3' column from a spatio-temporal DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing flattened 3D data.\n",
    "        grid_shape (tuple): The (latitude, longitude, time) dimensions of the grid.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 1D array of complex 3D DFT coefficients.\n",
    "    \"\"\"\n",
    "    z_vector = df['ColumnAmountO3'].values\n",
    "    return get_3d_fft_coefficients(z_vector, grid_shape)\n",
    "\n",
    "def compute_power_spectrum(covariance_func, grid_shape: tuple = (66, 130, 8)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the theoretical power spectrum from a stationary covariance function.\n",
    "\n",
    "    Under the assumption of a stationary and circulant process (the same assumption\n",
    "    made by the FFT), the power spectrum is the DFT of the autocorrelation function.\n",
    "    The autocorrelation function is equivalent to the first row/column of the\n",
    "    circulant covariance matrix Sigma.\n",
    "\n",
    "    Args:\n",
    "        covariance_func: A function that takes a displacement tuple (d_lat, d_lon, d_time)\n",
    "                         and returns a single covariance value. This function defines\n",
    "                         your estimated model for Sigma.\n",
    "        grid_shape (tuple): The (latitude, longitude, time) dimensions of the grid.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 1D array containing the real-valued power spectrum, which corresponds\n",
    "                    to the diagonal elements of the D * Sigma_hat * D^H matrix.\n",
    "    \"\"\"\n",
    "    n_lat, n_lon, n_time = grid_shape\n",
    "    \n",
    "    # Construct the first row/column of the theoretical covariance matrix.\n",
    "    # This is the autocorrelation function of the process.\n",
    "    autocorr_grid = np.zeros(grid_shape)\n",
    "    for i in range(n_lat):\n",
    "        for j in range(n_lon):\n",
    "            for k in range(n_time):\n",
    "                autocorr_grid[i, j, k] = covariance_func((i, j, k))\n",
    "                \n",
    "    # By the Wiener-Khinchin theorem, the power spectrum is the DFT of the\n",
    "    # autocorrelation function.\n",
    "    power_spectrum_grid = fft.fftn(autocorr_grid)\n",
    "    \n",
    "    # The power spectrum of a real-valued process is real. We take .real to\n",
    "    # discard negligible imaginary parts from numerical floating point errors.\n",
    "    return power_spectrum_grid.flatten().real\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c736512",
   "metadata": {},
   "source": [
    "just get the D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10793cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>ColumnAmountO3</th>\n",
       "      <th>Hours_elapsed</th>\n",
       "      <th>Time</th>\n",
       "      <th>Source_Latitude</th>\n",
       "      <th>Source_Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.8878</td>\n",
       "      <td>131.6768</td>\n",
       "      <td>258.35470</td>\n",
       "      <td>477720.866667</td>\n",
       "      <td>2024-07-01 00:52:00</td>\n",
       "      <td>2.887841</td>\n",
       "      <td>131.699000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.8878</td>\n",
       "      <td>131.6138</td>\n",
       "      <td>259.93120</td>\n",
       "      <td>477720.866667</td>\n",
       "      <td>2024-07-01 00:52:00</td>\n",
       "      <td>2.887996</td>\n",
       "      <td>131.636920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.8878</td>\n",
       "      <td>131.5508</td>\n",
       "      <td>259.26990</td>\n",
       "      <td>477720.866667</td>\n",
       "      <td>2024-07-01 00:52:00</td>\n",
       "      <td>2.888159</td>\n",
       "      <td>131.573580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.8878</td>\n",
       "      <td>131.4878</td>\n",
       "      <td>256.45093</td>\n",
       "      <td>477720.866667</td>\n",
       "      <td>2024-07-01 00:52:00</td>\n",
       "      <td>2.888175</td>\n",
       "      <td>131.511760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.8878</td>\n",
       "      <td>131.4248</td>\n",
       "      <td>255.34843</td>\n",
       "      <td>477720.866667</td>\n",
       "      <td>2024-07-01 00:52:00</td>\n",
       "      <td>2.888187</td>\n",
       "      <td>131.449420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68635</th>\n",
       "      <td>0.0278</td>\n",
       "      <td>123.8018</td>\n",
       "      <td>260.99780</td>\n",
       "      <td>477727.800000</td>\n",
       "      <td>2024-07-01 07:48:00</td>\n",
       "      <td>0.026326</td>\n",
       "      <td>123.783195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68636</th>\n",
       "      <td>0.0278</td>\n",
       "      <td>123.7388</td>\n",
       "      <td>259.03363</td>\n",
       "      <td>477727.800000</td>\n",
       "      <td>2024-07-01 07:48:00</td>\n",
       "      <td>0.026288</td>\n",
       "      <td>123.721146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68637</th>\n",
       "      <td>0.0278</td>\n",
       "      <td>123.6758</td>\n",
       "      <td>260.59440</td>\n",
       "      <td>477727.800000</td>\n",
       "      <td>2024-07-01 07:48:00</td>\n",
       "      <td>0.026334</td>\n",
       "      <td>123.658806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68638</th>\n",
       "      <td>0.0278</td>\n",
       "      <td>123.6128</td>\n",
       "      <td>260.73022</td>\n",
       "      <td>477727.800000</td>\n",
       "      <td>2024-07-01 07:48:00</td>\n",
       "      <td>0.026321</td>\n",
       "      <td>123.595980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68639</th>\n",
       "      <td>0.0278</td>\n",
       "      <td>123.5498</td>\n",
       "      <td>261.66772</td>\n",
       "      <td>477727.800000</td>\n",
       "      <td>2024-07-01 07:48:00</td>\n",
       "      <td>0.026358</td>\n",
       "      <td>123.533790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68640 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Latitude  Longitude  ColumnAmountO3  Hours_elapsed  \\\n",
       "0        2.8878   131.6768       258.35470  477720.866667   \n",
       "1        2.8878   131.6138       259.93120  477720.866667   \n",
       "2        2.8878   131.5508       259.26990  477720.866667   \n",
       "3        2.8878   131.4878       256.45093  477720.866667   \n",
       "4        2.8878   131.4248       255.34843  477720.866667   \n",
       "...         ...        ...             ...            ...   \n",
       "68635    0.0278   123.8018       260.99780  477727.800000   \n",
       "68636    0.0278   123.7388       259.03363  477727.800000   \n",
       "68637    0.0278   123.6758       260.59440  477727.800000   \n",
       "68638    0.0278   123.6128       260.73022  477727.800000   \n",
       "68639    0.0278   123.5498       261.66772  477727.800000   \n",
       "\n",
       "                      Time  Source_Latitude  Source_Longitude  \n",
       "0      2024-07-01 00:52:00         2.887841        131.699000  \n",
       "1      2024-07-01 00:52:00         2.887996        131.636920  \n",
       "2      2024-07-01 00:52:00         2.888159        131.573580  \n",
       "3      2024-07-01 00:52:00         2.888175        131.511760  \n",
       "4      2024-07-01 00:52:00         2.888187        131.449420  \n",
       "...                    ...              ...               ...  \n",
       "68635  2024-07-01 07:48:00         0.026326        123.783195  \n",
       "68636  2024-07-01 07:48:00         0.026288        123.721146  \n",
       "68637  2024-07-01 07:48:00         0.026334        123.658806  \n",
       "68638  2024-07-01 07:48:00         0.026321        123.595980  \n",
       "68639  2024-07-01 07:48:00         0.026358        123.533790  \n",
       "\n",
       "[68640 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848cb958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import fft\n",
    "\n",
    "def get_3d_fft_coefficients(data: np.ndarray, grid_shape: tuple) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the 3D Discrete Fourier Transform (DFT) of a 1D spatio-temporal data array.\n",
    "    This is a helper function used to define the transformation for one vector.\n",
    "    \n",
    "    CORRECTED: This version assumes the input data is flattened where time is the slowest\n",
    "    changing dimension, then latitude, and finally longitude is the fastest.\n",
    "    \"\"\"\n",
    "    expected_size = np.prod(grid_shape)\n",
    "    if data.size != expected_size:\n",
    "        raise ValueError(f\"Input data size is {data.size}, but expected {expected_size}.\")\n",
    "\n",
    "    # CORRECTED: The physical reshape order must be (time, lat, lon) based on the DataFrame structure.\n",
    "    # grid_shape (conceptual) is (lat, lon, time) -> (66, 130, 8)\n",
    "    # reshape_order (physical) is (time, lat, lon) -> (8, 66, 130)\n",
    "    reshape_order = (grid_shape[2], grid_shape[0], grid_shape[1])\n",
    "    grid_physical_order = data.reshape(reshape_order) # Current shape: (8, 66, 130)\n",
    "\n",
    "    # CORRECTED: Transpose the grid to the conceptual order (lat, lon, time) for analysis.\n",
    "    # Current axes: 0=time, 1=lat, 2=lon\n",
    "    # Target axes:  0=lat, 1=lon, 2=time\n",
    "    # We need to map the old axes (1, 2, 0) to the new axes (0, 1, 2)\n",
    "    grid_conceptual_order = np.transpose(grid_physical_order, (1, 2, 0))\n",
    "\n",
    "    grid_fft = fft.fftn(grid_conceptual_order)\n",
    "    \n",
    "    # CORRECTED: To ensure Y and Z are flattened in the same (time, lat, lon) order,\n",
    "    # we must transpose the coefficients back to the physical order before flattening.\n",
    "    # The inverse transpose of (1, 2, 0) is (2, 0, 1).\n",
    "    coeffs_physical_order = np.transpose(grid_fft, (2, 0, 1))\n",
    "\n",
    "    # Now, flatten the re-ordered grid to match the original Z's flattening order.\n",
    "    return coeffs_physical_order.flatten()\n",
    "\n",
    "def construct_3d_dft_matrix(grid_shape: tuple) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Constructs the explicit transformation matrix D for the 3D DFT.\n",
    "\n",
    "    The matrix D transforms a flattened data vector Z into its DFT coefficients Y\n",
    "    via the multiplication Y = DZ. Each column of D is the result of applying\n",
    "    the DFT to a standard basis vector.\n",
    "\n",
    "    WARNING: This function is extremely memory-intensive. For a grid of size\n",
    "    N = 66*130*8 = 68640, the resulting matrix D will have dimensions\n",
    "    (N, N), requiring over 75 GB of RAM. Use with caution.\n",
    "\n",
    "    Args:\n",
    "        grid_shape (tuple): The (latitude, longitude, time) dimensions of the grid.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The complex-valued (N, N) transformation matrix D.\n",
    "    \"\"\"\n",
    "    n_total = np.prod(grid_shape)\n",
    "    \n",
    "    # Create an identity matrix. Each column is a standard basis vector.\n",
    "    identity_matrix = np.eye(n_total)\n",
    "    \n",
    "    # Initialize the D matrix. It will be complex.\n",
    "    d_matrix = np.zeros((n_total, n_total), dtype=np.complex128)\n",
    "    \n",
    "    print(f\"Constructing ({n_total} x {n_total}) DFT matrix. This may take a long time...\")\n",
    "    \n",
    "    # Each column of D is the FFT of the corresponding column of the identity matrix.\n",
    "    for i in range(n_total):\n",
    "        basis_vector = identity_matrix[:, i]\n",
    "        d_matrix[:, i] = get_3d_fft_coefficients(basis_vector, grid_shape)\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"  ...processed column {i+1} of {n_total}\")\n",
    "            \n",
    "    print(\"Matrix construction complete.\")\n",
    "    return d_matrix\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == '__main__':\n",
    "    # --- Part 1: Using the efficient function with your full DataFrame ---\n",
    "    \n",
    "    full_grid_shape = (66, 130, 8)\n",
    "    full_total_size = np.prod(full_grid_shape)\n",
    "    \n",
    "    # Assume 'df2' is your DataFrame. For a runnable example, we create a sample df2.\n",
    "    print(f\"--- Demonstration with a full-sized sample DataFrame ({full_total_size} rows) ---\")\n",
    "    sample_data = {'ColumnAmountO3': np.random.randn(full_total_size)}\n",
    "    df2 = pd.DataFrame(sample_data)\n",
    "    \n",
    "    # This is how you would use the efficient function on your actual data\n",
    "    print(\"Calculating FFT coefficients from the full DataFrame using the efficient function...\")\n",
    "    y_from_df_function = get_3d_fft_coefficients(df2['ColumnAmountO3'].values, full_grid_shape)\n",
    "    print(f\"Successfully calculated {y_from_df_function.size} coefficients.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # --- Part 2: Demonstrating the D matrix construction on a small scale ---\n",
    "    \n",
    "    # WARNING: Using the full grid_shape for D will likely cause a MemoryError.\n",
    "    # We use a tiny grid to demonstrate the principle and verify correctness.\n",
    "    small_grid_shape = (2, 3, 2)\n",
    "    small_total_size = np.prod(small_grid_shape)\n",
    "    \n",
    "    print(f\"\\n--- Demonstrating D matrix construction on a small {small_grid_shape} grid ---\")\n",
    "\n",
    "    # 1. Construct the explicit D matrix for the small grid.\n",
    "    D = construct_3d_dft_matrix(small_grid_shape)\n",
    "    \n",
    "    # 2. Create a random data vector Z of the appropriate small size for verification.\n",
    "    z_vector_small = np.random.randn(small_total_size)\n",
    "    \n",
    "    # 3. --- Verification Step ---\n",
    "    # Compute Y using the direct function.\n",
    "    y_from_function = get_3d_fft_coefficients(z_vector_small, small_grid_shape)\n",
    "    \n",
    "    # Compute Y by multiplying by the constructed D matrix.\n",
    "    y_from_matrix = D @ z_vector_small\n",
    "    \n",
    "    # 4. Check if the results are numerically very close.\n",
    "    is_verified = np.allclose(y_from_function, y_from_matrix)\n",
    "    \n",
    "    print(\"\\n--- Verification ---\")\n",
    "    print(f\"Results are identical: {is_verified}\")\n",
    "    \n",
    "    if is_verified:\n",
    "        print(\"\\nThe D matrix was constructed correctly for the small grid.\")\n",
    "    else:\n",
    "        print(\"\\nError: The D matrix construction failed verification.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
