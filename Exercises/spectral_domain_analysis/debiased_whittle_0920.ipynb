{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c56dee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy                    # clone tensor\n",
    "import time\n",
    "\n",
    "# Custom imports\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels\n",
    "\n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "import typer\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO import configuration as config\n",
    "from GEMS_TCO import data_preprocess as dmbh\n",
    "\n",
    "import os\n",
    "from sklearn.neighbors import BallTree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6f300d",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96dfc8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joonwonlee/Documents/GEMS_DATA/pickle_2024/orbit_map24_07.pkl\n"
     ]
    }
   ],
   "source": [
    "lon_s = 123\n",
    "lon_e = 133\n",
    "step_lat = 0.044\n",
    "step_lon = 0.063\n",
    "\n",
    "lat_coords = np.arange( 5 -0.044- 0.0002, 0 -0.044, -0.044)\n",
    "lon_coords = np.arange( lon_e-step_lon- 0.0002, lon_s-step_lon, -step_lon)\n",
    "\n",
    "# Apply the shift as in the original code\n",
    "# These are the unique lat/lon values for the \"center_points\" grid\n",
    "final_lat_values = lat_coords + step_lat \n",
    "final_lon_values = lon_coords + step_lon \n",
    "\n",
    "# Create 2D grid with broadcasting\n",
    "#decrement = 0.00012\n",
    "decrement = 0 \n",
    "lat_grid = final_lat_values[:, None] + np.arange(len(final_lon_values)) * decrement  # shape: (228, 152)\n",
    "\n",
    "\n",
    "mac_data_path = config.mac_data_load_path\n",
    "years = [2024]  # years = [2023,2024]\n",
    "months = list( range(7,8))\n",
    "year = years[0]\n",
    "month = months[0]\n",
    "month_str = f\"{month:02d}\"  \n",
    "filename = f\"pickle_2024/orbit_map{str(year)[2:]}_{month_str}.pkl\"\n",
    "picklefile_path = Path(mac_data_path) / filename\n",
    "print(picklefile_path)\n",
    "\n",
    "with open(picklefile_path, 'rb') as pickle_file:\n",
    "    data_map_hour = pickle.load(pickle_file)\n",
    "\n",
    "# Base file path and settings\n",
    "# base_path = \"C:\\\\Users\\\\joonw\\\\TCO\\\\GEMS_data\"    MSI notebook\n",
    "\n",
    "mac_data_path = config.mac_data_load_path\n",
    "lat_start, lat_end, lon_start, lon_end = 0, 5, 123, 133\n",
    "step_lat, step_lon = 0.044, 0.063\n",
    "\n",
    "# df = pd.read_csv(\"C:\\\\Users\\\\joonw\\\\TCO\\\\GEMS_data\\\\data_2024\\\\data_24_07_0131_N510_E110120.csv\")  MSI notebook\n",
    "df = pd.read_csv(\"/Users/joonwonlee/Documents/GEMS_DATA/data_2024/data_24_07_0131_N05_E123133.csv\")  # MAC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b28c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "instance = dmbh.center_matching_hour(df, lat_start, lat_end, lon_start, lon_end)\n",
    "\n",
    "for year in years:        # years = [2023,2024]\n",
    "    for month in months:  \n",
    "        try:\n",
    "            # load pickle (dense ORI data)\n",
    "            pickle_path = os.path.join(mac_data_path, f'pickle_{year}')\n",
    "            input_filename = f\"orbit_map{str(year)[2:]}_{month_str}.pkl\"\n",
    "            input_filepath = os.path.join(pickle_path, input_filename)\n",
    "            with open(input_filepath, 'rb') as pickle_file:\n",
    "                loaded_map = pickle.load(pickle_file)\n",
    "            center_points = instance.make_center_points_wo_calibration(step_lat = step_lat, step_lon= step_lon)\n",
    "            coarse_cen_map = instance.coarse_by_center(loaded_map, center_points)\n",
    "\n",
    "            # Save pickle (coarse data)\n",
    "            output_filename = f\"coarse_cen_map_without_decrement_latitude{str(year)[2:]}_{month_str}.pkl\"\n",
    "            output_filepath = os.path.join(pickle_path, output_filename)\n",
    "            with open(output_filepath, 'wb') as pickle_file:\n",
    "                pickle.dump(coarse_cen_map, pickle_file)\n",
    "            \n",
    "            print(f\"Successfully processed and saved data for year {str(year)[2:]} month {month_str}.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File {input_filename} not found. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {input_filename}: {e}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20317e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = os.path.join(mac_data_path, f'pickle_{year}')\n",
    "output_filename = f\"coarse_cen_map_without_decrement_latitude{str(year)[2:]}_{month_str}.pkl\"\n",
    "output_filepath = os.path.join(pickle_path, output_filename)\n",
    "\n",
    "with open(output_filepath, 'rb') as pickle_file:\n",
    "    cbmap_ori = pickle.load(pickle_file)\n",
    "\n",
    "\n",
    "load_data_instance = GEMS_TCO.load_data('')\n",
    "\n",
    "df_day_list = []\n",
    "df_day_map_list = []\n",
    "for i in range(31):\n",
    "    cur_map, cur_df =load_data_instance.load_working_data_byday_wo_mm(cbmap_ori,[i*8, (i+1)*8])\n",
    "    df_day_list.append( cur_df )\n",
    "    df_day_map_list.append( cur_map )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95403c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_day_map_list[0]['y24m07day01_hm00:52'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7488b761",
   "metadata": {},
   "source": [
    "# Apply laplacian filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3f5570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.2000e-02,  1.2361e+02, -6.5655e+00,  2.1000e+01],\n",
       "        [ 7.2000e-02,  1.2368e+02, -3.0364e+00,  2.1000e+01],\n",
       "        [ 7.2000e-02,  1.2374e+02,  2.0616e+01,  2.1000e+01],\n",
       "        ...,\n",
       "        [ 2.8440e+00,  1.3149e+02,  5.0479e+00,  2.1000e+01],\n",
       "        [ 2.8440e+00,  1.3155e+02, -8.7733e+00,  2.1000e+01],\n",
       "        [ 2.8440e+00,  1.3161e+02, -3.4899e+00,  2.1000e+01]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def subset_tensor(df_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Subsets a tensor to a specific lat/lon range.\n",
    "    Columns are assumed to be [lat, lon, ozone, time].\n",
    "    \"\"\"\n",
    "    lat_mask = (df_tensor[:, 0] >= 0) & (df_tensor[:, 0] <= 2.9)\n",
    "    lon_mask = (df_tensor[:, 1] >= 123.5) & (df_tensor[:, 1] <= 131.7)\n",
    "    \n",
    "    # We apply the mask to the full tensor\n",
    "    df_sub = df_tensor[lat_mask & lon_mask].clone()\n",
    "    return df_sub\n",
    "\n",
    "def apply_laplacian_2d_valid_tensor(df_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies a 2D discrete Laplacian filter using 'mode=valid' on a PyTorch tensor.\n",
    "    The input tensor columns are assumed to be [lat, lon, ozone, time].\n",
    "    \"\"\"\n",
    "    # Extract unique coordinates and data\n",
    "    unique_lats = torch.unique(df_tensor[:, 0])\n",
    "    unique_lons = torch.unique(df_tensor[:, 1])\n",
    "    \n",
    "    lat_count = unique_lats.size(0)\n",
    "    lon_count = unique_lons.size(0)\n",
    "    \n",
    "    if df_tensor.size(0) != lat_count * lon_count:\n",
    "        raise ValueError(\"Tensor size does not match the grid dimensions (lat * lon).\")\n",
    "\n",
    "    ozone_data = df_tensor[:, 2].reshape(lat_count, lon_count)\n",
    "\n",
    "    # Define the 3x3 discrete Laplacian kernel as a PyTorch tensor\n",
    "    laplacian_kernel = torch.tensor([[0, 1, 0],\n",
    "                                     [1, -4, 1],\n",
    "                                     [0, 1, 0]], dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Reshape ozone data for conv2d: (batch_size, channels, height, width)\n",
    "    ozone_data_reshaped = ozone_data.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Apply convolution\n",
    "    laplacian_grid_cropped = F.conv2d(ozone_data_reshaped, laplacian_kernel, padding='valid')\n",
    "    laplacian_values = laplacian_grid_cropped.squeeze().flatten()\n",
    "\n",
    "    # Determine the new, cropped coordinates\n",
    "    cropped_lats = unique_lats[1:-1]\n",
    "    cropped_lons = unique_lons[1:-1]\n",
    "    \n",
    "    if len(cropped_lats) == 0 or len(cropped_lons) == 0:\n",
    "        return torch.empty(0, 4)\n",
    "\n",
    "    # Create a new meshgrid for the cropped coordinates\n",
    "    new_lon_grid, new_lat_grid = torch.meshgrid(cropped_lons, cropped_lats, indexing='xy')\n",
    "    \n",
    "    # Preserve time from the original tensor\n",
    "    time_value = df_tensor[0, 3].repeat(laplacian_values.size(0))\n",
    "\n",
    "    # Create the new tensor\n",
    "    new_tensor = torch.stack([new_lat_grid.flatten(), new_lon_grid.flatten(), laplacian_values, time_value], dim=1)\n",
    "    \n",
    "    return new_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0208e2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of aggregated day tensors: 31\n",
      "Shape of the first aggregated tensor: torch.Size([65536, 4])\n",
      "First aggregated tensor head:\n",
      "tensor([[ 7.2000e-02,  1.2361e+02, -6.5655e+00,  2.1000e+01],\n",
      "        [ 7.2000e-02,  1.2368e+02, -3.0364e+00,  2.1000e+01],\n",
      "        [ 7.2000e-02,  1.2374e+02,  2.0616e+01,  2.1000e+01],\n",
      "        [ 7.2000e-02,  1.2380e+02, -4.3127e+00,  2.1000e+01],\n",
      "        [ 7.2000e-02,  1.2386e+02, -1.0213e+00,  2.1000e+01]])\n"
     ]
    }
   ],
   "source": [
    "processed_tensor_map = []\n",
    "processed_df = []\n",
    "\n",
    "for day_map in df_day_map_list:\n",
    "    \n",
    "    tensors_to_aggregate = []\n",
    "    \n",
    "    for key, tensor in day_map.items():\n",
    "        # Step 1: Subset the tensor.\n",
    "        subsetted = subset_tensor(tensor)\n",
    "        \n",
    "        if subsetted.size(0) > 0:\n",
    "            laplacian_applied = apply_laplacian_2d_valid_tensor(subsetted)\n",
    "            if laplacian_applied.size(0) > 0:\n",
    "                tensors_to_aggregate.append(laplacian_applied)\n",
    "\n",
    "    # Step 3: Concatenate all the processed tensors from the 8-hour period.\n",
    "    if tensors_to_aggregate:\n",
    "        aggregated_day_tensor = torch.cat(tensors_to_aggregate, dim=0)\n",
    "        processed_df.append(aggregated_day_tensor)\n",
    "        \n",
    "    # The 'processed_tensors' list remains the same, storing the dictionary for each day.\n",
    "    processed_tensor_map.append(day_map)\n",
    "\n",
    "# Now, processed_df2 will contain a single aggregated tensor for each day (8-hour period).\n",
    "# You can inspect the results:\n",
    "print(\"Number of aggregated day tensors:\", len(processed_df))\n",
    "if processed_df:\n",
    "    print(\"Shape of the first aggregated tensor:\", processed_df[0].shape)\n",
    "    print(\"First aggregated tensor head:\")\n",
    "    print(processed_df[0][:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b75c72",
   "metadata": {},
   "source": [
    "# Multivariate Debiased Whittle Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f10c0ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a list of 8 tensors for multivariate test.\n",
      "\n",
      "--- Likelihood Calculation Complete ---\n",
      "Negative Log-Likelihood: 6482987008.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cmath\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cgn(u, n1, n2):\n",
    "    \"\"\"\n",
    "    Computes a 2D Bartlett window function (triangular window).\n",
    "    \n",
    "    Args:\n",
    "        u (tuple): A tuple of lag indices (u1, u2) as torch.Tensors.\n",
    "        n1 (int): The number of samples in the first dimension.\n",
    "        n2 (int): The number of samples in the second dimension.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The window value.\n",
    "    \"\"\"\n",
    "    u1, u2 = u\n",
    "    # Use torch operations for element-wise calculation\n",
    "    return (1 - torch.abs(u1) / n1) * (1 - torch.abs(u2) / n2) \n",
    "\n",
    "def cov_x(u1, u2, t, params):\n",
    "    \"\"\"\n",
    "    Computes the spatio-temporal autocovariance of the original process.\n",
    "    \n",
    "    Args:\n",
    "        u1 (torch.Tensor): The first spatial lag.\n",
    "        u2 (torch.Tensor): The second spatial lag.\n",
    "        t (torch.Tensor): The temporal lag.\n",
    "        params (list): A list of parameters for the covariance function.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The autocovariance value.\n",
    "    \"\"\"\n",
    "    sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "    \n",
    "    distance = (u1 / range_lat - advec_lat * t)**2 + (u2 / range_lon - advec_lon * t)**2 + (beta * t)**2\n",
    "\n",
    "    # Using torch.where for conditional logic on tensors\n",
    "    # This avoids issues with distance == 0\n",
    "    return torch.where(distance != 0, sigmasq * torch.exp(-(distance)), sigmasq + nugget)\n",
    "\n",
    "def cov_laplacian(u1, u2, t, params):\n",
    "    \"\"\"\n",
    "    Computes the autocovariance of the Laplacian-filtered process.\n",
    "    \n",
    "    Args:\n",
    "        u1 (torch.Tensor): The first lag index.\n",
    "        u2 (torch.Tensor): The second lag index.\n",
    "        t (torch.Tensor): The temporal lag.\n",
    "        params (list): A list of parameters for the covariance function.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The autocovariance value of the filtered process.\n",
    "    \"\"\"\n",
    "    delta1, delta2 = 0.044, 0.063\n",
    "    \n",
    "    # Define the 5-point stencil of the discrete Laplacian\n",
    "    stencil_weights = {(0, 0): -4, (0, 1): 1, (0, -1): 1, (1, 0): 1, (-1, 0): 1}\n",
    "    \n",
    "    # Initialize cov as a tensor of the correct shape to handle broadcasting\n",
    "    cov = torch.zeros_like(u1, dtype=torch.float32) # Ensure dtype matches\n",
    "    # Iterate through all pairs of points in the stencil\n",
    "    for (a, b), w_ab in stencil_weights.items():\n",
    "        for (c, d), w_cd in stencil_weights.items():\n",
    "            # Calculate the effective lag vector\n",
    "            lag_x = (u1 + a - c) * delta1\n",
    "            lag_y = (u2 + b - d) * delta2\n",
    "\n",
    "            # Add the weighted covariance term\n",
    "            cov += w_ab * w_cd * cov_x(lag_x, lag_y, t, params)\n",
    "    return cov\n",
    "\n",
    "def cn_bar(u1, u2, t, params, n1, n2):\n",
    "    \"\"\"\n",
    "    Computes the periodicized autocovariance by multiplying the\n",
    "    Laplacian covariance with a 2D Bartlett window.\n",
    "    \n",
    "    Args:\n",
    "        u1 (torch.Tensor): The first lag index.\n",
    "        u2 (torch.Tensor): The second lag index.\n",
    "        t (torch.Tensor): The temporal lag.\n",
    "        params (list): Model parameters.\n",
    "        n1 (int): The number of samples in the first dimension.\n",
    "        n2 (int): The number of samples in the second dimension.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The periodicized and windowed autocovariance value.\n",
    "    \"\"\"\n",
    "    # u1 and u2 are now tensors\n",
    "    u = (u1, u2)\n",
    "    return cov_laplacian(u1, u2, t, params) * cgn(u, n1, n2)\n",
    "\n",
    "def expected_periodogram_fft_multivariate(params, n1, n2, p):\n",
    "    \"\"\"\n",
    "    Computes the multivariate expected periodogram for ALL frequencies using a 2D FFT.\n",
    "    This method is much faster than the direct summation.\n",
    "    \n",
    "    Args:\n",
    "        params (list): Model parameters.\n",
    "        n1 (int): The number of samples in the first spatial dimension.\n",
    "        n2 (int): The number of samples in the second dimension.\n",
    "        p (int): The number of multivariate components.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: A 4D tensor (n1, n2, p, p) of expected periodogram matrices.\n",
    "    \"\"\"\n",
    "    delta1, delta2 = 0.044, 0.063\n",
    "    \n",
    "    # Create a 4D tensor to hold the term c_g,n * c_X\n",
    "    # Shape will be (n1, n2, p, p)\n",
    "    product_tensor = torch.zeros((n1, n2, p, p), dtype=torch.complex64)\n",
    "    \n",
    "    # Define time lags based on the number of components\n",
    "    t_lags = torch.arange(p, dtype=torch.float32)\n",
    "    \n",
    "    # Using torch.meshgrid to create tensors for u1 and u2 to enable vectorized operations\n",
    "    u1_mesh, u2_mesh = torch.meshgrid(torch.arange(n1, dtype=torch.float32), torch.arange(n2, dtype=torch.float32), indexing='ij')\n",
    "    \n",
    "    for q in range(p):\n",
    "        for r in range(p):\n",
    "            # Temporal lag\n",
    "            t = t_lags[q] - t_lags[r]\n",
    "            \n",
    "            # Compute the windowed autocovariance product c_g,n * c_X\n",
    "            # The formula in the paper is a sum over u, so we're building the term for each u\n",
    "            product_tensor[:, :, q, r] = cn_bar(u1_mesh, u2_mesh, t, params, n1, n2)\n",
    "            \n",
    "    # Perform the 2D FFT on the spatial dimensions for each component pair\n",
    "    fft_result = torch.fft.fft2(product_tensor, dim=(-4, -3))\n",
    "    # fft_result shape is (n1, n2, p, p)\n",
    "\n",
    "    # Normalization factor from the paper\n",
    "    normalization_factor = (delta1 * delta2) / (2 * cmath.pi)**2\n",
    "\n",
    "    # delta 1 and delta 2 are dx1 dx2 in continuous integral. \n",
    "    \n",
    "    expected_periodogram_tensor = fft_result * normalization_factor\n",
    "\n",
    "    # Use torch.fft.fftshift\n",
    "    return expected_periodogram_tensor\n",
    "\n",
    "\n",
    "def generate_Jvector(tensor_list, lat_col=0, lon_col=1, val_col=2):\n",
    "    \"\"\"\n",
    "    Generates a 3D tensor of DFT vectors from a list of tensors.\n",
    "    \n",
    "    Args:\n",
    "        tensor_list (list): A list of tensors, where each tensor represents one\n",
    "                            multivariate component.\n",
    "        lat_col (int): Index of the latitude column.\n",
    "        lon_col (int): Index of the longitude column.\n",
    "        val_col (int): Index of the value column (e.g., 'laplacian').\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: A 3D tensor (n1, n2, p) of DFT vectors for all frequencies.\n",
    "    \"\"\"\n",
    "    p = len(tensor_list)\n",
    "    if p == 0:\n",
    "        return torch.empty(0, 0, 0)\n",
    "        \n",
    "    fft_results = []\n",
    "    \n",
    "    # Determine grid dimensions from the first tensor\n",
    "    unique_lats = torch.unique(tensor_list[0][:, lat_col])\n",
    "    unique_lons = torch.unique(tensor_list[0][:, lon_col])\n",
    "    n1 = len(unique_lats)\n",
    "    n2 = len(unique_lons)\n",
    "    \n",
    "    for tensor in tensor_list:\n",
    "        # Create a grid from the flattened tensor\n",
    "        # Map original lat/lon values to grid indices\n",
    "        lat_map = {lat.item(): i for i, lat in enumerate(unique_lats)}\n",
    "        lon_map = {lon.item(): i for i, lon in enumerate(unique_lons)}\n",
    "\n",
    "        # Use the maps to get indices and reshape\n",
    "        lat_indices = [lat_map[lat.item()] for lat in tensor[:, lat_col]]\n",
    "        lon_indices = [lon_map[lon.item()] for lon in tensor[:, lon_col]]\n",
    "        \n",
    "        # Create the grid\n",
    "        data_grid = torch.zeros((n1, n2), dtype=tensor.dtype)\n",
    "        data_grid[lat_indices, lon_indices] = tensor[:, val_col]\n",
    "\n",
    "        # Perform 2D FFT\n",
    "        fft_results.append(torch.fft.fft2(data_grid))\n",
    "    \n",
    "    # Stack the 2D FFT results into a 3D tensor (n1, n2, p)\n",
    "    J_vector_tensor = torch.stack(fft_results, dim=2)\n",
    "    \n",
    "    # Apply the normalization factor\n",
    "    normalization_factor = 1.0 / (2 * cmath.pi)\n",
    "    \n",
    "    return J_vector_tensor * normalization_factor\n",
    "\n",
    "def likelihood_vectorized(params, tensor_list):\n",
    "    \"\"\"\n",
    "    Calculates the negative log-likelihood for the multivariate model\n",
    "    using a vectorized approach for improved performance.\n",
    "    \n",
    "    Args:\n",
    "        params (list): Model parameters.\n",
    "        tensor_list (list): A list of tensors representing the multivariate data.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The negative log-likelihood value.\n",
    "    \"\"\"\n",
    "    p = len(tensor_list)\n",
    "    if p == 0:\n",
    "        return torch.tensor(0.0, dtype=torch.float32)\n",
    "    \n",
    "    # Determine dimensions from the first tensor\n",
    "    unique_lats = torch.unique(tensor_list[0][:, 0])\n",
    "    unique_lons = torch.unique(tensor_list[0][:, 1])\n",
    "    n1 = len(unique_lats)\n",
    "    n2 = len(unique_lons)\n",
    "    n = n1 * n2\n",
    "\n",
    "    # Compute the expected periodogram tensor from the model\n",
    "    expected_periodogram = expected_periodogram_fft_multivariate(params, n1, n2, p)\n",
    "    \n",
    "    # Generate the J-vector tensor from the data\n",
    "    j_vector = generate_Jvector(tensor_list)\n",
    "    \n",
    "    # Reshape tensors for batch processing\n",
    "    I_omega_batch = expected_periodogram.reshape(-1, p, p)\n",
    "    J_omega_batch = j_vector.reshape(-1, p, 1)\n",
    "\n",
    "    # Add a small value to the real part of the diagonal for stability\n",
    "    reg_term = torch.linalg.norm(I_omega_batch, dim=(1, 2), keepdim=True) * 1e-10 * torch.eye(p, dtype=I_omega_batch.dtype)\n",
    "    I_omega_stable = torch.real(I_omega_batch) + reg_term\n",
    "    \n",
    "    # Compute the log-determinant of each matrix in the batch\n",
    "    log_det = torch.log(torch.det(I_omega_stable)).real\n",
    "\n",
    "    # Compute the inverse for each matrix in the batch\n",
    "    inv_I_batch = torch.linalg.inv(I_omega_stable)\n",
    "    \n",
    "    # Compute the quadratic form: J^H * I^-1 * J\n",
    "    term2_temp = torch.bmm(torch.conj(J_omega_batch.transpose(1, 2)), inv_I_batch)\n",
    "    term2_batch = torch.bmm(term2_temp, J_omega_batch).reshape(-1)\n",
    "\n",
    "    # The sum of log(det(I)) and the trace term over all frequencies\n",
    "    nll_batch = log_det + term2_batch.real\n",
    "\n",
    "    # Sum up all the log-likelihood terms\n",
    "    nll = torch.sum(nll_batch)\n",
    "\n",
    "    # Normalize the total negative log-likelihood\n",
    "    return nll / n\n",
    "\n",
    "# --- Example Usage with tensor_day1 ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Assume tensor_day1 is your input tensor\n",
    "    # To test the multivariate function, we'll split it into multiple time slices\n",
    "    # Here, we'll create a list of 4 tensors, representing 4 time slices\n",
    "    \n",
    "    # Find unique time values\n",
    "    unique_times = torch.unique(tensor_day1[:, 3])\n",
    "    time_slices_list = []\n",
    "    \n",
    "    for t_val in unique_times:\n",
    "        time_slices_list.append(tensor_day1[tensor_day1[:, 3] == t_val])\n",
    "\n",
    "    print(f\"Created a list of {len(time_slices_list)} tensors for multivariate test.\")\n",
    "\n",
    "    # Define some sample parameters\n",
    "    params = torch.tensor([1.0, 1.0, 1.0, 0.01, 0.01, 0.01, 0.01], requires_grad=True)\n",
    "\n",
    "# Calculate the negative log-likelihood\n",
    "nll_value = likelihood_vectorized(params, time_slices_list)\n",
    "\n",
    "print(\"\\n--- Likelihood Calculation Complete ---\")\n",
    "print(f\"Negative Log-Likelihood: {nll_value.item()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7837cbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1/400 ---\n",
      " Loss: 32286718.0\n",
      " Gradients: tensor([-1.0500e+01,  2.5297e+02,  3.9000e+01, -1.0750e+01,  3.2000e+01,\n",
      "        -1.4188e+01, -1.6063e+07])\n",
      "--- Epoch 101/400 ---\n",
      " Loss: 16558423.0\n",
      " Gradients: tensor([-1.6028e+03,  4.2513e+05, -5.2451e+03,  4.7245e+04,  9.2020e+03,\n",
      "         1.1484e+04, -4.2256e+06])\n",
      "--- Epoch 201/400 ---\n",
      " Loss: 15958496.0\n",
      " Gradients: tensor([-1.1995e+03,  3.0010e+05, -3.8144e+03,  3.8039e+04,  6.4340e+03,\n",
      "         7.6995e+03, -3.9247e+06])\n",
      "--- Epoch 301/400 ---\n",
      " Loss: 15490834.0\n",
      " Gradients: tensor([-1.1348e+03,  2.8400e+05, -3.6118e+03,  3.6053e+04,  6.0795e+03,\n",
      "         7.2925e+03, -3.6980e+06])\n",
      "FINAL STATE: Epoch 400, Loss: 15087420.0, \n",
      " vecc Parameters: [21.688, 0.339, 2.632, 0.181, 0.296, 0.309, 4.295]\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "# use adpating lr\n",
    "def run_full(params, optimizer, scheduler,  covariance_function, epochs=10 ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Run the training loop for the full likelihood model.\n",
    "\n",
    "    Args:\n",
    "        params (torch.Tensor): Model parameters.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for updating parameters.\n",
    "        scheduler (torch.optim.lr_scheduler): Learning rate scheduler.\n",
    "        epochs (int): Number of epochs to train.\n",
    "\n",
    "    Returns:\n",
    "        list: Final parameters and loss.\n",
    "        int: Number of epochs run.\n",
    "    \"\"\"\n",
    "\n",
    "    prev_loss= float('inf')\n",
    "\n",
    "    tol = 1e-3  # Convergence tolerance\n",
    "    for epoch in range(epochs):  \n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        loss = likelihood_vectorized(params, time_slices_list)\n",
    "        loss.backward()  # Backpropagate th e loss\n",
    "\n",
    "        if epoch%100 == 0:\n",
    "                # --- Check and print gradients and parameters at each iteration ---\n",
    "            print(f'--- Epoch {epoch+1}/{epochs} ---')\n",
    "            print(f' Loss: {loss.item()}')\n",
    "            print(f' Parameters: {params.detach().numpy()}')\n",
    "            print(f' Gradients: {params.grad}')\n",
    "        \n",
    "    \n",
    "        # Check if loss is NaN or Inf, and break if it is\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"Loss became NaN or Inf at epoch {epoch+1}. Stopping training.\")\n",
    "            print(f'Final Loss: {loss.item()}')\n",
    "            print(f'Final Gradients: {params.grad}')\n",
    "            break\n",
    "\n",
    "        \n",
    "\n",
    "        # Print gradients and parameters every 10th epoch\n",
    "        #if epoch % 10 == 0:\n",
    "        #    print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "        \n",
    "        # if epoch % 500 == 0:\n",
    "        #     print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "        \n",
    "        optimizer.step()  \n",
    "        scheduler.step()  \n",
    "        # Check for convergence\n",
    "        if abs(prev_loss - loss.item()) < tol:\n",
    "            print(f\"Converged at epoch {epoch}\")\n",
    "            print(f'Epoch {epoch+1}, : Loss: {loss.item()}, \\n vecc Parameters: {params.detach().numpy()}')\n",
    "            break\n",
    "\n",
    "        prev_loss = loss.item()\n",
    "\n",
    "    params = [torch.round(x*1000).detach().numpy()/1000 for x in params]\n",
    "    loss = (torch.round(loss*1000)/1000).item()\n",
    "    print(f'FINAL STATE: Epoch {epoch+1}, Loss: {loss}, \\n vecc Parameters: {params}')\n",
    "    return params + [loss], epoch\n",
    "\n",
    "# Convert the list of floats to a PyTorch tensor\n",
    "params = torch.tensor([20.0, 2.0, 3.0, 0.01, -0.001, 0.01, 2.01], requires_grad=True)\n",
    "lr, betas, eps, step_size, gamma = 0.03, (0.9, 0.99), 1e-08, 40, 0.8\n",
    "optimizer = torch.optim.Adam([params], lr=lr, betas=betas, eps=eps)\n",
    "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)  #\n",
    "\n",
    "a,b = run_full( params, optimizer, scheduler, None, epochs=400 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20cb1ea",
   "metadata": {},
   "source": [
    "lr 0.01. step50 epoch 200\n",
    "\n",
    "Loss: 22706554.000000\n",
    "FINAL STATE: Epoch 200, Loss: 22706554.0, \n",
    " vecc Parameters: [21.136, 0.79, 2.358, 0.122, -0.097, 0.0, 2.859]\n",
    "\n",
    " lr 0.01 step 100 epoch 600  smalll change after epoch 300\n",
    "\n",
    " --- Epoch 1/600 ---\n",
    " Loss: 32286718.0\n",
    " Gradients: tensor([-1.0500e+01,  2.5297e+02,  3.9000e+01, -1.0750e+01,  3.2000e+01,\n",
    "        -1.4188e+01, -1.6063e+07])\n",
    "--- Epoch 101/600 ---\n",
    " Loss: 22401716.0\n",
    " Gradients: tensor([-2.4288e+02,  3.1184e+04, -8.0125e+02, -2.1475e+02,  9.0000e+00,\n",
    "        -9.9756e+00, -7.7329e+06])\n",
    "--- Epoch 201/600 ---\n",
    " Loss: 17031722.0\n",
    " Gradients: tensor([ 2.6182e+05,  1.0712e+05, -1.9464e+06,  1.2475e+07,  1.0135e+08,\n",
    "         8.4488e+06, -6.8022e+06])\n",
    "--- Epoch 301/600 ---\n",
    " Loss: 16734517.0\n",
    " Gradients: tensor([ 3.2254e+05,  1.4013e+04, -4.5413e+05,  6.2497e+06,  2.3328e+07,\n",
    "        -8.1825e+06, -6.9858e+06])\n",
    "--- Epoch 401/600 ---\n",
    " Loss: 16704657.0\n",
    " Gradients: tensor([   249943.7500,     10871.4238,   -217640.0000,   7583911.0000,\n",
    "           723587.0000, -10843837.0000,  -6504515.5000])\n",
    "--- Epoch 501/600 ---\n",
    " Loss: 16663540.0\n",
    " Gradients: tensor([ 2.0924e+05,  9.9438e+03, -2.5152e+05,  7.5038e+06,  5.0710e+06,\n",
    "        -1.1627e+07, -6.2263e+06])\n",
    "FINAL STATE: Epoch 600, Loss: 16630042.0, \n",
    " vecc Parameters: [21.645, -0.011, 3.064, -0.004, 0.007, 0.007, 3.407]\n",
    "\n",
    " lr 0.03 50 600\n",
    "\n",
    " --- Epoch 1/600 ---\n",
    " Loss: 32286718.0\n",
    " Gradients: tensor([-1.0500e+01,  2.5297e+02,  3.9000e+01, -1.0750e+01,  3.2000e+01,\n",
    "        -1.4188e+01, -1.6063e+07])\n",
    "--- Epoch 101/600 ---\n",
    " Loss: 17367230.0\n",
    " Gradients: tensor([    59566.8125, -20491166.0000,    -45886.5000, -17058900.0000,\n",
    "         -2243651.7500,    468761.2500,  -5089638.0000])\n",
    "--- Epoch 201/600 ---\n",
    " Loss: 16874694.0\n",
    " Gradients: tensor([   -8683.5625,  2789644.5000,   -95100.8750,  1474869.8750,\n",
    "          567344.0000,   178033.7188, -4405246.0000])\n",
    "--- Epoch 301/600 ---\n",
    " Loss: 16666024.0\n",
    " Gradients: tensor([   -7147.0938,  2270397.0000,   -86895.5000,   936746.6875,\n",
    "          494532.1250,   173081.5625, -4305279.0000])\n",
    "--- Epoch 401/600 ---\n",
    " Loss: 16588262.0\n",
    " Gradients: tensor([   -6682.1250,  2102722.5000,   -84959.6875,   796439.0000,\n",
    "          478694.8750,   173921.0000, -4267694.5000])\n",
    "--- Epoch 501/600 ---\n",
    " Loss: 16561779.0\n",
    " Gradients: tensor([   -6450.6562,  2013594.0000,   -84132.5000,   725261.8750,\n",
    "          471525.3750,   174381.5938, -4255359.0000])\n",
    "FINAL STATE: Epoch 600, Loss: 16553778.0, \n",
    " vecc Parameters: [21.828, 0.184, 2.685, 0.073, 0.083, -0.104, 3.861]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "422a3dd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "likelihood_vectorized() missing 1 required positional argument: 'tensor_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFINAL STATE: Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m final_params, final_loss, epoch\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m a,b,c = \u001b[43mrun_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlikelihood_vectorized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mrun_full\u001b[39m\u001b[34m(params, optimizer, scheduler, likelihood_function, epochs)\u001b[39m\n\u001b[32m     31\u001b[39m optimizer.zero_grad()  \n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Calculate loss using the provided likelihood function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m loss = \u001b[43mlikelihood_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m loss.backward()  \u001b[38;5;66;03m# Backpropagate the loss\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# --- Check and print gradients and parameters at each iteration ---\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: likelihood_vectorized() missing 1 required positional argument: 'tensor_list'"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "# Convert the list of floats to a PyTorch tensor\n",
    "params = torch.tensor([1.0, 1.0, 1.0, 0.01, 0.01, 0.01, 0.01], requires_grad=True)\n",
    "lr, betas, eps, step_size, gamma = 0.1, (0.9, 0.99), 1e-08, 50, 0.5\n",
    "optimizer = torch.optim.Adam([params], lr=lr, betas=betas, eps=eps)\n",
    "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)  #\n",
    "\n",
    "def run_full(params, optimizer, scheduler, likelihood_function, epochs=10):\n",
    "    \"\"\"\n",
    "    Run the training loop for the full likelihood model.\n",
    "\n",
    "    Args:\n",
    "        params (torch.Tensor): Model parameters.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for updating parameters.\n",
    "        scheduler (torch.optim.lr_scheduler): Learning rate scheduler.\n",
    "        likelihood_function (callable): The likelihood function to be optimized.\n",
    "        epochs (int): Number of epochs to train.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Final parameters and loss, and the number of epochs run.\n",
    "    \"\"\"\n",
    "    prev_loss = float('inf')\n",
    "    tol = 1e-3  # Convergence tolerance\n",
    "    \n",
    "    # Store history for plotting\n",
    "    loss_history = []\n",
    "    param_history = []\n",
    "    grad_history = []\n",
    "\n",
    "    for epoch in range(epochs):  \n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        # Calculate loss using the provided likelihood function\n",
    "        loss = likelihood_function(params)\n",
    "        loss.backward()  # Backpropagate the loss\n",
    "        \n",
    "        # --- Check and print gradients and parameters at each iteration ---\n",
    "        print(f'--- Epoch {epoch+1}/{epochs} ---')\n",
    "        \n",
    "        # Check if loss is NaN or Inf, and break if it is\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"Loss became NaN or Inf at epoch {epoch+1}. Stopping training.\")\n",
    "            print(f'Final Loss: {loss.item()}')\n",
    "            print(f'Final Gradients: {params.grad}')\n",
    "            print(f'Final Parameters: {params.detach()}')\n",
    "            break\n",
    "\n",
    "        print(f'Loss: {loss.item():.6f}')\n",
    "        print(f'Parameters: {params.detach().numpy()}')\n",
    "        if params.grad is not None:\n",
    "            print(f'Gradients: {params.grad.numpy()}')\n",
    "            \n",
    "            # Check for NaN gradients\n",
    "            if torch.isnan(params.grad).any() or torch.isinf(params.grad).any():\n",
    "                print(f\"Gradients became NaN or Inf at epoch {epoch+1}. Stopping training.\")\n",
    "                break\n",
    "        else:\n",
    "            print('Gradients: Not computed yet (first epoch).')\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()  \n",
    "        scheduler.step()  \n",
    "        \n",
    "        # Check for convergence\n",
    "        if abs(prev_loss - loss.item()) < tol:\n",
    "            print(f\"\\nConverged at epoch {epoch+1}\")\n",
    "            print(f'Final State: Loss: {loss.item():.6f}, \\n Parameters: {params.detach().numpy()}')\n",
    "            break\n",
    "\n",
    "        prev_loss = loss.item()\n",
    "\n",
    "    # Final print and return\n",
    "    final_loss = loss.item()\n",
    "    final_params = params.detach().numpy()\n",
    "    \n",
    "    print(f'\\nFINAL STATE: Epoch {epoch+1}, Loss: {final_loss:.6f}, \\n Parameters: {final_params}')\n",
    "    \n",
    "    return final_params, final_loss, epoch\n",
    "\n",
    "a,b,c = run_full( params, optimizer, scheduler, likelihood_vectorized, epochs= 50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966de346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
