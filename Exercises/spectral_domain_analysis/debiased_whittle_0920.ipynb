{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c56dee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "# Add your custom path\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "import logging\n",
    "import argparse # Argument parsing\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import copy                    # clone tensor\n",
    "import time\n",
    "\n",
    "# Custom imports\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels\n",
    "\n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings \n",
    "from GEMS_TCO import load_data\n",
    "from GEMS_TCO import alg_optimization, alg_opt_Encoder\n",
    "from GEMS_TCO import configuration as config\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "import typer\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "from GEMS_TCO import configuration as config\n",
    "from GEMS_TCO import data_preprocess as dmbh\n",
    "\n",
    "import os\n",
    "from sklearn.neighbors import BallTree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6f300d",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96dfc8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/joonwonlee/Documents/GEMS_DATA/pickle_2024/orbit_map24_07.pkl\n"
     ]
    }
   ],
   "source": [
    "lon_s = 123\n",
    "lon_e = 133\n",
    "step_lat = 0.044\n",
    "step_lon = 0.063\n",
    "\n",
    "lat_coords = np.arange( 5 -0.044- 0.0002, 0 -0.044, -0.044)\n",
    "lon_coords = np.arange( lon_e-step_lon- 0.0002, lon_s-step_lon, -step_lon)\n",
    "\n",
    "# Apply the shift as in the original code\n",
    "# These are the unique lat/lon values for the \"center_points\" grid\n",
    "final_lat_values = lat_coords + step_lat \n",
    "final_lon_values = lon_coords + step_lon \n",
    "\n",
    "# Create 2D grid with broadcasting\n",
    "#decrement = 0.00012\n",
    "decrement = 0 \n",
    "lat_grid = final_lat_values[:, None] + np.arange(len(final_lon_values)) * decrement  # shape: (228, 152)\n",
    "\n",
    "\n",
    "mac_data_path = config.mac_data_load_path\n",
    "years = [2024]  # years = [2023,2024]\n",
    "months = list( range(7,8))\n",
    "year = years[0]\n",
    "month = months[0]\n",
    "month_str = f\"{month:02d}\"  \n",
    "filename = f\"pickle_2024/orbit_map{str(year)[2:]}_{month_str}.pkl\"\n",
    "picklefile_path = Path(mac_data_path) / filename\n",
    "print(picklefile_path)\n",
    "\n",
    "with open(picklefile_path, 'rb') as pickle_file:\n",
    "    data_map_hour = pickle.load(pickle_file)\n",
    "\n",
    "# Base file path and settings\n",
    "# base_path = \"C:\\\\Users\\\\joonw\\\\TCO\\\\GEMS_data\"    MSI notebook\n",
    "\n",
    "mac_data_path = config.mac_data_load_path\n",
    "lat_start, lat_end, lon_start, lon_end = 0, 5, 123, 133\n",
    "step_lat, step_lon = 0.044, 0.063\n",
    "\n",
    "# df = pd.read_csv(\"C:\\\\Users\\\\joonw\\\\TCO\\\\GEMS_data\\\\data_2024\\\\data_24_07_0131_N510_E110120.csv\")  MSI notebook\n",
    "df = pd.read_csv(\"/Users/joonwonlee/Documents/GEMS_DATA/data_2024/data_24_07_0131_N05_E123133.csv\")  # MAC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b28c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "instance = dmbh.center_matching_hour(df, lat_start, lat_end, lon_start, lon_end)\n",
    "\n",
    "for year in years:        # years = [2023,2024]\n",
    "    for month in months:  \n",
    "        try:\n",
    "            # load pickle (dense ORI data)\n",
    "            pickle_path = os.path.join(mac_data_path, f'pickle_{year}')\n",
    "            input_filename = f\"orbit_map{str(year)[2:]}_{month_str}.pkl\"\n",
    "            input_filepath = os.path.join(pickle_path, input_filename)\n",
    "            with open(input_filepath, 'rb') as pickle_file:\n",
    "                loaded_map = pickle.load(pickle_file)\n",
    "            center_points = instance.make_center_points_wo_calibration(step_lat = step_lat, step_lon= step_lon)\n",
    "            coarse_cen_map = instance.coarse_by_center(loaded_map, center_points)\n",
    "\n",
    "            # Save pickle (coarse data)\n",
    "            output_filename = f\"coarse_cen_map_without_decrement_latitude{str(year)[2:]}_{month_str}.pkl\"\n",
    "            output_filepath = os.path.join(pickle_path, output_filename)\n",
    "            with open(output_filepath, 'wb') as pickle_file:\n",
    "                pickle.dump(coarse_cen_map, pickle_file)\n",
    "            \n",
    "            print(f\"Successfully processed and saved data for year {str(year)[2:]} month {month_str}.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File {input_filename} not found. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {input_filename}: {e}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20317e3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type '<class 'torch.Tensor'>'; only Series and DataFrame objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m df_day_list = []\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m31\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     _, cur_df =\u001b[43mload_data_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_working_data_byday_wo_mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbmap_ori\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     df_day_list.append( cur_df )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GEMS_TCO-1/src/GEMS_TCO/__init__.py:211\u001b[39m, in \u001b[36mload_data.load_working_data_byday_wo_mm\u001b[39m\u001b[34m(self, coarse_dicts, idx_for_datamap)\u001b[39m\n\u001b[32m    208\u001b[39m     tmp = torch.from_numpy(tmp).double() \n\u001b[32m    210\u001b[39m     analysis_data_map[key_idx[i]] = tmp\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     aggregated_data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43maggregated_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m analysis_data_map, aggregated_data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/pandas/core/reshape/concat.py:382\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m op = \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/pandas/core/reshape/concat.py:448\u001b[39m, in \u001b[36m_Concatenator.__init__\u001b[39m\u001b[34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[39m\n\u001b[32m    445\u001b[39m objs, keys = \u001b[38;5;28mself\u001b[39m._clean_keys_and_objs(objs, keys)\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m ndims = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_ndims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m sample, objs = \u001b[38;5;28mself\u001b[39m._get_sample_object(objs, ndims, keys, names, levels)\n\u001b[32m    451\u001b[39m \u001b[38;5;66;03m# Standardize axis parameter to int\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/faiss_env/lib/python3.12/site-packages/pandas/core/reshape/concat.py:489\u001b[39m, in \u001b[36m_Concatenator._get_ndims\u001b[39m\u001b[34m(self, objs)\u001b[39m\n\u001b[32m    484\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (ABCSeries, ABCDataFrame)):\n\u001b[32m    485\u001b[39m         msg = (\n\u001b[32m    486\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcannot concatenate object of type \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    487\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33monly Series and DataFrame objs are valid\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    488\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m    491\u001b[39m     ndims.add(obj.ndim)\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ndims\n",
      "\u001b[31mTypeError\u001b[39m: cannot concatenate object of type '<class 'torch.Tensor'>'; only Series and DataFrame objs are valid"
     ]
    }
   ],
   "source": [
    "pickle_path = os.path.join(mac_data_path, f'pickle_{year}')\n",
    "output_filename = f\"coarse_cen_map_without_decrement_latitude{str(year)[2:]}_{month_str}.pkl\"\n",
    "output_filepath = os.path.join(pickle_path, output_filename)\n",
    "\n",
    "with open(output_filepath, 'rb') as pickle_file:\n",
    "    cbmap_ori = pickle.load(pickle_file)\n",
    "\n",
    "\n",
    "load_data_instance = GEMS_TCO.load_data('')\n",
    "\n",
    "df_day_list = []\n",
    "for i in range(31):\n",
    "    _, cur_df =load_data_instance.load_working_data_byday_wo_mm(cbmap_ori,[i*8, (i+1)*8])\n",
    "    df_day_list.append( cur_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8c37393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18126, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_day_list[0]['y24m07day01_hm00:52'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
