{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "# sys.path.append(gems_tco_path)\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings\n",
    "from GEMS_TCO import load_data_local_computer\n",
    "\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import copy                    # clone tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_resolution = [10,10]\n",
    "day = 5\n",
    "mm_cond_number = 20\n",
    "\n",
    "years = ['2024']\n",
    "month_range =[7,8]\n",
    "idx_for_datamap= [ 8*(day-1),8*day]\n",
    "\n",
    "instance = load_data_local_computer()\n",
    "map, ord_mm, nns_map= instance.load_mm20k_data_bymonthyear( lat_lon_resolution= lat_lon_resolution, mm_cond_number=mm_cond_number,years_=years, months_=month_range)\n",
    "analysis_data_map, aggregated_data = instance.load_working_data_byday( map, ord_mm, nns_map, idx_for_datamap=[0,8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5.0250, 110.0250, 273.3987,  21.0000],\n",
       "        [  5.0250, 119.5250, 262.7728,  21.0000],\n",
       "        [  9.5250, 110.5250, 267.5342,  21.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_data[ list([1,2,3]),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quick load from amarel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              sigmasq  range_lat  range_lon  advec_lat  advec_lon      beta  \\\n",
      "2024-07-01  24.793444   1.584529   1.718248   0.009089  -0.107299  0.131038   \n",
      "2024-07-02  24.424301   1.997055   1.942683   0.043588  -0.072679  0.137124   \n",
      "2024-07-03  26.009497   1.215236   1.558868   0.023392  -0.150548  0.199850   \n",
      "2024-07-04  24.701347   1.612308   1.822960  -0.164069  -0.237443  0.131595   \n",
      "2024-07-05  22.598671   2.901185   3.722327  -0.011729  -0.152072  0.072866   \n",
      "\n",
      "              nugget          loss  \n",
      "2024-07-01  2.717239  14068.529297  \n",
      "2024-07-02  1.513148  12357.715820  \n",
      "2024-07-03  2.890678  14948.140625  \n",
      "2024-07-04  3.636499  14786.204102  \n",
      "2024-07-05  2.397249  12096.261719  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sigmasq</th>\n",
       "      <th>range_lat</th>\n",
       "      <th>range_lon</th>\n",
       "      <th>advec_lat</th>\n",
       "      <th>advec_lon</th>\n",
       "      <th>beta</th>\n",
       "      <th>nugget</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-07-01</th>\n",
       "      <td>24.793444</td>\n",
       "      <td>1.584529</td>\n",
       "      <td>1.718248</td>\n",
       "      <td>0.009089</td>\n",
       "      <td>-0.107299</td>\n",
       "      <td>0.131038</td>\n",
       "      <td>2.717239</td>\n",
       "      <td>14068.529297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-02</th>\n",
       "      <td>24.424301</td>\n",
       "      <td>1.997055</td>\n",
       "      <td>1.942683</td>\n",
       "      <td>0.043588</td>\n",
       "      <td>-0.072679</td>\n",
       "      <td>0.137124</td>\n",
       "      <td>1.513148</td>\n",
       "      <td>12357.715820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-03</th>\n",
       "      <td>26.009497</td>\n",
       "      <td>1.215236</td>\n",
       "      <td>1.558868</td>\n",
       "      <td>0.023392</td>\n",
       "      <td>-0.150548</td>\n",
       "      <td>0.199850</td>\n",
       "      <td>2.890678</td>\n",
       "      <td>14948.140625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-04</th>\n",
       "      <td>24.701347</td>\n",
       "      <td>1.612308</td>\n",
       "      <td>1.822960</td>\n",
       "      <td>-0.164069</td>\n",
       "      <td>-0.237443</td>\n",
       "      <td>0.131595</td>\n",
       "      <td>3.636499</td>\n",
       "      <td>14786.204102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-05</th>\n",
       "      <td>22.598671</td>\n",
       "      <td>2.901185</td>\n",
       "      <td>3.722327</td>\n",
       "      <td>-0.011729</td>\n",
       "      <td>-0.152072</td>\n",
       "      <td>0.072866</td>\n",
       "      <td>2.397249</td>\n",
       "      <td>12096.261719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-06</th>\n",
       "      <td>25.594908</td>\n",
       "      <td>1.702692</td>\n",
       "      <td>2.255174</td>\n",
       "      <td>0.017462</td>\n",
       "      <td>-0.158125</td>\n",
       "      <td>0.098684</td>\n",
       "      <td>3.850205</td>\n",
       "      <td>14690.248047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-07</th>\n",
       "      <td>26.030510</td>\n",
       "      <td>1.261084</td>\n",
       "      <td>2.831952</td>\n",
       "      <td>0.054831</td>\n",
       "      <td>-0.343255</td>\n",
       "      <td>0.103045</td>\n",
       "      <td>4.596346</td>\n",
       "      <td>15342.459961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-08</th>\n",
       "      <td>26.043682</td>\n",
       "      <td>0.995279</td>\n",
       "      <td>1.629503</td>\n",
       "      <td>-0.019824</td>\n",
       "      <td>-0.411626</td>\n",
       "      <td>0.164296</td>\n",
       "      <td>2.751402</td>\n",
       "      <td>14857.195312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-09</th>\n",
       "      <td>24.052071</td>\n",
       "      <td>1.377774</td>\n",
       "      <td>2.357721</td>\n",
       "      <td>0.021439</td>\n",
       "      <td>-0.220316</td>\n",
       "      <td>0.142847</td>\n",
       "      <td>1.675457</td>\n",
       "      <td>12666.991211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-10</th>\n",
       "      <td>25.766109</td>\n",
       "      <td>1.392051</td>\n",
       "      <td>2.358171</td>\n",
       "      <td>0.026684</td>\n",
       "      <td>-0.077366</td>\n",
       "      <td>0.150648</td>\n",
       "      <td>3.821218</td>\n",
       "      <td>14987.769531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-11</th>\n",
       "      <td>23.945438</td>\n",
       "      <td>1.490333</td>\n",
       "      <td>2.470762</td>\n",
       "      <td>-0.009915</td>\n",
       "      <td>0.027429</td>\n",
       "      <td>0.137959</td>\n",
       "      <td>2.066264</td>\n",
       "      <td>13000.419922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-12</th>\n",
       "      <td>23.036034</td>\n",
       "      <td>2.299998</td>\n",
       "      <td>3.346955</td>\n",
       "      <td>-0.054281</td>\n",
       "      <td>0.114976</td>\n",
       "      <td>0.110155</td>\n",
       "      <td>1.604898</td>\n",
       "      <td>11485.496094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-13</th>\n",
       "      <td>22.790960</td>\n",
       "      <td>2.072518</td>\n",
       "      <td>3.616723</td>\n",
       "      <td>-0.130206</td>\n",
       "      <td>0.076944</td>\n",
       "      <td>0.135628</td>\n",
       "      <td>1.441895</td>\n",
       "      <td>11315.873047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-14</th>\n",
       "      <td>24.079025</td>\n",
       "      <td>2.077914</td>\n",
       "      <td>2.578654</td>\n",
       "      <td>-0.035028</td>\n",
       "      <td>0.072091</td>\n",
       "      <td>0.144720</td>\n",
       "      <td>2.405799</td>\n",
       "      <td>13138.958984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-15</th>\n",
       "      <td>22.556171</td>\n",
       "      <td>3.047949</td>\n",
       "      <td>3.821722</td>\n",
       "      <td>-0.051073</td>\n",
       "      <td>0.067158</td>\n",
       "      <td>0.109084</td>\n",
       "      <td>1.462631</td>\n",
       "      <td>10808.830078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-16</th>\n",
       "      <td>23.403471</td>\n",
       "      <td>2.888016</td>\n",
       "      <td>3.056899</td>\n",
       "      <td>-0.004253</td>\n",
       "      <td>0.005845</td>\n",
       "      <td>0.104761</td>\n",
       "      <td>2.019670</td>\n",
       "      <td>12012.943359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-17</th>\n",
       "      <td>24.978308</td>\n",
       "      <td>1.371159</td>\n",
       "      <td>2.236580</td>\n",
       "      <td>-0.068871</td>\n",
       "      <td>-0.126589</td>\n",
       "      <td>0.137412</td>\n",
       "      <td>3.044259</td>\n",
       "      <td>14286.230469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-18</th>\n",
       "      <td>23.328363</td>\n",
       "      <td>1.295417</td>\n",
       "      <td>3.319158</td>\n",
       "      <td>-0.079007</td>\n",
       "      <td>-0.109866</td>\n",
       "      <td>0.131408</td>\n",
       "      <td>2.737964</td>\n",
       "      <td>13417.033203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-19</th>\n",
       "      <td>23.913704</td>\n",
       "      <td>1.824143</td>\n",
       "      <td>2.503119</td>\n",
       "      <td>0.020213</td>\n",
       "      <td>0.016007</td>\n",
       "      <td>0.142548</td>\n",
       "      <td>2.095682</td>\n",
       "      <td>12876.714844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-20</th>\n",
       "      <td>23.171667</td>\n",
       "      <td>2.521096</td>\n",
       "      <td>3.594732</td>\n",
       "      <td>0.032805</td>\n",
       "      <td>-0.026624</td>\n",
       "      <td>0.092923</td>\n",
       "      <td>2.846266</td>\n",
       "      <td>12944.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-21</th>\n",
       "      <td>23.972263</td>\n",
       "      <td>2.328973</td>\n",
       "      <td>3.350626</td>\n",
       "      <td>-0.002169</td>\n",
       "      <td>-0.070489</td>\n",
       "      <td>0.109454</td>\n",
       "      <td>2.806951</td>\n",
       "      <td>13142.653320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-22</th>\n",
       "      <td>23.484762</td>\n",
       "      <td>1.773483</td>\n",
       "      <td>3.144358</td>\n",
       "      <td>0.106800</td>\n",
       "      <td>-0.146150</td>\n",
       "      <td>0.170165</td>\n",
       "      <td>2.292179</td>\n",
       "      <td>12951.943359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-23</th>\n",
       "      <td>22.399940</td>\n",
       "      <td>2.525347</td>\n",
       "      <td>3.945889</td>\n",
       "      <td>-0.004455</td>\n",
       "      <td>0.073785</td>\n",
       "      <td>0.144858</td>\n",
       "      <td>1.340997</td>\n",
       "      <td>10006.291016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-24</th>\n",
       "      <td>22.485428</td>\n",
       "      <td>1.960177</td>\n",
       "      <td>3.856450</td>\n",
       "      <td>0.042581</td>\n",
       "      <td>-0.149502</td>\n",
       "      <td>0.134382</td>\n",
       "      <td>1.423066</td>\n",
       "      <td>11153.117188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-25</th>\n",
       "      <td>22.398106</td>\n",
       "      <td>3.968451</td>\n",
       "      <td>3.945307</td>\n",
       "      <td>0.006230</td>\n",
       "      <td>-0.013954</td>\n",
       "      <td>0.047208</td>\n",
       "      <td>1.345503</td>\n",
       "      <td>8595.869141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-26</th>\n",
       "      <td>22.393942</td>\n",
       "      <td>2.544035</td>\n",
       "      <td>3.943803</td>\n",
       "      <td>-0.004113</td>\n",
       "      <td>0.031536</td>\n",
       "      <td>0.084661</td>\n",
       "      <td>1.334278</td>\n",
       "      <td>10097.978516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-27</th>\n",
       "      <td>22.484076</td>\n",
       "      <td>2.263680</td>\n",
       "      <td>3.848698</td>\n",
       "      <td>-0.026536</td>\n",
       "      <td>-0.070513</td>\n",
       "      <td>0.085162</td>\n",
       "      <td>1.953647</td>\n",
       "      <td>11613.490234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-28</th>\n",
       "      <td>22.687857</td>\n",
       "      <td>1.915375</td>\n",
       "      <td>3.615973</td>\n",
       "      <td>-0.024074</td>\n",
       "      <td>-0.078118</td>\n",
       "      <td>0.091293</td>\n",
       "      <td>2.331802</td>\n",
       "      <td>12408.534180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-29</th>\n",
       "      <td>22.405510</td>\n",
       "      <td>2.753298</td>\n",
       "      <td>3.919266</td>\n",
       "      <td>0.003522</td>\n",
       "      <td>-0.051619</td>\n",
       "      <td>0.069307</td>\n",
       "      <td>3.181403</td>\n",
       "      <td>12968.927734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-30</th>\n",
       "      <td>23.821211</td>\n",
       "      <td>2.505870</td>\n",
       "      <td>3.378460</td>\n",
       "      <td>-0.030410</td>\n",
       "      <td>-0.199047</td>\n",
       "      <td>0.127340</td>\n",
       "      <td>2.945880</td>\n",
       "      <td>13282.449219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-31</th>\n",
       "      <td>24.262573</td>\n",
       "      <td>3.082172</td>\n",
       "      <td>2.880464</td>\n",
       "      <td>0.059405</td>\n",
       "      <td>-0.190543</td>\n",
       "      <td>0.197513</td>\n",
       "      <td>5.303008</td>\n",
       "      <td>15539.535156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sigmasq  range_lat  range_lon  advec_lat  advec_lon      beta  \\\n",
       "2024-07-01  24.793444   1.584529   1.718248   0.009089  -0.107299  0.131038   \n",
       "2024-07-02  24.424301   1.997055   1.942683   0.043588  -0.072679  0.137124   \n",
       "2024-07-03  26.009497   1.215236   1.558868   0.023392  -0.150548  0.199850   \n",
       "2024-07-04  24.701347   1.612308   1.822960  -0.164069  -0.237443  0.131595   \n",
       "2024-07-05  22.598671   2.901185   3.722327  -0.011729  -0.152072  0.072866   \n",
       "2024-07-06  25.594908   1.702692   2.255174   0.017462  -0.158125  0.098684   \n",
       "2024-07-07  26.030510   1.261084   2.831952   0.054831  -0.343255  0.103045   \n",
       "2024-07-08  26.043682   0.995279   1.629503  -0.019824  -0.411626  0.164296   \n",
       "2024-07-09  24.052071   1.377774   2.357721   0.021439  -0.220316  0.142847   \n",
       "2024-07-10  25.766109   1.392051   2.358171   0.026684  -0.077366  0.150648   \n",
       "2024-07-11  23.945438   1.490333   2.470762  -0.009915   0.027429  0.137959   \n",
       "2024-07-12  23.036034   2.299998   3.346955  -0.054281   0.114976  0.110155   \n",
       "2024-07-13  22.790960   2.072518   3.616723  -0.130206   0.076944  0.135628   \n",
       "2024-07-14  24.079025   2.077914   2.578654  -0.035028   0.072091  0.144720   \n",
       "2024-07-15  22.556171   3.047949   3.821722  -0.051073   0.067158  0.109084   \n",
       "2024-07-16  23.403471   2.888016   3.056899  -0.004253   0.005845  0.104761   \n",
       "2024-07-17  24.978308   1.371159   2.236580  -0.068871  -0.126589  0.137412   \n",
       "2024-07-18  23.328363   1.295417   3.319158  -0.079007  -0.109866  0.131408   \n",
       "2024-07-19  23.913704   1.824143   2.503119   0.020213   0.016007  0.142548   \n",
       "2024-07-20  23.171667   2.521096   3.594732   0.032805  -0.026624  0.092923   \n",
       "2024-07-21  23.972263   2.328973   3.350626  -0.002169  -0.070489  0.109454   \n",
       "2024-07-22  23.484762   1.773483   3.144358   0.106800  -0.146150  0.170165   \n",
       "2024-07-23  22.399940   2.525347   3.945889  -0.004455   0.073785  0.144858   \n",
       "2024-07-24  22.485428   1.960177   3.856450   0.042581  -0.149502  0.134382   \n",
       "2024-07-25  22.398106   3.968451   3.945307   0.006230  -0.013954  0.047208   \n",
       "2024-07-26  22.393942   2.544035   3.943803  -0.004113   0.031536  0.084661   \n",
       "2024-07-27  22.484076   2.263680   3.848698  -0.026536  -0.070513  0.085162   \n",
       "2024-07-28  22.687857   1.915375   3.615973  -0.024074  -0.078118  0.091293   \n",
       "2024-07-29  22.405510   2.753298   3.919266   0.003522  -0.051619  0.069307   \n",
       "2024-07-30  23.821211   2.505870   3.378460  -0.030410  -0.199047  0.127340   \n",
       "2024-07-31  24.262573   3.082172   2.880464   0.059405  -0.190543  0.197513   \n",
       "\n",
       "              nugget          loss  \n",
       "2024-07-01  2.717239  14068.529297  \n",
       "2024-07-02  1.513148  12357.715820  \n",
       "2024-07-03  2.890678  14948.140625  \n",
       "2024-07-04  3.636499  14786.204102  \n",
       "2024-07-05  2.397249  12096.261719  \n",
       "2024-07-06  3.850205  14690.248047  \n",
       "2024-07-07  4.596346  15342.459961  \n",
       "2024-07-08  2.751402  14857.195312  \n",
       "2024-07-09  1.675457  12666.991211  \n",
       "2024-07-10  3.821218  14987.769531  \n",
       "2024-07-11  2.066264  13000.419922  \n",
       "2024-07-12  1.604898  11485.496094  \n",
       "2024-07-13  1.441895  11315.873047  \n",
       "2024-07-14  2.405799  13138.958984  \n",
       "2024-07-15  1.462631  10808.830078  \n",
       "2024-07-16  2.019670  12012.943359  \n",
       "2024-07-17  3.044259  14286.230469  \n",
       "2024-07-18  2.737964  13417.033203  \n",
       "2024-07-19  2.095682  12876.714844  \n",
       "2024-07-20  2.846266  12944.312500  \n",
       "2024-07-21  2.806951  13142.653320  \n",
       "2024-07-22  2.292179  12951.943359  \n",
       "2024-07-23  1.340997  10006.291016  \n",
       "2024-07-24  1.423066  11153.117188  \n",
       "2024-07-25  1.345503   8595.869141  \n",
       "2024-07-26  1.334278  10097.978516  \n",
       "2024-07-27  1.953647  11613.490234  \n",
       "2024-07-28  2.331802  12408.534180  \n",
       "2024-07-29  3.181403  12968.927734  \n",
       "2024-07-30  2.945880  13282.449219  \n",
       "2024-07-31  5.303008  15539.535156  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/Exercises/st_model/estimates\"\n",
    "\n",
    "# input_filename = \"vecc_extra_estimates_50_july24.pkl\"\n",
    "# input_filename = \"vecc_inter_estimates_1250_july24.pkl\"\n",
    "\n",
    "input_filename = \"vecc_inter_estimates_5000_july24.pkl\"\n",
    "# input_filename = \"estimation_200_july24.pkl\"\n",
    "input_filename = \"full_estimation_1250_july24.pkl\"\n",
    "input_filepath = os.path.join(input_path, input_filename)\n",
    "# Load pickle\n",
    "with open(input_filepath, 'rb') as pickle_file:\n",
    "    amarel_map1250= pickle.load(pickle_file)\n",
    "\n",
    "# Assuming df_1250 is your DataFrame\n",
    "df_1250 = pd.DataFrame()\n",
    "for key in amarel_map1250:\n",
    "    tmp = pd.DataFrame(amarel_map1250[key][0].reshape(1, -1), columns=['sigmasq', 'range_lat', 'range_lon', 'advec_lat', 'advec_lon', 'beta', 'nugget'])\n",
    "    tmp['loss'] = amarel_map1250[key][1]\n",
    "    df_1250 = pd.concat((df_1250, tmp), axis=0)\n",
    "\n",
    "# Generate date range\n",
    "date_range = pd.date_range(start='07-01-24', end='07-31-24')\n",
    "\n",
    "# Ensure the number of dates matches the number of rows in df_1250\n",
    "if len(date_range) == len(df_1250):\n",
    "    df_1250.index = date_range\n",
    "else:\n",
    "    print(\"The number of dates does not match the number of rows in the DataFrame.\")\n",
    "\n",
    "print(df_1250.head())\n",
    "df = df_1250\n",
    "# Save DataFrame to CSV\n",
    "output_filename = 'vecchia_inter_estimates_1250_july24.csv'\n",
    "output_csv_path = os.path.join(input_path, output_filename)\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization vecchia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Gradients: [-8.3266163e-01 -3.9050370e-01 -2.8237242e-01 -6.4676173e-04\n",
      " -3.2409048e-03  4.5490393e-01 -4.9839094e-01]\n",
      " Loss: 2548.8592623141867, Parameters: [ 2.51785984e+01  2.00787091e+00  2.10355711e+00  1.28148985e-03\n",
      " -7.98523873e-02  1.08808674e-01  3.60232282e+00]\n",
      "Converged at epoch 1\n",
      "Epoch 2, Gradients: [-8.3268803e-01 -3.9018026e-01 -2.8215161e-01 -6.4333249e-04\n",
      " -3.2253459e-03  4.5356691e-01 -4.9834314e-01]\n",
      " Loss: 2548.859166043867, full Parameters: [ 2.51787128e+01  2.00794053e+00  2.10362983e+00  1.28198182e-03\n",
      " -7.98518509e-02  1.08804606e-01  3.60241866e+00]\n",
      "FINAL STATE: Epoch 2, Gradients: [-8.3268803e-01 -3.9018026e-01 -2.8215161e-01 -6.4333249e-04\n",
      " -3.2253459e-03  4.5356691e-01 -4.9834314e-01]\n",
      " Loss: 2548.859166043867, full Parameters: [ 2.51787128e+01  2.00794053e+00  2.10362983e+00  1.28198182e-03\n",
      " -7.98518509e-02  1.08804606e-01  3.60241866e+00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 2.51787128e+01,  2.00794053e+00,  2.10362983e+00,  1.28198182e-03,\n",
       "        -7.98518509e-02,  1.08804606e-01,  3.60241866e+00], dtype=float32),\n",
       " 2548.859166043867]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance.run_full(params, optimizer, scheduler, instance.matern_cov_anisotropy_v05, epochs=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimize\n",
    "\n",
    "```    srun --cpus-per-task=3 --mem=5G --time=05:00:00 python /home/jl2815/tco/exercise_25/st_model/fit_st_vecc_v05_int414.py --v 0.5 --lr 0.01 --epochs 3000 --space \"20, 20\" --days 1 --mm-cond-number 10 --nheads 200 --params \"24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34\"    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "space 10 10   lr 0.03 for vecc and step size 80   then sec 21  15\n",
    "\n",
    "space 8 8    maybe lr 0.02    then sec 120 40\n",
    "             if lr 0.005   sec 250 (accuracy is worse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/opt/anaconda3/envs/faiss_env/bin/python /Users/joonwonlee/Documents/GEMS_TCO-1/Exercises/st_model/vecc_opt.py --space \"20,20\" --v 0.5 --lr 0.01 --epochs 3000 --days 1 --mm-cond-number 10 --nheads 10\n",
    "\n",
    "/opt/anaconda3/envs/faiss_env/bin/python /Users/joonwonlee/Documents/GEMS_TCO-1/Exercises/st_model/vecc_opt.py --space \"15,15\" --v 0.5 --lr 0.01 --epochs 3000 --days 1 --mm-cond-number 10 --nheads 10\n",
    "\n",
    "\n",
    "/opt/anaconda3/envs/faiss_env/bin/python /Users/joonwonlee/Documents/GEMS_TCO-1/Exercises/st_model/vecc_opt.py --space \"10,10\" --v 0.5 --lr 0.01 --epochs 2000 --days 1 --mm-cond-number 10 --nheads 20\n",
    "\n",
    "\n",
    "/opt/anaconda3/envs/faiss_env/bin/python /Users/joonwonlee/Documents/GEMS_TCO-1/Exercises/st_model/vecc_opt.py --space \"8,8\" --v 0.5 --lr 0.01 --epochs 2500 --days 1 --mm-cond-number 10 --nheads 100\n",
    "\n",
    "\n",
    "/opt/anaconda3/envs/faiss_env/bin/python /Users/joonwonlee/Documents/GEMS_TCO-1/Exercises/st_model/vecc_opt.py --space \"6,6\" --v 0.5 --lr 0.01 --epochs 2500 --days 1 --mm-cond-number 10 --nheads 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.446069469835466"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/opt/anaconda3/envs/faiss_env/bin/python /Users/joonwonlee/Documents/GEMS_TCO-1/Exercises/st_model/vecc_opt.py --space \"15,15\" --v 0.5 --lr 0.01 --epochs 3000 --days 1 --mm-cond-number 10 --nheads 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/opt/anaconda3/envs/faiss_env/bin/python /Users/joonwonlee/Documents/GEMS_TCO-1/Exercises/st_model/vecc_opt_hyper.py --space \"6,6\" --v 0.5 --lr 0.01 --epochs 2500 --days 1 --mm-cond-number 10 --nheads 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "params = torch.tensor(params, requires_grad=True)\n",
    "nheads= 200\n",
    "instance = kernels.model_fitting(\n",
    "    smooth=0.5,\n",
    "    input_map=analysis_data_map,\n",
    "    aggregated_data=aggregated_data,\n",
    "    nns_map=nns_map,\n",
    "    mm_cond_number=mm_cond_number,\n",
    "    nheads= nheads\n",
    ")\n",
    "\n",
    "# optimizer = optim.Adam([params], lr=0.01)  # For Adam\n",
    "optimizer, scheduler = instance.optimizer_fun( params, lr=0.01, betas=(0.9, 0.99), eps=1e-8, step_size=10, gamma=0.9)  \n",
    "instance.run_vecc_interpolate(params, optimizer, scheduler, instance.matern_cov_anisotropy_v05, epochs=3000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vecchia experiments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beat the b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/opt/anaconda3/envs/faiss_env/bin/python /Users/joonwonlee/Documents/GEMS_TCO-1/Exercises/vecc_beat2.py\n",
    "\n",
    "/opt/anaconda3/envs/faiss_env/bin/python /Users/joonwonlee/Documents/GEMS_TCO-1/Exercises/vecc_beat3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Day 1 data size per day: 1250.0 \n",
      "\n",
      "full: 14068.486752749093\n",
      "vecc t-1, t+1: 14026.824791852318\n",
      "vecc two lags: 14179.493975372936\n",
      "vecc two lags: 14152.955435277818\n",
      "vecc competitor: 14155.957972675671\n",
      "\n",
      "\n",
      " Day 1 full likelihood: 14068.486752749093\n",
      " parameters: [24.79344367980957, 1.584528923034668, 1.7182477712631226, 0.009088504128158092, -0.10729944705963135, 0.131037637591362, 2.717238664627075] \n",
      "\n",
      "Best approximation: vecc t-1, t+1 with abs_diff: 41.66196089677578\n",
      "Second best approximation: two lag with abs_diff: 84.46868252872446\n"
     ]
    }
   ],
   "source": [
    "lat_lon_resolution = [4,4]\n",
    "years = ['2024']\n",
    "month_range =[7,8]\n",
    "nheads = 200\n",
    "\n",
    "result_2 = [0]*3\n",
    "result_1 = [0]*3\n",
    "for day in range(1,2):\n",
    "    print(f'\\n Day {day} data size per day: { (200/lat_lon_resolution[0])*(100/lat_lon_resolution[0])  } \\n')\n",
    "\n",
    "    # parameters\n",
    "    mm_cond_number = 10+day\n",
    "    idx_for_datamap= [ 8*(day-1),8*day]\n",
    "    # params = [ 27.25, 2.18, 2.294, 4.099e-4, -0.07915, 0.0999, 3.65]   #200\n",
    "    params = list(df.iloc[day-1][:-1])\n",
    "    params = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "    # data\n",
    "    instance = load_data_local_computer()\n",
    "    map, ord_mm, nns_map= instance.load_mm20k_data_bymonthyear( lat_lon_resolution= lat_lon_resolution, mm_cond_number=mm_cond_number,years_=years, months_=month_range)\n",
    "    analysis_data_map, aggregated_data = instance.load_working_data_byday( map, ord_mm, nns_map, idx_for_datamap= idx_for_datamap)\n",
    "\n",
    "    # different approximations\n",
    "    key_order = [0,1,2,4,3,5,7,6]\n",
    "    keys = list(analysis_data_map.keys())\n",
    "    reordered_dict = {keys[key]: analysis_data_map[keys[key]] for key in key_order}\n",
    "\n",
    "    instance_ori = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "    out = instance_ori.full_likelihood(params, aggregated_data[:,:4],aggregated_data[:,2], instance_ori.matern_cov_anisotropy_v05)\n",
    "    print(f'full: {out}')  \n",
    "\n",
    "    out1 = instance_ori.vecchia_interpolation_1to6(params,  instance_ori.matern_cov_anisotropy_v05)\n",
    "    print(f'vecc t-1, t+1: {out1}')  \n",
    "\n",
    "    #out22 = instance_ori.vecchia_b2(params,  instance_ori.matern_cov_anisotropy_v05)\n",
    "    #print(f'vecc two lags: {out22}')  \n",
    "\n",
    "    instance = kernels.vecchia_experiment(0.5, reordered_dict, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "\n",
    "    out2 = instance.vecchia_b2(params,  instance.matern_cov_anisotropy_v05)\n",
    "    print(f'vecc two lags: {out2}')  \n",
    "\n",
    "    out3 = instance.vecchia_competitor(params,  instance.matern_cov_anisotropy_v05)\n",
    "    print(f'vecc competitor: {out3}')  \n",
    "\n",
    "\n",
    "    approx_map = {0: 'vecc t-1, t+1', 1: 'two lag', 2:'competitor'}\n",
    "\n",
    "    tmp_result = [ torch.abs(out-out1), torch.abs(out-out2) , torch.abs(out-out3)]\n",
    "    stacked_tensor = torch.stack(tmp_result)\n",
    "    top2_indices = torch.topk(stacked_tensor, 2, largest=False).indices\n",
    "\n",
    "    best_index = top2_indices[0].item()\n",
    "    second_best_index = top2_indices[1].item()\n",
    "\n",
    "    # Update the result for the best approximation\n",
    "    result_1[best_index] += 1\n",
    "    result_2[second_best_index] +=1\n",
    "    # Print the results\n",
    "    print(f'\\n\\n Day {day} full likelihood: {out}\\n parameters: {params.tolist()} \\n')\n",
    "    print(f'Best approximation: {approx_map[best_index]} with abs_diff: {stacked_tensor[best_index]}')\n",
    "    print(f'Second best approximation: {approx_map[second_best_index]} with abs_diff: {stacked_tensor[second_best_index]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invesitagate how many lags should I use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " day 1 data size per day: 1250.0 \n",
      "\n",
      "full: 14068.486752749093\n",
      "vecc t-1, t+1: 14026.824791852318\n",
      "vecc one lag : 14170.382029741002\n",
      "vecc two lags: 14152.955435277818\n",
      "vecc three lags: 14175.718607333947\n",
      "vecc four lags: 14176.37903307759\n",
      "\n",
      "\n",
      " Day 1 full likelihood: 14068.486752749093\n",
      " parameters: [24.79344367980957, 1.584528923034668, 1.7182477712631226, 0.009088504128158092, -0.10729944705963135, 0.131037637591362, 2.717238664627075] \n",
      "\n",
      "\n",
      "Best approximation: vecc t-1, t+1 with absolute difference 41.66196089677578\n",
      "Second best approximation: two lags with absolute difference 84.46868252872446\n"
     ]
    }
   ],
   "source": [
    "lat_lon_resolution = [4,4]\n",
    "years = ['2024']\n",
    "month_range =[7,8]\n",
    "nheads = 200\n",
    "\n",
    "result_2 = [0]*5\n",
    "result_1 = [0]*5\n",
    "for day in range(1,2):\n",
    "    print(f'\\n day {day} data size per day: { (200/lat_lon_resolution[0])*(100/lat_lon_resolution[0])  } \\n')\n",
    "\n",
    "    # parameters\n",
    "    mm_cond_number = 10+day\n",
    "    idx_for_datamap= [ 8*(day-1),8*day]\n",
    "    # params = [ 27.25, 2.18, 2.294, 4.099e-4, -0.07915, 0.0999, 3.65]   #200\n",
    "    params = list(df.iloc[day-1][:-1])\n",
    "    params = torch.tensor(params, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "    # data\n",
    "    instance = load_data_local_computer()\n",
    "    map, ord_mm, nns_map= instance.load_mm20k_data_bymonthyear( lat_lon_resolution= lat_lon_resolution, mm_cond_number=mm_cond_number,years_=years, months_=month_range)\n",
    "    analysis_data_map, aggregated_data = instance.load_working_data_byday( map, ord_mm, nns_map, idx_for_datamap= idx_for_datamap)\n",
    "\n",
    "    # different approximations\n",
    "    key_order = [0,1,2,4,3,5,7,6]\n",
    "    keys = list(analysis_data_map.keys())\n",
    "    reordered_dict = {keys[key]: analysis_data_map[keys[key]] for key in key_order}\n",
    "\n",
    "    instance_ori = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "    out = instance_ori.full_likelihood(params, aggregated_data[:,:4],aggregated_data[:,2], instance_ori.matern_cov_anisotropy_v05)\n",
    "    print(f'full: {out}')  # 15105\n",
    "\n",
    "    out1 = instance_ori.vecchia_interpolation_1to6(params,  instance_ori.matern_cov_anisotropy_v05)\n",
    "    print(f'vecc t-1, t+1: {out1}')  # 15105\n",
    "\n",
    "    instance = kernels.vecchia_experiment(0.5, reordered_dict, aggregated_data,nns_map,mm_cond_number, nheads)\n",
    "\n",
    "    out2 = instance.vecchia_b1(params,  instance.matern_cov_anisotropy_v05)\n",
    "    print(f'vecc one lag : {out2}')  # 15105\n",
    "\n",
    "    out3 = instance.vecchia_b2(params,  instance.matern_cov_anisotropy_v05)\n",
    "    print(f'vecc two lags: {out3}')  # 15105\n",
    "\n",
    "    out4 = instance.vecchia_b3(params,  instance.matern_cov_anisotropy_v05)\n",
    "    print(f'vecc three lags: {out4}')  # 15105\n",
    "\n",
    "    out5 = instance.vecchia_b4(params,  instance.matern_cov_anisotropy_v05)\n",
    "    print(f'vecc four lags: {out5}')  # 15105\n",
    "\n",
    "    approx_map = {0: 'vecc t-1, t+1', 1: 'one lag', 2:'two lags', 3:'three lags', 4:'four lags'}\n",
    "\n",
    "\n",
    "    tmp_result = [torch.abs(out-out1), torch.abs(out-out2), torch.abs(out-out3), torch.abs(out-out4), torch.abs(out-out5)]\n",
    "    stacked_tensor = torch.stack(tmp_result)\n",
    "    top2_indices = torch.topk(stacked_tensor, 2, largest=False).indices\n",
    "\n",
    "    best_index = top2_indices[0].item()\n",
    "    second_best_index = top2_indices[1].item()\n",
    "\n",
    "    # Update the result for the best approximation\n",
    "    result_1[best_index] += 1\n",
    "    result_2[second_best_index] +=1\n",
    "    # Print the results\n",
    "    print(f'\\n\\n Day {day} full likelihood: {out}\\n parameters: {params.tolist()} \\n')\n",
    "    print(f'Best approximation: {approx_map[best_index]} with abs_diff: {stacked_tensor[best_index]}')\n",
    "    print(f'Second best approximation: {approx_map[second_best_index]} with abs_diff: {stacked_tensor[second_best_index]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vecc t-1, t+1'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "approx_map[  torch.argmin( torch.stack(tmp_result)).item() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2588.8355, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2598.6200, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2596.4554, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2597.2973, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2597.4288, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2597.6245, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2597.7251, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 2 full 2588.835494538167 best 2-7.619876935808406\n",
      "tensor(2325.3057, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2338.3503, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2337.2224, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2337.3480, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2337.8504, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2337.7865, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2337.9979, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 3 full 2325.3057231697794 best 2-11.916687282841394\n",
      "tensor(2677.8738, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2711.6530, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2710.8470, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2710.0946, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2710.2561, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2710.2624, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2710.2837, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 4 full 2677.873782988168 best 3-32.22080760327162\n",
      "tensor(2607.6200, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2626.1323, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2628.0403, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2626.3039, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2626.3261, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2626.0761, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2626.3292, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 5 full 2607.620008884488 best 5-18.456108220521855\n",
      "tensor(2297.7041, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2317.1327, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2316.2551, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2315.8326, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2315.7545, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2315.6651, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2315.6782, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 6 full 2297.704062279004 best 5-17.961066431277686\n",
      "tensor(2595.6083, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2607.5411, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2607.3641, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2607.4065, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2607.4271, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2607.3450, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2607.2667, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2720.2230, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2726.0195, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2726.2736, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2727.3466, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2727.6813, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2727.5627, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2727.4288, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 8 full 2720.2230003749 best 1-5.79651889233719\n",
      "tensor(2716.2423, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2746.4358, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2743.5788, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2743.5225, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2743.4989, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2743.2579, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2743.1530, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2439.7181, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2465.3531, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2465.7143, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2465.1803, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2465.1571, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2465.1256, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2465.2253, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 10 full 2439.718051492361 best 5-25.407566057368513\n",
      "tensor(2632.4615, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2656.7989, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2654.6133, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2654.1847, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2654.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2654.1116, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2654.0519, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 11 full 2632.461532144173 best 4-21.540797109914365\n",
      "tensor(2397.5475, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2415.1000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2414.8049, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2414.2531, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2414.1802, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2414.0144, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2413.9669, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2254.0872, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2277.3050, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2278.2964, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2277.9706, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2277.8628, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2277.8632, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2277.9051, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 13 full 2254.0872290437924 best 1-23.21780800665192\n",
      "tensor(2273.8395, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2291.7017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2290.6677, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2291.7426, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2291.7406, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2291.8359, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2291.7217, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 14 full 2273.839546753164 best 2-16.828122013940174\n",
      "tensor(2444.7211, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2457.8423, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2455.5376, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2455.2565, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2454.9719, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2454.8507, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2454.6879, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2239.4959, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2256.8316, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2255.6135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2255.5940, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2255.7225, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2255.7953, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2255.7773, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 16 full 2239.495854029915 best 3-16.098146601861572\n",
      "tensor(2289.3533, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2306.6114, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2305.5030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2305.7232, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2305.8898, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2305.8676, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2305.8515, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 17 full 2289.353331686605 best 2-16.14970080096009\n",
      "tensor(2604.6595, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2609.6826, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2608.5096, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2608.2245, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2608.4528, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2608.3938, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2608.3466, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 18 full 2604.6594993738818 best 3-3.5649968806819743\n",
      "tensor(2501.7653, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2519.3117, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2519.5096, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2520.4348, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2520.3124, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2520.7086, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2520.4703, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 19 full 2501.7653357445975 best 1-17.546328131036717\n",
      "tensor(2383.8398, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2410.2557, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2409.4686, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2409.6487, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2409.6723, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2409.7001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2409.6403, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 20 full 2383.8398046939737 best 2-25.628830834950804\n",
      "tensor(2397.9914, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2416.0251, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2416.7615, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2416.9200, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2416.9514, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2416.9429, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2416.9958, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 21 full 2397.9913503612715 best 1-18.033776973373733\n",
      "tensor(2415.2363, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2426.0299, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2424.4697, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2423.9458, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2423.7174, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2423.8332, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2423.9201, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 22 full 2415.2362851612124 best 4-8.481101648688764\n",
      "tensor(2395.3138, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2416.4154, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2416.9620, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2417.0255, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2416.9522, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2417.0855, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2417.1347, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 23 full 2395.313819575629 best 1-21.101553463564414\n",
      "tensor(2221.0660, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2240.2560, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2239.4026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2239.2811, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2239.3526, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2239.2509, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2239.2417, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2259.2643, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2281.0549, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2278.8848, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2278.8007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2278.7219, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2278.6225, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2278.6297, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 25 full 2259.264339937321 best 5-19.358143734249097\n",
      "tensor(2036.1660, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2057.3812, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2055.4435, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2055.2120, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2055.2183, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2055.2493, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2055.2752, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 26 full 2036.16602521864 best 3-19.045997493149343\n",
      "tensor(2146.0333, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2160.5631, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2160.0290, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2159.6130, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2159.6851, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2159.7519, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2159.7405, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 27 full 2146.0333096528807 best 3-13.57972815117455\n",
      "tensor(2164.2665, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2189.2787, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2187.8623, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2187.6484, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2187.7190, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2187.6251, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2187.5509, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2349.6745, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2368.7387, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2367.5945, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2367.8489, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2368.1959, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2367.9423, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2368.1533, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 29 full 2349.674522763318 best 2-17.919993674066063\n",
      "tensor(2261.4948, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2280.9012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2278.3062, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2278.1144, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2277.9829, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2277.7629, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2277.9217, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 30 full 2261.494816838197 best 5-16.268066494493723\n",
      "tensor(2391.0470, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(2397.9122, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2396.5535, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2395.9974, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2396.0717, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2396.3047, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2396.4791, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 31 full 2391.047020052954 best 3-4.95039353568427\n",
      "tensor(3038.9627, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(3002.6027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3008.2816, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3008.1634, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3008.1979, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3008.2025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3008.1352, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      " day 32 full 3038.9627261779337 best 2-30.68107633844238\n"
     ]
    }
   ],
   "source": [
    "lat_lon_resolution = [10,10]\n",
    "\n",
    "head100map = defaultdict(list)\n",
    "nheads= 10\n",
    "mm_cond_number = 10\n",
    "\n",
    "result_2 = [0]*7\n",
    "result_1 = [0]*7\n",
    "for day in range(1,32):\n",
    "    \n",
    "\n",
    "    years = ['2024']\n",
    "    month_range =[7,8]\n",
    "    idx_for_datamap= [ 8*(day-1),8*day]\n",
    "\n",
    "    instance = load_data_local_computer()\n",
    "    map, ord_mm, nns_map= instance.load_mm20k_data_bymonthyear( lat_lon_resolution= lat_lon_resolution, mm_cond_number=mm_cond_number,years_=years, months_=month_range)\n",
    "    analysis_data_map, aggregated_data = instance.load_working_data_byday( map, ord_mm, nns_map, idx_for_datamap= idx_for_datamap)\n",
    "\n",
    "    params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "    params = torch.tensor(params, requires_grad=True)\n",
    "    instance = kernels.vecchia_experiment(0.5, analysis_data_map, aggregated_data, nns_map, mm_cond_number, nheads)\n",
    "\n",
    "\n",
    "    out = instance.full_likelihood(params, aggregated_data[:,:4],aggregated_data[:,2], instance.matern_cov_anisotropy_v05)\n",
    "    print(out)  # 15105\n",
    "\n",
    "    out1 = instance.vecchia_interpolation_1to6(params,  instance.matern_cov_anisotropy_v05)\n",
    "    print(f'vecc t-1, t+1: {out1}')  # 15105\n",
    "\n",
    "    out2 = instance.vecchia_b1(params,  instance.matern_cov_anisotropy_v05)\n",
    "    print(f'vecc one lag : {out2}')  \n",
    "\n",
    "    out3 = instance.vecchia_b2(params,  instance.matern_cov_anisotropy_v05)\n",
    "    print(f'vecc two lags: {out3}')  \n",
    "\n",
    "    out4 = instance.vecchia_b3(params,  instance.matern_cov_anisotropy_v05)\n",
    "    print(f'vecc three lags: {out4}')  \n",
    "\n",
    "    out5 = instance.vecchia_b4(params,  instance.matern_cov_anisotropy_v05)\n",
    "    print(f'vecc four lags: {out5}')  \n",
    "\n",
    "    out6 = instance.vecchia_b5(params, instance.matern_cov_anisotropy_v05)\n",
    "    print(f'vecc five lags: {out6}') \n",
    "\n",
    "    out7 = instance.vecchia_b6(params, instance.matern_cov_anisotropy_v05)\n",
    "    print(f'vecc six lags: {out7}')  \n",
    "\n",
    "    approx_map = {0: 'vecc t-1, t+1', 1: 'one lag', 2:'two lags', 3:'three lags', 4:'four lags', 5:'five lags' ,6:'six lags' }\n",
    "\n",
    "\n",
    "\n",
    "    tmp_result = [torch.abs(out-out1), torch.abs(out-out2), torch.abs(out-out3), torch.abs(out-out4), torch.abs(out-out5),  torch.abs(out-out6) , torch.abs(out-out7)]\n",
    "    stacked_tensor = torch.stack(tmp_result)\n",
    "    top2_indices = torch.topk(stacked_tensor, 2, largest=False).indices\n",
    "\n",
    "    best_index = top2_indices[0].item()\n",
    "    second_best_index = top2_indices[1].item()\n",
    "\n",
    "    # Update the result for the best approximation\n",
    "    result_1[best_index] += 1\n",
    "    result_2[second_best_index] +=1\n",
    "    # Print the results\n",
    "    print(f'\\n\\n Day {day} full likelihood: {out}\\n parameters: {params.tolist()} \\n')\n",
    "    print(f'Best approximation: {approx_map[best_index]} with abs_diff: {stacked_tensor[best_index]}')\n",
    "    print(f'Second best approximation: {approx_map[second_best_index]} with abs_diff: {stacked_tensor[second_best_index]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(10, 10): [3, 2, 11, 4, 1, 4, 0],\n",
       " (10, 20): [9, 4, 8, 3, 1, 2, 0],\n",
       " (10, 30): [10, 3, 4, 5, 1, 3, 0]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmap = {}  ## key (mm_cond_number, headn) values: b1 2 b3 b4 b5 b6 \n",
    "bmap[(10,10)] = [3,2,11,4,1,4,0]\n",
    "bmap[(10,20)] = [9,4,8,3,1,2,0]\n",
    "bmap[(10,30)] = [10,3,4,5,1,3,0]\n",
    "bmap[(10,40)] = [12,4,4,4,2,3,0]\n",
    "bmap[(10,50)] = [13,3,4,4,2,2,0]  # [0, 5, 7, 6, 2, 5, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Gradients: [  -9.955391  207.88681    -2.595461 -169.72203   273.39944   771.1119\n",
      "  -79.33867 ]\n",
      " Loss: 15135.7587890625, Parameters: [ 2.442e+01  1.920e+00  1.920e+00  1.000e-03 -4.500e-02  2.370e-01\n",
      "  3.340e+00]\n",
      "Epoch 101, Gradients: [ -1.4867018    0.10161591   1.1231489    3.3106918  -11.375813\n",
      "  -7.5312614    7.9387875 ]\n",
      " Loss: 15014.2587890625, Parameters: [ 2.5338207e+01  1.3597493e+00  1.7280715e+00  1.9441580e-02\n",
      " -1.6756412e-01  1.8955134e-01  3.3110123e+00]\n",
      "Converged at epoch 189\n",
      "Epoch 190, Gradients: [-1.2997863  -0.27633095 -0.09813547  0.49712753 -0.5989456  -1.545433\n",
      " -0.09783697]\n",
      " Loss: 15012.7626953125, Parameters: [ 2.5795250e+01  1.3089702e+00  1.6588148e+00  1.8806925e-02\n",
      " -1.6661680e-01  1.9616681e-01  3.1115689e+00]\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "params = torch.tensor(params, requires_grad=True)\n",
    "\n",
    "instance = torch_vecchia_exp(analysis_data_map, params, nns_map, mm_cond_number)\n",
    "\n",
    "# optimizer = optim.Adam([params], lr=0.01)  # For Adam\n",
    "optimizer = optim.Adam([params],lr=0.01, betas=(0.9, 0.8), eps=1e-8)\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.9)\n",
    "\n",
    "# Example function to compute out1\n",
    "def compute_out1(params):\n",
    "    # Compute the output using your function\n",
    "    # nll = instance.full_likelihood(params, instance.matern_advec_beta_cov )\n",
    "    nll = instance.vecchia_local4(params, instance.matern_cov_ani, 200 )\n",
    "    return nll\n",
    "\n",
    "# Training loop\n",
    "prev_loss = float('inf')\n",
    "tol = 1e-4  # Convergence tolerance\n",
    "for epoch in range(600):  # Number of epochs\n",
    "    optimizer.zero_grad()  # Zero the gradients \n",
    "    \n",
    "    loss = compute_out1(params)\n",
    "    loss.backward()  # Backpropagate the loss\n",
    "    \n",
    "    # Print gradients and parameters every 10th epoch\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "    \n",
    "    # print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "    \n",
    "    optimizer.step()  # Update the parameters\n",
    "    scheduler.step()\n",
    "    # Check for convergence\n",
    "    if abs(prev_loss - loss.item()) < tol:\n",
    "        print(f\"Converged at epoch {epoch}\")\n",
    "        print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "    \n",
    "        break\n",
    "    \n",
    "    prev_loss = loss.item()\n",
    "\n",
    "print('Training complete.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Gradients: [-10.6224985 200.28781    -6.4957714 -78.33738   149.36243   605.0917\n",
      " -81.693665 ]\n",
      " Loss: 15183.4765625, Parameters: [ 2.442e+01  1.920e+00  1.920e+00  1.000e-03 -4.500e-02  2.370e-01\n",
      "  3.340e+00]\n",
      "Epoch 101, Gradients: [-1.9947629  4.9151707  6.4719105 -1.286026  -2.0892706  2.9007835\n",
      "  1.0226293]\n",
      " Loss: 15077.75, Parameters: [25.32368     1.3661038   1.7502507   0.04647188 -0.12653434  0.17788467\n",
      "  3.327231  ]\n",
      "Converged at epoch 135\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "params = torch.tensor(params, requires_grad=True)\n",
    "\n",
    "instance = torch_vecchia_exp(analysis_data_map, params, nns_map, mm_cond_number)\n",
    "\n",
    "# optimizer = optim.Adam([params], lr=0.01)  # For Adam\n",
    "optimizer = optim.Adam([params],lr=0.01, betas=(0.9, 0.8), eps=1e-8)\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.9)\n",
    "\n",
    "# Example function to compute out1\n",
    "def compute_out1(params):\n",
    "    # Compute the output using your function\n",
    "    # nll = instance.full_likelihood(params, instance.matern_advec_beta_cov )\n",
    "    nll = instance.vecchia_local3(params, instance.matern_cov_ani )\n",
    "    return nll\n",
    "\n",
    "# Training loop\n",
    "prev_loss = float('inf')\n",
    "tol = 1e-4  # Convergence tolerance\n",
    "for epoch in range(500):  # Number of epochs\n",
    "    optimizer.zero_grad()  # Zero the gradients \n",
    "    \n",
    "    loss = compute_out1(params)\n",
    "    loss.backward()  # Backpropagate the loss\n",
    "    \n",
    "    # Print gradients and parameters every 10th epoch\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "    \n",
    "    # print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "    \n",
    "    optimizer.step()  # Update the parameters\n",
    "    scheduler.step()\n",
    "    # Check for convergence\n",
    "    if abs(prev_loss - loss.item()) < tol:\n",
    "        print(f\"Converged at epoch {epoch}\")\n",
    "        print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "        break\n",
    "    \n",
    "    prev_loss = loss.item()\n",
    "\n",
    "print('Training complete.') \n",
    "\n",
    " \n",
    "# vecchia local 2  332   epo 33.4s   25.55 2.61 2.68 0.16 0.03 2.7\n",
    "#    24.89 2.06 2.24 1.36e-2 -5.63 e-2 0.10113 3.75\n",
    "# vecchia cholesky local 380 epo 43.2  25 2.61 2.68 0.16 0.03 2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 200 x 8\n",
    "\n",
    "lr 0.001 without scheduler  same as lr, step_size, gamma  0.01 40 0.5  (9.8s)\n",
    "\n",
    " Loss: 2549.066650390625, full Parameters: [ 2.48777485e+01  2.05998826e+00  2.16013098e+00  2.20775465e-03\n",
    " -7.89414570e-02  1.05411254e-01  3.75236106e+00]\n",
    "\n",
    " lr 0.01  step size 40  betas 0.9 , 0.8 gamma 0.9  30 s\n",
    "\n",
    "  Loss: 2547.1728515625, full Parameters: [ 2.7377291e+01  2.2077193e+00  2.3204505e+00  1.0307773e-03\n",
    " -8.0311157e-02  9.8579854e-02  3.6677265e+00]\n",
    "\n",
    " lr 0.01  step size 10 betas 0.9 , 0.8 gamma 0.9  30 s\n",
    "  Loss: 2548.87841796875, full Parameters: [ 2.5092268e+01  2.0689390e+00  2.1694989e+00  2.0285936e-03\n",
    " -7.9028614e-02  1.0501490e-01  3.7373385e+00]\n",
    "Training full likelihood complete.   11.8 sc\n",
    "\n",
    " lr 0.01  step size 20 betas 0.9 , 0.8 gamma 0.9  30 s\n",
    " Loss: 2548.15283203125, full Parameters: [ 2.59814014e+01  2.12175608e+00  2.22699022e+00  1.73025124e-03\n",
    " -7.93599486e-02  1.02427535e-01  3.70715070e+00]\n",
    "\n",
    "\n",
    "\n",
    "lr 0.01  step size 20 beta 0.9 0.99 gamma 0.9\n",
    " Loss: 2548.18603515625, full Parameters: [ 2.5938652e+01  2.1110108e+00  2.2155209e+00  1.5893303e-03\n",
    " -7.9482891e-02  1.0297947e-01  3.6958976e+00]\n",
    " 21.6\n",
    "\n",
    "lr 0.01  step size 20 beta 0.9 0.8 gamma 0.9\n",
    " Loss: 2548.15283203125, full Parameters: [ 2.59814014e+01  2.12175608e+00  2.22699022e+00  1.73025124e-03\n",
    " -7.93599486e-02  1.02427535e-01  3.70715070e+00]\n",
    " 22.9 s\n",
    "\n",
    "lr 0.01  step size 10 beta 0.9 0.99 gamma 0.9\n",
    "Loss: 2548.95361328125, full Parameters: [ 2.5118145e+01  1.9827319e+00  2.0768294e+00  1.0898338e-03\n",
    " -8.0070712e-02  1.1034889e-01  3.5647078e+00]\n",
    "\n",
    "\n",
    "## 1250 x 8\n",
    "\n",
    "1250* 8 55m using constant learning rate 0.0001 \n",
    "Loss: 14068.798828125, full Parameters: [ 2.46198387e+01  1.61719894e+00  1.76454413e+00  8.55297223e-03\n",
    " -1.08275235e-01  1.28809512e-01  2.80795789e+00]\n",
    "\n",
    "1250* 8 10m 32s\n",
    "lr 0.01  step size 40 beta 0.9 0.8 gamma 0.9\n",
    "  Loss: 14068.1953125, full Parameters: [ 2.5030930e+01  1.6107724e+00  1.7573007e+00  8.8407323e-03\n",
    " -1.0820019e-01  1.2936097e-01  2.7430327e+00]\n",
    "Training full likelihood complete.\n",
    "\n",
    "9m 33s\n",
    "lr 0.01  step size 20 beta 0.9 0.8 gamma 0.9\n",
    " Loss: 14068.29296875, full Parameters: \n",
    " [ 2.4933689e+01  1.6009743e+00  1.7502663e+00  9.2404895e-03 -1.0737537e-01  1.2953614e-01 \n",
    "  2.7420275e+00]\n",
    "Training full likelihood complete.\n",
    "\n",
    "#### high resolution data might benefits from larger step size high resolution data often provides \n",
    "#### more stable gradients, so larger step size less likely to cause significant fluctuations\n",
    "14n 41.8s\n",
    "lr 0.01  step size 10 beta 0.9 0.99 gamma 0.9\n",
    "\n",
    "FINAL STATE: Epoch 199, \n",
    " Loss: 14068.8828125, full Parameters: \n",
    " [ 2.4707581e+01  1.6489888e+00  1.7993137e+00  8.4043797e-03 -1.0836436e-01  1.2655504e-01  \n",
    " 2.8416286e+00]\n",
    "\n",
    "#### beta 0.9 0.99 might be too conservative for high resolution data\n",
    "13m 44.8s\n",
    "lr 0.01  step size 20 beta 0.9 0.99 gamma 0.9\n",
    "\n",
    " Loss: 14068.318359375, full Parameters: [ 2.4938175e+01  1.6203119e+00  1.7678342e+00  8.6686825e-03\n",
    " -1.0813228e-01  1.2845081e-01  2.7731323e+00]\n",
    "\n",
    "\n",
    "18m\n",
    "lr 0.01  step size 40 beta 0.9 0.99 gamma 0.9\n",
    "\n",
    " Loss: 14067.970703125, full Parameters: [ 2.5205673e+01  1.6159834e+00  1.7630767e+00  8.7957922e-03\n",
    " -1.0802399e-01  1.2862283e-01  2.7390635e+00]\n",
    "\n",
    "9m 52s\n",
    "lr 0.01  step size 20 beta 0.9 0.8 gamma 0.9\n",
    "\n",
    "Loss: 14068.29296875, full Parameters: [ 2.4933689e+01  1.6009743e+00  1.7502663e+00  9.2404895e-03\n",
    " -1.0737537e-01  1.2953614e-01  2.7420275e+00]\n",
    "Training full likelihood complete."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
