{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "gems_tco_path = \"/Users/joonwonlee/Documents/GEMS_TCO-1/src\"\n",
    "sys.path.append(gems_tco_path)\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import GEMS_TCO\n",
    "from GEMS_TCO import kernels \n",
    "from GEMS_TCO import orderings as _orderings\n",
    "from GEMS_TCO import load_data_local_computer\n",
    "\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import copy                    # clone tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_resolution = [10,10]\n",
    "day = 1\n",
    "mm_cond_number = 20\n",
    "\n",
    "years = ['2024']\n",
    "month_range =[7,8]\n",
    "idx_for_datamap= [ 8*(day-1),8*day]\n",
    "\n",
    "instance = load_data_local_computer()\n",
    "map, ord_mm, nns_map= instance.load_mm20k_data_bymonthyear( lat_lon_resolution= lat_lon_resolution, mm_cond_number=mm_cond_number,years_=years, months_=month_range)\n",
    "analysis_data_map, aggregated_data = instance.load_working_data_byday( map, ord_mm, nns_map, idx_for_datamap=[0,8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate conditioning number\n",
    "\n",
    "10 seems best no more no less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor(110.0250), tensor(5.0250)), (tensor(110.0250), tensor(9.5250)), (tensor(114.5250), tensor(5.0250)), (tensor(114.5250), tensor(9.5250)), (tensor(119.5250), tensor(5.0250)), (tensor(119.5250), tensor(9.5250))]\n",
      "Indices in Tensor Frame:\n",
      "tensor([  1, 145, 160,  50,   2,   4])\n"
     ]
    }
   ],
   "source": [
    "sd = analysis_data_map['2024_07_y24m07day01_hm01:00']\n",
    "# Compute the required statistics\n",
    "# Compute the required statistics\n",
    "max_lat = torch.max(sd[:, 0])\n",
    "min_lat = torch.min(sd[:, 0])\n",
    "median_lat = torch.median(sd[:, 0])\n",
    "\n",
    "max_lon = torch.max(sd[:, 1])\n",
    "min_lon = torch.min(sd[:, 1])\n",
    "median_lon = torch.median(sd[:, 1])\n",
    "\n",
    "# Extract the 9 points along with their locations (indices)\n",
    "points = [\n",
    "    (min_lon, min_lat),\n",
    "    #(min_lon, median_lat),\n",
    "    (min_lon, max_lat),\n",
    "    (median_lon, min_lat),\n",
    "    #(median_lon, median_lat),\n",
    "    (median_lon, max_lat),\n",
    "    (max_lon, min_lat),\n",
    "    #(max_lon, median_lat),\n",
    "    (max_lon, max_lat)\n",
    "]\n",
    "print(points)\n",
    "\n",
    "indices = []\n",
    "for lon, lat in points:\n",
    "    condition = (sd[:, 0] == lat) & (sd[:, 1] == lon)\n",
    "    indices.append(torch.where(condition)[0])\n",
    "\n",
    "# Create the indices tensor\n",
    "indices_tensor = torch.cat(indices)\n",
    "\n",
    "print(\"Indices in Tensor Frame:\")\n",
    "print(indices_tensor)\n",
    "\n",
    "base_list = indices_tensor.clone().detach().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization vecchia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "params = torch.tensor(params, requires_grad=True)\n",
    "\n",
    "instance = kernels.model_fitting(\n",
    "    smooth=0.5,\n",
    "    input_map=analysis_data_map,\n",
    "    aggregated_data=aggregated_data,\n",
    "    nns_map=nns_map,\n",
    "    mm_cond_number=mm_cond_number\n",
    ")\n",
    "\n",
    "# optimizer = optim.Adam([params], lr=0.01)  # For Adam\n",
    "optimizer, scheduler = instance.optimizer_fun( params, lr=0.01, betas=(0.9, 0.99), eps=1e-8, step_size=10, gamma=0.9)  \n",
    "instance.run_vecc_local(params, optimizer, scheduler,epochs=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vecchia experiments here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_for_datamap = [0,8]\n",
    "\n",
    "class matern_advec_beta_torch_vecchia:\n",
    "    def __init__(self, analaysis_data_map: torch.Tensor, params: torch.Tensor, nns_map=nns_map, mm_cond_number=mm_cond_number):\n",
    "        \n",
    "        self.key_list = sorted(analysis_data_map)\n",
    "        self.input_map = analysis_data_map\n",
    "\n",
    "        self.mm_cond_number = mm_cond_number\n",
    "        self.nns_map = nns_map \n",
    "        self.input_map = analaysis_data_map\n",
    "        self.smooth = 0.5  \n",
    "        sample_df = analaysis_data_map[self.key_list[0]]\n",
    "\n",
    "        self.size_per_hour = len(sample_df)\n",
    "\n",
    "    def custom_distance_matrix(self, U, V):\n",
    "        # Efficient distance computation with broadcasting\n",
    "        spatial_diff = torch.norm(U[:, :2].unsqueeze(1) - V[:, :2].unsqueeze(0), dim=2)\n",
    "\n",
    "        temporal_diff = torch.abs(U[:, 2].unsqueeze(1) - V[:, 2].unsqueeze(0))\n",
    "        distance = (spatial_diff**2 + temporal_diff**2)  # move torch.sqrt to covariance function to track gradients of beta and avec\n",
    "        return distance\n",
    "    \n",
    "    def precompute_coords_ani(self, params, y: torch.Tensor, x: torch.Tensor)-> torch.Tensor:\n",
    "        sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "\n",
    "        if y is None or x is None:\n",
    "            raise ValueError(\"Both y and x_df must be provided.\")\n",
    "\n",
    "        x1 = x[:, 0]\n",
    "        y1 = x[:, 1]\n",
    "        t1 = x[:, 3]\n",
    "\n",
    "        x2 = y[:, 0]\n",
    "        y2 = y[:, 1]\n",
    "        t2 = y[:, 3]\n",
    "\n",
    "        # spat_coord1 = torch.stack((self.x1 , self.y1 - advec * self.t1), dim=-1)\n",
    "        spat_coord1 = torch.stack(( (x1 - advec_lat * t1)/range_lat, (y1 - advec_lon * t1)/range_lon ), dim=-1)\n",
    "        spat_coord2 = torch.stack(( (x2 - advec_lat * t2)/range_lat, (y2 - advec_lon * t2)/range_lon ), dim=-1)\n",
    "\n",
    "        U = torch.cat((spat_coord1, (beta * t1).reshape(-1, 1)), dim=1)\n",
    "        V = torch.cat((spat_coord2, (beta * t2).reshape(-1, 1)), dim=1)\n",
    "\n",
    "        distance = self.custom_distance_matrix(U,V)\n",
    "        non_zero_indices = distance != 0\n",
    "        return distance, non_zero_indices\n",
    "    \n",
    "    # anisotropic in three \n",
    "    def matern_cov_ani(self,params: torch.Tensor, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        sigmasq, range_lat, range_lon, advec_lat, advec_lon, beta, nugget = params\n",
    "        \n",
    "\n",
    "        distance, non_zero_indices = self.precompute_coords_ani(params, x,y)\n",
    "        out = torch.zeros_like(distance)\n",
    "\n",
    "        non_zero_indices = distance != 0\n",
    "        if torch.any(non_zero_indices):\n",
    "            out[non_zero_indices] = sigmasq * torch.exp(- torch.sqrt(distance[non_zero_indices]))\n",
    "        out[~non_zero_indices] = sigmasq\n",
    "\n",
    "        # Add a small jitter term to the diagonal for numerical stability\n",
    "        out += torch.eye(out.shape[0]) * nugget\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def full_likelihood(self,params: torch.Tensor, input_np: torch.Tensor, y: torch.Tensor, covariance_function) -> torch.Tensor:\n",
    "        input_arr = input_np[:, :4]\n",
    "        y_arr = y\n",
    "\n",
    "        # Compute the covariance matrix\n",
    "        cov_matrix = covariance_function(params=params, y=input_arr, x=input_arr)\n",
    "        \n",
    "        # Compute the log determinant of the covariance matrix\n",
    "        sign, log_det = torch.slogdet(cov_matrix)\n",
    "        #log_det = torch.log(torch.linalg.det(cov_matrix))\n",
    "        #if sign <= 0:\n",
    "        #    raise ValueError(\"Covariance matrix is not positive definite\")\n",
    "        \n",
    "        # Extract locations\n",
    "        locs = input_arr[:, :2]\n",
    "\n",
    "        # Compute beta\n",
    "        tmp1 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, locs))\n",
    "        tmp2 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, y_arr))\n",
    "        beta = torch.linalg.solve(tmp1, tmp2)\n",
    "\n",
    "        # Compute the mean\n",
    "        mu = torch.matmul(locs, beta)\n",
    "        y_mu = y_arr - mu\n",
    "\n",
    "        # Compute the quadratic form\n",
    "        quad_form = torch.matmul(y_mu, torch.linalg.solve(cov_matrix, y_mu))\n",
    "\n",
    "        # Compute the negative log likelihood\n",
    "        neg_log_lik = 0.5 * (log_det + quad_form)\n",
    "        # neg_log_lik = 0.5 * ( log_det )\n",
    "        return  neg_log_lik \n",
    "\n",
    "\n",
    "    def vecchia_local_full_cond(self, params: torch.Tensor, covariance_function) -> torch.Tensor:\n",
    "        neg_log_lik = 0.0\n",
    "\n",
    "                # Use below when working on local computer to avoid singular matrix\n",
    "        cur_heads = aggregated_data[:20, :]\n",
    "        neg_log_lik += self.full_likelihood(params, cur_heads, cur_heads[:, 2], covariance_function)\n",
    "\n",
    "        for idx in range(20,len(aggregated_data)):\n",
    "            current_row = aggregated_data[idx,:4]\n",
    "            current_y = aggregated_data[idx,2]\n",
    "            conditioning_data = aggregated_data[:idx,:4]\n",
    "\n",
    "            torch_arr = torch.vstack((current_row, conditioning_data))\n",
    "            y_and_neighbors = torch_arr[:, 2]\n",
    "            locs = torch_arr[:, :2]\n",
    "\n",
    "            cov_matrix = covariance_function(params=params, y= torch_arr, x= torch_arr)\n",
    "            # print(f'Condition number: {torch.linalg.cond(cov_matrix)}')\n",
    "            cov_xx = cov_matrix[1:, 1:]\n",
    "            # cov_xx_inv = torch.linalg.inv(cov_xx)\n",
    "\n",
    "            cov_yx = cov_matrix[0, 1:]\n",
    "\n",
    "                    # Compute the log determinant of the covariance matrix\n",
    "            # sign, log_det = torch.slogdet(cov_matrix)\n",
    "            # if sign <= 0:\n",
    "            #     raise ValueError(\"Covariance matrix is not positive definite\")\n",
    "        \n",
    "            # Compute beta\n",
    "\n",
    "\n",
    "            tmp1 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, locs))\n",
    "            tmp2 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, y_and_neighbors))\n",
    "            \n",
    "\n",
    "            beta = torch.linalg.solve(tmp1, tmp2)\n",
    "\n",
    "            mu = torch.matmul(locs, beta)\n",
    "            mu_current = mu[0]\n",
    "            mu_neighbors = mu[1:]\n",
    "\n",
    "            # Mean and variance of y|x\n",
    "            sigma = cov_matrix[0, 0]\n",
    "            \n",
    "            # cov_ygivenx = sigma - torch.matmul(cov_yx, torch.matmul(cov_xx_inv, cov_yx))\n",
    "            cov_ygivenx = sigma - torch.matmul(cov_yx, torch.linalg.solve(cov_xx, cov_yx))\n",
    "            # cond_mean_tmp = torch.matmul(cov_yx, cov_xx_inv)\n",
    "            cond_mean = mu_current + torch.matmul(cov_yx, torch.linalg.solve( cov_xx,(y_and_neighbors[1:] - mu_neighbors) ) )\n",
    "            \n",
    "            alpha = current_y - cond_mean\n",
    "            quad_form = alpha**2 * (1 / cov_ygivenx)\n",
    "            log_det = torch.log(cov_ygivenx)\n",
    "     \n",
    "            neg_log_lik += 0.5 * (log_det + quad_form) \n",
    "        return neg_log_lik\n",
    "\n",
    "\n",
    "    def vecchia_like_local_computer(self, params: torch.Tensor, covariance_function) -> torch.Tensor:\n",
    "        self.cov_map = defaultdict(list)\n",
    "        neg_log_lik = 0.0\n",
    "        \n",
    "        for time_idx in range(len(self.input_map)):\n",
    "            current_np = self.input_map[self.key_list[time_idx]]\n",
    "\n",
    "            # Use below when working on local computer to avoid singular matrix\n",
    "            #cur_heads = current_np[:21, :]\n",
    "            #neg_log_lik += self.full_likelihood(params, cur_heads, cur_heads[:, 2], covariance_function)\n",
    "\n",
    "            for index in range(0, self.size_per_hour):\n",
    "\n",
    "                \n",
    "                current_row = current_np[index].reshape(1, -1)\n",
    "                current_y = current_row[0, 2]\n",
    "\n",
    "                # Construct conditioning set\n",
    "                mm_neighbors = self.nns_map[index]\n",
    "                past = list(mm_neighbors)\n",
    "                data_list = []\n",
    "\n",
    "                if past:\n",
    "                    data_list.append(current_np[past])\n",
    "\n",
    "                if time_idx > 1:\n",
    "                    cov_matrix = self.cov_map[index]['cov_matrix']\n",
    "                    tmp_for_beta = self.cov_map[index]['tmp_for_beta']\n",
    "                    cov_xx_inv = self.cov_map[index]['cov_xx_inv']\n",
    "                    L_inv = self.cov_map[index]['L_inv']\n",
    "                    cov_ygivenx = self.cov_map[index]['cov_ygivenx']\n",
    "                    cond_mean_tmp = self.cov_map[index]['cond_mean_tmp']\n",
    "                    log_det = self.cov_map[index]['log_det']\n",
    "                    locs = self.cov_map[index]['locs']\n",
    "                    \n",
    "                    last_hour_np = self.input_map[self.key_list[time_idx - 1]]\n",
    "                    past_conditioning_data = last_hour_np[past + [index], :]\n",
    "                    data_list.append(past_conditioning_data)\n",
    "\n",
    "                    if data_list:\n",
    "                        conditioning_data = torch.vstack(data_list)\n",
    "                    else:\n",
    "                        conditioning_data = torch.empty((0, current_row.shape[1]), dtype=torch.float32)\n",
    "\n",
    "                    np_arr = torch.vstack((current_row, conditioning_data))\n",
    "                    y_and_neighbors = np_arr[:, 2]\n",
    "\n",
    "                    cov_yx = cov_matrix[0, 1:]\n",
    "\n",
    "                    tmp2 = torch.matmul(torch.matmul(L_inv, locs).T, torch.matmul(L_inv, y_and_neighbors))\n",
    "                    beta = torch.linalg.solve(tmp_for_beta, tmp2)\n",
    "\n",
    "                    mu = torch.matmul(locs, beta)\n",
    "                    mu_current = mu[0]\n",
    "                    mu_neighbors = mu[1:]\n",
    "                    \n",
    "                    # Mean and variance of y|x\n",
    "                    cond_mean = mu_current + torch.matmul(cond_mean_tmp, (y_and_neighbors[1:] - mu_neighbors))\n",
    "                    alpha = current_y - cond_mean\n",
    "                    quad_form = alpha**2 * (1 / cov_ygivenx)\n",
    "                    neg_log_lik += 0.5 * (log_det + quad_form)\n",
    "\n",
    "                    continue\n",
    "\n",
    "                if time_idx > 0:\n",
    "                    last_hour_np = self.input_map[self.key_list[time_idx - 1]]\n",
    "                    past_conditioning_data = last_hour_np[past + [index], :]\n",
    "                    data_list.append(past_conditioning_data)\n",
    "\n",
    "                if data_list:\n",
    "                    conditioning_data = torch.vstack(data_list)\n",
    "                else:\n",
    "                    conditioning_data = torch.empty((0, current_row.shape[1]), dtype=torch.float32)\n",
    "\n",
    "                np_arr = torch.vstack((current_row, conditioning_data))\n",
    "                y_and_neighbors = np_arr[:, 2]\n",
    "                locs = np_arr[:, :2]\n",
    "\n",
    "                cov_matrix = covariance_function(params=params, y= np_arr, x= np_arr)\n",
    "                # print(f'Condition number: {torch.linalg.cond(cov_matrix)}')\n",
    "                L = torch.linalg.cholesky(cov_matrix)\n",
    "                L11 = L[:1, :1]\n",
    "                L12 = torch.zeros(L[:1, 1:].shape)\n",
    "                L21 = L[1:, :1]\n",
    "                L22 = L[1:, 1:]\n",
    "                L11_inv = torch.linalg.inv(L11)\n",
    "                L22_inv = torch.linalg.inv(L22)\n",
    "\n",
    "                # First block: [L11_inv, L12]\n",
    "                upper_block = torch.cat((L11_inv, L12), dim=1)  # Concatenate along columns (dim=1)\n",
    "\n",
    "                # Second block: [-torch.matmul(torch.matmul(L22_inv, L21), L11_inv), L22_inv]\n",
    "                lower_left = -torch.matmul(torch.matmul(L22_inv, L21), L11_inv)\n",
    "                lower_block = torch.cat((lower_left, L22_inv), dim=1)  # Concatenate along columns (dim=1)\n",
    "\n",
    "                # Combine the upper and lower blocks\n",
    "                L_inv = torch.cat((upper_block, lower_block), dim=0)  # Concatenate along rows (dim=0)\n",
    "\n",
    "                cov_yx = cov_matrix[0, 1:]\n",
    "\n",
    "                tmp1 = torch.matmul(L_inv, locs)\n",
    "                tmp2 = torch.matmul(torch.matmul(L_inv, locs).T, torch.matmul(L_inv, y_and_neighbors))\n",
    "                tmp_for_beta = torch.matmul(tmp1.T, tmp1)\n",
    "                beta = torch.linalg.solve(tmp_for_beta, tmp2)\n",
    "\n",
    "                mu = torch.matmul(locs, beta)\n",
    "                mu_current = mu[0]\n",
    "                mu_neighbors = mu[1:]\n",
    "\n",
    "                # Mean and variance of y|x\n",
    "                sigma = cov_matrix[0, 0]\n",
    "                cov_xx = cov_matrix[1:, 1:]\n",
    "                cov_xx_inv = torch.linalg.inv(cov_xx)\n",
    "\n",
    "                cov_ygivenx = sigma - torch.matmul(cov_yx, torch.matmul(cov_xx_inv, cov_yx))\n",
    "                cond_mean_tmp = torch.matmul(cov_yx, cov_xx_inv)\n",
    "                cond_mean = mu_current + torch.matmul(cond_mean_tmp, (y_and_neighbors[1:] - mu_neighbors))\n",
    "                \n",
    "                alpha = current_y - cond_mean\n",
    "                quad_form = alpha**2 * (1 / cov_ygivenx)\n",
    "                log_det = torch.log(cov_ygivenx)\n",
    "                neg_log_lik += 0.5 * (log_det + quad_form)\n",
    "\n",
    "             \n",
    "                if time_idx == 1:\n",
    "                    self.cov_map[index] = {\n",
    "                        'tmp_for_beta': tmp_for_beta,\n",
    "                        'cov_xx_inv': cov_xx_inv,\n",
    "                        'cov_matrix': cov_matrix,\n",
    "                        'L_inv': L_inv,\n",
    "                        'cov_ygivenx': cov_ygivenx,\n",
    "                        'cond_mean_tmp': cond_mean_tmp,\n",
    "                        'log_det': log_det,\n",
    "                        'locs': locs\n",
    "                    }\n",
    "\n",
    "        return neg_log_lik  \n",
    "\n",
    "## add base_list three times  when time_idx =0 1 >0\n",
    "\n",
    "    def vecchia_extrapolate(self, params: torch.Tensor, covariance_function, cut_line=200) -> torch.Tensor:\n",
    "        self.cov_map = defaultdict(list)\n",
    "        neg_log_lik = 0.0\n",
    "\n",
    "        key_list = sorted(analysis_data_map)\n",
    "        cut_line = cut_line\n",
    "        heads = analysis_data_map[key_list[0]][:cut_line,:]\n",
    "        for time_idx in range(1, len(analysis_data_map)):\n",
    "            tmp = analysis_data_map[key_list[time_idx]][:cut_line,:]\n",
    "            heads = torch.cat( (heads,tmp), dim=0)\n",
    "\n",
    "        neg_log_lik += self.full_likelihood(params, heads, heads[:, 2], covariance_function)          \n",
    "        \n",
    "        for time_idx in range(len(self.input_map)):\n",
    "            current_np = self.input_map[self.key_list[time_idx]]\n",
    "\n",
    "            # Use below when working on local computer to avoid singular matrix\n",
    "            # cur_heads = current_np[:5, :]\n",
    "            # neg_log_lik += self.full_likelihood(params, cur_heads, cur_heads[:, 2], covariance_function)\n",
    "\n",
    "            for index in range(cut_line, self.size_per_hour):\n",
    "                current_row = current_np[index].reshape(1, -1)\n",
    "                current_y = current_row[0, 2]\n",
    "\n",
    "                # Construct conditioning set\n",
    "                mm_neighbors = self.nns_map[index]\n",
    "                past = list(mm_neighbors) \n",
    "                data_list = []\n",
    "\n",
    "                if past:\n",
    "                    data_list.append(current_np[past])\n",
    "\n",
    "                if time_idx > 1:\n",
    "                    cov_matrix = self.cov_map[index]['cov_matrix']\n",
    "                    tmp_for_beta = self.cov_map[index]['tmp_for_beta']\n",
    "                    cov_xx_inv = self.cov_map[index]['cov_xx_inv']\n",
    "            \n",
    "                    cov_ygivenx = self.cov_map[index]['cov_ygivenx']\n",
    "                    cond_mean_tmp = self.cov_map[index]['cond_mean_tmp']\n",
    "                    log_det = self.cov_map[index]['log_det']\n",
    "                    locs = self.cov_map[index]['locs']\n",
    "                    \n",
    "                    last_hour_np = self.input_map[self.key_list[time_idx - 1]]\n",
    "                    past_conditioning_data = last_hour_np[past + [index] , :]\n",
    "                    data_list.append(past_conditioning_data)\n",
    "\n",
    "                    if data_list:\n",
    "                        conditioning_data = torch.vstack(data_list)\n",
    "                    else:\n",
    "                        conditioning_data = torch.empty((0, current_row.shape[1]), dtype=torch.float32)\n",
    "\n",
    "                    np_arr = torch.vstack((current_row, conditioning_data))\n",
    "                    y_and_neighbors = np_arr[:, 2]\n",
    "\n",
    "                    cov_yx = cov_matrix[0, 1:]\n",
    "\n",
    "                    y_arr = y_and_neighbors\n",
    "                    tmp1 = tmp_for_beta\n",
    "                    tmp2 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, y_arr))\n",
    "                    beta = torch.linalg.solve(tmp1, tmp2)\n",
    "\n",
    "                    mu = torch.matmul(locs, beta)\n",
    "                    mu_current = mu[0]\n",
    "                    mu_neighbors = mu[1:]\n",
    "                    \n",
    "                    # Mean and variance of y|x\n",
    "                    cond_mean = mu_current + torch.matmul(cond_mean_tmp, (y_and_neighbors[1:] - mu_neighbors))\n",
    "                    alpha = current_y - cond_mean\n",
    "                    quad_form = alpha**2 * (1 / cov_ygivenx)\n",
    "                    neg_log_lik += 0.5 * (log_det + quad_form)\n",
    "\n",
    "                    continue\n",
    "\n",
    "                if time_idx > 0:\n",
    "                    last_hour_np = self.input_map[self.key_list[time_idx - 1]]\n",
    "                    past_conditioning_data = last_hour_np[past + [index], :]\n",
    "                    data_list.append(past_conditioning_data)\n",
    "\n",
    "                if data_list:\n",
    "                    conditioning_data = torch.vstack(data_list)\n",
    "                else:\n",
    "                    conditioning_data = torch.empty((0, current_row.shape[1]), dtype=torch.float32)\n",
    "\n",
    "                np_arr = torch.vstack((current_row, conditioning_data))\n",
    "                y_and_neighbors = np_arr[:, 2]\n",
    "                locs = np_arr[:, :2]\n",
    "\n",
    "                cov_matrix = covariance_function(params=params, y= np_arr, x= np_arr)\n",
    "                # print(f'Condition number: {torch.linalg.cond(cov_matrix)}')\n",
    "                cov_yx = cov_matrix[0, 1:]\n",
    "                        # Compute the log determinant of the covariance matrix\n",
    "                sign, log_det = torch.slogdet(cov_matrix)\n",
    "                # if sign <= 0:\n",
    "                #     raise ValueError(\"Covariance matrix is not positive definite\")\n",
    "            \n",
    "                y_arr = y_and_neighbors\n",
    "                # Compute beta\n",
    "                tmp1 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, locs))\n",
    "                tmp2 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, y_arr))\n",
    "                beta = torch.linalg.solve(tmp1, tmp2)\n",
    "\n",
    "                mu = torch.matmul(locs, beta)\n",
    "                mu_current = mu[0]\n",
    "                mu_neighbors = mu[1:]\n",
    "\n",
    "                # Mean and variance of y|x\n",
    "                sigma = cov_matrix[0, 0]\n",
    "                cov_xx = cov_matrix[1:, 1:]\n",
    "                cov_xx_inv = torch.linalg.inv(cov_xx)\n",
    "\n",
    "                cov_ygivenx = sigma - torch.matmul(cov_yx, torch.matmul(cov_xx_inv, cov_yx))\n",
    "                cond_mean_tmp = torch.matmul(cov_yx, cov_xx_inv)\n",
    "                cond_mean = mu_current + torch.matmul(cond_mean_tmp, (y_and_neighbors[1:] - mu_neighbors))\n",
    "                \n",
    "                alpha = current_y - cond_mean\n",
    "                quad_form = alpha**2 * (1 / cov_ygivenx)\n",
    "                log_det = torch.log(cov_ygivenx)\n",
    "                neg_log_lik += 0.5 * (log_det + quad_form)\n",
    " \n",
    "                if time_idx == 1:\n",
    "                    self.cov_map[index] = {\n",
    "                        'tmp_for_beta': tmp1,\n",
    "                        'cov_xx_inv': cov_xx_inv,\n",
    "                        'cov_matrix': cov_matrix,\n",
    "               \n",
    "                        'cov_ygivenx': cov_ygivenx,\n",
    "                        'cond_mean_tmp': cond_mean_tmp,\n",
    "                        'log_det': log_det,\n",
    "                        'locs': locs\n",
    "                    }\n",
    "        return neg_log_lik\n",
    "\n",
    "\n",
    "    def vecchia_local_extra_base(self, params: torch.Tensor, covariance_function) -> torch.Tensor:\n",
    "        self.cov_map = defaultdict(list)\n",
    "        neg_log_lik = 0.0\n",
    "        \n",
    "        for time_idx in range(len(self.input_map)):\n",
    "            current_np = self.input_map[self.key_list[time_idx]]\n",
    "\n",
    "            # Use below when working on local computer to avoid singular matrix\n",
    "            # cur_heads = current_np[:21, :]\n",
    "            # neg_log_lik += self.full_likelihood(params, cur_heads, cur_heads[:, 2], covariance_function)\n",
    "\n",
    "            for index in range(0, self.size_per_hour):\n",
    "                current_row = current_np[index].reshape(1, -1)\n",
    "                current_y = current_row[0, 2]\n",
    "\n",
    "                # Construct conditioning set\n",
    "                mm_neighbors = self.nns_map[index]\n",
    "                past = list(mm_neighbors) + base_list\n",
    "                data_list = []\n",
    "\n",
    "                if past:\n",
    "                    data_list.append(current_np[past])\n",
    "\n",
    "                if time_idx > 1:\n",
    "                    cov_matrix = self.cov_map[index]['cov_matrix']\n",
    "                    tmp_for_beta = self.cov_map[index]['tmp_for_beta']\n",
    "                    cov_xx_inv = self.cov_map[index]['cov_xx_inv']\n",
    "            \n",
    "                    cov_ygivenx = self.cov_map[index]['cov_ygivenx']\n",
    "                    cond_mean_tmp = self.cov_map[index]['cond_mean_tmp']\n",
    "                    log_det = self.cov_map[index]['log_det']\n",
    "                    locs = self.cov_map[index]['locs']\n",
    "                    \n",
    "                    last_hour_np = self.input_map[self.key_list[time_idx - 1]]\n",
    "                    past_conditioning_data = last_hour_np[past + [index] + base_list, :]\n",
    "                    data_list.append(past_conditioning_data)\n",
    "\n",
    "                    if data_list:\n",
    "                        conditioning_data = torch.vstack(data_list)\n",
    "                    else:\n",
    "                        conditioning_data = torch.empty((0, current_row.shape[1]), dtype=torch.float32)\n",
    "\n",
    "                    np_arr = torch.vstack((current_row, conditioning_data))\n",
    "                    y_and_neighbors = np_arr[:, 2]\n",
    "\n",
    "                    cov_yx = cov_matrix[0, 1:]\n",
    "\n",
    "                    y_arr = y_and_neighbors\n",
    "                    tmp1 = tmp_for_beta\n",
    "                    tmp2 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, y_arr))\n",
    "                    beta = torch.linalg.solve(tmp1, tmp2)\n",
    "\n",
    "                    mu = torch.matmul(locs, beta)\n",
    "                    mu_current = mu[0]\n",
    "                    mu_neighbors = mu[1:]\n",
    "                    \n",
    "                    # Mean and variance of y|x\n",
    "                    cond_mean = mu_current + torch.matmul(cond_mean_tmp, (y_and_neighbors[1:] - mu_neighbors))\n",
    "                    alpha = current_y - cond_mean\n",
    "                    quad_form = alpha**2 * (1 / cov_ygivenx)\n",
    "                    neg_log_lik += 0.5 * (log_det + quad_form)\n",
    "\n",
    "                    continue\n",
    "\n",
    "                if time_idx > 0:\n",
    "                    last_hour_np = self.input_map[self.key_list[time_idx - 1]]\n",
    "                    past_conditioning_data = last_hour_np[past + [index]+ base_list, :]\n",
    "                    data_list.append(past_conditioning_data)\n",
    "\n",
    "                if data_list:\n",
    "                    conditioning_data = torch.vstack(data_list)\n",
    "                else:\n",
    "                    conditioning_data = torch.empty((0, current_row.shape[1]), dtype=torch.float32)\n",
    "\n",
    "                np_arr = torch.vstack((current_row, conditioning_data))\n",
    "                y_and_neighbors = np_arr[:, 2]\n",
    "                locs = np_arr[:, :2]\n",
    "\n",
    "                cov_matrix = covariance_function(params=params, y= np_arr, x= np_arr)\n",
    "                # print(f'Condition number: {torch.linalg.cond(cov_matrix)}')\n",
    "                cov_yx = cov_matrix[0, 1:]\n",
    "                        # Compute the log determinant of the covariance matrix\n",
    "                sign, log_det = torch.slogdet(cov_matrix)\n",
    "                # if sign <= 0:\n",
    "                #     raise ValueError(\"Covariance matrix is not positive definite\")\n",
    "            \n",
    "                y_arr = y_and_neighbors\n",
    "                # Compute beta\n",
    "                tmp1 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, locs))\n",
    "                tmp2 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, y_arr))\n",
    "                beta = torch.linalg.solve(tmp1, tmp2)\n",
    "\n",
    "                mu = torch.matmul(locs, beta)\n",
    "                mu_current = mu[0]\n",
    "                mu_neighbors = mu[1:]\n",
    "\n",
    "                # Mean and variance of y|x\n",
    "                sigma = cov_matrix[0, 0]\n",
    "                cov_xx = cov_matrix[1:, 1:]\n",
    "                cov_xx_inv = torch.linalg.inv(cov_xx)\n",
    "\n",
    "                cov_ygivenx = sigma - torch.matmul(cov_yx, torch.matmul(cov_xx_inv, cov_yx))\n",
    "                cond_mean_tmp = torch.matmul(cov_yx, cov_xx_inv)\n",
    "                cond_mean = mu_current + torch.matmul(cond_mean_tmp, (y_and_neighbors[1:] - mu_neighbors))\n",
    "                \n",
    "                alpha = current_y - cond_mean\n",
    "                quad_form = alpha**2 * (1 / cov_ygivenx)\n",
    "                log_det = torch.log(cov_ygivenx)\n",
    "                neg_log_lik += 0.5 * (log_det + quad_form)\n",
    " \n",
    "                if time_idx == 1:\n",
    "                    self.cov_map[index] = {\n",
    "                        'tmp_for_beta': tmp1,\n",
    "                        'cov_xx_inv': cov_xx_inv,\n",
    "                        'cov_matrix': cov_matrix,\n",
    "               \n",
    "                        'cov_ygivenx': cov_ygivenx,\n",
    "                        'cond_mean_tmp': cond_mean_tmp,\n",
    "                        'log_det': log_det,\n",
    "                        'locs': locs\n",
    "                    }\n",
    "        return neg_log_lik\n",
    "    \n",
    "\n",
    "    def vecchia_interpolation_1to6(self, params: torch.Tensor, covariance_function, cut_line=200) -> torch.Tensor:\n",
    "        self.cov_map = defaultdict(list)\n",
    "        neg_log_lik = 0.0\n",
    "        key_list = sorted(analysis_data_map)\n",
    "        cut_line = cut_line\n",
    "        heads = analysis_data_map[key_list[0]][:cut_line,:]\n",
    "        for time_idx in range(1, len(analysis_data_map)):\n",
    "            tmp = analysis_data_map[key_list[time_idx]][:cut_line,:]\n",
    "            heads = torch.cat( (heads,tmp), dim=0)\n",
    "\n",
    "        neg_log_lik += self.full_likelihood(params, heads, heads[:, 2], covariance_function)          \n",
    "        \n",
    "        for time_idx in range(0,len(self.input_map)):\n",
    "            current_np = self.input_map[self.key_list[time_idx]]\n",
    "\n",
    "            # Use below when working on local computer to avoid singular matrix\n",
    "            for index in range(cut_line, self.size_per_hour):\n",
    "                current_row = current_np[index].reshape(1, -1)\n",
    "                current_y = current_row[0, 2]\n",
    "\n",
    "                # Construct conditioning set\n",
    "                mm_neighbors = self.nns_map[index]\n",
    "                past = list(mm_neighbors) \n",
    "                data_list = []\n",
    "\n",
    "                if past:\n",
    "                    data_list.append(current_np[past])\n",
    "\n",
    "                if time_idx > 0 and time_idx<7:\n",
    "                    last_hour_np = self.input_map[self.key_list[time_idx - 1]]\n",
    "                    past_conditioning_data = last_hour_np[past + [index], :]\n",
    "                    data_list.append(past_conditioning_data)\n",
    "\n",
    "                    last_hour_np = self.input_map[self.key_list[time_idx +1]]\n",
    "                    # if index==200:\n",
    "                    #     print(self.input_map[self.key_list[time_idx-6]])\n",
    "                    past_conditioning_data = last_hour_np[past + [index], :]\n",
    "                    data_list.append(past_conditioning_data)\n",
    "                \n",
    "                if data_list:\n",
    "                    conditioning_data = torch.vstack(data_list)\n",
    "                else:\n",
    "                    conditioning_data = torch.empty((0, current_row.shape[1]), dtype=torch.float32)\n",
    "\n",
    "                np_arr = torch.vstack((current_row, conditioning_data))\n",
    "                y_and_neighbors = np_arr[:, 2]\n",
    "                locs = np_arr[:, :2]\n",
    "\n",
    "                cov_matrix = covariance_function(params=params, y= np_arr, x= np_arr)\n",
    "                # print(f'Condition number: {torch.linalg.cond(cov_matrix)}')\n",
    "                cov_yx = cov_matrix[0, 1:]\n",
    "                        # Compute the log determinant of the covariance matrix\n",
    "                sign, log_det = torch.slogdet(cov_matrix)\n",
    "                # if sign <= 0:\n",
    "                #     raise ValueError(\"Covariance matrix is not positive definite\")\n",
    "            \n",
    "                y_arr = y_and_neighbors\n",
    "                # Compute beta\n",
    "                tmp1 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, locs))\n",
    "                tmp2 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, y_arr))\n",
    "                beta = torch.linalg.solve(tmp1, tmp2)\n",
    "\n",
    "                mu = torch.matmul(locs, beta)\n",
    "                mu_current = mu[0]\n",
    "                mu_neighbors = mu[1:]\n",
    "\n",
    "                # Mean and variance of y|x\n",
    "                sigma = cov_matrix[0, 0]\n",
    "                cov_xx = cov_matrix[1:, 1:]\n",
    "                cov_xx_inv = torch.linalg.inv(cov_xx)\n",
    "\n",
    "                cov_ygivenx = sigma - torch.matmul(cov_yx, torch.matmul(cov_xx_inv, cov_yx))\n",
    "                cond_mean_tmp = torch.matmul(cov_yx, cov_xx_inv)\n",
    "                cond_mean = mu_current + torch.matmul(cond_mean_tmp, (y_and_neighbors[1:] - mu_neighbors))\n",
    "                \n",
    "                alpha = current_y - cond_mean\n",
    "                quad_form = alpha**2 * (1 / cov_ygivenx)\n",
    "                log_det = torch.log(cov_ygivenx)\n",
    "                neg_log_lik += 0.5 * (log_det + quad_form)\n",
    " \n",
    "        return neg_log_lik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class torch_vecchia_exp(matern_advec_beta_torch_vecchia):\n",
    "    def __init__(self, analaysis_data_map: torch.Tensor, params: torch.Tensor, nns_map=nns_map, mm_cond_number=mm_cond_number):\n",
    "        super().__init__(analaysis_data_map, params, nns_map, mm_cond_number)\n",
    "        # Any additional initialization for dignosis class can go here\n",
    "\n",
    "\n",
    "    def vecchia_interpolation_1to6(self, params: torch.Tensor, covariance_function, cut_line) -> torch.Tensor:\n",
    "        self.cov_map = defaultdict(list)\n",
    "        neg_log_lik = 0.0\n",
    "        key_list = sorted(analysis_data_map)\n",
    "        cut_line = cut_line\n",
    "        heads = analysis_data_map[key_list[0]][:cut_line,:]\n",
    "        for time_idx in range(1, len(analysis_data_map)):\n",
    "            tmp = analysis_data_map[key_list[time_idx]][:cut_line,:]\n",
    "            heads = torch.cat( (heads,tmp), dim=0)\n",
    "\n",
    "        neg_log_lik += self.full_likelihood(params, heads, heads[:, 2], covariance_function)          \n",
    "        \n",
    "        for time_idx in range(0,len(self.input_map)):\n",
    "            current_np = self.input_map[self.key_list[time_idx]]\n",
    "\n",
    "            # Use below when working on local computer to avoid singular matrix\n",
    "            for index in range(cut_line, self.size_per_hour):\n",
    "                current_row = current_np[index].reshape(1, -1)\n",
    "                current_y = current_row[0, 2]\n",
    "\n",
    "                # Construct conditioning set\n",
    "                mm_neighbors = self.nns_map[index]\n",
    "                past = list(mm_neighbors) \n",
    "                data_list = []\n",
    "\n",
    "                if past:\n",
    "                    data_list.append(current_np[past])\n",
    "\n",
    "                if time_idx > 0 and time_idx<7:\n",
    "                    last_hour_np = self.input_map[self.key_list[time_idx - 1]]\n",
    "                    past_conditioning_data = last_hour_np[past + [index], :]\n",
    "                    data_list.append(past_conditioning_data)\n",
    "\n",
    "                    last_hour_np = self.input_map[self.key_list[time_idx +1]]\n",
    "                    # if index==200:\n",
    "                    #     print(self.input_map[self.key_list[time_idx-6]])\n",
    "                    past_conditioning_data = last_hour_np[past + [index], :]\n",
    "                    data_list.append(past_conditioning_data)\n",
    "                \n",
    "                if data_list:\n",
    "                    conditioning_data = torch.vstack(data_list)\n",
    "                else:\n",
    "                    conditioning_data = torch.empty((0, current_row.shape[1]), dtype=torch.float32)\n",
    "\n",
    "                np_arr = torch.vstack((current_row, conditioning_data))\n",
    "                y_and_neighbors = np_arr[:, 2]\n",
    "                locs = np_arr[:, :2]\n",
    "\n",
    "                cov_matrix = covariance_function(params=params, y= np_arr, x= np_arr)\n",
    "                # print(f'Condition number: {torch.linalg.cond(cov_matrix)}')\n",
    "                cov_yx = cov_matrix[0, 1:]\n",
    "                        # Compute the log determinant of the covariance matrix\n",
    "                sign, log_det = torch.slogdet(cov_matrix)\n",
    "                # if sign <= 0:\n",
    "                #     raise ValueError(\"Covariance matrix is not positive definite\")\n",
    "            \n",
    "                y_arr = y_and_neighbors\n",
    "                # Compute beta\n",
    "                tmp1 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, locs))\n",
    "                tmp2 = torch.matmul(locs.T, torch.linalg.solve(cov_matrix, y_arr))\n",
    "                beta = torch.linalg.solve(tmp1, tmp2)\n",
    "\n",
    "                mu = torch.matmul(locs, beta)\n",
    "                mu_current = mu[0]\n",
    "                mu_neighbors = mu[1:]\n",
    "\n",
    "                # Mean and variance of y|x\n",
    "                sigma = cov_matrix[0, 0]\n",
    "                cov_xx = cov_matrix[1:, 1:]\n",
    "                cov_xx_inv = torch.linalg.inv(cov_xx)\n",
    "\n",
    "                cov_ygivenx = sigma - torch.matmul(cov_yx, torch.matmul(cov_xx_inv, cov_yx))\n",
    "                cond_mean_tmp = torch.matmul(cov_yx, cov_xx_inv)\n",
    "                cond_mean = mu_current + torch.matmul(cond_mean_tmp, (y_and_neighbors[1:] - mu_neighbors))\n",
    "                \n",
    "                alpha = current_y - cond_mean\n",
    "                quad_form = alpha**2 * (1 / cov_ygivenx)\n",
    "                log_det = torch.log(cov_ygivenx)\n",
    "                neg_log_lik += 0.5 * (log_det + quad_form)\n",
    " \n",
    "        return neg_log_lik\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15183.4766, grad_fn=<AddBackward0>)\n",
      "tensor(14320.8662, grad_fn=<AddBackward0>)\n",
      "tensor(14320.8652, grad_fn=<AddBackward0>)\n",
      "tensor(-63.5469, grad_fn=<SubBackward0>) tensor(-63.5459, grad_fn=<SubBackward0>)\n",
      "day1 head 100 finish\n",
      "tensor(14320.8662, grad_fn=<AddBackward0>)\n",
      "tensor(12932.4326, grad_fn=<AddBackward0>)\n",
      "tensor(12932.4307, grad_fn=<AddBackward0>)\n",
      "tensor(-55.1963, grad_fn=<SubBackward0>) tensor(-55.1943, grad_fn=<SubBackward0>)\n",
      "day2 head 100 finish\n",
      "tensor(12932.4326, grad_fn=<AddBackward0>)\n",
      "tensor(15183.4766, grad_fn=<AddBackward0>)\n",
      "tensor(15183.4766, grad_fn=<AddBackward0>)\n",
      "tensor(-77.8057, grad_fn=<SubBackward0>) tensor(-77.8057, grad_fn=<SubBackward0>)\n",
      "day3 head 100 finish\n"
     ]
    }
   ],
   "source": [
    "lat_lon_resolution = [4,4]\n",
    "day = 4\n",
    "head100map = defaultdict(list)\n",
    "for day in range(1,4):\n",
    "    mm_cond_number = 10\n",
    "\n",
    "    years = ['2024']\n",
    "    month_range =[7,8]\n",
    "    idx_for_datamap= [ 8*(day-1),8*day]\n",
    "\n",
    "    instance = load_data_local_computer()\n",
    "    map, ord_mm, nns_map= instance.load_mm20k_data_bymonthyear( lat_lon_resolution= lat_lon_resolution, mm_cond_number=mm_cond_number,years_=years, months_=month_range)\n",
    "    analysis_data_map, aggregated_data = instance.load_working_data_byday( map, ord_mm, nns_map, idx_for_datamap= idx_for_datamap)\n",
    "\n",
    "    params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "    params = torch.tensor(params, requires_grad=True)\n",
    "    instance = torch_vecchia_exp(analysis_data_map, params, nns_map, mm_cond_number)\n",
    "    out = instance.full_likelihood(params, aggregated_data[:,:4],aggregated_data[:,2], instance.matern_cov_ani)\n",
    "    print(out0)  # 15105\n",
    "\n",
    "    out0 = instance.vecchia_local4(params, instance.matern_cov_ani,200)\n",
    "    print(out0)   # 15181.4775  15183 0,1 lage 15185 0,2\n",
    "    #out1 = instance.vecchia_local2(params, instance.matern_cov_ani)\n",
    "    #print(out1) \n",
    "\n",
    "    out1 = instance.vecchia_extrapolate(params, instance.matern_cov_ani, 200)\n",
    "    print(out1) \n",
    "\n",
    "    # out1 = instance.vecchia_interpolation_1to6(params, instance.matern_cov_ani, 200)\n",
    "    # print(out1)   # 15181.4775  15183 0,1 lage 15185 0,2\n",
    "    head100map[day]  = [out0, out1]\n",
    "    print( out-out0, out-out1)\n",
    "    print(f'day{day} head 100 finish')\n",
    "    # 1m 55sec day1\n",
    "    # 51193.6875  full  51067.4609 for 200 head\n",
    "    # 51100.9297 upgrade 200\n",
    "    # 51097.5546 800\n",
    "\n",
    "    # 51076.2344 for 300 head\n",
    "    # 51084  800 head\n",
    "\n",
    "# day3  55432.1406 full  55388.6562 800  55387.1641 200  55336 10\n",
    "# 55469 800 upgrade\n",
    "# 55486.7305  200 upgrade\n",
    "\n",
    "# future idx+1<8 and idx>2 and past if>0\n",
    "# 16 -6 -15 17 -9 33\n",
    "# if >1  4 -18 -28  11 25 27\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor(14238.2979, grad_fn=<AddBackward0>)\n",
    "day1 head 100 finish\n",
    "tensor(12882.5879, grad_fn=<AddBackward0>)\n",
    "day2 head 100 finish\n",
    "tensor(15119.4678, grad_fn=<AddBackward0>)\n",
    "day3 head 100 finish\n",
    "tensor(15028.9971, grad_fn=<AddBackward0>)\n",
    "day4 head 100 finish\n",
    "tensor(12730.4756, grad_fn=<AddBackward0>)\n",
    "day5 head 100 finish\n",
    "tensor(14921.2373, grad_fn=<AddBackward0>)\n",
    "day6 head 100 finish\n",
    "\n",
    "idx+1<8\n",
    "\n",
    "tensor(14257.3193, grad_fn=<MulBackward0>)\n",
    "tensor(14205.8691, grad_fn=<AddBackward0>)\n",
    "day1 head 100 finish\n",
    "tensor(12877.2363, grad_fn=<MulBackward0>)\n",
    "tensor(12851.4062, grad_fn=<AddBackward0>)\n",
    "day2 head 100 finish\n",
    "tensor(15105.6709, grad_fn=<MulBackward0>)\n",
    "tensor(15082.1377, grad_fn=<AddBackward0>)\n",
    "day3 head 100 finish\n",
    "\n",
    "tensor(15057.4072, grad_fn=<MulBackward0>)\n",
    "tensor(15003.6914, grad_fn=<AddBackward0>)\n",
    "day4 head 100 finish\n",
    "tensor(12729.2949, grad_fn=<MulBackward0>)\n",
    "tensor(12706.5537, grad_fn=<AddBackward0>)\n",
    "day5 head 100 finish\n",
    "tensor(14974.8105, grad_fn=<MulBackward0>)\n",
    "tensor(14888.0811, grad_fn=<AddBackward0>)\n",
    "day6 head 100 finish\n",
    "\n",
    "idx+1<8   idx>0\n",
    "42 18 9 48 17 7 \n",
    "\n",
    "tensor(14257.3193, grad_fn=<MulBackward0>)\n",
    "tensor(14215.7100, grad_fn=<AddBackward0>)\n",
    "day1 head 100 finish\n",
    "tensor(12877.2363, grad_fn=<MulBackward0>)\n",
    "tensor(12859.1016, grad_fn=<AddBackward0>)\n",
    "day2 head 100 finish\n",
    "tensor(15105.6709, grad_fn=<MulBackward0>)\n",
    "tensor(15096.2041, grad_fn=<AddBackward0>)\n",
    "day3 head 100 finish\n",
    "tensor(15057.4072, grad_fn=<MulBackward0>)\n",
    "tensor(15009.4727, grad_fn=<AddBackward0>)\n",
    "day4 head 100 finish\n",
    "tensor(12729.2949, grad_fn=<MulBackward0>)\n",
    "tensor(12712.2295, grad_fn=<AddBackward0>)\n",
    "day5 head 100 finish\n",
    "tensor(14974.8105, grad_fn=<MulBackward0>)\n",
    "tensor(14896.7686, grad_fn=<AddBackward0>)\n",
    "day6 head 100 finish\n",
    "\n",
    "idx+1<8 and idx>1\n",
    "30 6 -2 23 5 64\n",
    "\n",
    "tensor(14257.3193, grad_fn=<MulBackward0>)\n",
    "tensor(14227.0830, grad_fn=<AddBackward0>)\n",
    "day1 head 100 finish\n",
    "tensor(12877.2363, grad_fn=<MulBackward0>)\n",
    "tensor(12871.8301, grad_fn=<AddBackward0>)\n",
    "day2 head 100 finish\n",
    "tensor(15105.6709, grad_fn=<MulBackward0>)\n",
    "tensor(15107.1260, grad_fn=<AddBackward0>)\n",
    "day3 head 100 finish\n",
    "\n",
    "tensor(15057.4072, grad_fn=<MulBackward0>)\n",
    "tensor(15022.5693, grad_fn=<AddBackward0>)\n",
    "day4 head 100 finish\n",
    "tensor(12729.2949, grad_fn=<MulBackward0>)\n",
    "tensor(12724.7764, grad_fn=<AddBackward0>)\n",
    "day5 head 100 finish\n",
    "tensor(14974.8105, grad_fn=<MulBackward0>)\n",
    "tensor(14910.9521, grad_fn=<AddBackward0>)\n",
    "day6 head 100 finish\n",
    "\n",
    "idx+1<8 and idx>2\n",
    "16 -6 -15 17 -9 33\n",
    "\n",
    "tensor(14257.3193, grad_fn=<MulBackward0>)\n",
    "tensor(14241.9834, grad_fn=<AddBackward0>)\n",
    "day1 head 100 finish\n",
    "tensor(12877.2363, grad_fn=<MulBackward0>)\n",
    "tensor(12884.8262, grad_fn=<AddBackward0>)\n",
    "day2 head 100 finish\n",
    "tensor(15105.6709, grad_fn=<MulBackward0>)\n",
    "tensor(15120.7217, grad_fn=<AddBackward0>)\n",
    "day3 head 100 finish\n",
    "tensor(15057.4072, grad_fn=<MulBackward0>)\n",
    "tensor(15040.2734, grad_fn=<AddBackward0>)\n",
    "day4 head 100 finish\n",
    "tensor(12729.2949, grad_fn=<MulBackward0>)\n",
    "tensor(12738.7207, grad_fn=<AddBackward0>)\n",
    "day5 head 100 finish\n",
    "tensor(14974.8105, grad_fn=<MulBackward0>)\n",
    "tensor(14931.5791, grad_fn=<AddBackward0>)\n",
    "day6 head 100 finish\n",
    "\n",
    "idx+1<8 and idx>3\n",
    "-4 -22 -20 -1 -20 17\n",
    "\n",
    "tensor(14257.3193, grad_fn=<MulBackward0>)\n",
    "tensor(14261.0654, grad_fn=<AddBackward0>)\n",
    "day1 head 100 finish\n",
    "tensor(12877.2363, grad_fn=<MulBackward0>)\n",
    "tensor(12899.8008, grad_fn=<AddBackward0>)\n",
    "day2 head 100 finish\n",
    "tensor(15105.6709, grad_fn=<MulBackward0>)\n",
    "tensor(15135.3936, grad_fn=<AddBackward0>)\n",
    "day3 head 100 finish\n",
    "tensor(15057.4072, grad_fn=<MulBackward0>)\n",
    "tensor(15058.9043, grad_fn=<AddBackward0>)\n",
    "day4 head 100 finish\n",
    "tensor(12729.2949, grad_fn=<MulBackward0>)\n",
    "tensor(12749.0693, grad_fn=<AddBackward0>)\n",
    "day5 head 100 finish\n",
    "tensor(14974.8105, grad_fn=<MulBackward0>)\n",
    "tensor(14957.6123, grad_fn=<AddBackward0>)\n",
    "day6 head 100 finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14257.3193, grad_fn=<MulBackward0>)\n",
      "tensor(14192.7080, grad_fn=<AddBackward0>)\n",
      "day1 head finish\n",
      "tensor(12877.2363, grad_fn=<MulBackward0>)\n",
      "tensor(12859.2549, grad_fn=<AddBackward0>)\n",
      "day2 head finish\n",
      "tensor(15105.6709, grad_fn=<MulBackward0>)\n",
      "tensor(15094.1035, grad_fn=<AddBackward0>)\n",
      "day3 head finish\n",
      "tensor(15057.4072, grad_fn=<MulBackward0>)\n",
      "tensor(15014.3359, grad_fn=<AddBackward0>)\n",
      "day4 head finish\n",
      "tensor(12729.2949, grad_fn=<MulBackward0>)\n",
      "tensor(12713.2412, grad_fn=<AddBackward0>)\n",
      "day5 head finish\n",
      "tensor(14974.8105, grad_fn=<MulBackward0>)\n",
      "tensor(14894.7402, grad_fn=<AddBackward0>)\n",
      "day6 head finish\n",
      "tensor(15914.4395, grad_fn=<MulBackward0>)\n",
      "tensor(15894.6680, grad_fn=<AddBackward0>)\n",
      "day7 head finish\n",
      "tensor(15345.4297, grad_fn=<MulBackward0>)\n",
      "tensor(15312.1436, grad_fn=<AddBackward0>)\n",
      "day8 head finish\n",
      "tensor(13209.8857, grad_fn=<MulBackward0>)\n",
      "tensor(13196.6914, grad_fn=<AddBackward0>)\n",
      "day9 head finish\n",
      "tensor(15197.7314, grad_fn=<MulBackward0>)\n",
      "tensor(15158.7930, grad_fn=<AddBackward0>)\n",
      "day10 head finish\n",
      "tensor(13394.0176, grad_fn=<MulBackward0>)\n",
      "tensor(13349.6113, grad_fn=<AddBackward0>)\n",
      "day11 head finish\n",
      "tensor(12367.9697, grad_fn=<MulBackward0>)\n",
      "tensor(12352.4678, grad_fn=<AddBackward0>)\n",
      "day12 head finish\n",
      "tensor(12334.4551, grad_fn=<MulBackward0>)\n",
      "tensor(12336.3594, grad_fn=<AddBackward0>)\n",
      "day13 head finish\n",
      "tensor(13388.7969, grad_fn=<MulBackward0>)\n",
      "tensor(13354.2090, grad_fn=<AddBackward0>)\n",
      "day14 head finish\n",
      "tensor(11979.0537, grad_fn=<MulBackward0>)\n",
      "tensor(11971.5322, grad_fn=<AddBackward0>)\n",
      "day15 head finish\n",
      "tensor(12619.8984, grad_fn=<MulBackward0>)\n",
      "tensor(12590.7852, grad_fn=<AddBackward0>)\n",
      "day16 head finish\n",
      "tensor(14524.1982, grad_fn=<MulBackward0>)\n",
      "tensor(14467.9746, grad_fn=<AddBackward0>)\n",
      "day17 head finish\n",
      "tensor(13897.5332, grad_fn=<MulBackward0>)\n",
      "tensor(13869.3936, grad_fn=<AddBackward0>)\n",
      "day18 head finish\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "lat_lon_resolution = [4,4]\n",
    "\n",
    "head200map = defaultdict(list)\n",
    "for day in range(1,31):\n",
    "    mm_cond_number = 10\n",
    "\n",
    "    years = ['2024']\n",
    "    month_range =[7,8]\n",
    "    idx_for_datamap= [ 8*(day-1),8*day]\n",
    "\n",
    "    instance = load_data_local_computer()\n",
    "    map, ord_mm, nns_map= instance.load_mm20k_data_bymonthyear( lat_lon_resolution= lat_lon_resolution, mm_cond_number=mm_cond_number,years_=years, months_=month_range)\n",
    "    analysis_data_map, aggregated_data = instance.load_working_data_byday( map, ord_mm, nns_map, idx_for_datamap= idx_for_datamap)\n",
    "\n",
    "\n",
    "    params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "    params = torch.tensor(params, requires_grad=True)\n",
    "    instance = torch_vecchia_exp(analysis_data_map, params, nns_map, mm_cond_number)\n",
    "    out0 = instance.full_likelihood(params, aggregated_data[:,:4],aggregated_data[:,2], instance.matern_cov_ani)\n",
    "    print(out0)  # 15105\n",
    "\n",
    "    # out0 = instance.vecchia_local2(params, instance.matern_cov_ani)\n",
    "    # print(out0)   # 15181.4775  15183 0,1 lage 15185 0,2\n",
    "\n",
    "    out1 = instance.vecchia_local4(params, instance.matern_cov_ani, 200)\n",
    "    print(out1)   # 15181.4775  15183 0,1 lage 15185 0,2\n",
    "    head200map[day]  = [out0, out1]\n",
    "    print(f'day{day} finish')\n",
    "    # out0 = instance.vecchia_local3(params, instance.matern_cov_ani, 10)\n",
    "    # print(out0)   # 15181.4775  15183 0,1 lage 15185 0,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15105.6709, grad_fn=<MulBackward0>)\n",
      "tensor(15198.8496, grad_fn=<AddBackward0>)\n",
      "tensor(15183.4766, grad_fn=<AddBackward0>)\n",
      "tensor(15135.7588, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lat_lon_resolution = [4,4]\n",
    "day = 3\n",
    "mm_cond_number = 10\n",
    "\n",
    "years = ['2024']\n",
    "month_range =[7,8]\n",
    "idx_for_datamap= [ 8*(day-1),8*day]\n",
    "\n",
    "instance = load_data_local_computer()\n",
    "map, ord_mm, nns_map= instance.load_mm20k_data_bymonthyear( lat_lon_resolution= lat_lon_resolution, mm_cond_number=mm_cond_number,years_=years, months_=month_range)\n",
    "analysis_data_map, aggregated_data = instance.load_working_data_byday( map, ord_mm, nns_map, idx_for_datamap= idx_for_datamap)\n",
    "\n",
    "\n",
    "params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "params = torch.tensor(params, requires_grad=True)\n",
    "instance = torch_vecchia_exp(analysis_data_map, params, nns_map, mm_cond_number)\n",
    "out0 = instance.full_likelihood(params, aggregated_data[:,:4],aggregated_data[:,2], instance.matern_cov_ani)\n",
    "print(out0)\n",
    "\n",
    "# out0 = instance.vecchia_local_full_cond(params, instance.matern_cov_ani)\n",
    "# print(out0)   # 2588.0483 with heads 20 2587.3491 with heads 5   2580.9575 without heads\n",
    "\n",
    "out0 = instance.vecchia_local2(params, instance.matern_cov_ani)\n",
    "print(out0)      # 2593.35 # ingeneral underestimate is less risky than overestimate with hiher likeilhood\n",
    "\n",
    "out0 = instance.vecchia_local3(params, instance.matern_cov_ani,200)\n",
    "print(out0)\n",
    "\n",
    "out0 = instance.vecchia_local4(params, instance.matern_cov_ani,200)\n",
    "print(out0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Gradients: [  -9.955391  207.88681    -2.595461 -169.72203   273.39944   771.1119\n",
      "  -79.33867 ]\n",
      " Loss: 15135.7587890625, Parameters: [ 2.442e+01  1.920e+00  1.920e+00  1.000e-03 -4.500e-02  2.370e-01\n",
      "  3.340e+00]\n",
      "Epoch 101, Gradients: [ -1.4867018    0.10161591   1.1231489    3.3106918  -11.375813\n",
      "  -7.5312614    7.9387875 ]\n",
      " Loss: 15014.2587890625, Parameters: [ 2.5338207e+01  1.3597493e+00  1.7280715e+00  1.9441580e-02\n",
      " -1.6756412e-01  1.8955134e-01  3.3110123e+00]\n",
      "Converged at epoch 189\n",
      "Epoch 190, Gradients: [-1.2997863  -0.27633095 -0.09813547  0.49712753 -0.5989456  -1.545433\n",
      " -0.09783697]\n",
      " Loss: 15012.7626953125, Parameters: [ 2.5795250e+01  1.3089702e+00  1.6588148e+00  1.8806925e-02\n",
      " -1.6661680e-01  1.9616681e-01  3.1115689e+00]\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "params = torch.tensor(params, requires_grad=True)\n",
    "\n",
    "instance = torch_vecchia_exp(analysis_data_map, params, nns_map, mm_cond_number)\n",
    "\n",
    "# optimizer = optim.Adam([params], lr=0.01)  # For Adam\n",
    "optimizer = optim.Adam([params],lr=0.01, betas=(0.9, 0.8), eps=1e-8)\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.9)\n",
    "\n",
    "# Example function to compute out1\n",
    "def compute_out1(params):\n",
    "    # Compute the output using your function\n",
    "    # nll = instance.full_likelihood(params, instance.matern_advec_beta_cov )\n",
    "    nll = instance.vecchia_local4(params, instance.matern_cov_ani, 200 )\n",
    "    return nll\n",
    "\n",
    "# Training loop\n",
    "prev_loss = float('inf')\n",
    "tol = 1e-4  # Convergence tolerance\n",
    "for epoch in range(600):  # Number of epochs\n",
    "    optimizer.zero_grad()  # Zero the gradients \n",
    "    \n",
    "    loss = compute_out1(params)\n",
    "    loss.backward()  # Backpropagate the loss\n",
    "    \n",
    "    # Print gradients and parameters every 10th epoch\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "    \n",
    "    # print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "    \n",
    "    optimizer.step()  # Update the parameters\n",
    "    scheduler.step()\n",
    "    # Check for convergence\n",
    "    if abs(prev_loss - loss.item()) < tol:\n",
    "        print(f\"Converged at epoch {epoch}\")\n",
    "        print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "    \n",
    "        break\n",
    "    \n",
    "    prev_loss = loss.item()\n",
    "\n",
    "print('Training complete.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Gradients: [-10.6224985 200.28781    -6.4957714 -78.33738   149.36243   605.0917\n",
      " -81.693665 ]\n",
      " Loss: 15183.4765625, Parameters: [ 2.442e+01  1.920e+00  1.920e+00  1.000e-03 -4.500e-02  2.370e-01\n",
      "  3.340e+00]\n",
      "Epoch 101, Gradients: [-1.9947629  4.9151707  6.4719105 -1.286026  -2.0892706  2.9007835\n",
      "  1.0226293]\n",
      " Loss: 15077.75, Parameters: [25.32368     1.3661038   1.7502507   0.04647188 -0.12653434  0.17788467\n",
      "  3.327231  ]\n",
      "Converged at epoch 135\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "params = [24.42, 1.92, 1.92, 0.001, -0.045, 0.237, 3.34]\n",
    "params = torch.tensor(params, requires_grad=True)\n",
    "\n",
    "instance = torch_vecchia_exp(analysis_data_map, params, nns_map, mm_cond_number)\n",
    "\n",
    "# optimizer = optim.Adam([params], lr=0.01)  # For Adam\n",
    "optimizer = optim.Adam([params],lr=0.01, betas=(0.9, 0.8), eps=1e-8)\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma=0.9)\n",
    "\n",
    "# Example function to compute out1\n",
    "def compute_out1(params):\n",
    "    # Compute the output using your function\n",
    "    # nll = instance.full_likelihood(params, instance.matern_advec_beta_cov )\n",
    "    nll = instance.vecchia_local3(params, instance.matern_cov_ani )\n",
    "    return nll\n",
    "\n",
    "# Training loop\n",
    "prev_loss = float('inf')\n",
    "tol = 1e-4  # Convergence tolerance\n",
    "for epoch in range(500):  # Number of epochs\n",
    "    optimizer.zero_grad()  # Zero the gradients \n",
    "    \n",
    "    loss = compute_out1(params)\n",
    "    loss.backward()  # Backpropagate the loss\n",
    "    \n",
    "    # Print gradients and parameters every 10th epoch\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "    \n",
    "    # print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "    \n",
    "    optimizer.step()  # Update the parameters\n",
    "    scheduler.step()\n",
    "    # Check for convergence\n",
    "    if abs(prev_loss - loss.item()) < tol:\n",
    "        print(f\"Converged at epoch {epoch}\")\n",
    "        print(f'Epoch {epoch+1}, Gradients: {params.grad.numpy()}\\n Loss: {loss.item()}, Parameters: {params.detach().numpy()}')\n",
    "        break\n",
    "    \n",
    "    prev_loss = loss.item()\n",
    "\n",
    "print('Training complete.') \n",
    "\n",
    " \n",
    "# vecchia local 2  332   epo 33.4s   25.55 2.61 2.68 0.16 0.03 2.7\n",
    "#    24.89 2.06 2.24 1.36e-2 -5.63 e-2 0.10113 3.75\n",
    "# vecchia cholesky local 380 epo 43.2  25 2.61 2.68 0.16 0.03 2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 200 x 8\n",
    "\n",
    "lr 0.001 without scheduler  same as lr, step_size, gamma  0.01 40 0.5  (9.8s)\n",
    "\n",
    " Loss: 2549.066650390625, full Parameters: [ 2.48777485e+01  2.05998826e+00  2.16013098e+00  2.20775465e-03\n",
    " -7.89414570e-02  1.05411254e-01  3.75236106e+00]\n",
    "\n",
    " lr 0.01  step size 40  betas 0.9 , 0.8 gamma 0.9  30 s\n",
    "\n",
    "  Loss: 2547.1728515625, full Parameters: [ 2.7377291e+01  2.2077193e+00  2.3204505e+00  1.0307773e-03\n",
    " -8.0311157e-02  9.8579854e-02  3.6677265e+00]\n",
    "\n",
    " lr 0.01  step size 10 betas 0.9 , 0.8 gamma 0.9  30 s\n",
    "  Loss: 2548.87841796875, full Parameters: [ 2.5092268e+01  2.0689390e+00  2.1694989e+00  2.0285936e-03\n",
    " -7.9028614e-02  1.0501490e-01  3.7373385e+00]\n",
    "Training full likelihood complete.   11.8 sc\n",
    "\n",
    " lr 0.01  step size 20 betas 0.9 , 0.8 gamma 0.9  30 s\n",
    " Loss: 2548.15283203125, full Parameters: [ 2.59814014e+01  2.12175608e+00  2.22699022e+00  1.73025124e-03\n",
    " -7.93599486e-02  1.02427535e-01  3.70715070e+00]\n",
    "\n",
    "\n",
    "\n",
    "lr 0.01  step size 20 beta 0.9 0.99 gamma 0.9\n",
    " Loss: 2548.18603515625, full Parameters: [ 2.5938652e+01  2.1110108e+00  2.2155209e+00  1.5893303e-03\n",
    " -7.9482891e-02  1.0297947e-01  3.6958976e+00]\n",
    " 21.6\n",
    "\n",
    "lr 0.01  step size 20 beta 0.9 0.8 gamma 0.9\n",
    " Loss: 2548.15283203125, full Parameters: [ 2.59814014e+01  2.12175608e+00  2.22699022e+00  1.73025124e-03\n",
    " -7.93599486e-02  1.02427535e-01  3.70715070e+00]\n",
    " 22.9 s\n",
    "\n",
    "lr 0.01  step size 10 beta 0.9 0.99 gamma 0.9\n",
    "Loss: 2548.95361328125, full Parameters: [ 2.5118145e+01  1.9827319e+00  2.0768294e+00  1.0898338e-03\n",
    " -8.0070712e-02  1.1034889e-01  3.5647078e+00]\n",
    "\n",
    "\n",
    "## 1250 x 8\n",
    "\n",
    "1250* 8 55m using constant learning rate 0.0001 \n",
    "Loss: 14068.798828125, full Parameters: [ 2.46198387e+01  1.61719894e+00  1.76454413e+00  8.55297223e-03\n",
    " -1.08275235e-01  1.28809512e-01  2.80795789e+00]\n",
    "\n",
    "1250* 8 10m 32s\n",
    "lr 0.01  step size 40 beta 0.9 0.8 gamma 0.9\n",
    "  Loss: 14068.1953125, full Parameters: [ 2.5030930e+01  1.6107724e+00  1.7573007e+00  8.8407323e-03\n",
    " -1.0820019e-01  1.2936097e-01  2.7430327e+00]\n",
    "Training full likelihood complete.\n",
    "\n",
    "9m 33s\n",
    "lr 0.01  step size 20 beta 0.9 0.8 gamma 0.9\n",
    " Loss: 14068.29296875, full Parameters: \n",
    " [ 2.4933689e+01  1.6009743e+00  1.7502663e+00  9.2404895e-03 -1.0737537e-01  1.2953614e-01 \n",
    "  2.7420275e+00]\n",
    "Training full likelihood complete.\n",
    "\n",
    "#### high resolution data might benefits from larger step size high resolution data often provides \n",
    "#### more stable gradients, so larger step size less likely to cause significant fluctuations\n",
    "14n 41.8s\n",
    "lr 0.01  step size 10 beta 0.9 0.99 gamma 0.9\n",
    "\n",
    "FINAL STATE: Epoch 199, \n",
    " Loss: 14068.8828125, full Parameters: \n",
    " [ 2.4707581e+01  1.6489888e+00  1.7993137e+00  8.4043797e-03 -1.0836436e-01  1.2655504e-01  \n",
    " 2.8416286e+00]\n",
    "\n",
    "#### beta 0.9 0.99 might be too conservative for high resolution data\n",
    "13m 44.8s\n",
    "lr 0.01  step size 20 beta 0.9 0.99 gamma 0.9\n",
    "\n",
    " Loss: 14068.318359375, full Parameters: [ 2.4938175e+01  1.6203119e+00  1.7678342e+00  8.6686825e-03\n",
    " -1.0813228e-01  1.2845081e-01  2.7731323e+00]\n",
    "\n",
    "\n",
    "18m\n",
    "lr 0.01  step size 40 beta 0.9 0.99 gamma 0.9\n",
    "\n",
    " Loss: 14067.970703125, full Parameters: [ 2.5205673e+01  1.6159834e+00  1.7630767e+00  8.7957922e-03\n",
    " -1.0802399e-01  1.2862283e-01  2.7390635e+00]\n",
    "\n",
    "9m 52s\n",
    "lr 0.01  step size 20 beta 0.9 0.8 gamma 0.9\n",
    "\n",
    "Loss: 14068.29296875, full Parameters: [ 2.4933689e+01  1.6009743e+00  1.7502663e+00  9.2404895e-03\n",
    " -1.0737537e-01  1.2953614e-01  2.7420275e+00]\n",
    "Training full likelihood complete."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
